<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1" ident="GROBID" when="2024-11-15T07:11+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>For reasons of curiosity, we perused the two recent Oxford handbooks on legal history looking for discussions of digital methods in legal history. One of the fundamental decisions to be made when organizing such a handbook is defining which methodological approaches deserve an article of their own and which ones are to be understood rather as cross-cutting themes to be discussed in the context of many articles dedicated to other things. In the case of digital methods in legal history, this decision seems to have been a tough oneat one point, you can find a curious reference to a »chapter on ›Legal History and Digital Human-ities‹« (OHBLH 354), but in the final publication there is no such text.</p>
        <p>However, discussing digital methods in the context of other subjects has, in our opinion, the disadvantage that more systematic, methodological arguments cannot really be developed. Put more concretely, the most ›substantial‹ contributions regarding digital methods are, for whatever reason, those on »The Intellectual History of Law« by Assaf Likhovski, on »Taking the Long View« by Paul D. Halliday, on »Quantitative Legal History« by Daniel Klerman, and on »Indian Law« by Mitra Sharafi, all of which are in the Oxford Handbook on Legal History. (Equally surprising, there is no mention of digital methods at all in Angela Fernandez's »Legal History as The History of Legal Texts«.) However, even these articles do not really ›discuss‹ digital methods, rather they merely refer to them (and to some projects) as contributions of sorts to their respective fields of interest.</p>
        <p>Thus, if you are looking for digital methods in those handbooks, you can hardly find more than some namedropping passages where things like »digital mapping […], network analysis […], text analysis« (OHBLH 845f.) are mentioned, together with references to example projects where they have been employed but without any explanation as to:</p>
        <p>-why these methods are mentioned and not others, -what they are doing, to which end and under what circumstances, -what, possibly transformative, impact these methods have on the (respective sub-) field of legal history, and -what a scholar considering to apply these methods should be aware of.</p>
        <p>While the space for this is limited, the present Forum contribution tries to mitigate the scarcity of such discussions by presenting and discussing a few textual analyses that make use -for demonstration purposes -of digital methods. Some other methods of analysis, network analysis, and geo-mapping (among others), cannot be covered here, but we provide a link to an online bibliography where you can find them applied to legal history or a related domain, and discussed critically. A general discussion of digital perspectives beyond concrete methods of analysis concludes this contribution.</p>
        <p>Legal history is concerned with texts to an even greater extent than humanities in general.Through writing, norms achieve stability and communicability, and the vast majority of research in legal history deals with text. Therefore, in our exemplary analyses, we are focusing on a set of methods of textual analysis. More specifically, we will present an analysis using Structural Topic Modeling, followed by an analysis that further investigates one hypothesis resulting from this Topic Model in a corpus linguistics workbench called TXM.</p>
        <p>First of all, we have prepared all contributions to the two handbooks as a corpus: We have scraped (via copy-and-paste in the web browser) the plaintext from 107 articles via OUP's Oxford Handbooks Online site 1 and saved them as ›.txt‹ files (including notes and references, but without abstracts and keywords). Also, we have established a spreadsheet file (in ›.csv‹ format) with title, author, name of the corresponding plaintext file, and the following metadata fields for each contribution: how many authors the contribution has; their sex, affiliation, place and country of the affiliated institution; which of the two books the contribution features in; the DOI for the contribution, keywords, and abstract. This constituted a corpus of roughly 1,235,000 words (called ›tokens‹) formed out of a vocabulary of roughly 45,000 different basic words (or ›lemmata‹). 2</p>
        <p>Besides more general labels like ›text-mining‹ or ›network analyses‹, Topic Modeling is mentioned explicitly as a method in the handbooks (in Paul D. Halliday's »Legal History: Taking The Long View«, OHBLH 338), and we decided to use this method to illustrate some of the possibilities of quantitative Text Mining. Thus, we used the R language's stm package to apply a so-called Structural Topic Model (STM) to the two Oxford handbooks. 3 This technique enables researchers to discover topics within a larger collection of texts and to estimate their relationship to document metadata.</p>
        <p>But what exactly is a topic? Topic models treat topics as probability distributions over words, meaning that the estimated model returns several lists of words that have been identified computationally as having a high probability of occurring together. Anticipating our results, figure 1 presents an example for such a list as inferred from the two handbooks. It consists of words such as genocide, nazi, jewish, criminal, and tribunal, 4 which suggests that the topic encompasses the discourse on National Socialism (NS) and Law that is present in many handbook articles (e. g. Randall Lesaffer, »The Birth of European Legal History«; Michael Stolleis, »European Twentieth-Century Dictatorship and the Law«). The topic is displayed as a word cloud, which is a popular way of presenting Topic Modeling output.</p>
        <p>In order to estimate a meaningful STM, that is a set of such lists, we followed a trial-and-error process based on statistically-derived suggestions provided by the software.To determine the optimal topics number, one should test different models and consider the results in terms of interpretability with regards to the specific research question, and then possibly diverge from the merely statistical ›optimum‹. In the end, we opted for a 20-topic model with the estimated topics being displayed in the table presented in figure 2. In the later part on TXM, the tokens are also given in italics, but not always written in lower case, since their original spelling was retained for the TXM analysis.</p>
        <p>Figure 1 Forum forum Anselm Küsters, Laura Volkind, Andreas Wagner</p>
        <p>As the STM produces groups of words that merely have a high probability of occurring together, topics are usually referenced by their respective top-scoring words (according to various measures such as intra-group probability, distinctiveness visà-vis the other groups, etc).</p>
        <p>Since the actual reason underlying the groups' respective coherence is unknown to the STM, the researcher normally also assigns labels to the groups, as done in the right column of the table above. Usually, topics evoke specific associations, so that reasonable and coherent labels can be inferred relatively quickly. We give two examples. The seven most probable words for Topic 12 include empirically, marketplace, and economists, which clearly signals a proximity to Economic Legal History, 5 as, 5 Just as tokens are marked in a certain way (lower case, in italics) in a Topic Modeling analysis, topic labels are highlighted in the text in italics, but in capital letters. However, topics are not always recognizable at first sight. If a topic lacks a straightforward interpretation, it is helpful to read the texts that exhibit a large share of this topic in order to get a better sense for the proper interpretation of the word list and thus the appropriate label. This procedure had to be followed for most topics in the table above, since the specialized vocabulary and the wide topical variety made it relatively difficult to find intuitive common denominators.</p>
        <p>Finally, a well-known fact in Topic Modeling (and yet a common source of misunderstandings and criticism) is that topics do not necessarily have to describe a straightforward theme, in the sense of a subject matter, but that they can also form clusters of methodological words, days of weeks, person's names, or rhetorical devices. In our example, this happened in the case of Topic 13, which features many rhetorical terms (coincidentally, connect) and even metaphorical words (e.g. swan song, siren song) that were utilized in diverse articles, irrespective of the particular theme discussed. While scholars commonly use labels like Descriptive Language or Rhetorical Elements when dealing with such topics, we opt for the label Textual Analysis because the manual revisiting of the corpus and close reading revealed that the specific terms listed as Topic 13 often appear when scholars discuss their own (or others') textual analysis of certain sources (e.g. source X is particularly fruitful for the question of Y; X was found to be a particularly fruitful concept when analysing Y; studies on X have concerned themselves intensely with Y). Thus, Topic 13 should not be interpreted as reflecting textual analysis method or textual analysis as such, but as reflecting the rhetorical expressions frequently used when summarizing the results of such analyses. Note that, generally, the STM found all 20 topics without knowing that it deals with a set of legal historical articles and without any pre-coded definitions or lists of key terms. Yet it came to results that correspond, to a large extent, to the semantic and contextual meaning that the words actually exhibit in the corpus (e.g. sorting vattel, adenauer, and eichmann to different topics [7, 17 and 20], but adenauer, [de] gaulle and even [Paul] reuter to the same topic [17]).</p>
        <p>Besides inferring topical content, Topic Modeling allows us to structure large quantities of texts by providing different means of corpus level visualization. The most popular one relates to the expected proportion of the corpus that belongs to each topic. This is plotted for the estimated STM in figure 3. We see, for example, that the NS and Law topic (20) introduced in the beginning is actually a relatively minor proportion of the overall legalhistorical discourse. The most common topics refer to Roman Law (5), to a general topic full of words that historians commonly utilize for reporting about Textual Analysis (13), and -not surprisingly for handbooks that intend to present the evolution of a discipline and its state-of-the-art -to a topic on the History of Legal History (9).</p>
        <p>We now discuss estimating topic-metadata relationships, as the ability to plot these relationships is the key benefit of STMs. This feature has been used in the social science literature to model, for instance, the framing of international newspapers, Twitter feeds, and religious statements.foot_1 There are two ways in which the metadata can enter into our model: Whereas in topical prevalence, the metadata values of the various documents affect the frequency with which a topic is discussed in the respective document, in topical content, they influence the word probability distribution ›within‹ a specific topic in a document. In this example, we use the handbook variable (OHBLH vs. OHBELH) and the author's country as covariates in the topic prevalence portion of the model and the handbook variable again in the content portion.</p>
        <p>First, we would like to plot the change in topic proportion shifting from one handbook to the other. Since our covariate of interest is binary, we estimate the expected proportion of an article that belongs to a topic as a function of a first difference type estimate, where topic prevalence for each topic is contrasted for these two groups (OHBLH vs. OBHELH). Figure 4 gives the results. We see that Legal Scholarship in the 20 th century, Comparative Law, Textual Analysis, and Natural Law vs. Formalism are strongly discussed in the contributions to the OHBELH, while topics on Canon Law, Criminal Law, and Method of Legal History were largely associated with writers for the OHBLH.</p>
        <p>We can use the same method to investigate changes in topic proportion associated with the authors' countries of residence, since this information was also included as a covariate in the estimation of the STM. To give an example, we contrast authors that are located in the US with authors affiliated with German institutions. Inspecting the corpus reveals that, overall, there are 33 US-based authors and 14 Germany-based authors that have published articles in the two handbooks. When plotting topic prevalence for all 20 topics given in these two groups, it becomes clear that the country of residence has indeed some significant correlation to the author's choice of topics (fig. 5). USbased authors are more likely to write about Roman Law, Comparative Law and Natural Law vs. Formalism, whereas authors based in Germany tend to write on Canon Law, Economic Legal History, Marxist Legal History and EU Legal History. It should be noted, however, that these effects only indicate statistical correlations, not causations. For example, the authors might be writing about a certain subject mainly because the handbook editors have asked them to do so rather than because of the location of their institutional affiliation. Moreover, the relatively small sample size of our handbook corpus (typical Topic Modeling projects cover millions of tokens) increases the likelihood of sample selection bias. Forum forum Anselm Küsters, Laura Volkind, Andreas Wagner Finally, we can analyze the influence of the respective handbook as a topical content covariate. This allows us to investigate which words ›within‹ a certain topic are more associated with one handbook versus the other. In our analysis (not shown here), we plotted vocabulary differences by handbook for the NS and Law topic (20), whose top seven words as displayed in the general table are adolf, eichmann, immunity, nazis, persecution, israel, testimonies. However, as calculations make clear, the two handbooks treated this topic very differently. In particular, authors of the OHBELH were much more likely to use words such as state, national and german when writing about NS and Law (20), whereas OHBLH authors emphasized terms such as genocide and cultural. There might be an intuitive explanation for this: Whereas a volume that focuses on European legal history might be more inclined to refer to classic national histories of states and to their respective laws, a handbook trying to provide a global perspective on legal history is more likely to draw on aggregating meta-concepts like genocide and culture when referring to the legal system of the Third Reich. (In actual fact, something else is going on here -a factor related to the small sample size and that will be discussed in the next section.)</p>
        <p>But first let us acknowledge that estimating a Topic Model, such as the STM discussed in this review, has three important benefits not easily achievable by means of the classic close reading of texts: First, this method does not require the imposition of pre-defined categories and is thus somewhat shielded from bias -or at least, it isolates and makes more explicit the introduction of a schema of interpretation by the researcher. Second, topics are explicit, so other researchers can reproduce the analysis or challenge the labels associated with the topics. Third, the computational power allows us to understand and structure corpuses of texts that are difficult to grasp coherently for a single scholar due to their length. This might not be entirely true for the two handbooks analysed here, which ›only‹ encompass 2,374 pages, but it becomes much more relevant when dealing with, for instance, a large historical newspaper archive. Nevertheless, as has become clear as well, these quantitative techniques still depend on the researcher's judgment. They may serve as exploratory tools that stimulate new questions and hypotheses to be tested or complement -and not substituteexisting tools of legal historical research.</p>
        <p>Topic Modeling is a relatively recent method, and it is one in which many things are being accomplished without the assistance of the researcher. While this reduces chances of introducing bias, it also makes it harder for the researcher to provide interpretations or to avoid over-interpretation when she may be ignorant of all the steps involved.</p>
        <p>Therefore, we also want to present a more ›conventional‹ analysis of our OHB corpus using various functions of a powerful corpus linguistics platform. Corpus linguistics workbenches, or toolkits, like GATE, TXM or WebLicht allow the researcher to quickly gather statistics about aspects of language use in the assembled corpus. 7 Basically, one can see specific word forms or basic words ranked by their frequency (fig. 6). For what it is worth, the most frequent basic word in our corpus, the, comprising its specific forms the and The, occurs 73,149 times. The next most frequent words are of, and, in and the various forms of the verb be, all of them being so-called function words. The high frequencies of the content words law, legal, and history are also hardly surprising.</p>
        <p>In all likelihood, content words related to specific research questions are more interesting, but then of course it depends on the researcher's creativity and experience to translate his or her research question into query terms. Suppose the respective weight of justice and power is at issue. We can use TXM's ›index‹ and ›progression‹ tools to see that both terms cumulate more or less constantly over all the articles, but that the curve for power is more even and steeper, and that it totals at almost double the frequency of justice (1,164 vs 552 occurrences).</p>
        <p>A central function of corpus linguistics is the creation and contrasting analysis of sub-corpora. TXM allows us to create sub-corpora (a corpus being just a part of the full corpus) and partitions (a non-overlapping, collectively exhaustive set of sub-corpora) according to the metadata values that we have recorded beforehand. One could, for instance, partition by authors' sexes, and contrast, e. g. the mere number of words written by women (269,218) to those written by men (967,440; this would be even more dramatic when applied to the European handbook alone: 53,187 vs 577,862).</p>
        <p>Alternatively, one could partition the corpus according to the country that the author's affiliation is located in, or according to the affiliation itself, and again report the number of words per partition (fig. 7). Or, to enter a bit deeper into the linguistic aspect, one could contrast the partitions' vocabulary content rather than their mere size. TXM calculates a ›specificity score‹ for each word, based on the deviation of the actual from the expected number of its occurrences in a partition (given the partition size and the total number of occurrences in the whole corpus). 9 In this way, researchers can gain another perspective on the contrast between the two handbooks.</p>
        <p>Among the words specific to the European handbook (see also fig. 8), we see: -named entities, in particular the names of European nations (like France, Denmark, Sweden, but also as adjectives -German -and referring to historical entities Roman and Byzantine), -function words in other European languages that probably come from literature in those languages being cited (und, de, der, des, die, im, et, zur), and also -some words that seem to indicate subject matters more prominent in the European handbook than in the ›global‹ one (royal, king, church, kingdom, but also court, city, and town).</p>
        <p>In the list of words specific to the ›global‹ handbook, by contrast, the perspectives that seem to suggest themselves are (see also fig. 9): -very general (first and foremost history, historian and historical, past, or jurisprudence, research, and scholarship) and -methodological (the general analysis and inquiry, but also critical, realist / realism, and feminist), but there are also -some terms indicating concrete subject matters or fields of law (Islamic, environmental, violence, Jewish, possibly black).</p>
        <p>But let us come back to our NS and Law topic from the preceding section. For a more detailed assessment, we have queried 9 terms related to crimes against humanity (genocide, torture, deportation, displacement, rape, enslavement, persecution, cleansing, massacre) and a further 5 terms related to German National Socialism (NS, NSDAP, Nazi, Nazis, Nazism). We find that 7 of the 14 terms occur more than 10 times in the two handbooks. Looking up the specificity values of these 7 terms for some of the countries of the corpus' authors, the picture shown in figure 10 emerges.</p>
        <p>It is perhaps worth noting that there is a socalled ›banality‹ threshold within which fluctuations of usage of the terms are not really significant, and we have left this threshold at the default value (of ± 2.0, indicated by thin lines in the figure). We see that UK-/ US-based authors seem to avoid all the terms mentioned to a non-trivial degree; arguably, they do not treat the topic to any extent at all. Moreover, Australian and Finnish authors conspicuously refer exclusively to rape / displacement and, respectively, to torture, which none of the others seems to touch upon. This fact might indicate that it was (most likely) misleading to approach the topic solely from the perspective of crimes against humanity, assuming that many of the terms would typically occur together, which, if true, could have been motivated by this legal concept.</p>
        <p>Anyway, at least the numbers seem to confirm that German authors discuss the topic using the term NS, whereas Israeli authors rather use genocide and Nazi / Nazis. However, here we encounter again problems connected with the small sample size and selection bias alluded to above. Building a sub-corpus for only Israeli authors, partitioning that sub-corpus according to author, and then revisiting our topic's terms, we find that it is in fact only one single contribution that produces the particular profile of the ›Israeli way‹ of discussing the topic and using the vocabulary of genocide; an unsurprising result given the contribution's title: »Cultural Genocide: Between Law and History« by Leora Bilsky and Rachel Klagsbrun. It is quite likely that this even spills over and produces the would-be ›OHBLH way‹ of discussing it. And vice-versa, just one single contribution (Michael Stolleis' »European Twentieth-Century Dictatorship and the Law«) is responsible for the ›German‹ (and for the ›OHBELH‹) way of discussing the topic, mentioning terms such as NS more than Anselm Küsters, Laura Volkind, Andreas Wagner others. So it is certainly mistaken to infer from them either a rhetoric that would be characteristic to some extent for all authors of a certain national tradition or some preference in the respective editors' policy of inviting contributors that would adhere or not to a certain rhetoric! And whether the particular profiles of the two relevant contributions resulted from the chosen or requested topic, from developments that the authors may be involved in on their respective national level, or from the authors' idiosyncrasies cannot be decided by corpus linguistic means. Thus, one of the key takeaways is that relating findings of digital methods to research questions is something that requires scholarly interpretation, contextual knowledge, and close reading of the respective documents. (On the other hand, this makes the fact that STM was nevertheless capable of sorting the terms genocide and NS into the same topic in the first place all the more interesting.)</p>
        <p>Another key takeaway might be the following: Both Topic Modeling and more conventional corpus linguistics are most useful when assessing discourses instead of opinions or statements. The researcher's goal in using these methods should not be to understand what individual documents assert without reading them; nevertheless, such an approach could more plausibly be used to learn about various ways of talking and writing more easily discerned in large sections of a given discourse. Once made visible, it then becomes possi-ble to interpret and reflect about how these ways of talking and writing might frame certain subjects.</p>
        <p>With this in mind, we want to focus on more cross-cutting phenomena and offer a final example for this approach. As we have seen, the contrast between power and justice is ubiquitous and further investigation warranted. However, it would probably be more fruitful to return to the question posed at the very outset: How well established are digital methods and resources within the discipline? First of all, we can see that there is a steady occurrence of references to online resources (by http(s) or, less frequently, by doi), resulting in at least 225 references to online resources.</p>
        <p>Then, we can have TXM list all words that occur together with any word beginning with digit (in a ›window‹ of 20 words to the left and 20 words to the right). The most significant co-occurrent is humanity, certainly because ›digital humanities‹ is an established (and fashionable) term. Co-occurrents like opportunity (score: 5.3), possibility (2.7), access or accessible (5.7/2.4), available (5.8), and use (6.3) suggest that, if things digital are discussed, the attitude seems to be rather open and optimistic and there seems to be a certain focus on the ways in which resources are available in digital form. This last point is reaffirmed by the prominence of co-occurrents like archive(s), source, database, digitization, manuscript, newspaper, library, collection. Terms that might indicate a more skeptical attitude like issue, miss, serious seem to do so only in Figure 10: Specificity scores for »NS and Law« terms one instance. Figure 11 shows how we can see the immediate context of the respective occurrences in the list of concordances (bottom third of the image); furthermore, it shows how we can then select a passage (line 3, with digital being followed by miss after five words) and go back to the full text and read the passage in question in full (topmost third of the image). Here we see that it is Paul Halliday discussing the danger of ignoring sources like manuscripts that are not available in digital form merely for this reason.</p>
        <p>However, while both aspects -methods and resources -related to the digitization of legal history are represented in the handbooks, only the latter is featured prominently. Fifteen different authors (out of 100 in total) mention some aspect of digital research, and eight do so more than twice. But as we have seen, archives, collections, or databases occur quite frequently in the context of digit*, whereas references to digital tools or software are scarce. Only five authors (Likhovski, Halliday, Klerman, Sharafi, the four authors mentioned at the very outset of this review, plus Dirk Heirbaut in the OHBELH) mention these. Assaf Likhovski suggests that the most promising aspect of what he terms the digital revolution is not »the use of new tools to mine this data, but more modest projects: the creation of databases« that help to visualize data and the creation of new, curated, and interlinked teaching tools (OHBLH 160).</p>
        <p>However, given that the contributions to the handbooks do not indicate more than a handful of methods, not to mention that in many cases the authors merely refer to the special issue 10 on digital legal history of the Law &amp; History Review (2016), more should be done to address such deficits. There is a clear lack of attractive cases employing such methods, a lack of awareness of available methods, and a lack of opportunities to ›translate‹ digital methods and their technical details to layi. e. not-so-tech-savvy -scholars.</p>
        <p>Due to limitations of space, we are unable to discuss and offer examples of the two other methods mentioned in the handbooks: network analysis and geo-mapping. However, we would like to point out that quite a number of other methods might be relevant to legal historians. Digital humanities projects have already put ›Text Reuse Detection‹ or information extraction methods, such as ›Named Entity Recognition‹, to good use. And in the economically dynamic field of applied law, ›big players‹ like Westlaw, LexisNexis, or Bloomberg, as well as countless IT startups are developing their service portfolio and offer (or are researching) methods of citation recognition, argument mining and evaluation, and recommender systems for judges, litigant parties, or lawmakers. For all of the approaches mentioned above, we have established an online bibliography and are trying to list literature that is applicable to legal history and / or related fields -or at least introduce and discuss this literature critically. 11</p>
        <p>Even with respect to the resource-focused aspect of digitization, a critical discussion is still lacking. When building a digital resource, one has to check the context and profile of other related digital resources, and the selection of data at the very outset should be examined critically. Can the new resource link to other established resources? Is it capable of helping to establish some other resources? How does it participate, if at all, in a process of canonization or counter-canonization?</p>
        <p>Understanding ›data as capta‹, according to Johanna Drucker, draws attention to the process of acquisition and recording of data, where decisions about how to ask, what to record, what to 11 The bibliography can be found here: https://www.zotero.org/groups/ 2163790/digital_legal_history/items/ collectionKey/YEKDRSB9. ignore, and how to normalize must be made. Also, it is here that biases with regards to the relevance of non-canonized perspectives, opinions, and material come into play. With regards to the technical aspect, for instance, under which conditions are OCR techniques applicable and what are their (dis-)advantages? Or, more in terms of scholarly self-understanding, how does a project position itself with regard to crowdsourcing and the contributions of ›citizen scientists‹? Data modeling is another crucial point to consider and discuss even before starting the analysis. Are you dealing with a text or something else? If it is a text, is ›text‹ the best form in which to record the information for your project? Might tabular, relational, or semi-structured data be more appropriate? Do you normalize values (and if so, do you keep the original values or discard them?)? What kinds of metadata should go along with the records?</p>
        <p>In the following, we present a selection of questions that digital tools and methods should be submitted to once they come into the purview of legal history. (In the presentation of our STM and corpus linguistics examples above, we have at least hinted at how we would respond to some of the questions for those methods.)</p>
        <p>Since most methods accept data and additional configuration parameters, it is important to understand and critically reflect on the parameters used. At what point in the process does one feed a researcher's parameters into the method? Which effects are produced by a change in the parameters, and why would one (rightly or erroneously) enter one value rather than another under actual research conditions? Does the method / tool provide for repeated runs with varying parameters? How do you evaluate the quality of the results of different runs?</p>
        <p>In many cases, scholars add annotations to their data and it may be desirable to access these at various stages of the process. For instance, is there a standard data format to adhere to while entering the annotations, and is it possible to access, expose, or export intermediary results (e.g. scan images while you are still waiting for OCR or transcriptions)?</p>
        <p>For a number of methods, there is a considerable amount of complexity introduced by sophisticated mathematical algorithms, by the mere fact that parts of the process behave probabilistic / contingently, or by the sheer mass or multidimensionality of the data. It is good to know which parts of the process tend to become nontransparent, and why. Is one able to understand what the algorithm is doing -both in general and more specifically? Is it easy to comprehend what the operations performed on the data mean or represent in real life, or why one would want to do this with the specific data at hand? Finally, is it clear where the more ›objective‹ part of the process ends and where interpretation begins? How do you avoid reading more into your results than the information warrants? If you catch yourself over-interpreting, is it possible to operationalize the interpretation as another hypothesis, so that it can subsequently be checked and eventually be substantiated?</p>
        <p>While we have mostly pointed out questions that might possibly help to orient a critical discussion of digital methods and resources, we want to close by highlighting the opportunities that digital methods and resources present. As Mitra Sharafi (OHBLH 847), for example, pointed out, new large-scale digitization projects coordinated and funded by national and international consortia seem to piggyback on the technological advances that image acquisition and OCR are making. And the combination of technological advances and political initiatives may mean better chances for digitally preserving endangered cultural heritage, e. g. from small and / or remote archives or libraries. While the serial character of such cataloging and acquisition work is not completely new, the ratio between effort and benefit has shifted significantly. Moreover, the building momentum will hopefully benefit smaller institutions with valuable holdings yet limited funding as well. 12 Unlike the situation a few decades ago, once collections are available in digital form, it very often implies that they are internationallyeven globally -accessible and communicable. (The words available and accessible occur 216 times in the OHB corpus, the most frequent co-occurrents being parts of internet addresses like www, http, org, blogspot, thefacultylounge, jotwell, nytimes, washingtonpost, etc.) Besides the technical infrastructure, this communicability is facilitated by the establishment of international encoding standards like Unicode, RDA, TEI, and CIDOC CRM, which are transparently developed and recognized by cultural heritage institutions worldwide. 13 The main factor limiting the reach of digitized collections at the moment seems to be licensing and paywall arrangements, but sometimes it is also due to a lack of consideration for user diversity.</p>
        <p>Various authors in the OHB corpus acknowledge the new possibilities of searching data once it is available as digital full text data. What they have in mind, however, seem to be primarily ›classical‹ full-text searches of documents that previously could not be searched at all. There are (at least) two other important benefits worth mentioning: First, with searches being carried out by computer systems, linguistic and context searches are now possible (i.e. search X in all its grammatical forms, or search X near Y). Second, with collections granting access to standardized, machine-readable interfaces, federated searches have also become a reality (i.e. searches that query multiple repositories at the same time via mechanisms like OAI-PMH or SPARQL).</p>
        <p>This last point suggests that it will become easier to launch queries, or work with resources more generally, across disciplinary boundaries: Since most of the encoding standards alluded to above are developed independently of any given discipline or research community, the need for capabilities of translating disciplinary terms to those used by the repository standards is on the rise. Once this has been achieved, however, the same query should apply to related databases from other disciplines with relatively few and minor modifications.</p>
        <p>The preceding argument about linguistic searches (which are features of repository or of third-party software) suggests that the boundary between methods and resources sometimes seems to blur. Yet, there are important general opportunities related to digital methods as well. Of course, not all questions can be put to a large-scale corpus, but working at very large scales is a way of working that would not be possible without the opportunities that computer processing offers.</p>
        <p>Computer processability also means that data can be duplicated, reorganized, and revised without much effort. Thus, the process of scholarly as well as automatic analysis and annotation can be documented in very fine-grained ways. ›Open Science‹ refers to the possibilities (and ambition) to improve the openness, transparency, and reproducibility of research practice as a whole. Things like web annotation services, public collaboration platforms, versioning control systems, lab notebooks, data publication formats, data repositories, and data publication review literature are already available as tools contributing to this endeavor. 14 The same flexibility and connectedness also enable the accommodation of multiple dimensions and possibly conflicting interpretations of resources without forcing curators and editors to privilege one over the other(s). Instead, it opens the door to providing dynamic ways of presenting information, shifting emphases, and highlighting different interpretations according to the interests and questions that the users may have.</p>
        <p>Finally, in the discussion about Structural Topic Modeling, we have seen that one of the main advantages of digital tools is the promotion of what is referred to as serendipity. The new ways of seeing data, patterns, and relations suggested here are not only relevant to the field of legal history as such, but they also may stimulate questions and hypotheses that would otherwise not have occurred to anyone. These questions and hypotheses could then be investigated in novel or traditional ways, but that is another question for another time. Much work in the humanities is still being attributed to a kind of genius, for better or worse, and, just as they push us to make more explicit many other things that we have become used to presupposing or do implicitly, digital methods may very well turn out to organize and consolidate spaces for scholars' creativity, sponta-neity, and intuition. Ultimately, it is up to scholars to actively appropriate digital methods accordingly and establish this vision. After all, the goal is not to restrict ourselves to automatically generated and -in the end -more trivial and predictable ways of doing research, but rather to open up more and develop new avenues of analyzing sources.</p>
        <p>Anselm Küsters, Laura Volkind, Andreas Wagner</p>
        <p>Digital Humanities and the State of Legal History. A Text Mining Perspective</p>
        <p>The authors of the stm package provide a list of articles using STM at their website mentioned above.Forum forumAnselm Küsters, Laura Volkind, Andreas Wagner</p>
        <p>See, for example the British Library's Endangered Archives Programme at https://eap.bl.uk/.Forum forumAnselm Küsters, Laura Volkind, Andreas Wagner</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.0" ident="GROBID" when="2024-08-31T06:02+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this book are included in the book's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the book's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, express or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
        <p>The international monograph series "Fundamental Theories of Physics" aims to stretch the boundaries of mainstream physics by clarifying and developing the theoretical and conceptual framework of physics and by applying it to a wide range of interdisciplinary scientific fields. Original contributions in well-established fields such as Quantum Physics, Relativity Theory, Cosmology, Quantum Field Theory, Statistical Mechanics and Nonlinear Dynamics are welcome. The series also provides a forum for non-conventional approaches to these fields. Publications should present new and promising ideas, with prospects for their further development, and carefully show how they connect to conventional views of the topic. Although the aim of this series is to go beyond established mainstream physics, a high profile and open-minded Editorial Board will evaluate all This book is dedicated to Immanuel Kant (1724-1804 in Königsberg, Prussia) "Sapere aude!"</p>
        <p>Our perception of what is knowable and what is unknown, and, in particular, our viewpoint on randomness, lies at the metaphysical core of our worldview. This view has been shaped by the narratives created and provided by the experts through various sources-rational, effable, and (at least subjectively) ineffable ones.</p>
        <p>There are, and always have been, canonical narratives by the orthodox mainstream. Often orthodoxy delights itself in personal narcissism, which is administered and mediated by the attention economy, which in turn is nurtured by publicity and the desire of audiences "to know"-to attain "truth" in a final rather than in a procedural, preliminary sense. Alas, science is not in the position to provide final answers.</p>
        <p>Alternatively, the narrative is revisionistic. Already Emerson noted [200],"whoso would be a man must be a nonconformist …Nothing is at last sacred but the integrity of your own mind." But although iconoclasm, criticism, and nonconformism seem to be indispensable for progress, they bear the danger of diverting effort and attention to unworthy "whacky" attempts and degenerative research programs.</p>
        <p>Both orthodoxy as well as iconoclasts are indispensable elements of progress and different sides of the same coin. They define themselves through the respective other, and their interplay and interchange facilitate the possibility to obtain knowledge about Nature.</p>
        <p>And so it goes on and on; one is reminded of Nietzsche'seternal recurrence.1 It might always be like that; at least there is not the slightest indication that our theories settle and become canonized even for a human life span; let alone indefinitely. Indeed, any canonization might indicate a dangerous situation and be detrimental to science. Our universe seems to foster instability and change; indeed, volatility and compound interest is a universal feature of it.</p>
        <p>Physical and other unknowns might be systemic and inevitable, and actually quite enjoyable, features of science and human cognition. The sooner we learn how to perceive and handle them, the sooner we shall be able to exploit their innovative capacities.</p>
        <p>But there is more practical, pragmatic utility to randomness and indeterminism than just this epistemological joy. I shall try to explain this with two examples. Suppose that you want to construct a bridge, or some building of sorts. As you try to figure out the supporting framework, you might end up with the integral of some function which has no analytic solution you can figure out. Or even worse: The function is the result of some computation and has no closed analytic form which you know of. So, all you can do is to try to compute this function numerically.</p>
        <p>But this might be deceptive because the algorithm for numerical integration has to be "atypical" with respect to the function in the sense that all parts of the function are treated "unbiased." Suppose, for instance, that the function shows some periodicity. Then, if the integration would evaluate the function only at points which are in sync with that functional periodicity, this would result in a strong bias toward those functional values which fall within a particular sync period; and hence a bad approximation of the integral.</p>
        <p>Of course, if, in the extreme case, the function is almost constant, any kind of sampling of points-even very concentrated ones (even a single point), or periodic ones yield reasonable approximations. But "random" sampling alone guarantees that all kinds of functional scenarios are treated well and thus yield good approximations.</p>
        <p>Other examples for the utility of randomness are in politics. Random selection plays a role in aGedankenexperiment in which one is asked to sketch a theory of justice and appropriation of wealth if a veil of ignorance is kept over one's own status and destiny; or if one imagines being born into randomly selected families [422].</p>
        <p>And as far as the ancient Greeks are concerned, those who practiced their form of democracy have been (unlike us) quite aware that sooner or later, democracies deteriorate into oligarchies. This is almost inevitable: Because of mathematical mechanisms related to compound interestet cetera, an uninhibited growth tends to increase and accumulate wealth and political as well as economic power into fewer and fewer entities and individuals. We can see those aggregations of wealth and powers in action on all political scales, local and global. Two immediate consequences are misappropriations of all kinds of assets and means, as well as corruption.</p>
        <p>As the ancient Athenians watched similar tendencies in their times they came up with two solutions to neutralize the danger of tyranny by compounded power: one was ostracism, and the other one was sortition, the widespread random selection of official ministry as a remedy to curb corruption [271, p. 77]. As Aristotle noted, "the appointment of magistrates by lot is thought to be democratic, and the election of them oligarchical" [19, Politics IV, 1294 b 8, pp. 4408-4409]. The ancient Greeks used fairly sophisticated random selection procedures, algorithms, and machines calledvkgqxsqiom(kleroterion) for, say, the selection of lay judges [177,164]. Then and now, accountable and certified "randomized" selection procedures have been of great importance for the public affairs.</p>
        <p>Last but not least, I reserve a big thank you to Angela Lahee from Springer-Verlag, Berlin, for a most pleasant and efficient cooperation.</p>
        <p>Karl Svozil September 2017</p>
        <p>x Preface</p>
        <p>Fig. 1.1 (Wrong) physical proof that all nonzero natural numbers are primes law is therefore a statistical, not a mathematical, truth, for it depends on the fact that the bodies we deal with consist of millions of molecules, and that we never can get hold of single molecules." Another, ironic example is the (incorrect) physical "proof" that "all nonzero natural numbers are primes," graphically depicted in Fig. 1.1. This sarcastic anecdote should emphasize the epistemic incompleteness and transitivity of all of our constructions, suspended "in free thought;" and, in particular, the preliminarity of scientific findings.</p>
        <p>At first glance it seems that physics, and the sciences in general, are organized in a layered manner. Every layer, or level of description, has its own phenomenology, terminology, and theory. These layers are interconnected and ordered by methodological reductionism.</p>
        <p>Methodological reductionism proposes that earlier and less precise levels of (physical) descriptions can be reduced to, or derived from, more fundamental levels of physical description.</p>
        <p>For example, thermodynamics should be grounded in statistical physics. And classical physics should be derivable from quantum physics.</p>
        <p>Also, it seems that a situation can only be understood if it is possible to isolate and acknowledge the fundamentals from the complexities of collective motion; and, in particular, to solve a big problem which one cannot solve immediately by dividing it into smaller parts which one can solve, like subroutines in an algorithm.</p>
        <p>Already Descartes mentioned this method in his Discours de la méthode pour bien conduire sa raison et chercher la verité dans les sciences [165] (English translation: Discourse on the Method of Rightly Conducting One's Reason and of Seeking Truth) stating that (in a newer translation [167]) "[Rule Five:] The whole method consists entirely in the ordering and arranging of the objects on which we must concentrate our mind's eye if we are to discover some truth. We shall be following this method exactly if we first reduce complicated and obscure propositions step by step to simpler ones, and then, starting with the intuition of the simplest ones of all, try to ascend through the same steps to a knowledge of all the rest. . . . [Rule Thirteen:] If we perfectly understand a problem we must abstract it from every superfluous conception, reduce it to its simplest terms and, by means of an enumeration, divide it up into the smallest possible parts."</p>
        <p>A typical example for a successful application of Descartes' fifth and thirteenth rule is the method of separation of variables for solving differential equations [204]. For instance, Schrödinger, by his own account [450] with the help of Weyl, obtained the complete solutions of the Schrödinger equation for the hydrogen atom by separating the angular from the radial parts, solving them individually, and finally multiplying the separate solutions.</p>
        <p>So it seems that more fundamental microphysical theories should always be preferred over phenomenological ones.</p>
        <p>Yet, good arguments exist that this is not always a viable strategy. Anderson, for instance, points out [13] that "the ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. . . . The constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexity. The behaviour of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles. Instead, at each level of complexity entirely new properties appear, and the understanding of the new behaviours requires research which I think is as fundamental in its nature as any other."</p>
        <p>One pointy statement of Maxwell was related to his treatment of gas dynamics, in particular by taking only the mean values of quantities involved, as well as his implicit assumption that the distribution of velocities of gas molecules is continuous [234, p. 422]: "But I carefully abstain from asking the molecules which enter where they last started from. I only count them and register their mean velocities, avoiding all personal enquiries which would only get me into trouble. " Pattee argues that a hierarchy theory with at least two levels of description might be necessary to represent these conundra [384, p. 117]: "This is the same conceptual problem that has troubled physicists for so long with respect to irreversibility. How can a dynamical system governed deterministically by time-symmetric equations of motion exhibit irreversible behaviour? And of course there is the same conceptual difficulty in the old problem of free will: How can we be governed by inexorable natural laws and still choose to do whatever we wish? These questions appear paradoxical only in the context of single-level descriptions. If we assume one dynamical law of motion that is time reversible, then there is no way that elaborating more and more complex systems will produce irreversibility under this single dynamical description. I strongly suspect that this simple fact is at the root of the measurement problem in quantum theory, in which the reversible dynamical laws cannot be used to describe the measurement process. This argument is also very closely related to the logician's argument that any description of the truth of a symbolic statement must be in a richer metalanguage (i.e., more alternatives) than the language in which the proposition itself is stated. " Stöltzner and Thirring [489,493,529], in discussing Heisenberg's Urgleichung, which today is often referred to as Theory of Everything [34], at the top level of a "pyramid of laws," suggest three theses related to a "breakdown" to lower, phenomenologic, levels: "(i) The laws of any lower level . . . are not completely determined by the laws of the upper level though they do not contradict them. However, what looks like a fundamental fact at some level may seem purely accidental when looked at from the upper level. (ii) The laws of a lower level depend more on the circumstances they refer to than on the laws above. However, they may need the latter to resolve some internal ambiguities. (iii) The hierarchy of laws has evolved together with the evolution of the universe. The newly created laws did not exist at the beginning as laws but only as possibilities." In particular, the last thesis (iii) is in some proximity (but not sameness) to laws emerging from chaos in Chap. 9 (p. 39), as it refers also to spontaneous symmetry breaking.</p>
        <p>General reductionism as well as determinism does not necessarily imply predictability. Indeed, by reduction to the halting problem (and also related to the busy beaver function) certain structural consequences and behaviours may become unpredictable (cf. Sect. 6.2 on p. 30). As expressed by Suppes [497, p. 246], "such simple discrete elementary mechanical devices as Turing machines already have behaviour in general that is unpredictable."</p>
        <p>With regards to obtaining knowledge of physical or algorithmic universes, I encourage the reader to contemplate the notion of observation and measurement: what constitutes an observation, and how can we conceptualize measurement?</p>
        <p>In general terms measurement and observation can be understood as some kind of information transmission from some "object" to some "observer." Thereby the "observer" obtains knowledge about the "object." The quotation marks stand for the arbitrariness and conventionality of what constitutes an "object" and an "observer." These quotation marks will be omitted henceforth.</p>
        <p>Suppose that the observer is some kind of mechanistic or algorithmic agent, and not necessarily equipped with consciousness.</p>
        <p>In order to transmit information any observation needs to draw a distinction between the observed object and the observer. Because if there is no distinction, there cannot be any information transfer, no external world, and hardly any common object to speak about among individuals. (I am not saying that such distinction is absolutely necessary, but rather suggestive as a pragmatic approach.) Thereby, information is transferred back and forth through some hypothetical interface, forming a (Cartesian) cut; see Fig. 1.2 for a graphical depiction. Any such interface may comprise several layers of representation and abstraction. It could be symbolic or describable by information exchange. And yet, any such exchange of symbols and information, in order to take place is some universe, be it virtual or physical, has to ultimately take place as some kind of virtual or physical process.</p>
        <p>As we shall see, in many situations this view is purely conventional -say, by denoting the region on one side of the interface as "object," and the region on the other side of the interface as "observer." A priori it is not at all clear what meaning should be given to such a process of "give and take;" in particular, if the exchange and thus the information flow tends to be symmetric. In such cases, the observer-object may best be conceived in a holistic manner; and not subdivided as suggested by the interface. The situation will be discussed in Sect. 1.7 (p. 10) on nesting later.</p>
        <p>Another complication regarding the observer-object distinction arises if information of object-observer or object-object systems does not reside in the "local" properties of the individual constituents, but is relationally encoded by correlations between their joint properties. Indeed there exist states of multi-particle systems which are so densely (or rather, scarcely) coded that the only information which can be extracted from them is in terms of correlations among the particles. Thereby the state contains no information about single-particle properties.</p>
        <p>A typical example for this is quantum entanglement: there is no separate existence and apartness of certain entities (such as quanta of light) "tightly bundled together" by entanglement. Indeed, the entire state of multiple quanta could be expressed completely, uniquely and solely in terms of correlations (joint probability distributions) [58,365], or, by another term, relational properties [588], among observables belonging to the subsystems; irrespective of their relativistic spatio-temporal locations [464].</p>
        <p>1 Intrinsic and Extrinsic Observation Mode Consequently, as expressed by Bennett [287], one has "a complete knowledge of the whole without knowing the state of any one part. That a thing could be in a definite state, even though its parts were not. [[. . .]] It's not a complicated idea but it's an idea that nobody would ever think of from the human experience that we all have; and that is that a completely perfectly, orderly whole can have disorderly parts. " Schrödinger was the first physicist (indeed, the first individual) pointing this out. His German term was Verschränkung [452, pp. 827-844]; his English denomination entanglement [453]: "When two systems, of which we know the states by their respective representatives, enter into temporary physical interaction due to known forces between them, and when after a time of mutual influence the systems separate again, then they can no longer be described in the same way as before, viz. by endowing each of them with a representative of its own. I would not call that one but rather the characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought. By the interaction the two representatives (or ψ-functions) have become entangled."</p>
        <p>Conversely, if in a two-particle entanglement situation a single particle property is observed on one particle, this measurement entails a complete knowledge of the respective property on the other particle -but at the price of a complete destruction of the original entanglement [452, p. 844] a zero sum game of sorts.</p>
        <p>It is important to note that Schrödinger already pointed out that there is a tradeoff between (maximal) knowledge of relational or conditional properties (German Konditionalsätze) on the one hand, and single particle properties on the other hand; one can have one of them, but not both at the same time.</p>
        <p>This has far-reaching consequences.</p>
        <p>If the observer obtains "knowledge" about, say, a constituent of an entangled pair of particles whilst at the same time being unaware of the other constituent of that pair, this "knowledge" cannot relate to any definite property of the part observed. This is simply so because, from the earlier quotation, its parts are not in a definite state.</p>
        <p>This gets even more viral if one takes into account the possibility that any measured "property" might not reflect a definite property of the state of that particle prior to measurement. Because there is no "local" criterion guaranteeing that the object observed is not entangled with some other object(s) out there -in principle it could be in a relative, definite state with some other object(s) thousands of light years away.</p>
        <p>Worse still, this entanglement may come about a posteriori; that is after -in the relativistic sense lying "inside" the future light cone originating from the space-time point of the measurement -a situation often referred to as delayed choice.</p>
        <p>Surely, classical physics is not affected by such qualms: there, any definite state of a multipartite system can be composed from definite states of the subsystems. Therefore, if the subsystems are in a definite state it makes sense to talk about a definite property thereof. No complications arise from the fact that a classical system could actually serve as a subsystem of a larger physical state.</p>
        <p>Already at this stage perplexity and frustration might emerge. This is entirely common; and indeed some of the most renown and knowledgeable physicists have suggested -you would not guess it: to look the other way.</p>
        <p>For instance, Feynman stated that anybody asking [211, p. 129] "But how can it be like that?" will be dragged " 'down the drain', into a blind alley from which nobody has yet escaped."</p>
        <p>Two other physicists emphasize in their programmatic paper [228] entitled "Quantum theory needs no 'Interpretation' " not to seek any semantic interpretation of the formalism of quantum mechanics.</p>
        <p>These are just two of many similar suggestions. Bell [43] called them the 'why bother?'ers, in allusion to Dirac's suggestion "not be bothered with them too much" [175].</p>
        <p>Of course, people, in particular scientists, will never stop "making sense" out of the universe. (But of course they definitely have stopped talking about angels and demons [266], or gods [547] as causes for many events.)</p>
        <p>Other eminent quantum physicists like Greenberger are proclaiming that "quantum mechanics is magic."</p>
        <p>So, the insight that others have also struggled with similar issues may not come as great consolation. But it may help to adequately assess the situation.</p>
        <p>When it comes to the perception of systems -physical and virtual alike -there exist two modes of observations: The first, extrinsic mode, peeks at the system without interfering with the system.</p>
        <p>In terms of interfaces, there is only a one-way flow from the object toward the observer; nothing is exchanged in the other direction. This situation is depicted in Fig. 1.3. This mode can, for instance, be imagined as a non-interfering glance at the observed system "from the outside." That is, the observe is so "remote" that the disturbance from the observation is nil (fapp).</p>
        <p>This extrinsic mode is often associated with an asymmetric classical situation: a "weighty object" is observed with a "tiny force or probe." Thereby, fapp this weighty object is not changed at all, whereas the behaviour of the tiny probe can be used as a criterion for measurement. For the sake of an example, take an apple falling from a tree; thereby signifying the presence of a huge mass (earth) receiving very little attraction from the apple.</p>
        <p>The second intrinsic observation mode considers embedded observers bound by operational means accessible within the very system these observers inhabit.</p>
        <p>One of its features is the two-way flow of information across the interface between observer and object. This is depicted in Fig. 1. 4. This mode is characterized by the limits of such agents, both with respect to operational performance, as well as with regards to the (re)construction of theoretical models of representation serving as "explanations" of the observed phenomenology.</p>
        <p>Nesting [30,31] essentially amounts to wrapping up, or putting everything (the object-cut-observer) into, a bigger (relative to the original object) box and consider that box as the new object. It was put forward by von Neumann and Everett in the context of the measurement problem of quantum mechanics [206] but later became widely known as Wigner's friend [571]: Every extrinsic observation mode can be transformed into an intrinsic observation mode by "bundling" or "wrapping up" the object with the observer, thereby also including the interface; see Fig. 1.5 for a graphical depiction.</p>
        <p>Nesting can be iterated ad infinitum (or rather, ad nauseam), like a Russian doll of arbitrary depth, to put forward the idea that somebody's observer-cut-object conceptualization can be another agent's object. This can go on forever; until such time as one is convinced that, from the point of view of nesting, measurement is purely conventional; and suspended in a never-ending sequence of observer-cut-object layers of description.</p>
        <p>The thrust of nesting lies in the fact that it demonstrates quite clearly that extrinsic observers are purely fictional and illusory, although they may fapp exist.</p>
        <p>Moreover, irreversibility can only fapp emerge if the observer and the object are subject to uniform reversible motion. Strictly speaking, irreversibility is (provable) impossible for uniformly one-to-one evolutions. This (yet not fapp) eliminates the principle possibility for "irreversible measurement" in quantum mechanics. Of course, it is still possible to obtain strict irreversibility through the addition of some many-to-one process, such as nonlinear evolution: for instance, the function f (x) = x 2 maps both x and -x into the same value.</p>
        <p>A particular, "Russian doll" type nesting is obtained if one attempts to self-represent a structure.</p>
        <p>One is reminded of two papers by Popper [416,417] discussing Russell's paradox of Tristram Shandy [485]: In volume 1, Chap. 14, Shandy finds that he could publish two volumes of his life every year, covering a time span far shorter than the time it took him to write these volumes. This de-synchronization, Shandy concedes, will rather increase than diminish as he advances; one may thus have serious doubts about whether he will ever complete his autobiography. Hence Shandy will never "catch [417, p. 174], "Tristram Shandy tries to write a very full story of his own life, spending more time on the description of the details of every event than the time it took him to live through it. Thus his autobiography, instead of approaching a state when it may be called reasonably up to date, must become more and more hopelessly out of date the longer he can work on it, i.e. the longer he lives."</p>
        <p>For a similar argument Szangolies [526] employs the attempt to create a perfectly faithful map of an island; with the map being part of this very island -resulting in an infinite "Russian doll-type" regress from self-nesting, as depicted in Fig. 1.6. The origin of this map metaphor has been a sign in a shopping mall depicting a map of the mall with a "you are here" arrow [527].</p>
        <p>Note that the issue of complete self-representation by any infinite regress only is present in the intrinsic case -the map being located within the bounds of, and being part of, the island. Extrinsically -that is, if the map is located outside of the island it purports to represent -no self-reflexion, and no infinite regress and the associated issue of complete self-description occurs.</p>
        <p>Note also that Popper's and Szangolies's metaphors are different in that in Popper's case the situation expands, whereas Szangolies's example requires higher and higher resolutions as the iteration covers ever tinier regions. In both cases the metaphor breaks down for physical reasons -that is, for finite resolution, size or precision of the physical entities involved.</p>
        <p>Reflexive nesting has been long used in art. It is nowadays called the Droste effect after an advertisement for the cocoa powder of a Dutch brand displaying a nurse carrying a serving tray with a box with the same image.</p>
        <p>There are earlier examples. Already Giotto (di Bondone) in the 14th century used reflexivity in his Stefaneschi Triptych which on its front side portrays a priest presenting an image of itself (the Stefaneschi Triptych) to a saint. The 1956 lithograph "Prentententoonstelling" ("Print Gallery") by Escher depicts a young man standing in an exhibition gallery, viewing a print of some seaportthereby the print blends or morphs with the viewer's (exterior) surroundings. The presentation of reflexivity is incomplete: instead of an iteration of self-images it contains a circular white patch with Escher's monogram and signature. A "completion" has been suggested [471] by filling this lacking area of the lithograph with reflexive content.</p>
        <p>For a more recent installation, see Hofstadter's video camera [283, p. 490] which records a video screen picture of its own recording.</p>
        <p>A variant of nesting is chaining; that is, the serial composition of successive objects. In this case the cut between observer and object is placed between the "outermost, closest" object and an observer, as depicted in Fig. 1. 7.</p>
        <p>Chaining has been used by von Neumann [552,554,Chap. VI] to demonstrate that interface or cut can be shifted arbitrarily.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Empirical evidence can solely be drawn from operational procedures accessible to embedded observers. Embeddedness means that intrinsic observers have to somehow inspect and thus interact with the object, thereby altering both the observer as well as the object inspected.</p>
        <p>Physics shares this feature with computer science as well as the formalist, axiomatic approach to mathematics. There, consistency requirements result in limits of self-expressivity relative to the axioms [326,573] (if the formal expressive capacities are "great enough"). Indeed, as expressed by Gödel (cf. Ref. [549, p. 55] and [210, p. 554]), "a complete epistemological description of a language A cannot be given in the same language A, because the concept of truth of sentences of A cannot be defined in A. It is this theorem which is the true reason for the existence of undecidable propositions in the formal systems containing arithmetic."</p>
        <p>A generalized version of Cantor's theorem suggests that non-trivial (that is, nondegenerate, with more than one property) systems cannot intrinsically express all of its properties. For the sake of a formal example [573, p. 363], take any set S and some (non-trivial, non-degenerate) "properties" P of S. Then there is no onto function S -→ P S , whereby 1 P S represents the set of functions from S to P. Stated differently, suppose some (nontrivial, non-degenerate) properties; then the set of all conceivable and possible functional images or "expressions" of those properties is strictly greater than the domain or "description" thereof. 1 An equivalent function is S × S -→ P. Every function f : S -→ P S can be converted into an equivalent function g, with g : S × S -→ P, such that g(a 1 , a 2 ) = [ f (a 2 )](a 1 ) ∈ P. One may think of a 2 as some "index" running over all functions f . A typical example is taken from Cantor's proof that the (binary) reals are non-denumerable: Identify S = N and P = {0, 1}, then {0, 1} N can be identified with the binary reals in the interval</p>
        <p>For the sake of construction of a "non-expressible description" relative to the set of all functions f : S -→ P S , let us closely follow Yanofsky's scheme [573]: suppose that, for some non-trivial set of properties P we can define (that is, there exists) a "diagonal-switch" function δ : P -→ P without a fixed point, such that, for all p ∈ P, δ( p) = p. Then we may construct a nonf -expressible function u : S -→ P S by forming u(a) = δ(g(a, a)), (2.1)</p>
        <p>Because, in a proof by contradiction, suppose that some function h expresses u; that is, u(a 1 ) = h(a 1 , a 2 ). But then, by identifying a = a 1 = a 2 , we would obtain h(a, a) = δ(h(a, a)), thereby contradicting our property of δ. In summary, there is a limit to self-expressibility as long as one deals with systems of sufficiently rich expressibility.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>The vision that self-reflexivity may, through self-intervention amounting to paradoxical situations, impose some restrictions on the performance and the capacity of physical agents to know their own states, is a challenging one. It has continued to present a source of inspiration.</p>
        <p>Before beginning a brief review of the subject, let me recall an anecdote of Bocca della Veritá, the Mouth of Truth, a marble mask in the portico of Rome's Santa Maria in the Cosmedin church. According to Rucker's own account [437, p. 178], "Legend has it that God himself has decreed that anyone who sticks a hand in the mouth slot and then utters a false statement will never be able to pull the hand back out. But I have been there, and I stuck my hand in the mouth and said, "I will not be able to pull my hand back out." (May God forgive me!)"</p>
        <p>In a very similar manner as discussed earlier in Chap. 2 one can identify S with measurements M, and P with the set of possible outcomes O of these measurements. Alternatively, one may associate a physical state with P.</p>
        <p>For the sake of construction of a "non-measurable self-inspection" relative to all operational capacities let us again closely follow the scheme involving the nonexistence of fixed points. In particular, let us assume that it is not possible to measure properties without changing them. This can be formalized by introducing a disturbance function δ : O -→ O without a fixed point, such that, for all o ∈ O, δ(o) = o. Then we may construct a non-operational measurement u : M -→ O M by forming</p>
        <p>Again, because, in a proof by contradiction, suppose that some operational measurement h could express u; that is, u(m 1 ) = h(m 1 , m 2 ). But then, by identifying m = m 1 = m 2 , we would obtain h(m, m) = δ(h(m, m)), thereby again clearly contradicting our definition of δ.</p>
        <p>In summary, there is a limit to self-inspection, as long as one deals with systems of sufficiently rich phenomenology. One of the assumptions has been that there is no empirical self-exploration and self-examination without changing the sub-system to be measured. Because in order to measure a subsystem, one has to interact with it; thereby destroying at least partly its original state. This has been formalized by the introduction of a "diagonal-switch" function δ : P -→ P without a fixed point.</p>
        <p>In classical physics one could argue that, at least in principle, it would be possible to push this kind of disturbance to arbitrary low levels, thereby effectively and for all practical purposes (fapp) eliminating the constraints on, and limits from, self-observation. One way of modelling this would be a double pendulum; that is, two coupled oscillators, one of them (the subsystem associated with the "observed object") with a "very large" mass, and the other one of them (the subsystem associated with the "observer" or the "measurement apparatus") with a "very small" mass.</p>
        <p>In quantum mechanics, unless the measurement is a perfect replica of the preparation, or unless the measurement is not eventually erased, this possibility is blocked by the discreteness of the exchange of at least one single quantum of action. Thus there is an insurmountable quantum limit to the resolution of measurements, originating in self-inspection.</p>
        <p>Several authors have been concerned about reflexive measurements, and, in particular, possible restrictions and consequences from reflexivity. Their vision has been to obtain a kind of inevitable, irreducible indeterminism; because determinate states might be provable inconsistent.</p>
        <p>Possibly the earliest speculative note on intrinsic limits to self-perception is obtained in von Neumann's book on the Mathematical Foundations of Quantum Mechanics, just one year after the publication of Gödel's centennial paper [242] on the incompleteness of formal systems. Von Neumann notes that [554, Sect. 6.3, p. 438] ". . . the state of information of the observer regarding his own state could have absolute limitations, by the laws of nature." 1 It is unclear if he had recursion theoretic incompleteness in mind when talking about "laws of nature." Yet, von Neumann immediately dismissed this idea as a source of indeterminism in quantum mechanics and rather proceeded with the value indefiniteness of the state of individual parts of a system -comprising the object and the measurement apparatus combined -in (what Schrödinger later called) an entangled state (cf. the later Sects. 12.8 and 12.10).</p>
        <p>Probably the next author discussing similar issues was Popper who, in a two-part article on indeterminism in quantum physics and in classical physics [416,417] mentions that, like quantum physics, even classical physics "knows a similar kind of indeterminacy, also due to 'interference from within. ' " In what follows I shall just cite a few later attempts and survey articles with no claim of completeness. Indeed many authors appears to have had similar ideas independently; without necessarily being aware of each other. This is then reflected by a wide variety of publications and references. Many of the following references have already been discussed and listed in my previous reviews of that subject [499,516].</p>
        <p>Zwick's quantum measurement and Gödel's proof [595], cites, among other authors, Komar [317] and Pattee [384, p. 117] (cf. the quote on p. 4) as well as Lucas [345] (although the latter did only discuss related issues regarding minds-asmachines).</p>
        <p>According to his own draft notes written on a TWA in-flight paper on Feb. 4-6, 1974 [568] Wheeler imagined adding " "participant" to "undecidable propositions" to arrive at physics." Alas, by various records (cf. from Bernstein [59, p. 140-141] and Chaitin [499, p. 112], including this Author's private conversation with Wheeler), Gödel himself has not been very enthusiastic with regards to attempts to relate quantum indeterminism, and, in particular, with regards to quantum measurements, with logical incompleteness.</p>
        <p>More recently, Breuer has published a series of articles [72][73][74] on the impossibility of accurate self-measurements. Lately Mathen [356,357] as well as Szangolies [525,526] have taken up this topic again.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Having explored the limits and the "negative" effects of the type of self-exploration and self-examination embedded observers are bound to we shall now examine the "positive" side of self-description. In particular, we shall prove that, at least for "nontrivial" deterministic systems (in the sense of recursion theory and, by the Church-Turing thesis, capable of universal computation), it is possible to represent a complete theory or "blueprint" of itself within these very systems.</p>
        <p>Von Neumann created a cellular automaton model [555] which does exactly that: it is capable of universal communication, as well as of containing a "blueprint" or code of itself, as well as of "self-reproduction" based on this blueprint. Later such cellular automaton examples included Conway's Game of Life, or Wolfram's examples [575].</p>
        <p>To avoid any confusion one must differentiate between determinism and predictability [375]. As has already been pointed out by Suppes [497], any embodiment of a Turing machine, such as in ballistic n-body computation [510] is deterministic; and yet, due to the recursive undecidability of the halting problem, certain aspects of its behaviour, or phenomenology, are unpredictable.</p>
        <p>The possibility of a complete formal representation of a non-trivial system (capable of universal computation) within that very system is a consequence of the recursion theorem [579] and Kleene's s-m-n theorem: Denote the partial function g that is computed by the Turing-machine program with description i by ϕ i .</p>
        <p>Suppose that f : N -→ N is a total (defined on its entire domain) computable function. Then there exists an n 0 ∈ N such that ϕ f (n 0 ) = ϕ n 0 . For a proof, see Yanofsky [579].</p>
        <p>The s-m-n theorem states that every partial recursive function ϕ i (x, y) can be represented by a total recursive function r (i, x) such that ϕ i (x, y) = ϕ r (i,x) (y), thereby hard-wiring the input argument x of ϕ i (x, y) into the index of ϕ r (i,x) . Now we are ready to state that a complete formal representation or description of a non-trivial system (capable of universal computation) is given by the number n 0 of the computable function ϕ n 0 which always (that is, for all input x) outputs its own description; that is, ϕ n 0 (x) = n 0 .</p>
        <p>For the sake of a proof, suppose that p : N × N -→ N is the projection function p(m, n) = m. By the s-m-n theorem there exists a totally computable function r such that ϕ r (y) (x) = p(y, x) = y. And by the recursion theorem, there exists a complete description n 0 such that</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Chapter 5</p>
        <p>According to his own narrative, and totally unaware of Saint Augustine of Hippo's as well as Nicholas of Cusa's (aka Nicolaus Cusanus') notion of learned ignorance (Latin: docta ignorantia), the Baron Münchhausen pulled himself (and his horse) out of a mire by his own hair [88,Chap. 4]. (This story is not contained in Raspe's earlier collections [427].) In the following we shall be concerned with the question exactly why it is entirely hopeless to pursue the strategy suggested by the Baron Münchhausen; and why one should be concerned about this. More generally, is it (im)plausible to attempt to reach out into some external domain with purely intrinsic means; that is, by operational (from the point of view of intrinsic, embedded observers) capacities and means which cannot include any "extrinsic handle," or Archimedean point?</p>
        <p>Most likely everyone pursuing that kind of strategy -with the sole exception of the Baron Münchhausen -has drowned. But maybe something general can be learned from this flawed attempt of self-empowerment? And Münchhausen's vain attempt to lift himself entirely (and not only parts of himself, such as his hair) indicates that some internal means -tactics which rely entirely on operations referring to, and movements within himself, with rare exceptions1 -are useless.</p>
        <p>Epistemic issues resembling this metaphor have been called Münchhausen trilemma: as Albert has pointed out that, "if one wants a justification for everything, then one must also require a justification for those findings and premises which one has used to derive and justify the respective reasoning -or the relevant statements." 2With regards to the trilemma there seem to be only three alternatives or attempts of resolutions: either (i) an infinite regress in which each proof requires a further proof, ad infinitum; or (ii) circularity in which theory and proof support each other; very much like the ouroboros symbol, serpent or dragon eating its own tail; or (iii) a termination of justification at an arbitrary point of settlement by the introduction of axioms.</p>
        <p>When compared with the original goal of omni-justification of everything all three alternatives appear not very satisfactory. This is well in line with ancient scepticism, and Albert's three "resolutions" of the Münchhausen trilemma can be related to the tropes or Five Modes enumerated by Sextus Empiricus, in his Outlines of Pyrrhonism. These are in turn attributed to Diogenes Laertius and ultimately to Agrippa. These tropes are [548] (i) dissent and disagreement of conflicting arguments, such that conflict cannot be decided; as well as uncertainty about arguments; (ii) infinite regress; (iii) mean and relation dependence, relativity of arguments; (iv) assumption about the truth of axioms without providing argument; as well as (v) circularity -The truth asserted involves a vicious circle.</p>
        <p>If one pursues the axiomatic approach -by holding to some (at least preliminary) theory of everything, then what can and what cannot be expressed and formally proven is (means) relatives to the assumptions and axioms and derivation rules made. Once a formal framework is fixed, this framework constitutes a universe of expressions. If such a framework is "strong" or "sophisticated enough" it includes self-expressibility by its capacity to encode the terms occurring within it, and by substituting and applying these terms into functions which themselves are encodable. While there is nothing wrong with self-expressibility -actually it has been argued in Chap. 2 that physics is bound to reflexivity -some conceivable expressions are paradoxical, and need to be excluded for "security reasons;" in particular, to avoid contradictions. This results in provable limits to self-expressibility; limits which are even quantifiable [125,131]. This is very different from revelations about numbers, such as Srinivasa Ramanujan's inspirations. In such cases, no bounds to expressibility can be given. Indeed, expressibility by intuition may be unlimited. It cannot be ruled out that some agents, such as human minds, have a more direct access to truth than, say, an automated proof system.</p>
        <p>But trusting such claims is very problematic. The claims are not correct with respect to any axioms and derivation rules which one might have agreed upon as being valid, and therefore cannot be checked and found (in-)correct relative to the latter.</p>
        <p>Of course, one might say, that ultimately there has to be trust involved. Because also in the traditional, axiomatic ways, the axioms and derivation rules have to be trusted. (Ramsey theory might be an exception.)</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>While -depending on one's subjective optimism or pessimism often, sometimes or rarely -it is possible to predict the future, certain forecasting tasks, in particular, when it comes to self-reference, are provable unattainable, and will remain so forever. Why? Because some forecasting tasks would result in the following situation, frugally explained by Aaronson [2] "Turing imagined that there was a special machine that could solve the Halting Problem. Then he showed how we could have this machine analyse itself, in such a way that it has to halt if it runs forever, and run forever if it halts. Like a hound that finally catches its tail and devours itself, the mythical machine vanishes in a fury of contradiction. (That's the sort of thing you don't say in a research paper.)"</p>
        <p>Given two problems A and B. Let us say that if "a reduction from problem A (in)to problem B" exists (or "problem A is reducible to problem B") then the solution to problem B can be used to solve problem A. Indeed, one may think of B as some "oracle" or "subroutine" which can be used to solve A. Thereby, reduction from A into B is an algorithm for transforming problem A into another problem B. Therefore, when problem A is reducible to problem B, then a solution of problem A cannot be harder than a solution to problem B, since a solution to B provides a solution to A. Hence, a reduction from problem A to another problem B can be used to show that problem B is at least as difficult as problem A.</p>
        <p>More specifically, reduction (aka "algorithmic translation") from some unsolvable problem A (in particular, the halting problem) to problem B means the demonstration that the problem B in question is unsolvable by showing that the unsolvable problem A (in particular, the halting problem) can be reduced to it: that is, by showing that if we could compute a solution to problem B in question, we could use this solution to get a computable method for solving the unsolvable problem A (in particular, the halting problem) [435,Sect. 2.1,p. 34]. But there cannot exist such a computable method of solving A. Therefore problem B must be unsolvable as well.</p>
        <p>In what follows we shall follow previous reviews of that subject [499,516]; mostly in the context of classical mechanics. Thereby the standard method is a reduction from some form of recursion theoretic incompleteness (in particular, the halting problem) into some physical entity or decision problem. Here the term reduction also refers to the method to link physical undecidability by reducing it to logical undecidability. Logical undecidability, in turn, can be related to ancient antinomies -for instance "the liar:" already the Bible's Epistle to Titus 1:12, states that "one of Crete's own prophets has said it: 'Cretans are always liars, evil brutes, lazy gluttons.' He has surely told the truth." -as well as antinomies plaguing Cantor's naive set theory.</p>
        <p>A typical example for this strategy is the embedding of a Turing machine, or any type of computer capable of universal computation, into a physical system. As a consequence, the physical system inherits any type of unsolvability derivable for universal computers such as the unsolvability of the halting problem: because the computer or recursive agent is embedded within that physical system, so are its behavioural patterns.</p>
        <p>References [35, 119-121, 154-156, 197, 284, 302, 372, 497, 499, 573, 574]. contain concrete examples. The author used a similar reduction technique in the context of a universal ballistic computational model to argue that the n-body problem [171,413,563] may perform in an undecidable manner; that is, some observables may not be computable. Consequently, the associated series solutions [496,560,561] might not have computable rates of convergence; just like Chaitin's Ω [108,129,136], the halting probability for prefix-free algorithms on universal computers [109,111].</p>
        <p>Of course, at some point this method or metaphor becomes problematic, as universal computation requires the arbitrary allocation of time and -depending of the computational model -computational and/or memory space; that is, a potentially infinite totality. This is never achievable in realistic physical situations [79-81, 232, 233].</p>
        <p>One immediate consequence of reduction is the fact that, at least for sufficiently complex systems allowing the implementation of Peano arithmetic or universal computation, determinism does not imply predictability [497,499]. This may sound counterintuitive at first but is quite easy to understand in terms of the behaviour, the temporal evolution or phenomenology of a device or subsystem capable of universal computation.</p>
        <p>Let us, for the sake of a more explicit (but not formal and in a rather algorithmic way) demonstration what could happen, consider a supposedly and hypothetically universal predictor. We shall, by a proof by contradiction show, that the assumption of such a universal predictor (and some "innocent" side constructions) yields a complete contradiction. Therefore, if we require consistency, our only consolation -or rather our sole option -is to abandon the assumption of the existence of a universal predictor.</p>
        <p>The scheme of the proof by contradiction is as follows: the existence of a hypothetical halting algorithm capable of solving universal prediction will be assumed. More specifically, it will be (wrongly) assumed that a "universal predictor" exists which can forecast whether or not any particular program halts on any particular input. This could, for instance, be a subprogram of some suspicious super-duper macro library that takes the code of an arbitrary program as input and outputs 1 or 0, depending on whether or not the respective program halts. One may also think of it as a sort of oracle or black box analysing an arbitrary program in terms of its symbolic code and outputting one of two symbolic states, say, 1 or 0, referring to termination or nontermination of the input program, respectively.</p>
        <p>On the basis of this hypothetical halting algorithm one constructs another diagonalization program as follows: on receiving some arbitrary input program code (including its input code) as input, the diagonalization program consults the hypothetical halting algorithm to find out whether or not this input program halts. Upon receiving the answer, it does the exact opposite consecutively: If the hypothetical halting algorithm decides that the input program halts, the diagonalization program does not halt (it may do so easily by entering an infinite loop). Alternatively, if the hypothetical halting algorithm decides that the input program does not halt, the diagonalization program will halt immediately.</p>
        <p>The diagonalization program can be forced to execute a paradoxical task by receiving its own program code as input. This is so because, by considering the diagonalization program, the hypothetical halting algorithm steers the diagonalization program into halting if it discovers that it does not halt; conversely, the hypothetical halting algorithm steers the diagonalization program into not halting if it discovers that it halts.</p>
        <p>The contradiction obtained in applying the diagonalization program to its own code proves that this program and, in particular, the hypothetical halting algorithm as the single and foremost nontrivial step in the execution, cannot exist. A slightly revised form of the proof using quantum diagonalization operators holds for quantum diagonalization [512], as quantum information could be in a fifty-fifty fixed-point halting state. Procedurally, in the absence of any fixed-point halting state, the aforementioned task might turn into a nonterminating alteration of oscillations between halting and nonhalting states [303].</p>
        <p>A very general result about the incomputability of nontrivial functional properties is Rice's theorem (Cf. the Appendix Sect. A.5 on p. 174) stating that, given an algorithm, all functional properties (that is, some "nontrivial" input/output behavior which neither is true for every program, nor true for no program -that is, some programs show this behaviour, and others don't) of that algorithm are undecidable. Stated differently, given a program, there is no general algorithm predicting or determining whether the function it computes has or has not some property (which some programs have, and others do not have).</p>
        <p>One proof is by reduction to the halting problem; that is, a proof by contradiction: we construct a decision problem about functional properties by overlaying it with a primary halting problem. A the primary halting problem will in general be undecidable, so will be the compounded decision problem about function properties.</p>
        <p>Suppose (wrongly) that there exists a program predicting or determining whether or not, for any given program, the function it computes has or has not some particular property (which some programs have, and others do not have).</p>
        <p>Then we construct another program which first solves the halting program from some other arbitrary but definite program, then clears the memory, and after that, in a third step, runs a program which has the property which we are interested in. Now we apply this new program to the predictor. Suppose the other arbitrary but definite program terminates, then the predictor could in principle predict that the new program satisfies the property.</p>
        <p>Alas, if the other arbitrary but definite program does not halt (but for instance goes into an infinite loop), then our predictor will never be able to execute the two final steps of the new program -that is, clearing the memory and running the program with the property we are interested in. Therefore, predicting the functional property for the new three-step program constructed amounts to deciding the halting problem for the other arbitrary but definite program. This task is in general undecidable for arbitrary other but definite programs.</p>
        <p>More quantitatively one can interpret this unpredictability in terms of the busy beaver function [71,125,168,426], also discussed in Appendix A.7, which can be defined as a sort of "worst case scenario" as follows: suppose one considers all programs (on a particular computer) up to length (in terms of the number of symbols) n. What is the largest number producible by such a program before halting? (Note that non-halting programs, possibly producing an infinite number, e.g., by a non-terminating loop, do not apply.) This number may be called the busy beaver function of n.</p>
        <p>Consider a related question: what is the upper bound of running time -or, alternatively, recurrence time -of a program of length n bits before terminating or, alternatively, recurring? An answer to this question will explain just how long we have to wait for the most time-consuming program of length n bits to halt. That, of course, is a worst-case scenario. Many programs of length n bits will have halted long before the maximal halting time. We mention without proof [125,128] that this bound can be represented by the busy beaver function.</p>
        <p>Knowledge of the maximal halting time would solve the halting problem quantitatively because if the maximal halting time were known and bounded by any computable function of the program size of n bits, one would have to wait just a little longer than the maximal halting time to make sure that every program of length nalso this particular program, if it is destined for termination -has terminated. Otherwise, the program would run forever. Hence, because of the recursive unsolvability of the halting problem the maximal halting time cannot be a computable function. Indeed, for large values of n, the maximal halting time "explodes in a way which is unbounded by computability;" thereby growing faster than any computable function of n (such as the Ackermann function).</p>
        <p>By reduction, upper bounds for the recurrence of any kind of physical behaviour can be obtained; for deterministic systems representable by n bits, the maximal recurrence time grows faster than any computable number of n. This bound from below for possible behaviours may be interpreted quite generally as a measure of the impossibility to predict and forecast such behaviours by algorithmic means.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Induction is the inference of general rules "causing" and "generating" (in an algorithmic interpretation) physical behaviours from these very behaviours (without any extra assumptions) alone. Thus induction is "bottom up:" given the phenomena and how observers perceive them operationally, these observers somehow obtain the causes and rules which supposedly underlie these phenomena. Thereby we shall restrict ourselves to algorithmic methods of induction; We shall not consider others, such as intuition, guesses or oracles, or means other than intrinsic.</p>
        <p>Again it can be shown that for any deterministic system strong enough to support Peano arithmetic or universal computation, the induction problem for general algorithms (laws) or behaviours (phenomenology) is provable unsolvable. Induction is thereby reduced to the unsolvability of the rule inference problem [8,14,64,246,336]. This is the task to identify a rule or law reproducing the behaviour of a deterministic system by observing its input-output performance by purely algorithmic means (not by oracles or intuition).</p>
        <p>Informally, the algorithmic idea of the proof is to take any sufficiently powerful rule or method of induction and, by using it, to define some functional behaviour which is not identified by it. This amounts to a sort of diagonalization; that is, the construction of an algorithm which (passively) fakes the guesser by simulating some particular function until the guesser pretends to be able to guess the function correctly. In a second, diagonalization step, the faking algorithm then switches to a different functional behaviour to invalidate the guesser's guess.</p>
        <p>One can also relate this result to the recursive unsolvability of the halting problem, or in turn interpret it quantitatively in terms of the busy beaver function: there is no recursive bound on the time the guesser has to wait to make sure that the guess is correct. More generally, one could relate induction also to the problem of functional equivalence, which is provable undecidable [435,Sect. 2.1,pp. 33,34]: do two or more algorithms compute the same function? Two algorithms ϕ and ψ are equivalent if and only if they share a common domain (and image), and for any argument x of the domain, they are conditionally equal ϕ(x) = ψ(x); that is, both sides are meaningful at the same time and, if meaningful, they assume the same value.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>The following theorems of recursive analysis [7,564] have some implications for theoretical physics.</p>
        <p>• Specker's theorem of recursive analysis: There exist recursive monotone bounded sequences of rational numbers whose limit is no computable number [477].</p>
        <p>A concrete example of such a number is Chaitin's Omega number [103,109,129], also discussed in Appendix A.6, the halting probability for a computer (using prefix-free code), which can be defined by a sequence of rational numbers with no computable rate of convergence. • Specker's other theorem of recursive analysis: There exist a recursive real function which has its maximum in the unit interval at no recursive real number [478]. This has implications for the principle of least action [320]. • Wang's theorem of recursive analysis: The predicate "there exists a real number r such that whether or not G(r ) = 0" is recursively undecidable for G(x) in a class of functions which involves polynomials and the sine function [559]. This, again, has some bearing on the principle of least action. • Uncomputable solutions of differential equations: There exist uncomputable solutions of the wave equations for computable initial values [78,418]. • Ubiquity and pervasiveness of undecidability: On the basis of theorems of recursive analysis [433,442] many questions in dynamical systems theory are provable undecidable [107,157,280,487]. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>The following speculations resemble Darwin's and also Turing's "inversion of reason" -that is, "competence without comprehension" -forcefully put forward by the atheistic philosopher Daniel Dennett in his phrase "delere Auctorem Rerum Ut Universum Infinitum Noscas; aka DARW(=UU)IN: destroy the author of things in order to know the universe".</p>
        <p>The idea that the universe is lawless and grounded in Chaos, or a structureless void can be found in many mythologies and cosmogonies. For instance, in Chinese cosmogony hundun is identified with primordial chaos.</p>
        <p>In Greek mythology and cosmogony, c£ojchaos (or chasm, "gap, yawning" [531, p. 3]) has been considered the primordial "nonform" of the universe. In particular, Hesiod's Theogonia -Theogony "(not necessarily Hesiod's title) offers a brief account of the origins of the cosmos as preface to the extolling of Zeus' rule", thereby contrasting the "lawful" organization of the world of the gods "with the absence of such order in previous times" [531, p. 1]: "From the beginning, tell me which of these was first to come. Chasm it was, in truth, who was the very first" [277,[115][116]; or, in a different translation, "In truth, first of all Chasm came to be" [276,[115][116]. The latter author remarks in Footnote 7: "Usually [[Chasm is]] translated as "Chaos"; but that suggests to us, misleadingly, a jumble of disordered matter, whereas Hesiod's term indicates instead a gap or opening."</p>
        <p>Two centuries after Hesiod, Plato's Tomaeus stated that the god-demiurge "found everything visible in a state of turmoil, moving in a discordant and chaotic manner (prior to the intervention of the demiurge, there is chaos), so he led it from chaos to order, which he regarded as in all ways better" [412, p. 18,127; 30a] Also the Bible's Genesis [1.2] states that, after its creation by God "the earth was without form and void."</p>
        <p>Probably the first researcher speculating that all physical laws are not exact but emerge from, and are subject to, microphysical indeterminism, was Exner in fin de siècle Vienna: in his inaugural lecture "On Laws in Science and Humanities" as rector of the University of Vienna, held on October 15th, 1908, Exner suggests [209, p. 18] that there are no exact laws of nature; or, as Hanle puts it [262], "laws do not exist in nature but are formulated by man." In Exner's own words [209], ". . . in the region of the small, in time as in space, the physical laws are probably invalid . . . Therefore we have to perceive all so-called exact laws as probabilistic which are not valid with absolute certainty; but the more individual processes are involved the higher the certainty. All physical laws can be traced back to random processes on the molecular level, and from them the result follows according to the laws of probability theory. . ." 1 Indeed, Exner speculated, it could well be that the statistical laws do not necessitate nonprobabilistic, deterministic laws on the microlevel -it could well be that, in particular, on the microscopic level for individual particles, irreducible random events occur, giving rise to statistical macrolevel descriptions. Exner contemplates that this might be true even for classical physics such as collisions [209]. He also explicitly mentiones Boltzmann's methods of statistical physics.</p>
        <p>Egon von Schweidler, a colleague of Exner at the University of Vienna, might have been the first to interpret single radioactive decays as irreducibly random [459] (cf. Chap. 15, p. 129). And Schrödinger, the "scientific apprentice" of both Schweidler and Exner, later in his inaugural lecture in Zürich (Antrittsrede an der Universität Zürich, 9. Dezember 1922 [451]; English translation in [454, Chap. VI, pp. [107][108][109][110][111][112][113][114][115][116][117][118]), referred to Exner's indeterminism. So, essentially, both Exner's 1909 inaugural address as Rektor of the University of Vienna, as well as Schrödinger's 1922 inaugural address as chair professor for theoretical physics at the University of Zürich suggest the following: it is at least possible, if not preferable, to assume that all physical laws, classical and quantum alike, are emergent and correct only statistically and for large groups of outcomes, and at the microlevel are grounded in irreducible random individual events. As far as I know, these inaugural lectures are in German only and unavailable in their entirety in English; for excerpts and reviews see Refs. [262,[490][491][492]. 1 German original [209] ". . . im kleinen, der Zeit wie dem Raume nach, gelten die physikalischen Gesetze voraussichtlich nicht . . . So müssen wir also alle sogenannten exakten Gesetze nur als Durchschnittsgesetze auffassen die nicht mit absoluter Sicherheit gelten, wohl aber mit um so größerer Wahrscheinlichkeit aus je mehr Einzelvorgängen sie sich ergeben. Alle physikalischen Gesetze gehen zurück auf molekulare Vorgänge zufälliger Natur und aus ihnen folgt das Resultat nach den Gesetzen der Wahrscheinlichkeitsrechnung. . .'.</p>
        <p>Let us, for the sake of exposing an extreme position, contemplate on an infinite universe consisting of random bits -that is, these collection of bits are not only "lawless" in the sense that there does not exist any algorithm generating them, but they are, in a strictly formal way [103,133,355], also algorithmically incompressible. That is, its behaviour cannot be "compressed" by any algorithm or rule. One model of such a universe would be a single random real. We assume that the algorithmic incompressibility of encoded microphysical structures might be a quite appropriate formalization of primordial chaos.</p>
        <p>There are two ways how pseudo-lawfulness might be "revealed" to intrinsic observers:</p>
        <p>(i) Lawful substructures: It might be the case that these observers might have only restricted operational access to the entire random string, and merely perceive an orderly partial sequence (string) -that is, they accidentally live in a substructure of the random real which appears to be algorithmically compressible. Any such compression might be interpreted as a "law" governing this particular section of the universe. Calude, Meyerstein and Salomaa discuss universes which are lawless [106,114] and mention the possibility that we might be riding on a huge but finite segment of a random string which, to its inhabitants, appears to be lawful: "As our direct information refers to finite experiments, it is not out of question to discover local rules, functioning on large, but finite scales, even if the global behaviour of the process is truly random" [106, p. 1077]. These considerations are based on the finding that, "almost all real numbers, when expressed in any base, contain every possible digit or possible string of digits" [103, Theorem 6.1, p. 148] -even entire deterministic universes. There appear "spurious correlations" in the following sense: "very large databases have to contain arbitrary correlations. These correlations appear only due to the size, not the nature, of data" [113].</p>
        <p>Yanofsky [581] has discussed related scenarios, and has heuristically investigated the "extracted order that can be found in the chaos" by considering large matrices and finding patterns therein: Suppose, instead of a matrix, a long string (one might say a 1 × n matrix) whose entries are filled randomly and independently with decimal digits. The expected number of times any particular substring of m digits, say "123 . . . m," occurs within this larger string of length n is (nm + 1)(1/10) m . (ii) Emergence: The laws of nature might actually be "emergent" in a Ramsey-theory type way. Because just as "one cannot not communicate" [562, Sect. 2.24, p. 51] Ramsey theory [248,327,476] reveals that there exist properties and correlations for any kind of data, which do not depend on the way these date are generated or structured. This would also (but is not limited to) include c£oj; that is, universes which are not "lawful" and not generated by intent; and consisting of data which cannot be algorithmically compressed. Such inevitable correlations might be "interpreted" as "laws" in any data: any sufficiently large structure inevitably contains orderly substructures which can be conjectured to be "lawful" -just as the Elders looked up into the skies and "found" animal constellations there [247].</p>
        <p>Unlike the lawful substructures scenario, emergence does not presume local non-typicality.</p>
        <p>Every absolute claim of both irreducible determinism and indeterminism remains speculative and metaphysical. Because, due to the recursive undecidability of induction (the rule inference), one can never be sure if a phenomenology identified as deterministic -with a particular law or "theory of everything" [34] -"switches its course" and behaves differently, thereby disproving such claims. This is ultimately due to the fact that no recursive upper (algorithmic space/memory and runtime) bound exists for such an assertion. Conversely, any claim of absolute, irreducible indeterminism falls short of a proof that no laws exist relative to the phenomenology; for various reasons. Suppose the physical phenomena are coded into bit strings; then these bit strings are necessarily finite (there is no infinite operational precision). For finite bit strings always laws exist -think of a simple enumeration. One may also argue that, due to reduction from the halting problem, it cannot be guaranteed that no algorithmic compression exists -in general this bound will also be proportional to the worst-case scenario, which is a busy beaver type behaviour [128] -and thus nonrecursive in the length of the bit string. And finally, and also connected with worst-case space/memory and runtime -not all laws can in principle be enumerated (because there exist a potential infinity of them); and those few analized cannot be recursively asserted to not yield that particular bit string encoding the aforementioned phenomenology.</p>
        <p>Square-integrable functions can be approximated by a variety of rather different complete systems of orthogonal functions [18, Sect. 10.4, p. 649], such as, for instance, trigonometric functions, (Legendre) polynomials, or, more generally, due to the spectral theorem the system of eigenfunctions of certain normal operators. Are we thus justified to infer that such a particular function, because it can be written in these various forms, is actually "composed of," say, vibrations and oscillations in the case of Fourier analysis, or, alternatively, polynomials, or any other such complete set of orthogonal functions? At first sight it might be tempting to assume just that. But a second thought reveals that these choices of functional sets (and thus of normal operators) are purely conventional. They might, from the practical point of view, be As has been noted already in the preface, in order to cope with subjective projections of the mind, as well as with wishful thinking, Freud advised analysts to adopt a contemplative strategy of evenly-suspended attention [224,225]; and, in particular, to be aware of the dangers caused by ". . . the temptation of projecting outwards some of the peculiarities of his own personality, which he has dimly perceived, into the field of science, as a theory having universal validity; he will bring the psycho-analytic method into discredit, and lead the inexperienced astray." [224] 2 And the late Jaynes warns and disapproves of the Mind Projection Fallacy [290,291], pointing out that "we are all under an ego-driven temptation to project our private thoughts out onto the real world, by supposing that the creations of one's own imagination are real properties of Nature, or that one's own ignorance signifies some kind of indecision on the part of Nature."</p>
        <p>For a recent neurophysiological finding corroborating the possibility to induce hallucinations by perceptual priors and expectations see Ref. [419].</p>
        <p>So, it may not be entirely unreasonable to speculate that our own universe might be grounded in c£oj -chaos (or chasm, "gap, yawning" [531, p. 3]). Those "laws" which we purport to "discover" might be spurious reflections of our own minds, desperately attempting to "make sense" of the phenomena.</p>
        <p>One is reminded of Fritz Lang's remark in Godard's movie Le mépris (Contempt), approximately 14 min into that movie: "Jerry, don't forget. The gods have not created man. Man has created gods." And Schrödinger, in Nature and the Greeks, quotes fragments of Xenophanes as follows [456, p. 71]: "(Fr. 15) Yes, and if the oxen or horses or lions had hands and could paint with their hands, and produce works of art as men do, horses would paint the forms of the gods like horses, and oxen like oxen and make their bodies in the image of their several kinds. (Fr. 16) The Ethiopians make their gods black and snubnosed; the Thracians say theirs have blue eyes and red hair."</p>
        <p>For the sake of a bold demonstration take some recent findings in machine learning. In particular, consider the interpretation of photographic images by neural networks, also called deep dreaming. Depending of the class of objects the network has handled and has been trained to recognize in the past, it "projects" or interprets images presented to it according to its expectations and (trained) knowledge. Thereby [374],</p>
        <p>"even a relatively simple neural network can be used to over-interpret an image, just like as children we enjoyed watching clouds and interpreting the random shapes." An ironic and less sophisticated example is graphically depicted in Fig. 9.1.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Chapter 10 "Shut Up and Calculate" One of the biggest dangers in presenting quantum unknowns might be sophism; a wasteful exercise in fruitless scholasticism and mysticism [589]; on a par with magic [588, p. 631]. Thereby the current quantum mechanical formalism is presented as incomprehensible [211, p. 129] and, as Jaynes put it, as [291], "a peculiar mixture describing in part realities of Nature, in part incomplete human information about Nature -all scrambled up by Heisenberg and Bohr into an omelette that nobody has seen how to unscramble." Even Einstein conceded to Schrödinger, ". . . the main point was, so to speak, buried by the erudition."1 -and that was about his own co-authored "EPR"-paper [196] which he did not write, and was unhappy with the way it turned out [285, p. 175]! What is even more dicomforting, in this author's opinion, is the reaction: whereas quantum sophism presents the theory as a sort of deep "hocus pocus" [522], there is an alternative perception of quantum mechanics as a sort of shallow "nothingburger;" thereby disallowing or at least discouraging semantical questions about the meaning, as well as about the epistemology and ontology of the quantum formalism. In this way quantum theory is reduced to an almost trivial execution of functional analysis.</p>
        <p>The latter approach has a long tradition. Already in 1989 Mermin stated [362,363] "If I were forced to sum up in one sentence what the Copenhagen interpretation says to me, it would be 'Shut up and calculate!' " Well in line with this witticism are articles entitled "Quantum theory needs no 'Interpretation' [228]," or "Interpretations of quantum theory: A map of madness [93]," or assurances that "Quantum theory is a well-defined local theory with a clear interpretation. No "measurement problem" or any other foundational matters are waiting to be settled [201]." Dirac suggested "not be bothered with them too much" [175]. Indeed, already Sommerfeld had warned his students not to get into these issues, and Feynman [211, p. 129] predicted the "perpetual torment that results from [[the question]], 'But how can it be like that?' which is a reflection of uncontrolled but utterly vain desire to see [[quantum mechanics]] in terms of an analogy with something familiar." Therefore he advised his audience, "Do not keep saying to yourself, if you can possibly avoid it, 'But how can it be like that?' because you will get 'down the drain', into a blind alley from which nobody has yet escaped."</p>
        <p>But heresy has continued. Clauser [of the Clauser-Horne-Shimony-Holt (CHSH) inequalities [145]], in a noteworthy paper [144], pointed out the dogmatism of "evangelical theoreticians . . . their ecumenical leadership, and especially given Bohr's strong leadership, the net legacy of their arguments is that the overwhelming majority of the physics community accepted Bohr's "Copenhagen" interpretation as gospel, and totally rejected Einstein's viewpoint." At some point Clauser got thrown out of the office by the impatient Feynman (who often liked to market himself as "cool"). "A very powerful . . . stigma began to develop within the physics community towards anyone who sacrilegiously was critical of quantum theory's fundamentals. . . . The net impact of this stigma was that any physicist who openly criticized or even seriously questioned these foundations (or predictions) was immediately branded as a 'quack.' " Clauser continues by noticing, "To be sure, there remained alive a minority of the theory's founders (notably Einstein, Schrödinger, and de Broglie) who were still critical of the theory's foundations. These men were obviously not quacks. Indeed, they all had Nobel Prizes! Instead, gossip among physicists branded these men 'senile.' "</p>
        <p>As time passed by, another, more optimistic phase of the perception of quantum foundations followed, which, however, might not have sufficiently and critically reflected the previous evangelical theoreticians' orthodoxy. On the contrary, quantum mechanics has been marketed to the public and to policy makers alike as a hocuspocus type capacity [522].</p>
        <p>This author believes [504] that interpretation is to the formalism what a scaffolding in architecture and building construction is to the completed building. Very often the scaffolding has to be erected because it is an indispensable part of the building process. Once the completed building is in place, the scaffolding is torn down and the opus stands in its own full glory. No need for auxiliary scaffold any more. But beware of those technicians who claim to be able to erect skyscrapers without any of those poles and planks!</p>
        <p>In addition, when it comes to claims of applicability of the formalism, and its ontological commitments, the suppression of semantic content in favour of mere syntax makes us vulnerable: in many ways the formalism could be extended to domains in which it cannot be applied safely and properly. Thereby, the resulting certifications, alleged capacities and predictions could be wrong. Hence, if it comes to utilize the formalism, interpretation serves not only as scaffolding, but also provides guiding principles and precautionary methods of evaluation and application.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>In what follows a very brief account of reversible evolution and, in particular, reversible computation by permutation will be presented. We shall follow Mermin's account [368] (also available as his Lecture Notes on Quantum Computation [367]) and introduce reversible computation in terms of vector spaces: Thereby the computational states, and the state evolution are represented as elements of Cartesian standard bases, and permutation matrices acting on these base vectors, respectively.</p>
        <p>Let us repeat and rehearse some conventions involving the representation and creation of state related entities.</p>
        <p>A ket vector |x can be represented by a column vector, that is, by vertically arranged tuples of scalars, or, equivalently, as n × 1 matrices; that is,</p>
        <p>Their linear span is a one-dimensional subspace.</p>
        <p>A bra vector x| from the dual space can be represented by a row vector, that is, by horizontally arranged tuples of scalars, or, equivalently, as 1 × n matrices; that is,</p>
        <p>Their linear spans</p>
        <p>are one-dimensional subspaces of the base space V and the dual space V * , respectively.</p>
        <p>If |x is a unit vector, the associated orthogonal projection E x of V onto M can be written as the dyadic product, or tensor product, or outer product</p>
        <p>is the projection associated with |x .</p>
        <p>If the vector x is not normalized, then the associated projection is</p>
        <p>The product (state) of two ket vectors |x ≡ x 1 , x 2 , . . . , x n and |y ≡ y 1 , y 2 , . . . , y n can, up to normalization, be written as</p>
        <p>x n y 1</p>
        <p>x n y 2 . . .</p>
        <p>The product (state) of two bra vectors x| ≡ x 1 , x 2 , . . . , x n and y| ≡ y 1 , y 2 , . . . , y n can, up to normalization, be written as</p>
        <p>A more restricted universe than a quantized one would be rendered by real finite dimensional Hilbert spaces R n , and by the permutations -more precisely, orthonormal (orthogonal) transformations; that is, a one-to-one (injective) transformation of identical (co)domains R n preserving the scalar product therein. An even greater restriction comes with a discretization of states as elements of Cartesian standard bases and the use of permutation matrices.</p>
        <p>Recall that a function f (x) = y from a set X to a set Y maps inputs (or arguments) x from X into outputs (or values) y from Y such that each element of X has a single and thus unique output. X is called the domain and Y is called the codomain. The image f (X ) of the entire domain X is a subset of the codomain Y .</p>
        <p>A function</p>
        <p>As a consequence, if f is one-to-one it can be "inverted" (and thus its action "undone") by another function f -1 from its image f [X ] into its domain X such that f -1 (y) = x if f (x) = y. Therefore, the functional mapping can be inverted through</p>
        <p>A function f is onto, or surjective if every element y in its codomain Y corresponds to some (not necessarily unique) element x of its domain, such that y = f (x). In this case, the functional image is the codomain.</p>
        <p>A function f is bijective, or a one-to-one correspondence if it is both one-to-one (injective) and onto (surjective).</p>
        <p>A function f is a permutation if it is a one-to-one correspondence (bijective), and if the domain X is identical with the codomain Y = X .</p>
        <p>Usually, the (co)domain is a finite set. The symmetric group S(n) on a finite set of n elements (or symbols) is the group whose elements are all the permutations of the n elements, and whose group operation is the composition of such permutations. The identity is the identity permutation. The permutations are bijective functions from the set of elements onto itself. The order (number of elements) of S(n) is n!.</p>
        <p>Cayley's theorem [436] states that every group G can be imbedded as -equivalently, is isomorphic to -a subgroup of the symmetric group; that is, it is isomorphic to some permutation group. In particular, every finite group G of order n can be imbedded as -equivalently, is isomorphic to -a subgroup of the symmetric group S(n).</p>
        <p>Stated pointedly: permutations exhaust the possible structures of (finite) groups. The study of subgroups of the symmetric groups is no less general than the study of all groups.</p>
        <p>A particular case where the codomain needs not to be finite is quantum mechanics. In quantum mechanics, the (co)domain will be identified with the Hilbert spaces. We will restrict our attention to complex finite dimensional Hilbert spaces C n with the Euclidean scalar product. In one of the axioms of quantum mechanics the evolution is identified with some isometric permutation preserving the scalar product (or, equivalently, a mapping of one orthomodular basis into another one); that is, with unitary transformations U, for which the adjoint (the conjugate transpose) is the inverse; that is,</p>
        <p>We shall now turn our attention to an even more restricted type of universe whose evolution is based upon permutations [193] on countable or even finite (co)domains [368]. Thereby we shall identify these (co)domains with very particular sets of unit vectors in R n : the Cartesian standard bases; namely all those ket (that is, column) vectors |x with a single coordinate being one, and all other components zero.</p>
        <p>Suppose further that elements of the set {1, 2, . . . , n} of natural numbers are identified with the elements of the Cartesian standard bases</p>
        <p>The symmetric group S(n) of all permutations of n basis elements of B can then be represented by the set of all (n × n) permutation matrices carrying only a single "1" in all rows and columns; all other entries vanish.</p>
        <p>For the sake of an example, consider the two-dimensional case with n = 2,</p>
        <p>Then there exist only two permutation matrices, interpretable as the identity and the not matrix, respectively:</p>
        <p>Note that the way these matrices are constructed follows the scheme of defining unitary transformations in terms of sums of basis state changes [460]. Indeed, all the n! permutation matrixes transforming the n basis elements of the Cartesian standard basis B = {|e 1 , |e 2 , . . . , |e n } in n dimensions can be constructed by varying the sums of such basis state changes. More explicitly, consider in Cauchy's two-line notation the jth permutation</p>
        <p>so that the input i is mapped into σ j (i), with 1 ≤ i ≤ n; then the jth permutation matrix can be defined by</p>
        <p>(11.9)</p>
        <p>Permutations cannot give rise to coherent superposition and entanglement -the latter one being just particular, non-factorizable superpositions in the multiple particle context. Syntactically this is due to the fact that, for a finite number of bits, permutation matrices contain only a single entry in each row and each column.</p>
        <p>The following question arises naturally: is the set of permutations for arbitrary largedimensional computationally universal in the sense of Turing; that is: can such a system of permutations compute all recursively enumerable functions [55,222,537]?</p>
        <p>The three-bit Fredkin gate is universal with respect to the class of Boolean functions; that is, functions of binary inputs with binary output. Universality here means that any Boolean function can be constructed by the serial composition Fredki gates. Likewise, the three-bit Toffoli gate is universal with respect to the class of Boolean functions. Its permutation matrix is Indeed, the Fredkin and the Toffoli gates are equivalent up to permutations; and so is any quasi-diagonal matrix with one entry in 2 × 2 matrix block form X, and all other entries 1 in the diagonal.</p>
        <p>This author is not aware of any concrete, formal derivation of Turing universality from universality with respect to Boolean functions. Indeed, how could input-output circuits encode the kind of substitution and self-reference encountered in recursion theory [473][474][475]? One could conjectured that, of one allows an arbitrary sequence of Boolean functions, then this would entail universal Turing computability [69,378], but this is still a far cry from coding, say, the Ackermann function in terms of reversible gates.</p>
        <p>While it is true that, at least in principle, Leibniz's binary atoms of information suffice for the construction of higher-dimensional entities, it is not entirely unreasonable to consider 3-ary, 4-ary, and, in general d-ary atoms of information. One conjecture would be that the set of universal operations with respect to d-ary generalisations of binary functions -that is, functions</p>
        <p>Quantum computing is about generalized states, which can be in a superposition of classical states; and about generalized permutations; that is, about bijections in complex vector spaces. For this it is sufficient to consider classical reversible computation, "augmented" with gates producing coherent superpositions of a classical bit (such as the Hadamard gate or quantum Fourier transforms) [371,466].</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Quantum Mechanics in a Nutshell</p>
        <p>At the moment, there exists a loosely bundled canon of quantum rules subsumed under the term quantum mechanics or quantum theory. It includes reversible as well as irreversible processes, and is prima facie inconsistent. As already von Neumann [552,554] and later Everett [30,206,545] noted, there cannot be any irreversible measurement process nested in a ubiquitous uniformly reversible evolution of the quantum state. Both von Neumann and Everett called the former, irreversible, discontinuous change the "process 1"; and the latter, reversible, continuous, deterministic change the "process 2," respectively. Stated differently, there cannot exist any irreversible many-to-one measurement scenario (other than pragmatic fappness) in a reversible one-to-one environment.</p>
        <p>Hence, if one wants to maintain irreversible measurements, then (at least within the quantum formalism) one is faced with the following dilemma: either quantum mechanics must be augmented with some irreversible, many-to-one state evolution, thereby spoiling the ubiquitous, universal reversible one-to-one state evolution; or the assumption of the co-existence of a ubiquitous, uniform reversible one-to-one state evolution on the one hand with some irreversible many-to-one "wave function collapse," (by another wording, "reduction of the state vector") throughout measurement on the other hand, yields a complete contradiction.</p>
        <p>How is such a situation handled in other areas? Every system of logic which is self-contradictory (inconsistent) -such that a proposition as well as its negation is postulated; or can be derived from the postulates -in particular, in a formal axiomatic system, is detrimental and disastrous. Because by the principle of explosion (Latin: ex falso quodlibet) any invocation of a statement as well as of its negation yields every proposition true. This can be motivated by supposing that both "P" as well as "not P" are true. Then the proposition "P or anything" is true (because at least "P" is true). Now suppose that also "not P" holds. But then, in order for "P or anything" to be true, "anything" needs to be true. However, if anything is derivable, then such a system lacks any descriptive or predictive capacity. In this respect it is quite convenient that quantum mechanics does not represent a formal system in the strict logical sense.</p>
        <p>With regards to the persistence and scientific reception of inconsistencies within theoretical domains one is reminded of Cantorian "naive" set theory [116,117]; whereby a set, or aggregate, was defined as follows [118, p. 85]: "By an "aggregate" (Menge) we are to understand any collection into a whole (Zusammenfassung zu einem Ganzen) M of definite and separate objects m of our intuition or our thought. These objects are called the "elements" of M." Despite its well known inconsistencies (e.g., Russell's paradox, [288] defining a "set of all sets that are not members of themselves"), it was embraced by researchers of the time with unabated enthusiasm. Hilbert, for instance, stated that [278] "Wherever there is any hope of salvage, we will carefully investigate fruitful definitions and deductive methods. We will nurse them, strengthen them, and make them useful. No one shall drive us out of the paradise which Cantor has created for us." Indeed, the different forms of (un)countable infinities still present a marvel of early "naive" set theory.</p>
        <p>Another source of perplexity remains irreversibility in statistical physics [381]; in particular, issues related to the second law of thermodynamics [375] in view of microphysical irreversibility. As already pointed out in Sect. 1.1, for the second law of thermodynamics to hold Maxwell advised to avoid [234, p. 422]: "all personal enquiries [[of Molecules]] which would only get me into trouble." A recent discussion [84,158,380,431] on the exorcism of Maxwell's demon [189,190,332] is witness of the ongoing debate.</p>
        <p>Many practitioners either tend to look the other way, or take a pragmatic stance expressed quite voluptuously by Heaviside [272,Sect. 225]: "I suppose all workers in mathematical physics have noticed how the mathematics seems made for the physics, the latter suggesting the former, and that practical ways of working arise naturally.</p>
        <p>As suggested by Dirac [173] and explored by von Neumann [552,554], quantum mechanics has been formalized in terms of Hilbert spaces.</p>
        <p>Many researchers have attempted to at least partially derive this kind of quantum formalism from other principles, mostly informational (cf., e.g., Refs. [569,588], and [239, Part II], to name but a few). Indeed, as Lakatos has pointed out [324], the contemporary researchers cannot know which ideas will prevail, and will ultimately result in progressive research programs. Therefore it appears prudent to pursue varied research programs in parallel.</p>
        <p>In the following we shall present a very brief, somewhat revisionist, view on quantum mechanics. It is based on pure quantum states representable as dichotomic value assignments on, equivalently, a (normalized) system of orthonormal basis vectors, the associated set of projection operators, or the associated set of subspaces of a Hilbert space. (Fapp a Hilbert space is a vector space with a scalar product.) Vector spaces are needed for the manipulation of vectors, such as vector additions and superpositions. (For the rest of this chapter, suppose that we are "riding" a single vector of a high dimensional Hilbert space, thereby qualifying as "members of the church of the larger Hilbert space.") By the spectral theorem, observables can be represented by the weighted spectral sums of such pure (mutually orthogonal) quantum states as well. Any non-degenerate spectral sum represents a maximal measurement. We may call this, or rather the set of orthogonal projection operators in the spectral sum, a context.</p>
        <p>Quantum complementarity is the feature that two different contexts cannot be directly measured simultaneously.</p>
        <p>Scalar products are needed for defining the relational property of vectors, such as orthogonality and collinearity. They allow projections of vectors onto arbitrary non-zero subspaces. Thereby they grant a particular view on the quantum state, as seen from another quantum state -or, equivalently, the proposition represented by the respective vector or associated projection operator.</p>
        <p>Ultimately, scalar products facilitate the definition of frame functions which can be interpreted as quantum probabilities. This is necessary because, at least from dimension three onwards, the tight intertwining (pasting) of such maximal views or contexts does not allow quantum probabilities to be defined by the convex sum of two-valued measures. These two-valued measures could, if they existed, be interpreted as non-contextual truth assignments. As it turns out, relative to reasonable side assumptions, any such classical strategy fails, simply because, from dimension three onwards, such two-valued measures do not exist for more than a single context.</p>
        <p>Suppose we are given a Hilbert space of sufficient dimension. That is, its dimension coincides with the maximal number of mutually exclusive outcomes of any experiment we wish to formalize.</p>
        <p>It is "reasonable" to define a physical state of an object by the maximal empirical (information) content in principle accessible to an observer by any sort of operational means available to this observer. In Dirac's words [173, pp. 11-12], "A state of a system may be defined as an undisturbed motion that is restricted by as many conditions or data as are theoretically possible without mutual interference or contradiction. In practice the conditions could be imposed by a suitable preparation of the system, consisting perhaps in passing it through various kinds of sorting apparatus, such as slits and polarimeters, the system being left undisturbed after the preparation."</p>
        <p>Schrödinger, in his Generalbeichte [452, Footnote 1, p. 845] (general confession) of 1935, pointed out that [539,Sect. 6,p. 328] "Actually [[in truth]]-so they saythere is intrinsically only awareness, observation, measurement. If through them I have procured at a given moment the best knowledge of the state of the physical object that is possibly attainable in accord with natural laws, then I can turn aside as meaningless any further questioning about the "actual state," inasmuch as I am convinced that no further observation can extend my knowledge of it-at least, not without an equivalent diminution in some other respect (namely by changing the state, see below)." 1 No further justification is given here.</p>
        <p>A quantum state is thus identified with a maximal co-measurable (or co-preparable) entity. This is based on complementarity: not all conceivable quantum physical properties are co-measurable. (For classical models of complementarity, see, for instance, Moore's discrete-valued automaton analogue of the Heisenberg uncertainty principle [373,446,499], as well as Wright's generalized urn model [578], and partition logics in general [511].)</p>
        <p>In the Hilbert space formulation of quantum mechanics a state is thus formalized by two entities; some structural elements, and a measure on these elements [520]:</p>
        <p>(I) equivalently, (i) an orthonormal basis of Hilbert space; (ii) a set of mutually orthogonal projection operators corresponding to an orthonormal basis called context; (iii) a maximal observable, or maximal operator, or maximal transformation whose spectral sum contains the set of mutual orthogonal projection operators from the aforementioned basis; (iv) a maximal Boolean subalgebra [249,300,376,420] of the quantum logic also called a block;</p>
        <p>(II) as well as a two-valued (0-1) measure (or, used synonymously, valuation, or truth assignment) on all the aforementioned entities, singling out or selecting one of them such that this measure is one on exactly one of them, and zero on all the others. Another way of formalizing a state would be to single out a particular vector of the basis referred to earlier -the one which is actually "true;" that is, whose measurement (deterministically) indicates that the system is in this state.</p>
        <p>However, one cannot "not measure" the accompanying context of a particular set of orthogonal vectors which, together with the state vector, completes a basis. One can deny it, or look the other way, but the permutation quantum evolution presented below presents no way for "blissful ignorance:" any "beam dump" is fapp irreversible and only fapp formalizable by taking partial traces, whereas in principle the information about the rest of the context remains intact.</p>
        <p>A non-degenerate quantum observable is identified with all properties of a state, less the two-valued measure, and formalized by (i) an orthonormal basis of Hilbert space; (ii) a set of mutually orthogonal projection operators corresponding to an orthonormal basis called context; (iii) a maximal observable, or maximal operator, or maximal transformation whose spectral sum contains the set of mutual orthogonal projection operators from the aforementioned basis; (iv) a maximal Boolean subalgebra [249,300,376,420] of the quantum logic also called block.</p>
        <p>This correspondence (ex measure) between a quantum state and a quantum observable is reflected in the formalism itself: Any maximal observable can be decomposed into a spectral sum, with the orthogonal projection operators forming a corresponding orthonormal basis, or, synonymously, by a context or a block.</p>
        <p>The isometric state permutation rule postulates that the quantum state evolves in a deterministic way by isometric (length preserving) state permutation. [Throughout this book we shall denote a bijection between the same set (continuum) as permutation.] This can be equivalently understood as a linear transformation preserving the inner product, or as change of orthonormal bases/contexts/blocks [260,Sect. 74] (see also [460]). The formalization is in terms of unitary operators.</p>
        <p>Suppose that the quantum mechanical (unitary) permutation is ubiquitous and thus valid universally. Then, stated pointedly, "reversibility rules."</p>
        <p>This assumption is strongly supported by a nesting argument [30,31] first put forward by Everett, and later by Wigner [571] (cf. Sect. 1.7 on p. 10). Because it is quite reasonable that any observing agent, when combined with the object this agent observes (including the cut/interface), should form a system that is quantized again; thereby implying a time evolution which is governed by isometric state permutation.</p>
        <p>With the assumption of uniform validity of state the quantum evolution by isometric permutativity, many-to-one processes are excluded. In particular, formation of mixed states from pure states, as well as irreversible measurements, and the associated "state reduction" (or "wave function collapse") contradict the isometric state permutation rule, and cannot take place in this regime.</p>
        <p>Usually a "state reduction" occurs during an irreversible measurement. It is associated with a transition from a state which is in a non-trivial coherent (or, by an equivalent term, linear) superposition -that is, a linear combination -a multiplicity of more than one states</p>
        <p>No one-to-one process such as a permutation can produce this n-to-1 transition.</p>
        <p>Again any "generation" of a mixed state from pure states by "tracing out" certain components of the state is disallowed, since this amounts to a loss of information, and does not correspond to any invertible (reversible) transformation. Conversely, one could "purify" any mixed state, but this process is nonunique.</p>
        <p>Already Dirac referred to the principle of superposition of states [173, pp. 11-12], "whenever the system is definitely in one state we can consider it as being partly in each of two or more other states. The original state must be regarded as the result of a kind of superposition of the two or more new states, in a way that cannot be conceived on classical ideas."</p>
        <p>The superposition principle can be formalized by linear combinations as follows: suppose two states, which can be formally represented by orthonormal bases</p>
        <p>Then each member |e i of the first basis can be represented as a linear combination or coherent superposition or superposition of elements of the second basis by</p>
        <p>and vice versa.</p>
        <p>For normalization reasons which are motivated by probability interpretations, the absolute squares of the coefficients α i j must add up to 1; that is,</p>
        <p>With this normalization, the dyadic (tensor) product of |e i is always of trace class one; that is,</p>
        <p>Superpositions of pure states -resulting in a pure state -should not be confused with mixed states, such as, for instance,</p>
        <p>which are the linear combination of dyadic (tensor) products |f i f j | of pure states |f i and |f j such that Tr(ρ) = 1 and Tr(ρ 2 ) &lt; 1.</p>
        <p>In classical physics any compound system -the whole -can be composed from its parts by separation and specification of the parts individually.</p>
        <p>This "factoring" of states of multiple constituent parts into products of individual single particle states need no longer be possible in quantum mechanics (although it is not excluded in particular quasi-classical cases): in general, any strategy to obtain the entire state of the whole system of many particles by considering the states of the individual particles fails. This is a consequence of the quantum mechanical possibility to superpose states of multiple particles; that is, to add together arbitrarily weighted (subject to normalization) products of single particle states to form a new, valid, state. Classically, these states are "unreachable" by reversible evolutions-by-permutation, but quantum mechanically it is quite straightforward to create such a superposition through unitary transformations. Arguably the most prominent one is a Hadamard transformation corresponding to a 50:50 beam splitter.</p>
        <p>Probably the first to discuss this quantum feature (in the context of the measurement process) was von Neumann, stating that, "If I is in the state ϕ(q) and I I in the state ξ(r ), then I + I I is in the state Φ(q, r ) = ϕ(q)ξ(r ). If on the other hand I + I I is in a state Φ(q, r ) which is not a product ϕ(q)ξ(r ), then I and I I are mixtures and not states, but Φ establishes a one-to-one correspondence between the possible values of certain quantities in I and in I I . [554, Sect. VI.2, pp. [436][437] . . . all "probability dependencies" which may exist between the two systems disappear as the information is reduced to the sole knowledge of . . . the separated systems I and I I . But if one knows the state of I precisely, as also that of I I , "probability questions" do not arise, and then I + I I , too, is precisely known [554, Sect. VI.2, p. 426]". 2 Unfortunately the translation uses the two English phrases "probability dependencies" as well as "probability questions" for von Neumann's German expression "Wahrscheinlichkeitsabhängigkeit." Maybe it would be better to translate these by "probabilistic correlations."</p>
        <p>In a series of German [452] and English [453,455] papers Schrödinger emphasized that [539, Sect. 10, p. 332] "The whole is in a definite state, the parts taken individually are not." 3Both von Neumann and Schrödinger thought of this as a sort of a zero-sum game, very much like complementary observables: due to the scarcity and fixed amount of information which merely gets permuted during state evolution, one can either have total knowledge of the individual parts; with zero relational knowledge of the correlations and relations among the parts; or conversely one can have total knowledge of the correlation and relations among the parts; but know nothing about the properties of the individual parts. Stated differently, any kind of mixture between the two extremes can be realized for an ensemble of multiple particles or parts:</p>
        <p>(i) either the properties of the individual parts are totally determined; in this case the relations and correlations among the parts remain indeterminate, (ii) or the relations and correlations among the parts are totally determined; but then the properties of the individual parts remain indeterminate.</p>
        <p>For classical particles only the first case can be realized. The latter case is a genuine quantum mechanical feature.</p>
        <p>Everett expressed this by saying that, in general (that is, with the exception of quasi-classical states) [206], "a constituent subsystem cannot be said to be in any single well-defined state, independently of the remainder of the composite system."</p>
        <p>The entire state of multiple quanta can be expressed completely in terms of correlations or joint probability distributions [365,576], or, by another term, relational properties [587,588], among observables belonging to the subsystems. As pointedly stated by Bennett [287] in quantum physics the possibility exists "that you have a complete knowledge of the whole without knowing the state of any one part. That a thing can be in a definite state, even though its parts were not. . . . It's not a complicated idea but it's an idea that nobody would ever think of."</p>
        <p>Schrödinger called such states in German verschränkt, and in English entangled. In the context of multiple particles the formal criterion for entanglement is that an entangled state of multiple particles (an entangled multipartite state) cannot be represented as a product of states of single particles.</p>
        <p>The sort of "zero-sum game" mentioned earlier is complementary with regards to encoding information into relations-correlations versus individual properties: due to the scarcity and fixed amount of information which merely gets permuted during state evolution, one can either have total knowledge of the individual parts; with zero relational knowledge of the correlations and relations among the parts; or conversely one can have total knowledge of the correlation and relations among the parts; but then one learns nothing about the properties of the individual parts. Stated differently, any kind of mixture between the following two extremes can be realized for a ensemble of multiple particles or parts:</p>
        <p>(i) individuality: either the properties of the individual parts are totally determined; in this case the relations and correlations among the parts remain indeterminate; in probability theory one may say that the parts are independent [261, Sect. 45] (ii) entanglement: or the relations and correlations among the parts are totally determined; but then the properties of the individual parts remain indeterminate.</p>
        <p>For classical particles only the first, individual, case can be realized. The latter, entangled, case is a genuine quantum mechanical feature. Thereby, interaction entangles any formerly individual parts at the price of losing their individuality, and measurements on individual parts destroys entanglement and "enforces value-definiteness" of the individual constituent parts. Suppose one starts out with a factorable case. Then an entangled state is obtained by a unitary transformation of the factorable state. Its inverse transformation leads back from the entangled state to the factorable state; through a continuum of non-maximal entangled intermedium states. This may go back and forth -from individuality to entanglement and then back to individuality -an arbitrary number of times.</p>
        <p>In purely formal terms; that is, on the syntactic level, this can be quite well understood: a pure state of, say, k particles with n states per particle can, be written as</p>
        <p>and not</p>
        <p>In particular, this is only valid if</p>
        <p>For the sake of a concrete demonstration [368, Sect. 1.5], consider a general state in four-dimensional Hilbert space. It can be written as a vector in C 4 , which can be parameterized by</p>
        <p>and suppose (wrongly) (12.7) that all such states can be written in terms of a tensor product of two quasi-vectors in</p>
        <p>A comparison of the coordinates in (12.7) and (12.8) yields</p>
        <p>By taking the quotient of the two first and the two last equations, and by equating these quotients, one obtains</p>
        <p>, and thus α 1 α 4 = α 2 α 3 . (12.10)</p>
        <p>How can we imagine this? As in many cases, states in the Bell basis, and, in particular, the Bell state, serve as a sort of Rosetta Stone for an understanding of this quantum feature. The Bell state |Ψ -is a typical example of an entangled state; or, more generally, states in the Bell basis can be defined and, with |0 = 1, 0 and |1 = 0, 1 , encoded by</p>
        <p>(12.11) For instance, in the case of |Ψ -a comparison of coefficient yields</p>
        <p>and thus entanglement, since</p>
        <p>This shows that |Ψ -cannot be considered as a two particle product state. Indeed, the state can only be characterized by considering the relative properties of the two particles -in the case of |Ψ -they are associated with the statements [588]: "the quantum numbers (in this case "0" and "1") of the two particles are different in (at least) two orthogonal directions."</p>
        <p>The Bell basis symbolizing entanglement and non-individuality can, in an ad hoc manner, be generated from a non-entangled, individual state symbolized by elements of the Cartesian standard basis in 4-dimensional real space R 4</p>
        <p>(12.14) by arranging the coordinates (12.11) of the Bell basis as row or column vectors, thereby forming the respective unitary transformation</p>
        <p>Successive application of U and its inverse U transforms an individual, nonentangled state from the Cartesian basis back and forth into an entangled, nonindividual state from the Bell basis. For the sake of another demonstration, consider the following perfectly cyclic evolution which permutes all (non-)entangled states corresponding to the Cartesian and Bell bases:</p>
        <p>This evolution is facilitated by U of Eq. (12.15), as well as by the following additional unitary transformation [460]:</p>
        <p>(12.17)</p>
        <p>One of the ways thinking of this kind of "breathing in and out of individuality and entanglement" is in terms of sampling and scrambling information, as quoted from Chiao [251, p. 27] (reprinted in [350]): "Nothing has really been erased here, only scrambled!" Indeed, mere re-coding or "scrambling," and not erasure or creation of information, is tantamount to, and an expression and direct consequence of, the unitary evolution of the quantum state.</p>
        <p>So far, quantum theory lacks probabilities. These will be introduced and compared to classical probabilities next. Indeed, for the sake of appreciating the novel features of quantum probabilities and correlations, as well as the (joint) expectations of quantum observables, a short excursion into classical probability theory is useful.</p>
        <p>Already George Boole, although better known for his symbolic logic calculus of propositions aka Laws of Thought [66], pointed out that the probabilities of certain events, as well as their (joint) occurrence are subject to linear constraints [45-50, 66, 67, 163, 181-183, 221, 257, 258, 328, 421, 424, 524, 541-543]. A typical problem considered by Boole was the following [67, p. 229]: "Let p 1 , p 2 , . . . , p n represent the probabilities given in the data. As these will in general not be the probabilities of unconnected events, they will be subject to other conditions than that of being positive proper fractions, . . .. Those other conditions will, as will hereafter be shown, be capable of expression by equations or inequations reducible to the general form a 1 p 1 + a 2 p 2 + • • • + a n p n + a ≥ 0, a 1 , a 2 , . . . , a n , a being numerical constants which differ for the different conditions in question. These . . . may be termed the conditions of possible experience." Independently, Bell [40] derived some bounds on classical joint probabilities which relate to quantized systems insofar as they can be tested and falsified in the quantum regime by measuring subsets of compatible observables (possibly by Einstein-Podolsky-Rosen type [196] counterfactual inference) -one at a time -on different subensembles prepared in the same state. Thereby, in hindsight, it appears to be a bitter turn of history of thought that Bell, a staunch classical realist, who found wanting [41] previous attempts [552,554], created one of the most powerful theorems used against (local) hidden variables. The present form of the "Bell inequalities" is due to Wigner [572] (cf. Sakurai [439, pp. 241-243] and Pitowsky [397,Footnote 13]. Fine [215] later pointed out that deterministic hidden variables just amount to suitable joint probability functions.</p>
        <p>In referring to a later paper by Bell [42], Froissart [143,227] proposed a general constructive method to produce all "maximal" (in the sense of tightest) constraints on classical probabilities and correlations for arbitrary physical configurations. This method uses all conceivable types of classical correlated outcomes, represented as matrices (or higher dimensional objects) which are the vertices [227, p. 243] "of a polyhedron which is their convex hull. Another way of describing this convex polyhedron is to view it as an intersection of half-spaces, each one corresponding to a face. The points of the polyhedron thus satisfy as many inequations as there are faces. Computation of the face equations is straightforward but tedious." That is, certain "optimal" Bell-type inequalities can be interpreted as defining half-spaces ("below-above," "inside-outside") which represent the faces of a convex correlation polytope.</p>
        <p>Later Pitowsky pointed out that any Bell-type inequality can be interpreted as Boole's condition of possible experience [396][397][398][399][400]407]. Pitowsky does not quote Froissart but mentions [396, p. 1556] that he had been motivated by a (series of) paper(s) by Garg and Mermin [235] (who incidentally did not mention Froissart either) on Farkas' Lemma. Their concerns were linear constraints on pair distributions, derivable from the existence of higher-order distributions; constraints which turn out to be Bell-type inequalities; derivable as facets of convex correlation polytopes. The Garg and Mermin paper is important because it concentrates on the "inverse" problem: rather than finding high-order distributions from low-order ones, they consider the question of whether or not those high-order distributions could return random variables with first order distributions as marginals. One of the examples mentioned [235, p. 2] are "three dichotomic variables each of which assumes either the value 1 or -1 with equal probability, and all the pair distributions vanish unless the members of the pair have different values, then any third-order distribution would have to vanish unless all three variables had different values. There can therefore be no third-order distribution." (I mention this also because of the similarity with Specker's parable of three boxes [479,521].) A very similar question had also been pursued by Vorob'ev [556] and Kellerer [304,305], who inspired Klyachko [312], as neither one of the previous authors are mentioned. [To be fair, in the reference section of an unpublished previous paper [311] Klyachko mentions Pitowsky two times; one reference not being cited in the main text.]</p>
        <p>The gist of the classical strategy is to obtain all conceivable probabilities by a convex polytope method: any classical probability distribution can be written as a convex sum of all of the conceivable "extreme" cases. These "extreme" cases can be interpreted as classical truth assignments; or, equivalently, as two-valued states. A two-valued state is a function on the propositional structure of elementary observables, assigning any proposition the values "0" and "1" if they are (for a particular "extreme" case) "false" or "true," respectively. "Extreme" cases are subject to criteria defined later in Sect. 12.9.4. The first explicit use [502,506,511,521] (see Pykacz [423] for an early use of two-valued states) of the polytope method for deriving bounds using two-valued states on logics with intertwined contexts seems to have been for the pentagon logic, discussed in Sect. 12.9.8.3) and cat's cradle logic (also called "Käfer," the German word for "bug," by Specker), discussed in Sect. 12.9.8.4. More explicitly, suppose that there be as many, say, k, "weights" λ 1 , . . . , λ k as there are two-valued states (or "extreme" cases, or truth assignments, if you prefer this denominations). Then convexity demands that all of these weights are positive and sum up to one; that is, λ 1 , . . . , λ k ≥ 0, and</p>
        <p>Suppose further that for any particular, say, the ith, two-valued state (or the ith "extreme" case, or the ith truth assignment, if you prefer this denomination), all the, say, m, "relevant" terms -relevance here merely means that we want them to contribute to the linear bounds denoted by Boole as conditions of possible experience, as discussed in Sect. 12.9.6 -are "lumped" or combined together and identified as vector components of a vector |x i in an m-dimensional vector space R m ; that is,</p>
        <p>Note that any particular convex [see Eq. (12.18)] combination</p>
        <p>of the k weights λ 1 , . . . , λ k yields a valid -that is consistent, subject to the criteria defined later in Sect. 12.9.4 -classical probability distribution, characterized by the vector |w(λ 1 , . . . , λ k ) . These k vectors |x 1 , . . . , |x k can be identified with vertices or extreme points (which cannot be represented as convex combinations of other vertices or extreme points), associated with the k two-valued states (or "extreme" cases, or truth assignments). Let V = {|x 1 , . . . , |x k } be the set of all such vertices. For any such subset V (of vertices or extreme points) of R m , the convex hull is defined as the smallest convex set in R m containing V [230, Sect. 2.10, p. 6]. Based on its vertices a convex V-polytope can be defined as the subset of R m which is the convex hull of a finite set of vertices or extreme points</p>
        <p>A convex H-polytope can also be defined as the intersection of a finite set of half-spaces, that is, the solution set of a finite system of n linear inequalities:</p>
        <p>with the condition that the set of solutions is bounded, such that there is a constant c such that |x ≤ c holds for all |x ∈ P. A i are matrices and |b are vectors with real components, respectively. Due to the Minkoswki-Weyl "main" representation theorem [22,230,254,274,361,449,590] every V-polytope has a description by a finite set of inequalities. Conversely, every H-polytope is the convex hull of a finite set of points. Therefore the H-polytope representation in terms of inequalities as well as the V-polytope representation in terms of vertices, are equivalent, and the term convex polytope can be used for both and interchangeably. A k-dimensional convex polytope has a variety of faces which are again convex polytopes of various dimensions between 0 and k -1. In particular, the 0-dimensional faces are called vertices, the 1-dimensional faces are called edges, and the k -1-dimensional faces are called facets.</p>
        <p>The solution of the hull problem, or the convex hull computation, is the determination of the convex hull for a given finite set of k extreme points V = {|x 1 , . . . , |x k } in R m (the general hull problem would also tolerate points inside the convex polytope); in particular, its representation as the intersection of half-spaces defining the facets of this polytope -serving as criteria of what lies "inside" and "outside" of the polytope -or, more precisely, as a set of solutions to a minimal system of linear inequalities. As long as the polytope has a non-empty interior and is full-dimensional (with respect to the vector space into which it is imbedded) there are only inequalities; otherwise, if the polytope lies on a hyperplane one obtains also equations.</p>
        <p>For the sake of a familiar example, consider the regular 3-cube, which is the convex hull of the 8 vertices in R 3 of V = 0, 0, 0 , 0, 0, 1 , 0, 1, 0 , 1, 0, 0 , 0, 1, 1 , 1, 1, 0 , 1, 0, 1 , 1, 1, 1</p>
        <p>. The cube has 8 vertices, 12 edges, and 6 facets. The half-spaces defining the regular 3-cube can be written in terms of the 6 facet inequalities 0</p>
        <p>Finally the correlation polytope can be defined as the convex hull of all the vertices or extreme points |x 1 , . . . , |x k in V representing the (k per two-valued state) "relevant" terms evaluated for all the two-valued states (or "extreme" cases, or truth assignments); that is,</p>
        <p>The convex H-polytope -associated with the convex V-polytope in (12.23)which is the intersection of a finite number of half-spaces, can be identified with Boole's conditions of possible experience.</p>
        <p>A similar argument can be put forward for bounds on expectation values, as the expectations of dichotomic E ∈ {-1, +1}-observables can be considered as affine transformations of two-valued states v ∈ {0, 1}; that is, E = 2v -1. One might even imagine such bounds on arbitrary values of observables, as long as affine transformations are applied. Joint expectations from products of probabilities transform non-linearly, as, for instance</p>
        <p>So, given some bounds on (joint) expectations; these can be translated into bounds on (joint) probabilities by substituting 2v i -1 for expectations E i . The converse is also true: bounds on (joint) probabilities can be translated into bounds on (joint) expectations by</p>
        <p>This method of finding classical bounds must fail if, such as for Kochen-Specker configurations, there are no or "too few" (such that there exist two or more atoms which cannot be distinguished by any two-valued state) two-valued states. In this case one my ease the assumptions; in particular, abandon admissibility, arriving at what has been called non-contextual inequalities [92].</p>
        <p>Henceforth a context will be any Boolean (sub-)algebra of experimentally observable propositions. The terms block or classical mini-universe will be used synonymously.</p>
        <p>In classical physics there is only one context -and that is the entire set of observables. There exist models such as partition logics [184,506,511] -realizable by Wright's generalized urn model [578] or automaton logic [444][445][446]499], -which are still quasi-classical but have more than one, possibly intertwined, contexts. Two contexts are intertwined if they share one or more common elements. In what follows we shall only consider contexts which, if at all, intertwine at a single atomic proposition.</p>
        <p>For such configurations Greechie has proposed a kind of orthogonality diagram [249,300,523] in which 1. entire contexts (Boolean subalgebras, blocks) are drawn as smooth lines, such as straight (unbroken) lines, circles or ellipses; 2. the atomic propositions of the context are drawn as circles; and 3. contexts intertwining at a single atomic proposition are represented as nonsmoothly connected lines, broken at that proposition.</p>
        <p>In Hilbert space realizations, the straight lines or smooth curves depicting contexts represent orthogonal bases (or, equivalently, maximal observables, Boolean subalgebras or blocks), and points on these straight lines or smooth curves represent elements of these bases; that is, two points on the same straight line or smooth curve represent two orthogonal basis elements. From dimension three onwards, bases may intertwine [240] by possessing common elements.</p>
        <p>In what follows we shall use notions of "truth assignments" on elements of logics which carry different names for related concepts:</p>
        <p>1. The quantum logic community uses the term two-valued state; or, alternatively, valuation for a total function v on all elements of some logic holds for all orthonormal bases (contexts, blocks) of the logic based on H. 3. A dichotomic total function v : L → [0, 1] will be called strongly admissible if a. within every context C = {a i , i ∈ N}, a single atom a j is assigned the value one: v(a j ) = 1; and b. all other atoms in that context are assigned the value zero: v(a i = a j ) = 0.</p>
        <p>Physically this amounts to only one elementary proposition being true; the rest of them are false. (One may think of an array of mutually exclusively firing detectors.) c. Non-contextuality, stated explicitly]: The value of any observable, and, in particular, of an atom in which two contexts intertwine, does not depend on the context. It is context-independent.</p>
        <p>4. In order to cope with value indefiniteness (cf. Sect. 12.9.8.7), a weaker form of admissibility has been proposed [3][4][5][6] which is no total function but rather is a partial function which may remain undefined (indefinite) on some elements of L: A dichotomic partial function v : L → [0, 1] will be called admissible if the following two conditions hold for every context C of L:</p>
        <p>c. the value assignments of all other elements of the logic not covered by, if necessary, successive application of the admissibility rules, are undefined and thus the atom remains value indefinite.</p>
        <p>Unless otherwise mentioned (such as for contextual value assignments or admissibility discussed in Sect. 12.9.8.7) the quantum logical (I), Gleason type (II), strong admissibility (III) notions of two-valued states will be used. Such two valued states (probability measures) are interpretable as (pre-existing) truth assignments; they are sometimes also referred to as a Kochen-Specker value assignment [583].</p>
        <p>A caveat seems to be in order from the very beginning: in what follows correlation polytopes arise from classical (and quasi-classical) situations. The considerations are relevant for quantum mechanics only insofar as the quantum probabilities could violate classical bounds; that is, if the quantum tests violote those bounds by "lying outside" of the classical correlation polytope.</p>
        <p>There exist at least two good reasons to consider (correlation) polytopes for bounds on classical probabilities, correlations and expectation values:</p>
        <p>1. they represent a systematic way of enumerating the probability distributions and deriving constraints -Boole's conditions of possible experience -on them; 2. one can be sure that these constraints and bounds are optimal in the sense that they are guaranteed to yield inequalities which are best criteria for classicality.</p>
        <p>It is not evident to see why, with the methods by which they have been obtained, Bell's original inequality [41,42] or the Clauser-Horne-Shimony-Holt inequality [145] should be "optimal" at the time they were presented. Their derivation involve estimates which appear ad hoc; and it is not immediately obvious that bounds based on these estimates could not be improved. The correlation polytope method, on the other hand, offers a conceptually clear framework for a derivation of all classical bounds on higher-order distributions.</p>
        <p>Polytopes?</p>
        <p>What can enter as terms in such correlation polytopes? To quote Pitowsky [397, p. 38], "Consider n events A 1 , A 2 , . . . , A n , in a classical event space . . . Denote p i = probability(A i ), p i j = probability(A i ∩ A j ), and more generally p i</p>
        <p>We assume no particular relations among the events. Thus A 1 , . . . , A n are not necessarily distinct, they can be dependent or independent, disjoint or non-disjoint etc." However, although the events A 1 , . . . , A n may be in any relation to one another, one has to make sure that the respective probabilities, and, in particular, the extreme cases -the two-valued states interpretable as truth assignments -properly encode the logical or empirical relations among events. In particular, when it comes to an enumeration of cases, consistency must be retained. For example, suppose one considers the following three propositions: A 1 : "it rains in Vienna," A 3 : "it rains in Vienna or it rains in Auckland." It cannot be that A 2 is less likely than A 1 ; therefore, the two-valued states interpretable as truth assignments must obey p(A 2 ) ≥ p(A 1 ), and in particular, if A 1 is true, A 2 must be true as well. (It may happen though that A 1 is false while A 2 is true.) Also, mutually exclusive events cannot be true simultaneously.</p>
        <p>These admissibility and consistency requirements are considerably softened in the case of non-contextual inequalities [92], where subclassicality -the requirement that among a complete (maximal) set of mutually exclusiver observables only one is true and all others are false (equivalent to one important criterion for Gleason's frame function [240]) -is abandoned. To put it pointedly, in such scenarios, the simultaneous existence of inconsistent events such as A 1 : "it rains in Vienna," A 2 : "it does not rain in Vienna" are allowed; that is, p("it rains in Vienna") = p("it does not rain in Vienna") = 1. The reason for this rather desperate step is that, for Kochen-Specker type configurations, there are no classical truth assignments satisfying the classical admissibility rules; therefore the latter are abandoned. (With the admissibility rules goes the classical Kolmogorovian probability axioms even within classical Boolean subalgebras.)</p>
        <p>It is no coincidence that most calculations are limited -or rather limit themselves because there is no formal reasons to go to higher orders -to the joint probabilities or expectations of just two observables: there is no easy "workaround" of quantum complementarity. The Einstein-Podolsky-Rosen setup [196] offers one for just two complementary contexts at the price of counterfactuals, but there seems to be no generalization to three or more complementary contexts in sight [448].</p>
        <p>As pointed out earlier, Froissart and Pitowsky, among others such as Tsirelson, have sketched a very precise algorithmic framework for constructively finding all conditions of possible experience. In particular, Pitowsky's later method [397][398][399][400]407], with slight modifications for very general non-distributive propositional structures such as the pentagon logic [506,511,521], goes like this:</p>
        <p>1. define the terms which should enter the bounds; 2. a. if the bounds should be on the probabilities: evaluate all two-valued measures interpretable as truth assignments; b. if the bounds should be on the expectations: evaluate all value assignments of the observables; c. if (as for non-contextual inequalities) the bounds should be on some predefined quantities: evaluate all value definite pre-assigned quantities; 3. arrange these terms into vectors whose components are all evaluated for a fixed two-valued state, one state at a time; one vector per two-valued state (truth assignment), or (for expectations) per value assignments of the observables, or (for non-contextual inequalities) per value-assignment; 4. consider the set of all obtained vectors as vertices of a convex polytope; 5. solve the convex hull problem by computing the convex hull, thereby finding the smallest convex polytope containing all these vertices. The solution can be represented as the half-spaces (characterizing the facets of the polytope) formalized by (in)equalities -(in)equalities which can be identified with Boole's conditions of possible experience.</p>
        <p>Froissart [227] and Tsirelson [143] are not much different; they arrange joint probabilities for two random variables into matrices instead of "delineating" them as vectors; but this difference is notational only. We shall explicitly apply the method to various configurations next.</p>
        <p>In what follows we shall enumerate several (non-)trivial -that is, non-Boolean in the sense of pastings [249,300,376,420] of Boolean subalgebras. Suppose some points or vertices in R n are given. The convex hull problem of finding the smallest convex polytope containing all these points or vertices, given the latter, will be solved evaluated with Fukuda's cddlib package cddlib-094h [229] (using GMP [223]) implementing the double description method [22,23,231].</p>
        <p>The case of a single variable has two extreme cases: false ≡ 0 and true ≡ 1, resulting in the two vertices 0 as well as 1 , respectively. The corresponding hull problem yields a probability "below 0" as well as "above 1," respectively; thus solution this rather trivial hull problem yields 0 ≤ p 1 ≤ 1. For dichotomic expectation values ±1 a similar argument yields -1 ≤ E 1 ≤ 1.</p>
        <p>The next trivial case is just two dichotomic (two values) observables and their joint probability. The respective logic is generated by the pairs (overline indicates negation) a 1 a 2 , a 1 ā2 , ā1 a 2 , ā1 ā2 , representable by a single Boolean algebra 2 4 , whose atoms are these pairs: a 1 a 2 , a 1 ā2 , ā1 a 2 , ā1 ā2 . For single Boolean algebras with k atoms, there are k two-valued measures; in this case k = 4.</p>
        <p>For didactive purposes this case has been covered ad nauseam in Pitowsky's introductions [396][397][398][399][400]407]; so it is just mentioned without further discussion: take the probabilities two observables p 1 and p 2 , and a their joint variable p 12 and "bundle" them together into a vector p 1 , p 2 , p 1 ∧ p 2 ≡ p 12 = p 1 p 2 of threedimensional vector space. Then enumerate all four extreme cases -the two-valued states interpretable as truth assignments -involving two observables p 1 and p 2 , and a their joint variable p 12 very explicitly false-false-false, false-true-false, true-falsefalse, and true-true-true, or by numerical encoding, 0-0-0, 0, 1, 0, 1, 0, 0, and 1-1-1, yielding the four vectors</p>
        <p>Solution of the hull problem for the polytope</p>
        <p>yields the "inside-outside" inequalities of the half-spaces corresponding to the four facets of this polytope:</p>
        <p>For the expectation values of two dichotomic observables ±1 a similar argument yields</p>
        <p>Very similar calculations, taking into account three observables and their joint probabilities and expectations, yield and</p>
        <p>(12.31)</p>
        <p>The first non-trivial (in the sense that quantum probabilities and expectations violate the classical bounds) instance occurs for four observables in an Einstein-Podolski-Rosen type "explosion" setup [196], where n observables are measured on both sides, respectively. To obtain a feeling, Fig. 12.1a depicts the Greechie orthogonality diagram of the 2 particle 2 observables per particle situation. Figure 12.1b enumerates all two-valued states thereon.</p>
        <p>At this point it might be interesting to see how exactly the approach of Froissart and Tsirelson blends in [143,227]. The only difference to the Pitowsky method -which enumerates the (two particle) correlations and expectations as vector components -is that Froissart and later and Tsirelson arrange the two-particle correlations and expectations as matrix components; so both differ only by notation. For instance, Froissart explicitly mentions [227, pp. 242-243] 10 extremal configurations of the two-particle correlations, associated with 10 matrices p 13 = p 1 p 3 p 14 = p 1 p 4 p 23 = p 2 p 3 p 24 = p 2 p 4 (12.33) containing 0s and 1s (the indices "1, 2" and "3, 4" are associated with the two sides of the Einstein-Podolsky-Rosen "explosion"-type setup, respectively), arranged in Pitowsky's case as vector</p>
        <p>For probability correlations the number of different matrices or vectors is 10 (and not 16 as could be expected from the 16 two-valued measures), since, as enumerated in Table 12.1 some such measures yield identical results on the two-particle correlations; in particular, v 1 , v 2 , v 3 , v 4 , v 5 , v 9 , v 13 yield identical matrices (in the Froissart case) or vectors (in the Pitowsky case). The calculation for the facet inequalities for two observers and three measurement configurations per observer is straightforward and yields 684 inequalities [148,407,469]. If one considers (joint) expectations one arrives at novel ones which are not of the Clauser-Horne-Shimony-Holt type; for instance [469, p. 166, Eq. ( 4)],</p>
        <p>Table 12.1 The 16 two-valued states on the 2 particle two observables per particle configuration, as drawn in Fig. 12.1b. Two-particle correlations appear green. There are 10 different such configurations, painted in red # a 1 a 2 a 3 a 4 a 13 a 14 a 23 a 24 v 1 0 0 0 0 0 0 0 0 v 2 0 0 0 1 0 0 0 0 v 3 0 0 1 0 0 0 0 0 v 4 0 0 1 1 0 0 0 0 v 5 0 1 0 0 0 0 0 0 v 6 0 1 0 1 0 0 0 1 v 7 0 1 1 0 0 0 1 0 v 8 0 1 1 1 0 0 1 1 v 9 1 0 0 0 0 0 0 0 v 10 1 0 0 1 0 1 0 0 v 11 1 0 1 0 1 0 0 0 v 12 1 0 1 1 1 1 0 0 v 13 1 1 0 0 0 0 0 0 v 14 1 1 0</p>
        <p>As already mentioned earlier, these bounds on classical expectations [469] translate into bounds on classical probabilities [148,407] (and vice versa) if the affine transformations</p>
        <p>Here a word of warning is in order: if one only evaluates the vertices from the joint expectations (and not also the single particle expectations), one never arrives at the novel inequalities of the type listed in Eq. (12.35), but obtains 90 facet inequalities; among them 72 instances of the Clauser-Horne-Shimony-Holt inequality form, such as</p>
        <p>They can be combined to yield (see also Ref. [469, p. 166, Eq. ( 4)])</p>
        <p>(12.37)</p>
        <p>For the general case of n qubits, algebraic methods different than the hull problem for polytopes have been suggested in Refs. [404,443,567,594].</p>
        <p>In the following we shall present a series of logics whose contexts (representable by maximal observables, Boolean subalgebras, blocks, or orthogonal bases) are intertwined; but "not much:" by assumption and for convenience, contexts intertwine in only one element; it does not happen that two contexts are pasted [249,300,376,420] along two or more atoms. (They nevertheless might be totally identical.) Such intertwines -connecting contexts by pasting them together -can only occur from Hilbert space dimension three onwards, as contexts in lower-dimensional spaces cannot have the same element unless they are identical.</p>
        <p>In Sect. 12.9.8.3 we shall first study the "firefly case" with just two contexts intertwined in one atom; then, in Sect. 12.9.8.3, proceed to the pentagon configuration with five contexts intertwined cyclically, then, in Sect. 12.9.8.4, paste two such pentagon logics to form a cat's cradle (or, by another term, Specker's bug) logic; and finally, in Sect. 12.9.8.6, connect two Specker bugs to arrive at a logic which has a so "meagre" set of states that it can no longer separate two atoms. As pointed out already by Kochen and Specker [314, p. 70,] this is no longer imbeddable into some Boolean algebra. It thus cannot be represented by a partition logic; and thus has neither any generalized urn and finite automata models nor classical probabilities separating different events. The case of logics allowing no two valued states will be covered consecutively.</p>
        <p>Cohen presented [147, pp. 21-22] a classical realization of the first logic with just two contexts and one intertwining atom: a firefly in a box, observed from two sides of this box which are divided into two windows; assuming the possibility that sometimes the firefly does not shine at all. This firefly logic, which is sometimes also denoted by L 12 because it has 12 elements (in a Hasse diagram) and 5 atoms, with the contexts defined by {a 1 , a 2 , a 5 } and {a 3 , a 4 , a 5 } is depicted in Fig. 12.2.</p>
        <p>The five two-valued states on the firefly logic are enumerated in Table 12.2 and depicted in Fig. 12.3.</p>
        <p>These two-valued states induce [506] a partition logic realization [184, 511] {{{1}, {2, 3}, {4, 5}}, {{1}, {2, 5}, {3, 4}}} which in turn induce all classical probability</p>
        <p>Fig. 12.3 Two-valued measures on the firefly logic. Filled circles indicate the value "1" interpretable as "true"</p>
        <p>Fig. 12.4 Classical probabilities on the firefly logic with two contexts, as induced by the two-valued states, and subject to</p>
        <p>distributions, as depicted in Fig. 12.4. No representation in R 3 is given here; but this is straightforward (just two orthogonal tripods with one identical leg), or can be read off from logics containing more such intertwined fireflies; such as in Fig. 12.6.</p>
        <p>Admissibility of two-valued states imposes conditions and restrictions on the twovalued states already for a single context (Boolean subalgebra): if one atom is assigned the value 1, all other atoms have to have value assignment(s) 0. This is even more so for intertwining contexts. For the sake of an example, consider two firefly logics pasted along an entire block, as depicted in Fig. 12.5. For such a logic we can state a "true-and-true implies true" rule: if the two-valued measure at the "outer extremities" is 1, then it must be 1 at its center atom.</p>
        <p>We shall pursue this path of ever increasing restrictions through construction of pasted; that is, intertwined, contexts. This ultimately yields to non-classical logics which have no separating sets of two-valued states; and even, as in Kochen-Specker type configurations, to logics which do not allow for any two valued state interpretable as preassigned truth assignments.</p>
        <p>Let us proceed by pasting more firefly logics together in "closed circles." The next possibilities -two firefly logics forming either a triangle or a square Greechie orthogonal diagram -have no realization in three dimensional Hilbert space. The</p>
        <p>next diagram realizably is obtained by a pasting of three firefly logics. It is the pentagon logic (also denoted as orthomodular house [300, p. 46, Fig. 4.4] and discussed in Ref. [50]; see also Birkhoff's distributivity criterion [57, p. 90, Theorem 33], stating that, in particular, if some lattice contains a pentagon as sublattice, then it is not distributive [60]) which is subject to an old debate on "exotic" probability measures [577]. In terms of Greechie orthogonality diagrams there are two equivalent representations of the pentagon logic: one as a pentagon, as depicted [521] in Fig. 12.6 and one as a pentagram; thereby the indices of the intertwining edges (the non-intertwining ones follow suit) are permuted as follows: 1 → 1, 9 → 5, 7 → 9, 5 → 3, 3 → 7. From a Greechie orthogonality point of view the pentagon representation is preferable over the pentagram, because the latter, although appearing more "magic," might suggest the illusion that there are more intertwining contexts and observables as there actually are.</p>
        <p>As pointed out by Wright [577, p 268] the pentagon has 11 "ordinary" twovalued states v 1 , . . . , v 11 , and one "exotic" dispersionless state v e , which was shown by Wright to have neither a classical nor a quantum interpretation; all defined on the 10 atoms a 1 , . . . , a 10 . They are enumerated in Table 12.3. and depicted in Fig. 12.7.</p>
        <p>These two-valued states directly translate into the classical probabilities depicted in Fig. 12.8.</p>
        <p>The pentagon logic has quasi-classical realizations in terms of partition logics [184,506,511], such as generalized urn models [577,578] or automaton logics [444][445][446]499]. An early realization in terms of three-dimensional (quantum) Hilbert space can, for instance, be found in Ref. [523, pp. 5392-5393]; other such parametrizations are discussed in Refs. [24,85,86,312].</p>
        <p>The full hull problem, including all joint expectations of dichotomic ±1 observables yields 64 inequalities enumerated in the supplementary material; among them The pasting of two pentagon logics results in ever tighter conditions for two-valued measures and thus truth value assignments: consider the Greechie orthogonality diagram of a logic drawn in Fig. 12.9. Specker [481] called this the "Käfer" (bug) Logic because of the similar shape with a bug. It has been introduced in 1963( 5) by Kochen and Specker [313, Fig. 1, p. 182]; and subsequently used as a subset of the diagrams Γ 1 , Γ 2 and Γ 3 demonstrating the existence of quantum propositional structures with the "true implies true" property (cf. Sect. 12.9.8.5), the non-existence of any two-valued state (cf. Sect. 12.9.8.7), and the existence of a non-separating set of two-valued states (cf. Sect. 12.9.8.6), respectively [314].</p>
        <p>Pitowsky called it (part of [429]) "cat's cradle" [403,405] (see also Refs. for early discussions). A partition logic, as well as a Hilbert space realization can be found in Refs. [511,523]. There are 14 two-valued states which are listed in Table 12.4.</p>
        <p>As already Pták and Pulmannová [420, p. 39, Fig. 2.4.6] as well as Pitowsky [403,405] have pointed out, the reduction of some probabilities of atoms at intertwined contexts yields [521, p. 285, Eq. (11.2)]</p>
        <p>A better approximation comes from the explicit parameterization of the classical probabilities on the atoms a 1 and a 7 , derivable from all the mutually disjoined twovalued states which do not vanish on those atoms, as depicted in Fig. 12.10: p 1 = λ 1 + λ 2 + λ 3 , and p 7 = λ 7 + λ 10 + λ 13 . Because of additivity the 14 positive weights λ 1 , . . . , λ 14 ≥ 0 must add up to 1; that is, 14</p>
        <p>Fig. 12.10 Classical probabilities on the Specker bug (cat's cradle) logic;</p>
        <p>. . , λ 14 ≤ 1, taken from Ref. [521]. The two-valued states i = 1, . . . , 14 can be identified by taking λ j = δ i, j for all j = 1, . . . 14</p>
        <p>For two-valued measures this yields the "1-0" or "true implies false" rule [515]: if a 1 is true, then a 7 must be false. For the sake of another proof by contradiction, suppose a 1 as well as a 7 were both true. This would (by the admissibility rules) imply a 3 , a 5 , a 9 , a 11 to be false, which in turn would imply both a 4 as well as a 10 , which have to be true in one and the same context -a clear violation of the admissibility rules stating that within a single context there can only be atom which is true. This property, which has already been exploited by Kochen and Specker [314, Γ 1 ] to construct both a logic with a non-separating, as well as one with a non-existent set of two valued states. These former case will be discussed in the next section. For the time being, instead of drawing all two valued states separately, Fig. 12.10 enumerates the classical probabilities on the Specker bug (cat's cradle) logic.</p>
        <p>The hull problem yields 23 facet inequalities; one of them relating p 1 to p 7 : p 1 + p 2 + p 7 + p 6 ≥ 1+ p 4 , which is satisfied, since, by subadditivity, p 1 + p 2 = 1p 3 , p 7 + p 6 = 1p 5 , and p 4 = 1p 5p 3 . This is a good example of a situation in which considering just Boole-Bell type inequalities do not immediately reveal important aspects of the classical probabilities on such logics.</p>
        <p>A restricted hull calculation for the joint expectations on the six edges of the Greechie orthogonality diagram yields 18 inequalities; among them</p>
        <p>(12.43)</p>
        <p>A tightened "true implies 3-times-false" logic depicted in Fig. 12.11 has been introduced by Yu and Oh [584]. As can be derived from admissibility in a straightforward manner, the set of 24 two-valued states [536] enforces at most one of the four atoms h 0 , h 1 , h 2 , h 3 to be 1. Therefore, classically p h 0 + p h 1 + p h 2 + p h 3 ≤ 1. This can also be explicitly demonstrated by noticing that, from the 24 two-valued states, exactly 3 acquire the value 1 on each one of the four atoms h 0 , h 1 , h 2 , and h 3 ; The set of two-valued states enforces at most one of the four atoms h 0 , h 1 , h 2 , h 3 to be 1. The logic has a (quantum) realization in R 3 consisting of the 25 projections; associated with the one dimensional subspaces spanned by the 13 vectors from the origin (0, 0, 0) to</p>
        <p>, respectively [584] also the respective two-valued states are different for these four different atoms h 0 , h 1 , h 2 , and h 3 . More explicitly, suppose the set of two-valued states is enumerated in such a way that the respective probabilities on the atoms h 0 , h 1 , h 2 , and h 3 are p h 0 = λ 1 +λ 2 +λ 3 , p h 1 = λ 4 +λ 5 +λ 6 , p h 2 = λ 7 +λ 8 +λ 9 , and p h 3 = λ 10 +λ 11 +λ 12 . Because of additivity the 24 positive weights λ 1 , . . . , λ 24 ≥ 0 must add up to 1; that is, 24 i=1 λ i = 1. Therefore [compare with Eq. (12.42)],</p>
        <p>Tkadlec has noted [536] that Fig. 12.11 contains 3 Specker bug subdiagrams per atom h i , thereby rendering the "true implies 3-times-false" property. For instance, for h 1 the three Specker bugs are formed by the three sets of contexts (missing non-interwining atoms should be added)</p>
        <p>A small extension of the Specker bug logic by two contexts extending from a 1 and a 7 , both intertwining at a point c renders a logic which facilitates that, whenever a 1 is true, so must be an atom b 1 , which is element in the context {a 7 , c, b 1 }, as depicted in Fig. 12.12. The reduction of some probabilities of atoms at intertwined contexts yields (q 1 , q 7 are the probabilities on b 1 , b 7 , respectively), additionally to Eq. (12.41),</p>
        <p>which, as can be derived also explicitly by taking into account admissibility, implies that, for all the 112 two-valued states, if p 1 = 1, then [from Eq. (12.41)] p 7 = 0, and q 1 = 1 as well as q 7 = 1q 1 = 0. Besides the quantum mechanical realization of this logic in terms of propositions identified with projection operators corresponding to vectors in three-dimensional Hilbert space Tkadlec and this author [523, p. 5387, Fig. 4] (see also Tkadlec [533,p. 206,Fig. 1]) have given an explicit collection of such vectors. As Tkadlec has observed (cf. Ref. [523, p. 5390], and Ref. [535, p.]), the original realization suggested by Kochen and Specker [314] appears to be a little bit "buggy" as they did not use the right angle between a 1 and a 7 , but this could be rectified. Notice that, if a second Specker bug logic is placed along b 1 and b 7 , just as in the Kochen-Specker Γ 3 logic [314, p. 70], this imposes an additional "true implies false" condition; together with the "true implies false" condition of the first logic this implies the fact that a 1 and a 7 can no longer be separated by some two-valued state: whenever one is true, the other one must be true as well, and vice versa. This Kochen-Specker logic Γ 3 will be discussed in the next Sect. 12.9.8.6.</p>
        <p>Notice further that if we manage to iterate this process in such a manner that, with every ith iteration we place another Kochen-Specker Γ 3 logic along b i , while at the same time increasing the angle between b i and b 1 , then eventually we shall arrive at a situation in which b 1 and b i are part of a context (in terms of Hilbert space: they correspond to orthogonal vectors). But admissibility disallows two-valued measures with more than one, and in particular, two "true" atoms within a single block. As a consequence, if such a configuration is realizable (say, in 3-dimensional Hilbert space), then it cannot have any two-valued state satisfying the admissibility criteria. This is the Kochen-Specker theorem, as exposed in the Kochen-Specker Γ 3 logic [314, p. 69], which will be discussed in Sect. 12.9.8.7.</p>
        <p>As we are heading toward logics with less and less "rich" set of two-valued states we are approaching a logic depicted in Fig. 12.13 which is a combination of two Specker bug logics linked by two external contexts. It is the Γ 3 -configuration of Kochen-Specker [314, p. 70] with a set of two-valued states which is no longer separating: In this case one obtains the "one-one" and "zero-zero rules" [515], stating that a 1 occurs if and only if b 1 occurs (likewise, a 7 occurs if and only if b 7 occurs): Suppose v is a two-valued state on the Γ 3 -configuration of Kochen-Specker. Whenever v(a 1 ) = 1, then v(c) = 0 because it is in the same context {a 1 , c, b 7 } as a 1 . Furthermore, because of Eq. (12.41), whenever v(a 1 ) = 1, then v(a 7 ) = 0. Because b 1 is in the same context {a 7 , c, b 1 } as a 7 and c, because of admissibility, v(b 1 ) = 1. Conversely, by symmetry, whenever v(b 1 ) = 1, so must be v(a 1 ) = 1. Therefore it can never happen that either one of the two atoms a 1 and b 1 have different dichotomic values. (Eq. 12.46 is compatible with these value assignments.) The same is true for the pair of atoms a 7 and b 7 .</p>
        <p>Note that one needs two Specker bug logics tied together (at their "true implies false" extremities) to obtain non-separability; just extending one to the Kochen-Specker Γ 1 logic [314, p. 68] of Fig. 12.12 discussed earlier to obtain "true implies true" would be insufficient. Because in this case a consistent two-valued state exists for which v(b 1 ) = v(b 7 ) = 1 and v(a 1 ) = v(a 7 ) = 0, thereby separating a 1 from b 1 , and vice versa. A second Specker bug logic is needed to eliminate this case; in particular, v(b 1 ) = v(b 7 ) = 1. The logic has a (quantum) realization in R 3 consisting of the 27 projections associated with the one dimensional subspaces spanned by the vectors from the origin (0, 0, 0) to the 13 points mentioned in Fig. 12.9, the 3 points mentioned in Fig. 12.12, as well as Besides the quantum mechanical realization of this logic in terms of propositions which are projection operators corresponding to vectors in three-dimensional Hilbert space suggested by Kochen and Specker [314], Tkadlec has given [533, p. 206, Fig. 1] an explicit collection of such vectors (see also the proof of Proposition 7.2 in Ref. [523, p. 5392]).</p>
        <p>The "1-1" or "true implies true" rule can be taken as an operational criterion for quantization: Suppose that one prepares a system to be in a pure state corresponding to a 1 , such that the preparation ensures that v(a 1 ) = 1. If the system is then measured along b 1 , and the proposition that the system is in state b 1 is found to be not true, meaning that v(b 1 ) = 1 (the respective detector does not click), then one has established that the system is not performing classically, because classically the set of two-valued states requires non-separability; that is, v(a 1 ) = v(b 1 ) = 1. With the Tkadlec directions taken from Figs. 12.9 and 12.12,</p>
        <p>1, 0 so that the probability to find a quantized system prepared along |a 1 and measured along |b 2]. More recently Hardy [70,264,265] as well as Cabello and García-Alcaine and others [24,90,95,96,99,138] discussed such scenarios. These criteria for non-classicality are benchmarks aside from the Boole-Bell type polytope method, and also different from the full Kochen-Specker theorem.</p>
        <p>As every algebra imbeddable in a Boolean algebra must have a separating set of two valued states, this logic is no longer "classical" in the sense of "homomorphically (structure-preserving) imbeddable." Nevertheless, two-valued states can still exist. It is just that these states can no longer differentiate between the pairs of atoms (a 1 , b 1 ) as well as (a 7 , b 7 ). Partition logics and their generalized urn or finite automata models fail to reproduce two linked Specker bug logics resulting in a Kochen-Specker Γ 3 logic even at this stage. Of course, the situation will become more dramatic with the non-existence of any kind of two-valued state (interpretable as truth assignment) on certain logics associate with quantum propositions.</p>
        <p>Complementarity and non-distributivity is not enough to characterize logics which do not have a quasi-classical (partition logical, set theoretical) interpretation. While in a certain, graph coloring sense the "richness/scarcity" and the "number" of twovalued homomorphisms" yields insights into the old problem of the structural property [152] by separating quasi-classical from quantum logics, the problem of finding smaller, maybe minimal, subsets of graphs with a non-separating set of two-valued states still remains an open challenge.</p>
        <p>The "true implies true" rule is associated with chromatic separability; in particular, with the impossibility to separate two atoms a 7 and b 7 with less than four colors. A proof is presented in Fig. 12.14. That chromatic separability on the unit sphere requires 4 colors is implicit in Refs. [245,269]. Fig. 12.14 Proof (by contradiction) that chromatic separability of two linked Specker bug (cat's cradle) logics Γ 3 cannot be achieved with three colors. In particular, a 7 and b 7 cannot be separated, as this would result in the depicted inconsistent coloring: suppose a red/green/blue coloring with chromatic admissibility ("all three colors occur only once per context or block or Boolean subalgebra") is possible. Then, if a 7 is colored red and b 7 is colored green, c must be colored blue. Therefore, a 1 must be colored red. Therefore, a 4 as well as a 10 must be colored red (similar for green on the second Specker bug), contradicting admissibility</p>
        <p>Gleason's theorem [240] was a response to Mackey's problem to "determine all measures on the closed subspaces of a Hilbert space" contained in a review [351] of Birkhoff and von Neumann's centennial paper [62] on the logic of quantum mechanics. Starting from von Neumann's formalization of quantum mechanics [552,554], the quantum mechanical probabilities and expectations (aka the Born rule) are essentially derived from (sub)additivity among the quantum context; that is, from subclassicality: within any context (Boolean subalgebra, block, maximal observable, orthonormal base) the quantum probabilities sum up to 1.</p>
        <p>Gleason's finding caused ripples in the community, at least of those who cared and coped with it [41,151,180,301,314,401,434,591]. (I recall having an argument with Van Lambalgen around 1983, who could not believe that anyone in the larger quantum community had not heard of Gleason's theorem. As we approached an elevator at Vienna University of Technology's Freihaus building we realized there was also one very prominent member of Vienna experimental community entering the cabin. I suggested to stage an example by asking; and voila. . .) With the possible exception of Specker who did not explicitly refer to the Gleason's theorem in independently announcing that two-valued states on quantum logics cannot exist [479] -he must have made up his mind from other arguments and preferred to discuss scholastic philosophy; at that time the Swiss may have had their own biotope -Gleason's theorem directly implies the absence of two-valued states. Indeed, at least for finite dimensions [11,12], as Zierler and Schlessinger [591, p. 259, Example 3.2] (even before publication of Bell's review [41]) noted, "it should also be mentioned that, in fact, the non-existence of two-valued states is an elementary geometric fact contained quite explicitly in [ In what follows we shall consider Hilbert spaces of dimension n = 3 and higher. Suppose that the quantum system is prepared to be in a pure state associated with the unit vector |x , or the projection operator |x x|.</p>
        <p>As all self-adjoint operators have a spectral decomposition [260, Sect. 79], and the scalar product is (anti)linear in its arguments, let us, instead of T, only consider one-dimensional orthogonal projection operators E 2 i = E i = |y i y i | (formed by the unit vector |y i which are elements of an orthonormal basis {|y 1 , . . . , |y n }) occurring in the spectral sum of T = n≥3 i=1 λ i E i , with I n = n≥3 i=1 E i . Thus if T is restricted to some one-dimensional projection operator E = |y y| along |y , then Gleason's main theorem states that any frame function reduces to the absolute square of the scalar product; and in real Hilbert space to the square of the angle between those vectors spanning the linear subspaces corresponding to the two projectors involved; that is (note that E is self-adjoint),</p>
        <p>Hence, unless a configuration of contexts is not of the star-shaped Greechie orthogonality diagram form -meaning that they all share one common atom; and, in terms of geometry, meaning that all orthonormal bases share a common vector -and the two-valued state has value 1 on its centre, as depicted in Fig. 12.15, there is no way how any two contexts could have a two-valued assignment; even if one context has one: it is just not possible by the continuous, cos 2 -form of the quantum probabilities. That is (at least in this author's believe) the watered down version of the remark of Zierler and Schlessinger [591, p. 259, Example 3.2].</p>
        <p>When it comes to the absence of a global two-valued state on quantum logics corresponding to Hilbert spaces of dimension three and higher -where contexts or blocks can be intertwined or pasted [376] to form chains -Kochen and Specker [314] pursued a very concrete, "constructive" (in the sense of finitary mathematical objects Fig. 12. 15 Greechie diagram of a star shaped configuration with a variety of contexts, all intertwined in a single "central" atom; with overlaid two-valued state (bold black filled circle) which is one on the centre atom and zero everywhere else (see also Refs. [3,5,6]) but not in the sense of physical operationalizability [79]) strategy: they presented finite logics realizable by vectors (from the origin to the unit sphere) spanning onedimensional subspaces, equivalent to observable propositions, which allowed for lesser and lesser two-valued state properties. For the reason of non-imbedability is already enough to consider two linked Specker bugs logics Γ 3 [314, p. 70], as discussed in Sect. 12.9.8.6.</p>
        <p>Kochen and Specker went further and presented a proof by contradiction of the non-existence of two-valued states on a finite number of propositions, based on their Γ 1 "true implies true" logic [314, p. 68] discussed in Fig. 12.12, iterating them until they reached a complete contradiction in their Γ 2 logic [314, p. 69]. As has been pointed out earlier, their representation as points of the sphere is a little bit "buggy" (as could be expected from the formation of so many bugs): as Tkadlec has observed, Kochen-Specker diagram Γ 2 it is not a one-to-one representation of the logic, because some different points at the diagram represent the same element of corresponding orthomodular poset (cf. Ref. [523, p. 5390], and Ref. [535, p.]).</p>
        <p>The early 1990s saw an ongoing flurry of papers recasting the Kochen-Specker proof with ever smaller numbers of, or more symmetric, configurations of observables (see Refs. [17, 83, 96, 97, 112, 307, 340, 364, 385, 386, 390, 391, 408, 472, 523, 533-535, 557, 558, 583, 593] for an incomplete list). Arguably the most compact such logic is one in four-dimensional space suggested by Cabello, Estebaranz and García-Alcaine [91,96,385]. It consists of 9 contexts, with each of the 18 atoms tightly intertwined in two contexts. Its Greechie orthogonality diagram is drawn in Fig. 12. 16.</p>
        <p>In a parity proof by contradiction consider the particular subset of real fourdimensional Hilbert space with a "parity property," consisting of 18 atoms a 1 , . . . , a 18 in 9 contexts, as depicted in Fig. 12.16. Note that, on the one hand, each atom/point/ vector/projector belongs to exactly two -that is, an even number of -contexts; that is, it is biconnected. Therefore, any enumeration of all the contexts occurring in the graph depicted in Fig. 12.16 would contain an even number of 1s assigned. Because, due to non-contextuality and biconnectivity, any atom a with v(a) = 1 along one Fig. 12. 16 The most compact way of deriving the Kochen-Specker theorem in four dimensions has been given by Cabello, Estebaranz and García-Alcaine [96]. The configuration consists of 18 biconnected (two contexts intertwine per atom) atoms a 1 , . . . , a 18 in 9 contexts. It has a (quantum) realization in R 4 consisting of the 18 projections associated with the one dimensional subspaces spanned by the vectors from the origin (0, 0, 0, 0) to a 1 = (0, 0, 1, -1) , a 2 = (1, -1, 0, 0) ,</p>
        <p>, a 6 = (1, 0, -1, 0) , a 7 = (0, 1, 0, -1) , a 8 = (1, 0, 1, 0) , a 9 = (1, 1, -1, 1) , a 10 = (-1, 1, 1, 1) , a 11 = (1, 1, 1, -1) , a 12 = (1, 0, 0, 1) , a 13 = (0, 1, -1, 0) , a 14 = (0, 1, 1, 0) , a 15 = (0, 0, 0, 1) , a 16 = (1, 0, 0, 0) , a 17 = (0, 1, 0, 0) , a 18 = (0, 0, 1, 1) , respectively [92, Fig. 1] (for alternative realizations see Refs. [91,92]) context must have the same value 1 along the second context which is intertwined with the first one -to the values 1 appear in pairs. Alas, on the other hand, in such an enumeration there are nine -that is, an odd number of -contexts. Hence, in order to obey the quantum predictions, any twovalued state (interpretable as truth assignment) would need to have an odd number of 1s -exactly one for each context. Therefore, there cannot exist any two-valued state on Kochen-Specker type graphs with the "parity property."</p>
        <p>More concretely, note that, within each one of those 9 contexts, the sum of any state on the atoms of that context must add up to 1. That is, due to additivity (12.24) and (12.25) one obtains a system of 9 equations Because v(a i ) ∈ {0, 1} the sum in (12.48) must add up to some natural number M. Therefore, Eq. (12.48) is impossible to solve in the domain of natural numbers, as on the left and right hand sides there appear even (2M) and odd (9) numbers, respectively.</p>
        <p>Of course, one could also prove the nonexistence of any two-valued state (interpretable as truth assignment) by exhaustive attempts (possibly exploiting symmetries) to assign values 0s and 1s to the atoms/points/vectors/projectors occurring in the graph in such a way that both the quantum predictions as well as context independence is satisfied. This latter method needs to be applied in cases with Kochen-Specker type diagrams without the "parity property;" such as in the original Kochen-Specker proof [314]. (However, admissibility (IV) is too weak for a proof of this type, as it allows also a third, value indefinite, state, which spoils the arguments [6].)</p>
        <p>This result, as well as the original Kochen-Specker theorem, is state independent insofar as it applies to an arbitrary quantum state. One could reduce the size of the proof by assuming a particular state. Such proofs are called state-specific or state dependent. By following Cabello, Estebaranz and García-Alcaine [96, Eqs. ( 10)-( 19), p. 185] their state independent proof utilizing the logic depicted in Fig. 12.16 can be transferred to a state-specific proof as follows: suppose that the quantum (or quanta, depending upon the physical realization) is prepared in the state v(a 1 ) = 1, (12.49) so that any two-valued state must obey the admissibility rules 17 Greechie orthogonality diagram of a state-specific proof of the Kochen-Specker theorem based on the assumption that the physical system is in state a 1 , such that v(a 1 ) = 1. The additivity and admissibility constraints (12.51) represent different "reduced" (or "truncated") contexts, because all states v(a</p>
        <p>The additivity relations (12.47) reduce to seven equations (two equations encoding contexts a and f are satisfied trivially)</p>
        <p>(12.51)</p>
        <p>The configuration is depicted in Fig. 12.17. As all atoms remain to be biconnected and there are 7, that is, an odd number, of equations, value indefiniteness can be proven by a similar parity argument as before. One could argue that the "primed" contexts in (12.51) are not complete because those contexts are "truncated." However, every completion would result in vectors orthogonal to a 1 ; and therefore their values must again be zero.</p>
        <p>Graph coloring allows another view on value (in)definiteness. The chromatic number of a graph is defined as the least number of colors needed in any total coloring of a graph; with the constraint that two adjacent vertices have distinct colors.</p>
        <p>Suppose that we are interested in the chromatic number of graphs associated with both (i) the real and (ii) the rational three-dimensional unit sphere.</p>
        <p>More generally, we can consider n-dimensional unit spheres with the same adjacency property defined by orthogonality. An orthonormal basis will be called context (block, maximal observable, Boolean subalgebra), or, in this particular area, a n-clique. Note that for any such graphs involving n-cliques the chromatic number of this graph is at least be n (because the chromatic number of a single n-clique or context is n).</p>
        <p>Thereby vertices of the graph are identified with points on the three-dimensional unit sphere; with adjacency defined by orthogonality; that is, two vertices of the graph are adjacent if and only if the unit vectors from the origin to the respective two points are orthogonal.</p>
        <p>The connection to quantum logic is this: any context (block, maximal observable, Boolean subalgebra, orthonormal basis) can be represented by a triple of points on the sphere such that any two unit vectors from the origin to two distinct points of that triple of points are orthogonal. Thus graph adjacency in logical terms indicates "belonging to some common context (block, maximal observable, Boolean subalgebra, orthonormal basis)."</p>
        <p>In three dimensions, if the chromatic number of graphs is four or higher, there does not globally exist any consistent coloring obeying the rule that adjacent vertices (orthogonal vectors) must have different colors: if one allows only three different colors, then somewhere in that graph of chromatic number higher than three, adjacent vertices must have the same colors (or else the chromatic number would be three or lower).</p>
        <p>By a similar argument, non-separability of two-valued states -such as encountered in Sect. 12.9.8.6 with the Γ 3 -configuration of Kochen-Specker [314, p. 70] -translates into non-differentiability by colorings with colors less or equal to the number of atoms in a block (cf. Fig. 12.14).</p>
        <p>Godsil and Zaks [245,269] proved the following results:</p>
        <p>1. the chromatic number of the graph based on points of real-valued unit sphere is four [245, Lemma 1.1]. 2. he chromatic number of rational points on the unit sphere</p>
        <p>We shall concentrate on (i) and discuss (ii) later. As has been pointed out by Godsil in an email conversation from March 13, 2016 [244], "the fact that the chromatic number of the unit sphere in R 3 is four is a consequence of Gleason's theorem, from which the Kochen-Specker theorem follows by compactness. Gleason's result implies that there is no subset of the sphere that contains exactly one point from each orthonormal basis."</p>
        <p>Indeed, any coloring can be mapped onto a two-valued state by identifying a single color with "1" and all other colors with "0." By reduction, all propositions on two-valued states translate into statements about graph coloring. In particular, if the chromatic number of any logical structure representable as graph consisting of n-atomic contexts (blocks, maximal observables with n outcomes, Boolean subalgebras 2 n , orthonormal bases with n elements) -for instance, as Greechie orthogonality diagram of quantum logics -is larger than n, then there cannot be any globally consistent two-valued state (truth value assignment) obeying adjacency (aka admissibility). Likewise, if no two-valued states on a logic which is a pasting of n-atomic contexts exist, then, by reduction, no global consistent coloring with n different colors exists. Therefore, the Kochen-Specker theorem proves that the chromatic number of the graph corresponding to the unit sphere with adjacency defined as orthogonality must be higher than three.</p>
        <p>Based on Godsil and Zaks finding that the chromatic number of rational points on the unit sphere S 3 ∩ Q 3 is three [245, Lemma 1.2] -thereby constructing a twovalued measure on the rational unit sphere by identifying one color with "1" and the two remaining colors with "0" -there exist "exotic" options to circumvent Kochen-Specker type constructions which have been quite aggressively (Cabello has referred to this as the second contextuality war [94]) marketed by allegedly "nullifying" [369] the respective theorems under the umbrella of "finite precision measurements" [32,75,76,146,306,366]: the support of vectors spanning the one-dimensional subspaces associated with atomic propositions could be "diluted" yet dense, so much so that the intertwines of contexts (blocks, maximal observables, Boolean subalgebras, orthonormal bases) break up; and the contexts themselves become "free and isolated." Under such circumstances the logics decay into horizontal sums; and the Greechie orthogonality diagrams are just disconnected stacks of previously intertwined contexts. As can be expected, proofs of Gleason-or Kochen-Specker-type theorems do no longer exist, as the necessary intertwines are missing.</p>
        <p>The "nullification" claim and subsequent ones triggered a lot of papers, some cited in [32]; mostly critical -of course, not to the results of Godsil and Zaks's finding (ii); how could they? -but to their physical applicability. Peres even wrote a parody by arguing that "finite precision measurement nullifies Euclid's postulates" [392], so that "nullification" of the Kochen-Specker theorem might have to be our least concern.</p>
        <p>Maybe one could, with all due respect, speak of "extensions" of the Kochen-Specker theorem by looking at situations in which a system is prepared in a state |x x| along direction |x and measured along a non-orthogonal, non-collinear projection |y y| along direction |y . Those extensions yield what may be called [286,401] indeterminacy. Indeterminacy may be just another word for contextuality; but, as has been suggested by the realist Bell, the latter term implicitly implies that there "is something (rather than nothing) out there," some "pre-existing observable" which, however, needs to depend on the context of the measurement. To avoid such implicit assumption we shall henceforth use indeterminacy rather than contextuality.</p>
        <p>Pitowsky's logical indeterminacy principle [401,Theorem 6,p. 226] states that, given two linearly independent non-orthogonal unit vectors |x and |y in R</p>
        <p>That is, if a system of three mutually exclusive outcomes (such as the spin of a spin-1 particle in a particular direction) is prepared in a definite state |x corresponding to v(|x ) = 1, then the state v(|y ) along some direction |y which is neither collinear nor orthogonal to |x cannot be (pre-)determined, because, by an argument via some set of intertwined rays Γ (|x , |y ), both cases would lead to a complete contradiction.</p>
        <p>The proofs of the logical indeterminacy principle presented by Pitowsky and Hrushovski [286,401] is global in the sense that any ray in the set of intertwining rays Γ (|x , |y ) in-between |x and |y -and thus not necessarily the "beginning and end points" |x and |y -may not have a pre-existing value. (If you are an omnirealist, substitute "pre-existing" by "non-contextual:" that is, any ray in the set of intertwining rays Γ (|x , |y ) may violate the admissibility rules and, in particular, non-contextuality.) Therefore, one might argue that the cases (i) as well as (ii); that is, v(|x ) = v(|y ) = 1. as well as v(|x ) = 1 and v(|y ) = 0 might still be predefined, whereas at least one ray in Γ (|x , |y ) cannot be pre-defined. (If you are an omni-realist, substitute "pre-defined" by "non-contextual.") This possibility has been excluded in a series of papers [3][4][5][6] localizing value indefiniteness. Thereby the strong admissibility rules coinciding with two-valued states which are total function on a logic, have been generalized or extended (if you prefer "weakened") in such away as to allow for value definiteness. Essentially, by allowing the two-valued state to be a partial function on the logic, which need not be defined any longer on all of its elements, admissability has been defined by two rules (IV) of Sect. 12.9.4: if v(|x ) = 1, then a measurement of all the other observables in a context containing |x must yield the value 0 for the other observables in this context -as well as counterfactually, in all contexts including |x and in mutually orthogonal rays which are orthogonal to |x , such as depicted as the star-shaped configuration in Fig. 12.15. Likewise, if all propositions but one, say the one associated with |x , in a context have value 0, then this proposition |x is assigned the value 1; that is,</p>
        <p>However, as long as the entire context contains more than two atoms, if v(|x ) = 0 for some proposition associated with |x , any of the other observables in the context containing |x could still yield the value 1 or 0. Therefore, these other observables need not be value definite. In such a formalism, and relative to the assumptionsin particular, by the admissibility rules allowing for value indefiniteness -sets of intertwined rays Γ (|x , |y ) can be constructed which render value indefiniteness of property |y y| if the system is prepared in state |x (and thus v(|x ) = 1). More specifically, sets of intertwined rays Γ (|x , |y ) can be found which demonstrate that, in accord with the "weak" admissibility rules (IV) of Sect. 12.9.4, in Hilbert spaces of dimension greater than two, in accord with complementarity, any proposition which is complementary with respect to the state prepared must be value indefinite [3][4][5][6].</p>
        <p>Clifton replied with this (rhetorical) question after I had asked if he could imagine any possibility to somehow "operationalize" the Kochen-Specker theorem.</p>
        <p>Indeed, the Kochen-Specker theorem -in particular, not only non-separability but the total absence of any two-valued state -has been resilient to attempts to somehow "measure" it: first, as alluded by Clifton, its proof is by contraction -any assumption or attempt to consistently (in accordance with admissibility) construct two-valued state on certain finite subsets of quantum logics provably fails.</p>
        <p>Second, the very absence of any two-valued state on such logics reveals the futility of any attempt to somehow define classical probabilities; let alone the derivation of any Boole's conditions of physical experience -both rely on, or are, the hull spanned by the vertices derivable from two-valued states (if the latter existed) and the respective correlations. So, in essence, on logics corresponding to Kochen-Specker configurations, such as the Γ 2 -configuration of Kochen-Specker [314, p. 69], or the Cabello, Estebaranz and García-Alcaine logic [91,96] depicted in Fig. 12.16 which (subject to admissibility) have no two-valued states, classical probability theory breaks down entirely -that is, in the most fundamental way; by not allowing any two-valued state.</p>
        <p>It is amazing how many papers exist which claim to "experimentally verify" the Kochen-Specker theorem. However, without exception, those experiments either prove some kind of Bell-Boole of inequality on single-particles (to be fair this is referred to as "proving contextuality;" such as, for instance, Refs. [36,98,267,268,309]); or show that the quantum predictions yield complete contradictions if one "forces" or assumes the counterfactual co-existence of observables in different contexts (and measured in separate, distinct experiments carried out in different subensembles; e.g., Refs. [91,250,383,467,468]; again these lists of references are incomplete.)</p>
        <p>Of course, what one could still do is measuring all contexts, or subsets of compatible observables (possibly by Einstein-Podolsky-Rosen type [196] counterfactual inference) -one at a time -on different subensembles prepared in the same state by Einstein-Podolsky-Rosen type [196] experiments, and comparing the complete sets of results with classical predictions [250]. For instance, multiplying all products of dichotomic ±1 observables within contexts, and summing up the results in parity proofs such as for the Cabello, Estebaranz and García-Alcaine logic depicted in Fig. 12.16 must yield differences between the classical and the quantum predictions -in this case parity odd and even, respectively.</p>
        <p>If one is willing to drop admissibility altogether while at the same time maintaining non-contextuality -thereby only assuming that the hidden variable theories assign values to all the observables [54,Sect. 4,p. 375], thereby only assuming non-contextuality [92], one arrives at contextual inequalities [16]. Of course, these value assignments need to be much more general as the admissibility requirements on two-valued states; allowing all 2 n (instead of just n combinations) of contexts with n atoms; such as 1 -1 -1 -• • • -1, or 0 -0 -• • • -0. For example, Cabello has suggested [92] to consider fourth order correlations within all the contexts (blocks; really within single maximal observables) constituting the logic considered by Cabello, Estebaranz and García-Alcaine [91,96], and depicted as a Greechie orthogonality diagram in Fig. 12.16. For the sake of demonstration, consider a Greechie (orthogonality) diagram of a finite subset of the continuum of blocks or contexts imbeddable in four-dimensional real Hilbert space without a two-valued probability measure. More explicitly, the correlations are with nine tightly interconnected contexts a = {a 1 , a 2 , a 3 , a 4 }, b = {a 4 , a 5 , a 6 , a 7 }, c = {a 7 , a 8 , a 9 , a 10 }, d = {a 10 , a 11 , a 12 , a 13 }, e = {a 13 , a 14 , a 15 , a 16 }, f = {a 16 , a 17 , a 18 , a 1 }, g = {a 6 , a 8 , a 15 , a 17 } h = {a 3 , a 5 , a 12 , a 14 }, i = {a 2 , a 9 , a 11 , a 18 }, respectively.</p>
        <p>A hull problem can be defined as follows: (i) assume that each one of the 18 (partially counterfactual) observables a 1 , a 2 , . . . , a 18 independently acquires either the definite value "-1" or "+1," respectively. There are 2 18 = 262144 such cases. Note that, essentially, thereby all information on the intertwine structure is eliminated (the only remains are in the correlations taken in the next step), as one treats all observables to belong to a large Boolean algebra of 18 atoms a 1 , a 2 , . . . , a 18 ; (ii) form all the 9 four-order correlations according to the context (block) structure a 1 a 2 a 3 a 4 , a 4 a 5 a 6 a 7 , . . . , a 2 a 9 a 11 a 18 , respectively; (iii) then evaluate (by multiplication) each one of these nine observables according to the valuations created in (i); (iv) for each one of the 2 18 valuations form a 9-dimensional vector (E 1 = a 1 a 2 a 3 a 4 , E 2 = a 4 a 5 a 6 a 7 , . . . , E 9 = a 2 a 9 a 11 a 18 ) which contains all the values computed in (iii), and consider them as vertices (of course, there will be many duplicates which can be eliminated) defining a correlation polytope; (v) finally, solve the hull problem for this polytope. The resulting 274 inequalities and 256 vertices (a reverse vertex computation reveals 256 vertices; down from 2 18 ) confirms Cabello's [92] as well as other bounds [521, Eq. ( 8)]; among them</p>
        <p>Similar calculations for the pentagon and the Specker bug logics, by "bundling" the 3rd order correlations within the contexts (blocks, 3-atomic Boolean subalgebras), yield 32 (down from 2 10 = 1024 partially duplicate) vertices and 10 "trivial" inequalities for the bug logic, as well as 128 (down from 2 13 = 8192 partially duplicate) vertices and 14 "trivial" inequalities for the Specker bug logic.</p>
        <p>Since from Hilbert space dimension higher than two there do not exist any twovalued states, the (quasi-)classical Boolean strategy to find (or define) probabilities via the convex sum of two-valued states brakes down entirely. Therefore, as this happened to be [172,173,295,551,552,554], the quantum probabilities have to be "derived" or postulated from entirely new concepts, based upon quantities -such as vectors or projection operators -in linear vector spaces equipped with a scalar product. One guiding principle should be that, among those observables which are simultaneously co-measurable (that is, whose projection operators commute), the classical probability theory should hold.</p>
        <p>Historically, what is often referred to as Born rule for calculating probabilities, has been a statistical re-interpretation of Schrödinger's wave function [68, Footnote 1, Anmerkung bei der Korrektur, p. 865], as outlined by Dirac [172,173] (a digression: a small piece [176] on "the futility of war" by the late Dirac is highly recommended; I had the honour listening to the talk personally), Jordan [295], von Neumann [551,552,554], and Lüders [89,346,347].</p>
        <p>Rather than stating it as axiom of quantum mechanics, Gleason [240] derived the Born rule from elementary assumptions; in particular from subclassicality: within contexts -that is, among mutually commuting and thus simultaneously comeasurable observables -the quantum probabilities should reduce to the classical, Kolmogorovian, form. In particular, the probabilities of propositions corresponding to observables which are (i) mutually exclusive (in geometric terms: correspond to orthogonal vectors/projectors) as well as (ii) simultaneously co-measurable observables are (i) non-negative, (ii) normalized, and (iii) finite additive as in Eqs. (12.24) and (12.25); that is, probabilities (of atoms within contexts or blocks) add up to one [259,Sect. 1].</p>
        <p>As already mentioned earlier, Gleason's paper made a high impact on those in the community capable of comprehending it [41,151,180,301,314,401,434,591]. Nevertheless it might not be unreasonable to state that, while a proof of the Kochen-Specker theorem is straightforward, Gleason's results are less attainable. However, in what follows we shall be less concerned with either necessity nor with mixed states, but shall rather concentrate on sufficiency and pure states. (This will also rid us of the limitations to Hilbert spaces of dimensions higher that two.)</p>
        <p>Recall that pure states [172,173] as well as elementary yes-no propositions [62,552,554] can both be represented by (normalized) vectors in some Hilbert space. If one prepares a pure state corresponding to a unit vector |x (associated with the one-dimensional projection operator E x = |x x|) and measures an elementary yesno proposition, representable by a one-dimensional projection operator E y = |y y| (associated with the vector |y ), then Gleason notes [240, p. 885]</p>
        <p>Since in Euclidean space, the projection E y of |y on A = span(|x ) is the dot product (both vectors |x , |y are supposed to be normalized) |x x|y = |x cos ∠(|x , |y ), Gleason's observation amounts to the well-known quantum mechanical cosine square probability law referring to the probability to find a system prepared a in state in another, observed, state. (Once this is settled, all self-adjoint observables follow by linearity and the spectral theorem.)</p>
        <p>In this line of thought, "measurement" contexts (orthonormal bases) allow "views" on "prepared" contexts (orthonormal bases) by the respective projections.</p>
        <p>For the sake of demonstration, suppose some unit vector |ρ corresponding to a pure quantum state (preparation) is selected. For each one-dimensional closed subspace corresponding to a one-dimensional orthogonal projection observable (interpretable as an elementary yes-no proposition) E = |e e| along the unit vector |e , define w ρ (|e ) = | e|ρ | 2 to be the square of the length | ρ|e | of the projection of |ρ onto the subspace spanned by |e .</p>
        <p>The reason for this is that an orthonormal basis {|e i } "induces" an ad hoc probability measure w ρ on any such context (and thus basis). To see this, consider the length of the orthogonal (with respect to the basis vectors) projections of |ρ onto all the basis vectors |e i , that is, the norm of the resulting vector projections of |ρ onto the basis vectors, respectively. This amounts to computing the absolute value of the Euclidean scalar products e i |ρ of the state vector with all the basis vectors.</p>
        <p>In order that all such absolute values of the scalar products (or the associated norms) sum up to one and yield a probability measure as required in Eqs. (12.24) and (12.25), recall that |ρ is a unit vector and note that, by the Pythagorean theorem, these absolute values of the individual scalar products -or the associated norms of the vector projections of |ρ onto the basis vectors -must be squared. Thus the value w ρ (|e i ) must be the square of the scalar product of |ρ with |e i , corresponding to the square of the length (or norm) of the respective projection vector of |ρ onto |e i . For complex vector spaces one has to take the absolute square of the scalar product; that is,</p>
        <p>Pointedly stated, from this point of view the probabilities w ρ (|e i ) are just the (absolute) squares of the coordinates of a unit vector |ρ with respect to some orthonormal basis {|e i }, representable by the square | e i |ρ | 2 of the length of the vector projections of |ρ onto the basis vectors |e i -one might also say that each orthonormal basis allows "a view" on the pure state |ρ . In two dimensions this is illustrated for two bases in Fig. 12.18. The squares come in because the absolute values of the individual components do not add up to one; but their squares do. These considerations apply to Hilbert spaces of any, including two, finite dimensions. In this non-general, ad hoc sense the Born rule for a system in a pure state and an elementary proposition observable (quantum encodable by a one-dimensional projection operator) can be motivated by the requirement of additivity for arbitrary finite dimensional Hilbert space.</p>
        <p>thereby motivating the use of the absolute value (modulus) squared of the amplitude for quantum probabilities on pure states 12.9.9.</p>
        <p>In what follows quantum configurations corresponding to the logics presented in the earlier sections will be considered. All of them have quantum realizations in terms of vectors spanning one-dimensional subspaces corresponding to the respective onedimensional projection operators.</p>
        <p>The appendix contains a detailed derivation of two-particle correlation functions. It turns out that, whereas on the singlet state the classical correlation function (B.1) E c,2,2 (θ) = 2 π θ -1 is linear, the quantum correlations (B.11) and (B.23) are of the "stronger" cosine form E q,2 j+1,2 (θ) ∝cos(θ). A stronger-than-quantum correlation would be a sign function E s,2,2 (θ) = sgn(θ -π/2) [321].</p>
        <p>When translated into the most fundamental empirical level -to two clicks in 2 × 2 = 4 respective detectors, a single click on each side -the resulting differences</p>
        <p>signify a critical difference with regards to the occurrence of joint events: both classical and quantum systems perform the same at the three points θ ∈ {0, π 2 , π}. In the region 0 &lt; θ &lt; π 2 , ΔE is strictly positive, indicating that quantum mechanical systems "outperform" classical ones with regard to the production of unequal pairs "+-" and "-+," as compared to equal pairs "++" and "--." This gets largest at θ max = arcsin(2/π) ≈ 0.69; at which point the differences amount to 38% of all such pairs, as compared to the classical correlations. Conversely, in the region π 2 &lt; θ &lt; π, ΔE is strictly negative, indicating that quantum mechanical systems "outperform" classical ones with regard to the production of equal pairs "++" and "--," as compared to unequal pairs "+-" and "-+." This gets largest at θ min = πarcsin(2/π) ≈ 2.45. Stronger-than-quantum correlations [414,415] could be of a sign functional form E s,2,2 (θ) = sgn(θ -π/2) [321].</p>
        <p>In correlation experiments these differences are the reason for violations of Boole's (classical) conditions of possible experience. Therefore, it appears not entirely unreasonable to speculate that the non-classical behaviour already is expressed and reflected at the level of these two-particle correlations, and not in need of any violations of the resulting inequalities.</p>
        <p>Violation of Boole's (classical) conditions of possible experience by the quantum probabilities, correlations and expectations are indications of some sort of nonclassicality; and are often interpreted as certification of quantum physics, and quantum physical features [395,540]. Therefore it is important to know the extent of such violations; as well as the experimental configurations (if they exist [478]) for which such violations reach a maximum.</p>
        <p>The basis of the min-max method are two observations [212]:</p>
        <p>1. Boole's bounds are linear -indeed linearity is, according to Pitowsky [400], the main finding of Boole with regards to conditions of possible (nowadays classical physical) experience [66,67] -in the terms entering those bounds, such as probabilities and nth order correlations or expectations. 2. All such terms, in particular, probabilities and nth order correlations or expectations, have a quantum realization as self-adjoint transformations. As coherent superpositions (linear sums and differences) of self-adjoint transformations are again self-adjoint transformations (and thus normal operators), they are subject to the spectral theorem. So, effectively, all those terms are "bundled together" to give a single "comprehensive" (with respect to Boole's conditions of possible experience) observable. 3. The spectral theorem, when applied to self-adjoint transformations obtained from substituting the quantum terms for the classical terms, yields an eigensystem consisting of all (pure or non-pure) states, as well as the associated eigenvalues which, according to the quantum mechanical axioms, serve as the measurement outcomes corresponding to the combined, bundled, "comprehensive," observables. (In the usual Einstein-Podolsky-Rosen "explosion type" setup these quantities will be highly non-local.) The important observation is that this "comprehensive" (with respect to Boole's conditions of possible experience) observable encodes or includes all possible one-by-one measurements on each one of the single terms alone, at least insofar as they pertain to Boole's conditions. 4. By taking the minimal and the maximal eigenvalue in the spectral sum of this comprehensive observable one therefore obtains the minimal and the maximal measurement outcomes "reachable" by quantization.</p>
        <p>Thereby, Boole's conditions of possible experience are taken as given and for granted; and the computational intractability of their hull problem [399] is of no immediate concern, because nothing need to be said of actually finding those conditions of possible experience, whose calculation may grow exponential with the number of vertices. Note also that there might be a possible confusion of the term "minmax principle" [260,Sect. 90] with the term "maximal operator" [260,Sect. 84]. And finally, this is no attempt to compute general quantum ranges, as for instance discussed by Pitowsky [396,402,406] and Tsirelson [141][142][143].</p>
        <p>Indeed, functional analysis provides a technique to compute (maximal) violations of Boole-Bell type inequalities [213,214]: the min-max principle, also known as Courant-Fischer-Weyl min-max principle for self-adjoint transformations (cf. Ref. [260,Sect. 90], Ref. [430, pp. 75ff], and Ref. [528,Sect. 4.4,pp. 142ff]), or rather an elementary consequence thereof: by the spectral theorem any bounded self-adjoint linear operator T has a spectral decomposition T = n i=1 λ i E i , in terms of the sum of products of bounded eigenvalues times the associated orthogonal projection operators. Suppose for the sake of demonstration that the spectrum is non-degenerate. Then we can (re)order the spectral sum so that λ 1 ≥ λ 2 ≥ • • • ≥ λ n (in case the eigenvalues are also negative, take their absolute value for the sort), and consider the greatest eigenvalue.</p>
        <p>In quantum mechanics the maximal eigenvalue of a self-adjoint linear operator can be identified with the maximal value of an observation. Thereby, the spectral theorem supplies even the state associated with this maximal eigenvalue λ 1 : it is the eigenvector (linear subspace) |e 1 associated with the orthogonal projector E i = |e 1 e 1 | occurring in the (re)ordered spectral sum of T.</p>
        <p>With this in mind, computation of maximal violations of all the Boole-Bell type inequalities associated with Boole's (classical) conditions of possible experience is straightforward:</p>
        <p>1. take all terms containing probabilities, correlations or expectations and the constant real-valued coefficients which are their multiplicative factors; thereby excluding single constant numerical values O(1) (which could be written on "the other" side of the inequality; resulting if what might look like "T ( p 1 , . . . , p n , p 1,2 , . . . , p 123 , . . .) ≤ O( 1)" (usually, these inequalities, for reasons of operationalizability, as discussed earlier, do not include higher than 2rd order correlations), and thereby define a function T ; 2. in the transition "quantization" step T → T substitute all classical probabilities and correlations or expectations with the respective quantum selfadjoint operators, such as for two spin-1 2 particles enumerated in Eq. (B.6),</p>
        <p>], E c → E q = p 12++ + p 12--p 12+-p 12-+ , as demanded by the inequality. Note that, since the coefficients in T are all real-valued, and because</p>
        <p>for arbitrary self-adjoint transformations A, B, the real-valued weighted sum T of self-adjoint transformations is again self-adjoint. 3. Finally, compute the eigensystem of T; in particular the largest eigenvalue λ max and the associated projector which, in the non-degenerate case, is the dyadic product of the "maximal state" |e max , or E max = |e max e max |. 4. In a last step, maximize λ max (and find the associated eigenvector |e max ) with respect to variations of the parameters incurred in step (ii).</p>
        <p>The min-max method yields a feasible, constructive method to explore the quantum bounds on Boole's (classical) conditions of possible experience. Its application to other situations is feasible. A generalization to higher-dimensional cases appears tedious but with the help of automated formula manipulation straightforward.</p>
        <p>The quantum expectation can be directly computed from spin state operators. For spin- 1 2 particles, the relevant operator, normalized to eigenvalues ±1, is</p>
        <p>The eigenvalues are -1, -1, 1, 1 and 0; with eigenvectors for</p>
        <p>-e -i(θ 1 +θ 2 ) , 0, 0, 1 , 0, -e -i(θ 1 -θ 2 ) , 1, 0 , e -i(θ 1 +θ 2 ) , 0, 0, 1 , 0, e -i(θ 1 -θ 2 ) , 1, 0 , (12.55) respectively.</p>
        <p>If the states are restricted to Bell basis states</p>
        <p>) and the respective projection operators are E Ψ ∓ and E Φ ∓ , then the correlations, reduced to the projected operators</p>
        <p>for E Φ -, and cos(θ 1 + θ 2 ) for E Φ + .</p>
        <p>The ease of this method can be demonstrated by (re)deriving the Tsirelson bound [141] of 2 √ 2 for the quantum expectations of the Clauser-Horne-Shimony-Holt inequalities (12.32) (cf. Sect. 12.9.8.2), which compare to the classical bound 2. First note that the two-particle projection operators along directions ϕ 1 = ϕ 2 = π 2 and θ 1 , θ 2 , as taken from Eqs. (B.6) and (B.3), are</p>
        <p>(12.56)</p>
        <p>Adding these four orthogonal projection operators according to the parity of their signatures ± 1 ± 2 yields the expectation value</p>
        <p>(12.57)</p>
        <p>Forming the Clauser-Horne-Shimony-Holt operator</p>
        <p>The eigenvalues λ 1,2 = ∓2 1sin(θ 1 -θ 2 ) sin(θ 3 -θ 4 ),</p>
        <p>for θ 1 -θ 2 = θ 3 -θ 4 = ± π 2 , yield the Tsirelson bounds ±2 √ 2. In particular, for</p>
        <p>and the eigenvalues are</p>
        <p>with the associated eigenstates (0, 0, 1, 0) , (0, 1, 0, 0) , (i, 0, 0, 1) , (-i, 0, 0, 1) , respectively. Note that, by comparing the components [368, p. 18] the eigenvectors associated with the eigenvalues reaching Tsirelson's bound are entangled, as could have been expected.</p>
        <p>If one is interested in the measurements "along" Bell states, then one has to consider the projected operators E Ψ ∓ (CHSH)E Ψ ∓ and E Φ ∓ (CHSH)E Φ ∓ on those states which yield extrema at</p>
        <p>(12.61)</p>
        <p>For</p>
        <p>and Eq. (12.61) yields the Tsirelson bound</p>
        <p>Again it should be stressed that these violations might be seen as a "build-up;" resulting from the multiple addition of correlations which they contain.</p>
        <p>Note also that, only as single context can be measured on a single system, because other context contain incompatible, complementary observables. However, as each observable is supposed to have the same (counterfactual) measurement outcome in any context, different contexts can be measured on different subensembles prepared in the same state such that, with the assumptions made (in particular, existence and context independence), Boole's conditions of possible experience should be valid for the averages over each subsensemble -regardless of whether they are comeasurable or incompatible and complementary. (This is true for instance for models with partition logics, such as generalized urn or finite automaton models.)</p>
        <p>In a similar way two-particle correlations of a spin-one system can be defined by the operator S 1 introduced in Eq. (B.13)</p>
        <p>Plugging in these correlations into the Klyachko-Can-Biniciogolu-Shumovsky inequality [312] in Eq. (12.40) yields the Klyachko-Can-Biniciogolu-Shumovsky operator</p>
        <p>Taking the special values of Tkadlec [532], as enumerated in Cartesian coordinates in Fig. 12.6, which, is spherical coordinates, are</p>
        <p>, yields eigenvalues of KCBS in -2.49546, 2.2288, -1.93988, 1.93988, -1.33721, 1.33721, -0.285881, 0.285881, 0.266666 (12.64) all violating Eq. (12.40).</p>
        <p>As a final exercise we shall compute the quantum bounds on the Cabello, Estebaranz and García-Alcaine logic [91,96] which can be used in a parity proof of the Kochen-Specker theorem in 4 dimensions, as depicted in Fig. 12.16 (where also a representation of the atoms as vectors in R 4 suggested by Cabello [92, Fig. 1] is enumerated), as well as the dichotomic observables [92, Eq. ( 2)] A i = 2|a i a i | -I 4 is used. The observables are then "bundled" into the respective contexts to which they belong; and the context summed according to the contextual inequalities from the Hull computation (12.52), and introduced by Cabello [92, Eq. ( 1)]. As a result (we use Cabello's notation and not ours), (12.65) The resulting 4 4 = 256 eigenvalues of T have numerical approximations as ordered numbers -6.94177 ≤ -6.67604 ≤ • • • ≤ 5.78503 ≤ 6.023, neither of which violates the contextual inequality (12.52) and Ref. [92, Eq. ( 1)].</p>
        <p>When reading the book of Nature, she obviously tries to tell us something very sublime yet simple; but what exactly is it? As mentioned earlier it seems that often discussants approach this particular book not with evenly-suspended attention [224,225] but with strong -even ideologic [144] or evangelical [589] -(pre)dispositions. This might be one of the reasons why Specker called this area "haunted" [482]. With these provisos we shall enter the discussion.</p>
        <p>Already in 1935 -possibly based to the Born rule for computing quantum probabilities which differ from classical probabilities on a global scale involving complementary observables, and yet coincide within contexts -Schrödinger pointed out (cf. also Pitowsky [400, footnote 2, p. 96]) that [539, p. 327] "at no moment does there exist an ensemble of classical states of the model that squares with the totality of quantum mechanical statements of this moment." 4 This seems to be the gist of what can be learned from the quantum probabilities: they cannot be accommodated entirely within a classical framework.</p>
        <p>What can be positively said? Quantum mechanics grant operational access merely to a single context (block, maximal observable, orthonormal basis, Boolean subalgebra); and for all that operationally matters, all observables forming that context can be simultaneously value definite. (It could formally be argued that an entire star of contexts intertwined in a "true" proposition must be value definite, as depicted in Fig. 12.15.) A single context represents the maximal information encodable into a quantum system. This can be done by state preparation.</p>
        <p>Beyond this single context one can get "views" on that single state in which the quantized system has been prepared. But these "views" come at a price: value indefiniteness. (Value indefiniteness is often expressed as "contextuality," but this view is distractive, as it suggests some existing entity which is changing its value; depending on how -that is, along which context -it is measured.)</p>
        <p>This situation might not be taken as a metaphysical conundrum, but perceived rather Socratically: it should come as no surprise that intrinsic [500], emdedded [538] observers have no access to all the information they subjectively desire, but only to a limited amount of properties their system -be it a virtual or a physical universeis capable to express. Therefore there is no omniscience in the wider sense of "all that observers want to know" but rather than "all that is operationally realizable."</p>
        <p>Anything beyond this narrow "local omniscience covering a single context" in which the quantized system has been prepared appears to be a subjective illusion which is only stochastically supported by the quantum formalism -in terms of Gleason's "projective views" on that single, value definite context. Experiments may enquire about such value indefinite observables by "forcing" a measurement upon a system not prepared or encoded to be "interrogated" in that way. However, these "measurements" of non-existing properties, although seemingly possessing viable outcomes which might be interpreted as referring to some alleged "hidden" properties, cannot carry any (consistent classical) content pertaining to that system alone.</p>
        <p>To paraphrase a dictum by Peres [388], unprepared contexts do not exist; at least not in any operationally meaningful way. If one nevertheless forces metaphysical existence upon (value) indefinite, non-existing, physical entities the price, hedged into the quantum formalism, is stochasticity.</p>
        <p>The quantum measurement problem is at the heart of today's quantum random number generators. Thus everybody relying on that technology has to be concerned with this seemingly philosophical issues of observer-object relation. And anybody denying its existence (aka Austin Powers' "if you got an issue here's a tissue") is tantamount to building a bridge with a new material whose properties and construction objectives are largely unknown -thereby relying on assurances of most experts which are solely based on heuristics.</p>
        <p>Presently the quantum mechanical observer-object theory is a "canvas with many facets and nuances." There is no one accepted view of the measurement problem. The Ansätze proposed include, but are not limited to (I) collapse models: modification of quantum mechanics by the inclusion of some additional non-linear, irreversible transformation accounting for von Neumann's process 1, and possible also for the transformation of pure states into mixed ones [226,237,238,544]. (II) Exner-Schrödinger thesis: all laws; in particular, also the unitary time evolution of the quantum state, have to be understood merely statistically, and are not valid individually [209,262,451]. (III) Noncollapse Schrödinger-type quantum jellification without observation or measurement: in Schrödinger's own words [457, pp. 19,20],</p>
        <p>] form for, let me say, a quarter of an hour, we should find our surroundings rapidly turning into a quagmire, or sort of a featureless jelly or plasma, all contours becoming blurred, we ourselves probably becoming jelly fish. . . . nature is prevented from rapid jellification only by our perceiving or observing it. And I wonder that he is not afraid, when he puts a ten-pound-note {his wrist-watch} into his drawer in the evening, he might find it dissolved in the morning, because he has not kept watching it." (IV) Non-collapse Everettian type relative state formalism [30,33,205,206,208,544,545]: Everett realized that, due to nesting, there cannot occur any kind of irreversibility, but just the formation of entanglement. Entanglement in turn induces relational properties between objects and observers (a the price of individual properties of those parties). Given a pure state and a measurement not compatible with it, the coherent superposition decomposition of the state in terms of the eigenstates of the measurement operator all constitute a valid observer (observing agent) who subjectively experiences the outcome of the measurement as unique. Maybe Everett would have even granted observers the capacity of being in a coherent superposition yet having the illusory subjective experience of uniqueness; but this is highly speculative; as well as all further interpretations of his writings in terms of splitting worlds theory [207]. (V) Non-collapse and intrinsic incompleteness: already von Neumann mentions (and immediately discards this as a solution of the measurement problem) the possibility that [554, Sect. 6.2, p. 426] ". . . the result of the measurement is indeterminate, because the state of the observer before the measurement is not known exactly. It is conceivable that such a mechanism might function, because the state of information of the observer regarding his own state could have absolute limitations, by the laws of nature." 5 Breuer has discussed this possibility in a series of papers [72][73][74]. This is not dissimilar to self-nesting, as discussed in Sect. 1.8. (VI) Non-collapse entanglement (zero sum scenario, excluding consciousness): the extrinsic state representation of the combined object and observer system is pure and entangled, while intrinsically both the observer and the object, mistakenly perceived individually, are in mixed states. This line of thought might have been best expressed by London and Bauer [341,342] who base their presentation on von Neumann's treatment of the measurement process [552, Chap. VI] (echoed also in Everett's [206] and Wigner's [571] papers). Related ideas can be found in Schrödinger's accounts on entanglement [452,453,455], which in turn have been influenced by a (nowadays famous) paper by Einstein, Podolsky and Rosen [196].</p>
        <p>(i) Initial phase: this conceptualization of the measurement process starts with the supposition that initially the entire system consists of two isolated systems O and A, identified with an object and the measurement apparatus, respectively. Initially, if the respective states are pure and denoted by |ψ O as well as |ψ A , then the wave function of the entire system can be composed from the individual parts by multiplication; that is,</p>
        <p>In this initial phase the object as well as the measurement apparatus are separated and know nothing about each other, their joint state |ψ O&amp; A being non-entangled and without any relational properties. Suppose further, for the sake of simplicity, that both the object as well as the measurement device have an equal number k of mutually exclusive states |ψ O,i as well as |ψ A, j , with 1 ≤ i, j ≤ k, respectively. The operator A corresponding to the measurement device should have a spectral resolution of E = k j=1 a j |ψ A, j ψ A, j |. (ii) Interaction phase: in order to obtain information about each other, both object and the measurement apparatus have to interact with each other. This interaction is supposed to be representable by a unitary transformation. During this interaction, the initial state |ψ O&amp; A is transformed into a coherent superposition, a sum of products of individual states of O and A, so that the state after the interaction phase is</p>
        <p>Preferably, in a measurement, states of the measurement apparatus "should get aligned" with states of the object, such that ϕ i j ≈ δ i j ϕ ii and</p>
        <p>This state |ψ O&amp; A will in generally not be factorizable as a non-entangled product state of individual states of O and A. Suppose further that it is not the case; that is, suppose that |ψ O&amp; A is entangled. (iii) interpretation phase: depending on our viewpoint, conventions and inclinations, i. on the one hand, |ψ O&amp; A can either be perceived from the outside -that is, extrinsically -and thus appear as pure entangled state of the combined system of object and the measurement apparatus, encoding relational information (that is, statistical correlations) among these subsystems, but lacking complete information of individual subsystems; ii. or, on the other hand, from the intrinsic point of view of individual subsystems, |ψ O&amp; A can be analysed in terms of the individual components by forcing some type of individuality (e.g., by taking the partial trace with respect to one subsystem) upon the subsystems. In the latter case of individuality forcing, as the wave function lacks complete information about the individual subsystems, the respective undefined subsystem properties are value indefinite. As a side effect of individuality forcing, entanglement is fapp destroyed, and |ψ O&amp; A undergoes a change back to a non-entangled state |ψ O&amp; A , subject to the relational information contained in |ψ O&amp; A .</p>
        <p>One may ask how individuality can be enforced upon |ψ O&amp; A . This can be done by context translation [505] (for related ideas see Refs. [333,520]); that is, by "translating" or "transforming" a misaligned measurement context into one which can be analysed, possibly by a third measurement device. This translation process introduces stochasticity through the (supposedly many) degrees of freedom of the outside measurement apparatus. Thereby, context translation involves fapp irreversibility [348] for macroscopic measurement devices [202,461]. Yet in principle the chaining or nesting of measurements results in a (potentially infinite) regress.</p>
        <p>If there is a regress, when does it stop? This can be answered by considering the smallest isolated system encompassing the original object O as well as the measurement apparatus A. Already Baumann [37] and Zeh [586] have pointed out that, strictly speaking, because of the high density of energy niveaus in macroscopic systems, the interactions between macroscopic systems are effective even at astronomical distances. Therefore these systems are exceedingly difficult to isolate; and any system which includes all (nested) observers would encompass the universe as a whole.</p>
        <p>Most importantly, whatever the measurement outcome of measurements on individual parts of the entangled object-measurement apparatus system, this cannot correspond to some pre-existing, definite value solely residing within the bounds of the observed object, because the information encoded in the entangled state is (also, and in the extreme case solely and exclusively) in the relational properties of its constituent parts; and not about the individual states of these constituents. Therefore, if one forces individuality, one has to add additional information from the environment, in particular, the measurement apparatus, which is not present in the original state. The author is inclined to adopt this view of the measurement process -it is just like zooming in and out of the situation: if one looks at it from an extrinsic, outside, disentangled perspective (if one can afford such a view) -that is, as an isolated holistic system including the observer and the object, as well as the cut between them, the system is in a pure, well-defined state. However, if one "zooms into" this system, and takes an embedded, intrinsic point of view, then the individual constituents of the system -in particular, the object as well as the observer -are underdefined and value definite. "Forcing individuality" upon these constituents requires additional input from the environment (via context translation), thereby introducing auxiliary bits which do not reflect any property of those constituents.</p>
        <p>(VII) consciousness causes state reduction: this scenario is identical to the previous one but employs nesting until the level of consciousness of the observer. At this level, awareness by consciousness is then assumed to be essentially irreversible. That is, it is assumed that one cannot "unthink" the perception of a measurement. This point of view has been suggested by London and Bauer [341,342] as well as Wigner [571]; although the latter one may have developed a different stance on this subject later [203]. For a critical discussion, see Ref. [582].</p>
        <p>What does it mean "to ride on a particular section" of a vector in high dimensional Hilbert space? Can two such sectors of one and the same vector constitute an observerobject system? Where is the cut, the interface between those sections? We suggest here that indeed it might fapp be possible to make a distinction between observer and object; where both parties "ride" the same state. This distinction is formally specified by Everett's relative states; that is, it involves entangled states.</p>
        <p>What does it mean that a particular (quantum) entity is value indefinite? It means that relative to, or with respect to, a particular physical resource or physical means, the respective entity cannot be entirely, that is, completely and totally, defined. In short: any proposition about physical value indefiniteness is means relative.</p>
        <p>If such an entity is "observed" nevertheless, then this "observation" must necessarily introduce, add, input, and include, other specifications "outside" of the object.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>There are many accounts of the roots of quantum indeterminism. Take, for instance, Vaidman's recent review [544], leaning toward deterministic ontology.</p>
        <p>In any case, in 1926, Born [570, p. 54] suggested that "from the standpoint of our quantum mechanics, there is no quantity which in any individual case causally fixes the consequence of the collision; but also experimentally we have so far no reason to believe that there are some inner properties of the atom which condition a definite outcome for the collision. Ought we to hope later to discover such properties . . . and determine them in individual cases? Or ought we to believe that the agreement of theory and experiment -as to the impossibility of prescribing conditions? . . . I myself am inclined to give up determinism in the world of atoms." 1A quantum mechanical gap of casuality can be realized by a half-silvered mirror [292,484,498], with a 50:50 chance of transmission and reflection, as depicted in Fig. 13.1. A gap certified by quantum value indefiniteness necessarily has to operate with more than two exclusive outcomes [5]. Reference [3] presents such a qutrit configuration.</p>
        <p>One may object to the orthodox view [589] of quantum indeterminism by pointing out that it is merely based on a belief without proof. It is not at all clear where exactly the randomness generated by a half-silvered mirror resides; that is, where the stochasticity comes from, and what are its origin. (Often vacuum fluctuations originating from the second, empty, input port are mentioned, but, pointedly stated [236, p. 249], these "mysterious vacuum fluctuations . . . may be regarded as sugar coating for the bitter pill of quantum theory.") Fig. 13.1 (Colour online) A gap created by a quantum coin toss. A single quantum (symbolized by a black circle from a source (left crossed circle) impinges on a semi-transparent mirror (dashed line), where it is reflected and transmitted with a 50:50 chance. The two final states are indicated by grey circles. The exit ports of the mirror can be coded by 0 and 1, respectively More generally, any irreversible measurement process, and, in particular, any associated 'collapse,' or, by another denomination, 'reduction' of the quantum state (or the wave function) to the post-measurement state is a postulate which appears to be means relative in the following sense.</p>
        <p>The beam splitter setup is not irreversible at all because a 50:50 mirror has a quantum mechanical representation as a permutation of the state, such as a unitary Hadamard transformation; that is, with regard to the quantum state evolution the beam splitter acts totally deterministic; it can be represented by a one-to-one function, a permutation. (Experimentally, this can be demonstrated by serially concatenating two such 50:50 mirrors so that the output ports of the first mirror are the input ports of the second mirror. The result (modulo an overall phase) is a Mach-Zehnder interferometer reconstructing the original quantum state.) Formally -that is, within quantum theory proper, augmented by the prevalent orthodox 'Copenhagen-type' interpretation -it is not too difficult to locate the origin of randomness at the beam splitter configuration: it is (i) the possibility that a quantum state can be in a coherent superposition of classically distinct and mutually exclusive (outcome or scattering) states of a single quantum; and</p>
        <p>(ii) the possibility that an irreversible measurement ad hoc and ex nihilo stochastically 'chooses' or 'selects' one of these classically mutually exclusive properties, associated with a measurement outcome. This, according to the orthodox interpretation of quantum mechanics, is an irreducible indeterministic many-to-one process -it transforms the coherent superposition of a multitude of (classically distinct) properties into a single, classical property. This latter assumption (ii) is sometimes referred to as the reduction postulate.</p>
        <p>Already Schrödinger has expressed his dissatisfaction with both assumptions (i) and (ii), and, in particular, with the quantum mechanical concept of ontological existence of coherent superposition, in various forms at various stages of his life: he polemicized against the uncritical perception of the quantum formalism (i) by quoting the burlesque situation of a cat which is supposed to be in a coherent superposition between death and life [452]. He also noted the curious fact that, as a consequence of (i) and in the absence of measurement and state reduction (ii), according to quantum mechanics we all (as well as the physical universe in general), would become quantum jelly [457].</p>
        <p>What in the orthodox scriptures of quantum mechanics often is referred to as "irreversible measurement" remains conceptually unclear, and is inconsistent with other parts of quantum theory. Indeed, it is not even clear that, ontologically, an irreversible measurement exists! Wigner [571] and, in particular, Everett [206,208] put forward ontologic arguments against irreversible measurements by extending the cut between a quantum object and the classical measurement apparatus to include both object as well as the measurement apparatus in a uniform quantum description. As this latter situation is described by a permutation (i.e. by a unitary transformation), irreversibility, and what constitutes 'measurement' is lost. Indeed, the reduction postulate (ii) and the uniform unitarity of the quantum evolution cannot both be true, because the former essentially yields a many-to-one mapping of states, whereas uniform unitarity merely amounts to a one-to-one mapping, that is, a permutation, of states. In no way can a many-to-one mapping 'emerge' from any sort of concatenation of one-to-one mappings! Stated differently, according to the reduction postulate (ii), information is lost; whereas, according to the unitary state evolution, no information is ever lost. So, one of these postulates must be ontologically wrong (they may be epistemically justified for all practical purposes [44], though). In view of this situation, I am (to use Born's dictum [68, p. 866]) inclined to give up the reduction postulate disrupting permutativity, and, in particular, unitarity, in the world of single quantum phenomena, in favour of the latter; that is, in favour of permutativity, and, in particular, unitarity.</p>
        <p>The effort to do so may be high, as detailed beam recombination analysis of a Stern-Gerlach device (the spin analogue of a beam splitter in the Mach-Zehnder interferometer) shows [202,461]. Nonetheless, experiments (and proposals) to "undo" quantum measurements abound [137,252,275,323,389,394,462,463,585]. Thus we could say that for all practical purposes [43], that is, relative to the physical means [375] available to resolve the huge number of degrees of freedom involving a macroscopic measurement apparatus, measurements appear to be irreversible, but a close inspection reveals that they are not. So, irreversibility of quantum measurements appears to be epistemic and means relative, subjective and conventional; but not ontic. (As already argued by Maxwell, this is just the same for the second law of thermodynamics [375].)</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>As stated by Milonni [370,p. xiii] and emphasized by others [174,195], ". . . there is no vacuum in the ordinary sense of tranquil nothingness. There is instead a fluctuating quantum vacuum." One of the observable vacuum effects is the spontaneous emission of radiation [565]: ". . . the process of spontaneous emission of radiation is one in which "particles" are actually created. Before the event, it consists of an excited atom, whereas after the event, it consists of an atom in a state of lower energy, plus a photon."</p>
        <p>Recent experiments achieve single photon production by spontaneous emission [87,308,322,441,486], for instance by electroluminescence. Indeed, most of the visible light emitted by the sun or other sources of blackbody radiation, including incandescent bulbs, is due to spontaneous emissions [370, p. 78].</p>
        <p>Just as in the beam splitter case discussed earlier the quantum (field theoretic) formalism can be used to compute (scattering) probabilities -that is, expectations for occurrences of individual events, or mean frequencies for large groups of quantabut remains silent for single outcomes.</p>
        <p>Alas, also in the quantum field theoretic case, unitarity, and thus permutations, govern the state evolution. Thus, for similar reasons mentioned earlier -mainly the uniformity of the validity of unitary quantum evolutions -the ontological status of indeterminism remains uncertain.</p>
        <p>If we follow the quantum canon, any such emission is an irreducible, genuine instance of creation coming from nothing (ex nihilo); more precisely, in theological terms, the spontaneous emission of light and other particles amounts to an instance of creatio continua. (This is also true for the stimulated emission of a quantum.)</p>
        <p>A (fapp postulated) gap of determinism based on vacuum fluctuations is schematically depicted in Fig. 14.1. It consists of an atom in an excited state, which transits into a state of lower energy, thereby producing a photon. The photon (non-)creation can be coded by the symbols 0 and 1, respectively. Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Egon von Schweidler, a colleague of Exner at the University of Vienna, interpreted Rutherford's (1902) decay law as merely probabilistically -thereby allowing deviations for small sample sizes [459]. Exner, a staunch indeterminist [209], might have convinced both Schweidler and Schrödinger that single decay processes occur irreducibly random, and that they had to give up determinism in the world of atoms (cf. Sect. 9.2, p. 40) [82,217,262,490,491]. For a review an early attempts to "explain" radioactivity, see Refs. [318,319] Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Chapter 16</p>
        <p>The physical theories of classical mechanics, electrodynamics and gravitation (relativity theory) have been developed alongside classical analysis. Thereby assumptions about the formal mathematical models for theoretical physics had to be made which were partly (to some degree of accuracy) corroborated empirically; and partly a mere convenience.</p>
        <p>In particular, classical continuum physics employed mathematical objects -the continuum of real and complex numbers -which, from a logical, recursion theoretic, and algorithmic point of view, has turned out to be highly nontrivial, to say the least. For instance, as is argued in the Appendix A, with probability one, an arbitrary real number turns out to be incomputable, and even algorithmically incompressible -that is, random. Stated differently, almost all elements of a continuum are not attainable by any operational physical process. They require unlimited (in terms of computation space, time et cetera) resources.</p>
        <p>When contemplating the use of nonconstructive means for physical models, two questions are imminent:</p>
        <p>(i) Are these nonconstructive continuum models a faithful representation of the physical systems in the sense that they do not underrepresent -that is, do they systems? include and comprise essential operational features of these physical systems. (ii) Are these nonconstructive continuum models a faithful representation of the physical systems in the sense that they do not overrepresent; that is, that they do not introduce entities, properties, capacities and features which have no correspondence in the empirical data? If they allege and suggest capacities -such as irreducible randomness and computability beyond the universal Turing-type -can these capacities be utilized and (technologically) harvested for "supertasks" [53,187,188,188,352,530] which go beyond the finite capacities usually ascribed to physical systems? (iii) What kind of verification, if any at all, can be given for nonconstructive means?</p>
        <p>The term "(non)constructive" is used here in its metamathematical meaning [63,77,78,354].</p>
        <p>Pointedly stated, if some theory is the double of some physical system (or vice versa [21]) we have to differentiate between properties of the theory and properties of the physical system. And we have to make sure that we do not over-represent physical facts by formalisms which contain elements which have no correspondence to the former. Because if we are not careful enough we fall pray of Jaynes' Mind Projection Fallacy mentioned in Sect. 9.5 (p. 42).</p>
        <p>Another issue is the applicability of mathematical models or methods which somehow implicity or explicitly rely on infinities. For instance, Cantor's diagonalization technique which is often used to prove the undenumerability of the real unit interval relies on an infinite process [79] which is nonoperational. Again the issue of supertasks mentioned earlier arises. It may not be totally unjustified to consider the question of whether or not theoretical physics should allow for such infinities unsettled. The issue has been raised already by Eleatic philosophy [253,331,440], and may be with us forever.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Rather than giving a detailed account on the origin and varieties of classical determinism -which is a fascinating topic of its own [185,186,255,282,494,495,566] -a very brief sketch of some of its concepts will be given.</p>
        <p>The principle of sufficient reason states that [495] "a thing cannot come to existence without a cause which produces it . . . that for everything that happens there must be a reason which determines why it is thus and not otherwise." This principle is related to another one which, in Diderot's Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers, has been discussed as follows [218]: "The law of continuity is a principle that we owe to Mr. Leibniz, which informs us that nothing jumps in nature and that one thing cannot pass from one state to another without passing through all the other states that can be conceived of between them. This law issues, according to Mr. Leibniz, from the axiom of sufficient reason. Here is the deduction. Every state in which a being finds itself must possess sufficient reason why this body finds itself in this state rather than in any other state; and this reason can only be found in its prior state. The prior state therefore contained something which gave birth to the actual state which it followed, and in such a way that these two states are so bound that it is impossible to place another in between them, for if there was a state between the actual state and that which immediately preceded it, nature would have left the first state even before it had been determined by the second to abandon the first; thus there would be no sufficient reason why it would sooner proceed to this state than to another."</p>
        <p>Note that irreversible many-to-one evolution is not excluded in this scheme, because in principle it is possible that many different states may evolve into a single one state; very much like the square function f (x) = x 2 maps both, say, x = ±2 into 4. If we interpret these concepts algorithmically and in terms of an evolution which amounts to a one-to-one permutation, then we arrive at a sort of hermetic and closed "clockwork universe" or virtual reality in which everything that happens is pre-determined by its past state, and ultimately by its initial state.</p>
        <p>First of all it should be stated up-front that, as is always the case in formalizations, the following definitions and discussions merely apply to models of physical systems, and not to the physical systems themselves. Furthermore, indeterminism is just the absence or even negation of determinism.</p>
        <p>Determinism can be informally but very generally defined by the property that [382, Chapter on Indeterministic Physical Systems] "the fixing of one aspect of the system fixes some other. . . . In a (temporally) deterministic physical system, the present state of the system determines its future states". Alternatively one may say that the present determines both past and future [566]: "determinism reigns when the state of the system at one time fixes the past and future evolution of the system.". Here uniqueness plays a crucial role: deterministic systems evolve uniquely. If the past is also assumed to be determined by the present, then this amounts to an injective (one-to-one) state evolution; that is, essentially to a permutation of the state.</p>
        <p>In classical continuum physics ordinary differential equations are a means to express the dynamics of a system. Thus determinism could formally be defined in terms of unique solutions of differential equations. In this approach determinism is essentially reduced to the purely mathematical question regarding the uniqueness of the solution of a differential equation.</p>
        <p>According to the Picard-Lindelöf theorem an initial value problem (also called the Cauchy problem) defined by a first order ordinary differential equation of the form y (t) = f (t, y(t)) and the initial value y(t 0 ) = y 0 has a unique solution if f satisfies the Lipschitz condition and is continuous as a function of t.</p>
        <p>A mapping f satisfies (global/local) Lipschitz continuity (or, used synonymously, Lipschitz condition) with finite positive constant 0 &lt; k &lt; ∞ if it increases the distance between any two points y 1 and y 2 (of its entire domain/some neighbourhood) by a factor at most k [20,Sect. 4.3,p. 272]:</p>
        <p>That is, f may be nonlinear as long as it does not separate different points "too much." f must lie within the "outward cone" spanned by the two straight lines with slopes ±k.</p>
        <p>An initial value problem defined by a second order linear ordinary differential equation of the form y (t) +a 1 (t)y (t) +a 0 (t)y = b(t) and the initial values y(t 0 ) = y 0 and y (t 0 ) = y 1 has a unique solution if the functions a 1 , a 0 and b are continuous.</p>
        <p>Systems of higher order ordinary differential equations which are normal are equivalent to first order normal systems of ordinary differential equations [61,Theorem 4,p. 180]. Therefore, uniqueness criteria of such higher order normal systems can be reduced to uniqueness criteria for first-order ordinary differential equations, which is essentially Lipschitz continuity.</p>
        <p>There are other definitions of determinism via ordinary differential equations which (mostly implicitly) do not requiring Lipschitz continuity. Consequently (weak) solutions may exist, which may result in nonunique solutions.</p>
        <p>The history of determinism abounds in proposals for indeterminism by nonunique solutions to ordinary differential equations. These proposals, if they are formalized, are mostly "exotic" in the sense that they do not satisfy the criteria for uniqueness of solutions mentioned earlier.</p>
        <p>There are a plethora of such "examples of indeterminism in classical mechanics;" in particular, discussed by Poisson in 1806, Duhamel in 1845, Bertrand in 1878, and Boussinesq in 1879 [162,494].</p>
        <p>In 1873, Maxwell identified a certain kind of instability at singular points as rendering a gap in the natural laws [359, pp. 440]: ". . . when an infinitely small variation in the present state may bring about a finite difference in the state of the system in a finite time, the condition of the system is said to be unstable. It is manifest that the existence of unstable conditions renders impossible the prediction of future events, if our knowledge of the present state is only approximate, and not accurate."</p>
        <p>Figure 17.1 (see also Frank's Fig. 1 in Chap. III, Sect. 13) depicts a one dimensional gap configuration envisioned by Maxwell [359,p. 443]: a "rock loosed by frost and balanced on a singular point of the mountain-side, . . . ." On top, the rock is in perfect balanced symmetry. A small perturbation or fluctuation causes this symmetry to be broken, thereby pushing the rock either to the left or to the right hand side of the potential divide. This dichotomic alternative can be coded by 0 and by 1, respectively.</p>
        <p>One may object to this scenario of spontaneous symmetry breaking for physical reasons; that is, by maintaining that, if indeed the symmetry is perfect, there is no movement, and the particle or rock stays on top of the tip (potential).</p>
        <p>However, any slightest movement -either through a microscopic asymmetry or imbalance of the particle, or from fluctuations of any form, either in the particle's position due to quantum zero point fluctuations, or by the surrounding environment of the particle -might topple the particle over the tip; thereby spoiling the original Fig. 17.1 (Color online) A gap created by a black particle sitting on top of a potential well. The two final states are indicated by grey circles. Their positions can be coded by 0 and 1, respectively symmetry. For instance, any collision of gas molecules with the rock may push the latter over the edge by thermal fluctuations.</p>
        <p>Maxwell's scenario resembles Norton's dome [159,216,329,379,566], and a similar configuration studied already by Boussinesq in 1879 [494, pp. 176-178] which violates Lipschitz continuity: The ordinary differential equation of motion (for its derivation and motivation, we refer to the literature) associated with the Norton dome is given by y = √ y.</p>
        <p>(17.</p>
        <p>2)</p>
        <p>It can be readily verified by insertion that (17.2) has two solutions, namely (i) a trivial one y 1 (t) = 0 for all times t, and (ii) a weak solution which can be interpreted as distribution:</p>
        <p>, where H symbolizes the (Heaviside) unit step function. Note that the left hand side of (17.2) needs to be interpreted as a generalized function (or distribution); that is, as a linear functional integrated over a test function ϕ which in this case could be 1):</p>
        <p>The right hand side of Eq. (17.2) contains the square root of this distribution, in particular the square root of the unit step function. One way of interpretation would be in terms of theta-sequences such as</p>
        <p>x . The square root of the unit step function could also be understood in terms of Colombeau theory [149]; or one might just define 1 144 (t -T ) 4 H (t -T )</p>
        <p>Colombeau theory provides another rich source of pseudo-indeterminism [150] as it deals with situations in which "tiny micro-irregularities" are "mollified" and "blown up" to "macro-scales" [9,28].</p>
        <p>The Picard-Lindelöf theorem, which applies for first-order ordinary differential equations, cannot be directly applied to this second order ordinary differential equation. Therefore we have to use the aforementioned method of conversion of a higher order ordinary differential equation into systems of first order ordinary differential equations [140, Sect. II.D, pp. [94][95][96]. Suppose the initial value (or Cauchy) problem is y (t) = f (t, y(t), y (t)) with y(t 0 ) = a 0 and y (t 0 ) = a 1 . (17.4) This equation can be rewritten into a coupled pair of equation, with v = y :</p>
        <p>The only modification for the Lipschitz condition is that instead of the absolute value of the numerical difference we have to use the difference in the plane</p>
        <p>For the rewritten Picard-Lindelöf theorem we have to assume that f (t, y, v) is continuous as a function of t, and that the modified Lipschitz condition holds: for finite positive constant 0</p>
        <p>A generalization to higher orders is straightforward.</p>
        <p>In the Norton dome case, f (t, y, v) is identified with y 1 2 . This function does not satisfy the Lipschitz condition for y = 0, as its slope is d f dy = 1 2 y -1 2 which diverges for y = 0; hence no finite bound k exists at that point: f (t, y, v) = y 1 2 grows "too fast" for y approaching 0.</p>
        <p>Similar considerations apply to other configurations violating Lipschitz continuity [216,494].</p>
        <p>There are other instances of classical determinism, all involving infinities of some sorts [382, Chapter on Indeterministic Physical Systems]. Neither shall be mentioned nor discuss here.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>incompressible) "information" contained therein. That is, if the initial value is computable -that is neither incomputable nor random -then the evolution is not chaotic but merely sensitive to the computable initial value.</p>
        <p>The question of whether physical initial values are computable or incomputable or even random (in the formal sense discussed in the Appendix A) is a nonoperational assumption and thus metaphysical. Very pointedly stated, with regards to the ontology and the type of randomness involved, deterministic chaos is sort of "garbage-in, garbage-out processes."</p>
        <p>In what may be considered as the beginning of deterministic chaos theory, Poincaré was forced to accept a gradual, that is epistemic (albeit not an ontologic in principle), departure from the deterministic position: sometimes small variations in the initial state of the bodies could lead to huge variations in their evolution at later times. In Poincaré's own words [413,Chap. 4, Sect. II, pp. [56][57], "If we would know the laws of Nature and the state of the Universe precisely for a certain time, we would be able to predict with certainty the state of the Universe for any later time. But [[ . . . ]] it can be the case that small differences in the initial values produce great differences in the later phenomena; a small error in the former may result in a large error in the latter. The prediction becomes impossible and we have a 'random phenomenon.' " See also Maxwell' observation of a metastabile state at singular points discussed in Sect. 17.4 earlier.</p>
        <p>Symbolic dynamics [27,310,339] and ergodic theory [153,192,393] has identified the Poincaré map near a homocyclic orbit, the horseshoe map [470], and the shift map as equivalent origins of classical deterministic chaotic motion, which is characterized by a computable evolution law and the sensitivity and instability with respect to variations of the initial value [15,338,465].</p>
        <p>This scenario can be demonstrated by considering the shift map σ as it pushes up "dormant" information residing in the successive bits of the initial state represented by the sequence s = 0.(bit 1)(bit 2)(bit 3) • • • , thereby truncating the bits before the comma:</p>
        <p>Suppose a measurement device operates with a precision of, say, two bits after the comma, indicated by a two bit window of measurability; thus initially all information beyond the second bit after the comma is hidden to the experimenter. Consider two initial states s = [0.(bit 1)(bit 2)](bit 3) • • • and s = [0.(bit 1)(bit 2)](bit 3) • • • , where the square brackets indicate the boundaries of the window of measurability (two bits in this case). Initially, as the representations of both states start with the same two bits after the comma [0.(bit 1)(bit 2)], these states appear operationally identical and cannot be discriminated experimentally. Suppose further that, after the second bit, when compared, the successive bits (bit i) and (bit i) in both state representations at identical positions i = 3, 4, . . . are totally independent and uncorrelated. After just two iterations of the shift map σ, s and s may result in totally different, diverging observables σ(σ(s)) = [0.(bit 3)(bit 4)](bit 5) • • • and σ(σ(s )) = [0.(bit 3) (bit 4) ](bit 5) • • • . Suppose, as has been mentioned earlier, that the initial values are presumed, that is, hypothesized as chosen uniformly from the elements of a continuum, then almost all (that is, of measure one) of them are not representable by any algorithmically compressible number; in short, they are random (Sect. A.2 on p. 171).</p>
        <p>Thus in this scenario of classical, deterministic chaos the randomness resides in the assumption of the continuum; an assumption which might be considered a convenience (for instance, for the sake of applying the infinitesimal calculus). Yet no convincing physically operational evidence supporting the necessity of the full structure of continua can be given. If the continuum assumption is dropped, then what remains is Maxwell's and Poincaré's observation of the unpredictability of the behaviour of a deterministic system due to instabilities and diverging evolutions from almost identical initial states [349].</p>
        <p>There exist series solutions of the n-body problem [496,560,561]. From deterministic chaos theory -that is, from the great sensibility to changes in the initial values -it should be quite clear that the convergence of these series solutions could be extremely slow [170,171]. However, one could go one step further and argue that, at least for systems capable of universal computation, in general there need not exist any computable criterion of convergence of these series [477]. This can be achieved by embedding a model of (ballistic) universal computation into an n-body system [510].</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Partition Logics, Finite Automata and Generalized Urn Models</p>
        <p>Complementarity was first encountered in quantum mechanics. It is a phenomenon also understandable in classical terms; and although "it's not a complicated idea but it's an idea that nobody would ever think of" in analogy to entanglement [287] one might say what follows we shall present finite deterministic models featuring complementarity. The type of complementarity discussed in this chapter grew out of an attempt to understand quantum complementarity by some finite, deterministic, quasi-classical (automaton) model [373].</p>
        <p>We shall do this by sets of partitions L of a given set with more than two elements. Suppose one identifies arbitrary elements {x 1 , . . . , x k } of some partition with the proposition "The properties x 1 , or, . . ., or x k are true." Then each partition in L can be associated with a Boolean algebra or, synonymously, with a context, or block. Arbitrary partitions of L can be intertwined or pasted together [249,263,300,376] in their common elements. This pasting construction yields a partition logic.</p>
        <p>For the sake of getting a better intuition of partition logic and their relation to complementarity, two quasi-classical models will be discussed: (i) generalized urn models [577,578] or, equivalently [506,511], (ii) the (initial) state identification problem of finite deterministic automata [104,184,373,446,499] which are in an unknown initial state.</p>
        <p>Both quasi-classic examples mimic complementarity to the extent that even quasiquantum cryptography can be performed with them [509] as long as one sticks to the rules (limiting measurements to certain types), and as long as value indefiniteness is not a feature of the protocol [38,519], that is, for instance, the Bennett and Brassard 1984 protocol [56] can be implemented with generalized urn models, whereas the Ekert protocol [198] cannot. The (initial) state identification problem for finite deterministic (Mealy) automata is the following: suppose one is presented with a (blackbox containing a) single copy of a finite deterministic automaton whose specifications are completely given with the exception of the state it is initially in: find that initial state by the input/output analysis of experiments with that automaton.</p>
        <p>Then, as already pointed out by Moore, "there exists a [[finite and deterministic]] machine such that any pair of its states are distinguishable, but there is no simple experiment which can determine what state the machine was in at the beginning of the experiment" [373, Theorem 1, p. 138].</p>
        <p>Wright's generalized urn model can be sketched by considering black balls with symbols in different colours drawn simultaneously on it. Perception of these colours are all "exclusive" or "complementary" by assuming that one looks at the ball with (coloured) glasses which are capable of transmitting only a single colour. Therefore, only the symbol in the respective colour is visible; all the symbols in different colours merge with the black background and are therefore unrecognizable. Suppose there are a lot of balls of many types (with various colours and an equal number of symbols per colour) in an urn. The question or task is this: Suppose one single ball is drawn from that urn; what is this particular type of ball or "ball state?" Formally, a generalized urn model U = U, C, L , Λ is characterized as follows. Consider an ensemble of balls with black background colour. Printed on these balls are some colour symbols from a symbolic alphabet L. The colours are elements of a set of colours C. A particular ball type is associated with a unique combination of mono-spectrally (no mixture of wavelength) coloured symbols printed on the black ball background. Let U be the set of ball types. We shall assume that every ball contains just one single symbol per colour. (Not all conceivable types of balls; i.e., not all colour/symbol combinations, may be present in this ensemble, though.)</p>
        <p>Let |U | be the number of different types of balls, |C| be the number of different mono-spectral colours, |L| be the number of different output symbols. Consider the deterministic "output" or "lookup" function Λ(u, c) = v, u ∈ U , c ∈ C, v ∈ L, which returns one symbol per ball type and colour. One interpretation of this lookup function Λ is as follows. Consider a set of |C| eyeglasses build from filters for the |C| different colours. Let us assume that these mono-spectral filters are "perfect" in that they totally absorb light of all other colours but a particular single one. In that way, every colour can be associated with a particular eyeglass and vice versa.</p>
        <p>The following considerations (largely based on [506,511]) apply only to partition logics which have "enough" -that is, a separating set of -two-valued states. A logic L has a separating set of two-valued states if for every a, b ∈ L with a = b there is a two-valued state s such that s(a) = s(b); that is, different propositions are distinguishable by some state [523].</p>
        <p>The connection between those toy models and partition logics can be achieved by "inverting" the set of two-valued states as follows.</p>
        <p>1. In the first step, every atom of this lattice is indexed or labelled by some natural number, starting from "1" to "n", where n stands for the number of lattice atoms. The set of atoms is denoted by A = {1, 2, . . . , n}. 2. Then, all two-valued states of this lattice are labelled consecutively by natural numbers, starting from "v 1 " to "v r ", where r stands for the number of two-valued states. The set of states is denoted by V = {v 1 , v 2 , . . . , v r }. 3. Now partitions are defined as follows. For every atom, a set is created whose members are the index numbers or "labels" of the two-valued states which are "true" or take on the value "1" on this atom. More precisely, the elements p i (a) of the partition P j corresponding to some atom a ∈ A are defined by</p>
        <p>The partitions are obtained by taking the unions of all p i which belong to the same subalgebra P j . That the corresponding sets are indeed partitions follows from the properties of two-valued states: two-valued states (are "true" or) take on the value "1" on just one atom per subalgebra and ("false" or) take on the value "0" on all other atoms of this subalgebra. 4. Let there be t partitions labelled by "1" through "t". The partition logic is obtained by a pasting of all partitions P j , 1 ≤ j ≤ t. 5. In the following step, a corresponding generalized urn model or automaton model is obtained from the partition logic just constructed.</p>
        <p>a. A generalized urn model is obtained by the following identifications (see also [577, p. 271]).</p>
        <p>i. Take as many ball types as there are two-valued states; i.e., r types of balls. ii. Take as many colours as there are subalgebras or partitions; i.e., t colours. iii. Take as many symbols as there are elements in the partition(s) with the maximal number of elements; i.e., max 1≤ j≤t |P j | ≤ n. To make the construction easier, we may just take as many symbols as there are atoms; i.e., n symbols. (In most cases, much less symbols will suffice).</p>
        <p>Label the symbols by s l . Finally, take r "generic" balls with black background. Now associate with every measure a different ball type. (There are r two-valued states, so there will be r ball types.) iv. The ith ball type is painted by coloured symbols as follows: Find the atoms for which the ith two-valued state v i is 1. Then paint the symbol corresponding to every such lattice atom on the ball, thereby choosing the colour associated with the subalgebra or partition the atom belongs to. If the atom belongs to more than one subalgebra, then paint the same symbol in as many colours as there are partitions or subalgebras the atom belongs to (one symbol per subalgebra). This completes the construction. b. A Mealy automaton is obtained by the following identifications (see also [499, pp. 154-155]). i. Take as many automaton states as there are two-valued states; that is, r automaton states. ii. Take as many input symbols as there are subalgebras or partitions; i.e., t symbols. iii. Take as many output symbols as there are elements in the partition(s)</p>
        <p>with the maximal number of elements (plus one additional auxiliary output symbol " * ", see below); i.e., max 1≤ j≤t |P j | ≤ n + 1. iv. The output function is chosen to match the elements of the state partition corresponding to some input symbol. Alternatively, let the lattice atom a q ∈ A must be an atom of the subalgebra corresponding to the input i l . Then one may choose an output function such as</p>
        <p>Here, the additional output symbol " * " is needed. v. The transition function is r -to-1 (e.g., by δ(s, i) = s 1 , s, s 1 ∈ S, i ∈ I ), i.e., after one input the information about the initial state is completely lost. This completes the construction.</p>
        <p>The universe of possible partition logics [184,446,499,511] is huge; and so are the conceivable probability measures [521] on them. In what follows we shall restrict our attention to partition logics containing partitions with equal numbers of elements.</p>
        <p>Let us, for the sake of illustration, just mention as an example a set of partitions of the set {1, 2, 3}:</p>
        <p>The term "{1}" corresponds to the proposition "1 is true." Every partition forms a 2-atomic Boolean subalgebra. It results in three Boolean algebras "spanned" by the atoms {1}, not({1}) = {2, 3}, {2}, not({2}) = {1, 3}, and {3}, not({3}) = {1, 2}, which are not intertwined and thus form a horizontal sum of three Boolean subalgebras 2 3 . This is equivalent to a quantum logic of, say, spin-1 2 particles whose spin is measured along three distinct directions [501].</p>
        <p>Complementarity is obtained by realizing that one has to make choices: each choice of a particular partition corresponds to a type of measurement made. The set of (sometimes intertwined) partitions represents the "universe of conceivable measurements."</p>
        <p>The propositional structure depicted in Fig. 19.1(i) has no two-valued (admissible [3,5,6]) state: The supposition that one element is "1" forces the remaining two to be "0," thus leaving the "adjacent" block without a "1" (there cannot be only zeroes in a context). This means that it has no representation as a quasi-classical partition logic.</p>
        <p>The logic depicted in Fig. 19.1(ii) has sufficiently many (indeed four) two-valued measures to be representable by a partition logic [519]. Indeed, a concrete partition logic obtained by the earlier construction based on the inversion of the</p>
        <p>Another example [506,507,511] is a logic which is already mentioned by Kochen and Specker [314] (this is a subgraph of their Γ 1 discussed in Sect. 12.9.8.4) whose automaton partition logic is depicted in Fig. 19.2. There are 14 dispersion-free states which are listed in Table 12.4. The associated generalized urn model is listed in Table 19.1.</p>
        <p>With regards to quantum logic, partition logics share some common features but lack others. For instance, not all partition logics can be represented as sublogics of some quantum logic: as a counterexample take the partition logic depicted in Fig. 19.1(ii), which has no representation in R 3 . The central concern here is representability: since atoms in quantum logics can be identified with nonzero vectors or their associated projectors, the partition logic needs to have a geometric interpretation (embedding in vector space) preserving or rather representing the partition logical structure. Nevertheless, all finite sublogics of quantum logics with a separating set of twovalued states are equivalent to partition logics [139].</p>
        <p>On the other hand, the Kochen-Specker theorem (cf. Sect. 12.9.8.7 on p. 97; in particular, the quantum sublattice depicted in Fig. 12.6) asserts that there exist sublogics of quantum logics which have no two-valued state at all. As has already been noted earlier, in a very precise and formal way, this can be identified with either contextuality [517,518] or with value indefiniteness [286,401]. This is all "bad news for partition logics" because although these quantum mechanical sublogics can be embedded in some (even low-dimensional) vector space, they have no two-valued state at all -alas, a separating set of two-valued state would be needed for a construction or characterization of any partition logics. Indeed it is even possible to show that, with reasonable side assumptions such as noncontextuality, there exist constructive proofs demonstrating that there is no value definiteness -that is, no two-valued state -beyond a single proposition and its negation [3,5,6] (cf. Sect. 12.9.8.7 on p. 97). Whether or not partition logics have empirical relevance remains an open question.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Chapter 20</p>
        <p>Since theological nomenclature hardly belongs to the standard repertoire of physicists but will be used later, as some termini technici will be mentioned upfront. Thereby we will mainly follow Philipp Frank's (informal) definitions of gaps and miracles [219,220].</p>
        <p>In the theological context, creatio ex nihilo often refers to the 'initial boot up of the universe;' whereas creatio continua stands for the permanent intervention of the divine throughout past, present, and future. Alas, as we will be mainly interested with physical events, we shall refer to creatio ex nihilo, or just ex nihilo, as something coming from nothing; in particular, from no intrinsic [500] causation (and thus rather consider the theological creatio continua; apologies for this potential confusion). Ex nihilo denies, and is in contradiction, to the principle of sufficient reason (cf Sect. 17.1, p. 135), stating that nothing is without intrinsic causation, and vice versa.</p>
        <p>According to Frank [219,220,Sect. II,12], a gap stands for the incompleteness of the laws of nature, which allow for the occurrence of events without any unique natural (immanent, intrinsic) cause, and for the possible intervention of higher powers [219,220,Sect. II,9]: "Under certain circumstances they do not say what definitely has to happen but allow for several possibilities; which of these possibilities comes about depends on that higher power which therefore can intervene without violating laws of nature."</p>
        <p>Many scientists, among them Poisson, Duhamel, Bertrand, and Boussinesq [162,494], have considered such gaps as a possibility of free will even before the advent of quantum mechanics. Maxwell may have anticipated a scenario related to deterministic chaos (cf. Chap. 18, p. 141) by considering singular points and instability of motion with respect to very small variations of initial states, whereas Boussinesq seemed to have stressed rather the nonuniqueness of solutions of certain ordinary differential equations [162,343,494]. This is different from a direct breach or 'rapture' of the laws of nature [219,220,Sect. II,10]; also referred to as ontological gap by a forced intervention in the otherwise uniformly causal connection of events [438, Sect. 3.C.3, Type II]. An example for an ontological gap would be the sudden ad hoc turn of a celestial object which would otherwise have proceeded along a trajectory governed by the laws of inertia and gravitation.</p>
        <p>Sometimes, certain correlations are subjectively and semantically experienced as synchronicity, that is, with a purpose -the events are not causally connected but "stand to one another in a meaningful relationship of simultaneity" [296,299]. A more personal example is Jung's experience of a solid oak table suddenly split right across, soon followed by a strong steel knife breaking in pieces for no apparent reason [297, 298, pp. 111-2, 104-5].</p>
        <p>In what follows we shall adopt Frank's conceptualization of a miracle [219, 220, Sect. II, 15] as a gap (in Frank's sense cited above) which is exploited according to a plan or purpose; so a 'higher power' interacts via the incompleteness (lack of determinacy) of the laws of nature to pursue an intention.</p>
        <p>Note that this notion of miracle is different from the common acceptation quoted by Voltaire, according to which a miracle is the violation of divine and eternal laws [549,Sect. 330].</p>
        <p>An oracle (if it exists) is conceptualized by an agent capable of a decision or an emanation (such as a random number) which cannot be produced by a universal computer. Again, we take up Frank's conception of a gap by realizing oracles via gaps.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>In what follows the term transcendence refers to an entity or agent beyond all physical laws. (It is not used in the Kantian sense.) In contrast, immanence refers to all operational, intrinsic physical means available to embedded observers [500,538] from within some universe.</p>
        <p>Suppose that transcendent agents, interact with a(n) (in)deterministic universe via suitable interfaces. In what follows we shall refer to the transcendental universe as the beyond.</p>
        <p>For the sake of metaphorical models, take Eccles' mind-brain model [191], or consider a virtual reality, and, more particular, a computer game. In such a gaming universe, various human players are represented by avatars. There, the universe is identified with the game world created by an algorithm (essentially, some computer program), and the transcendental agent is identified with the human gamer. The interface consists of any kind of device and method connecting a human gamer with the avatar. Like the god Janus in the Roman mythology, an interface possesses two faces or handles: one into the universe, and a second one into the beyond.</p>
        <p>Human players constantly input or inject choices through the interface, and vice versa. In this hierarchical, dualistic scenario, such choices need not solely (or even entirely) be determined by any conditions of the game world: human players are transcendental with respect to the context of the game world, and are subject to their own universe they live in (including the interface). Nevertheless the game world itself is totally deterministic in a very specific way: it allows the player's input from beyond; but other than that it is created by a computation. One may think of a player as a specific sort of indeterministic (with respect to intrinsic means) oracle, or subprogram, or functional library.</p>
        <p>Another algorithmic metaphor is an operating system, or a real-time computer system, serving as context. (This is different from a classical Turing machine, whose emphasis is not on interaction with some user-agent.) The user is identified with the agent. Any user not embedded within the context is thus transcendent with respect to this computation context. In all these cases the real-time computer system acts deterministically on any input received from the agent. It observes and obeys commands of the agent handed over to it via some interface. An interface could be anything allowing communication between the real-time computer system and the (human) agent; say a touch screen, a typewriter(/display), or any brain-computer interface.</p>
        <p>The mere existence of gaps in the causal fabric cannot be interpreted as sufficient evidence for the existence of providence or free will, because these gaps may be completely supplied by creatio continua.</p>
        <p>As has already been observed by Frank [219, Kapitel III, Sects. 14 and 15], in order for any miracle or free will to manifest itself through any such gap in the natural laws, it needs to be systematic, according to a plan and intentional (German planmäßig). If there were no possibilities to inject information or other matter or content into the universe from beyond through such gaps, there would be no possibility to manipulate the universe, and therefore no substantial choice.</p>
        <p>Alas, intentionality may turn out to be difficult or even impossible to prove. How can one intrinsically decide between chance on the one hand, and providence, or some agent executing free will through the gap interface, on the other hand? The interface must in both cases employ gaps in the intrinsic laws of the universe, thereby allowing steering and communicating with it in a feasible, consistent manner. That excludes any kind of immanent predictability of the signals emanating from it. (Otherwise, the behaviour across the interface would be predictable and deterministic.) Hence, for an embedded observer [538] employing intrinsic means which are operationally available in his universe [500], no definite criterion can exist to either prove or falsify claims regarding mere chance (by creatio continua) versus the free choice of an agent. Both cases -free will of some agent as well as complete chance -express themselves by irreducible intrinsic indeterminism.</p>
        <p>Suppose an agent or gamer is immersed in such dualistic environment and experiences "both of its sides" through the interface but has no knowledge thereof. (C.f the metaphors "we are the dead on vacation" by Godard [241], or of the "brain in the vat" employed by Descartes [166,Second meditation,[26][27][28][29] and Putnam [422, Chap. 1], among others.) Then the agent's knowledge of the beyond amounts to ineffability [294]. However, ineffability is neither necessary nor sufficient for dualism; and could also be a mere subjective illusion, constructed by the agent in a deparate attempt to make sense and create meaning from his sensory perceptions, very much like brain hallucinations [419]. And yet, ineffability might present some hint on metaphysics.</p>
        <p>For the sake of an example, suppose for a moment that we would possess a sort of 'Ark of the Covenant,' an oracle which communicates to us the will of the beyond, and, in particular, of divinity. How could we be sure of that? (Sarfatti, in order to investigate the paranormal, attempted to build what he called an Eccles telegraph by connecting a radioactive source to a typewriter.) This situation is not dissimilar to problems in recognizing hypercomputation, that is, computational capacities beyond universal computation [334]; in particular also to zero knowledge proofs [65,425].</p>
        <p>This chapter is for those who neither want to bother with details nor have time for the expositions and the explanatory rants of previous parts; or as a teaser to go deeper into the subject and to want to know more.</p>
        <p>First and foremost, from the rational, scientific point of view, there is and never will be anything like "absolute randomness." Knowledge of absolute randomness, if it "exists" in some platonic realm of ideas, is ineffable and thus strictly metaphysical and metamathematical, as it is blocked by various theorems about the impossibility of induction (cf. Chap. 7, p. 35ff), forecasting (cf. Chap. 6, p. 29ff), and representation (cf. Sect. A.4, p. 173ff). Any proof of such theorems, and thus their validity, is only relative to the assumptions made.</p>
        <p>Claims regarding "absolute randomness" -and, for the same reasons, "absolute determinism" -in physics should therefore be met with utmost skepticism. Such postures might serve as a heuristic principle, a sign-post, but they do not signify anything beyond the contemporary, most likely transient (some might even say spurious), worldview, as well as the personal and subjective preferences of the individual issuing them. Like all constructions of the mind and society, physical theories are suspended in free thought -an echo chamber of sorts.</p>
        <p>Whoever trusts a physical random number generator has to trust the assurances of the physical authorities that it indeed performs as claimed -in this case, that it produces random numbers. The authorities in turn base their judgement on personal inclinations [68, p. 866] and in metaphysical assertions [589]; as well as on their trust on the theories and models of functioning of such devices. Theories and models are considered trustworthy if they satisfy a "reasonable" and "meaningful" catalogue of criteria; but never more than that.</p>
        <p>One such reasonable criterion is the requirement that it should at least in principle be possible to locate the (re)source for randomness or indeterminism. Unfortunately both in quantum mechanics, as well as for classical systems, there is no such consolidated agreement about the physical resources of randomness. Therefore, whenever a physical random number generator is employed, one has to bear in mind the insecure, and means relative, performance of this device. It is not that, pragmatically and for all practical purposes, it would not be usable. But it could fail in particular circumstances one has little idea about, and control of.</p>
        <p>There are three classes or types of quantum indeterminism: complementarity (cf. Sect. 12.3 and Chap. 19), value indefiniteness (often, referred to as contextuality after the realist Bell; cf. Sect. 12.9.8.7, p. 97ff), as well as single measurement outcomes and events; all of them tied to the quantum measurement problem (cf. Sect. 12.10, p. 118ff). Thus quantum random number generators are subject to some form of the quantum measurement problem, which lies at the heart of an ongoing debate -a debate which has been declared (re)solved or superfluous by various self-proclaimed authorities for a variety of conflicting reasons. Alas, quantum mechanics, despite being immensely useful for the prediction and comprehension of certain phenomena, formally operates with an inconsistent set of rules; in particular, pertaining to measurement. As has already been pointed out by von Neumann, the assumption of irreversible measurements contradicts the unitary deterministic evolution of the quantum state. (Inconsistencies, even in the core of mathematics, such as in Cantor's set theory, should be rather a reason for consideration and prudence but not cause too much panic -after all, as noted earlier, those constructions of our mind are suspended in our free thought.) Some supposedly "active" elements such as beam splitters are represented by perfectly deterministic (unitary, that is, distance preserving permutations, such as the Hadamard gate) evolutions (cf. Chap. 11 and Sect. 12.5). Therefore they cannot be directly identified as quantum resource for indeterminism.</p>
        <p>The measurement process in quantum mechanics appears to be related to entanglement and individuation: in order to be able to know from each other, the measurement apparatus has to acquire knowledge about the object; and in order to do so, the former has to interact with the latter. Thereby entanglement in the form of relational properties of object and apparatus is created. Because of the permutativity (one-to-one-ness) of the entire process (resulting in a sort of zero-sum game) these relationally definite properties (or, by another term, statistical correlations) come at the price of the indefiniteness of the individual, constituent parts -the original object as well as the measurement device are in no definite individual state any longer. If one forces individuality upon them (by some later measurement on the individual parts), then the outcome cannot be totally (but may be partly) pre-determined by the state of the constituent parts before that measurement. Thereby it may be justified to say that "the measurement creates the outcome which is indeterminate before." But this is a rather trivial statement expressing the fact that the outside environment with its supposedly huge number of degrees of freedom, in particular also the measurement device, has contributed to the outcome. The author's impression is that Bohr and his followers may never have understood the true reason for value indefiniteness: the scarcity and constancy of information encoded into the quantum state; and the entanglement across the Heisenberg cut between object and measurement device. This scarcity also shows up in "static" Kochen-Specker type theorems [6,314,401] expressing the fact that only a single maximal observable or context is defined at any time.</p>
        <p>Classical (in)determinism depends on its definition, and on the assumptions made. One of these assumptions is the existence of the continuum -not only as formal convenience but as a physical entity. Almost all elements of a continuum are random (cf. Sect. A.2, p. 171ff). Any computable form of evolution "revealing" the algorithmic information content "buried" in a single supposedly random real physical entity (e.g., initial values) corresponds to a form of deterministic chaos (cf. Chap. 18, p. 141ff). If the assumption of the physical existence of the continuum is dropped in favour of constructive, computable entities, then what remains from these indeterministic scenarios is the high sensitivity of the system behaviour on variations of initial states.</p>
        <p>Another form of model-induced classical indeterminism is due to representation and formalization of classical physical systems in terms of differential equations. In such cases the question of uniqueness of its solutions is crucial. Nonunique solutions indicate indeterminism. However, the requirement of Lipschitz continuity guarantees uniqueness in many cases which appear to be indeterministic (due to the possibility of weak solutions) without this property (cf. Sect. 17.4, p. 137ff).</p>
        <p>Should one prefer physical (re)sources of randomness over mathematical pseudorandom? Of course, "anyone who considers arithmetical methods of producing random digits is . . . in a state of sin" [553, p. 768]. And yet, some desired features of randomness can be formally certified even for such computable entities.</p>
        <p>For instance, take Borel normality; that is, the property that every subsequence of length n occurs in a "large" b-ary sequence with frequency b -n . Almost all real numbers are normal to a given base b; in particular, all random sequences are Borel normal [102]. Yet, individual (even computable) numbers are hard to "pin down" as being normal; and no well-known mathematical constant, such as e or log 2, is known to be normal to any integer base. Also the normality of π, the ratio of the circumference to the diameter of a "perfect" (platonic) circle, remains conjectural [26], although particular digits are directly computable [25]. Von Neumann's paper [553] quoted earlier contains a way to eliminate bias (and thus establish Borel normality up to length 1) of a binary sequence (essentially a partitioning of the sequence into subsequences of length 2, followed by a mapping of 00, 11 → ∅, 01 → 0, 10 → 1); but only if this sequence is generated by independent physical events. Physical independence may be easy to obtain for all practical purposes, but difficult in principle.</p>
        <p>On the other hand, Champernowne's number 0.12345678910. . ., obtained by concatenating the decimal representations of the natural numbers in order, as well as the Copeland-Erdos constant 0.2357111317192329. . ., obtained by concatenating the prime numbers in order, are both Borel normal in base 10. So, if Borel normality suffices for the particular task, then it might be better to consider such carefully chosen pseudo-random numbers (cf. Ref. [110] for comparisons with certain quantum random sources).</p>
        <p>There exist situations which are perplexing yet not very helpful for practical purposes: Chaitin's Ω (cf. Sect. A.6, p. 176ff) is also Borel normal in any base, and additionally it is provable random. Algorithms for computing the very first couple of digits of Ω [109,111] exist; alas the rate of convergence of the sum yielding Ω is so bad (in terms of time and other computational capacities worse than any computable function of the d-ary place) it is incomputable.</p>
        <p>Whatever one's personal inclinations toward (in)determinism may be -one might characterize our situation either as an ocean of unknowns with a few islands of preliminary predictables; or, conversely, as a sea of determinism with the occasional islands or gaps of an otherwise lawful behaviour -every such inclination remains strictly means relative, metaphysical and subjective. Maybe such preferences says more about the person than the situation; because a person's stance is often determined by the subconscious desires, hopes and fears driving that individual. Choose one, and choose wisely for your needs; or even better, "if you can possibly avoid it [211, p. 129]," choose none, and remain conscious about the impossibility to know.</p>
        <p>Let me finally quote the late Planck [410] concluding that [409, p. 539] (see also Earman [186, p. 1372]) ". . . the law of causality is neither right nor wrong, it can be neither generally proved nor generally disproved. It is rather a heuristic principle, a sign-post (and to my mind the most valuable sign-post we possess) to guide us in the motley confusion of events and to show us the direction in which scientific research must advance in order to attain fruitful results. As the law of causality immediately seizes the awakening soul of the child and causes him indefatigably to ask "Why?" so it accompanies the investigator through his whole life and incessantly sets him new problems. For science does not mean contemplative rest in possession of sure knowledge, it means untiring work and steadily advancing development." 1Planck also emphasized the joy of and the motivation from the unknown [411]: "We will never come to a completion, to the final. Scientific work will never cease. It would be bad if it stopped. For if there were no more problems one would put his hands in his lap and his head to rest, and would not work anymore. And rest is stagnation, and rest is death -in a scientific sense. The fortune of the investigator is not to have the truth, but to gain the truth. And in this progressive successful search for truth, lies the real satisfaction. Of course, the search for itself is not satisfactory. It must be successful. And this successful research is the source of every effort, and also the source of every spiritual enjoyment. When the source dries up, when the truth is found, then it is over, then one can fall asleep mentally and physically. But that is taken care of, that we don't experience this, and therein persists our happiness." 2Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>For the sake of an orientation for the reader a very brief expose of formal definitions related to indeterminism and randomness is offered. Mostly the concepts will be presented without proofs.</p>
        <p>Let us start with the set R of real numbers [see for instance, Refs. [179,194,488], or Ref. [281] (in German)]. Reals will be coded by, or written as, infinite decimals. By Cantor's theorem using diagonalization [488,Sect. 3.5.1, the set of real numbers R (or, say, the real unit interval [0, 1]) is nondenumerable. More generally -because one needs not proceed along the diagonal and process the entries therein to produce a real which does not occur in any type of enumeration of reals -the same result can be obtained by applying self-reference and the existence of some map without any fixed points [579, p. 368], As a result R cannot be brought into a one-to-one correspondence with the natural numbers N (such as, for instance, the integers Z or the rationals Q). Sets of this R type will be called continua. Sets of type N will be called denumerable.</p>
        <p>But we can go quantitatively further than that: we can show that, from the point of view of measure theory, denumerable sets are "meagre;" that is, almost all reals are not in any such set of denumerable numbers [488,Sect. 3.5.2,. For a sketch of a proof suppose that we are "covering" the i'th element of the denumerable set with an interval δ -i ε, with 1 &lt; δ &lt; ∞ and ε a tiny number. Summing over all such cover intervals can be readily performed, as the respective set is denumerable. By the geometric series summation formula, the entire length covered is at most (for nonoverlapping intervals) εδ -1 /(1 -δ -1 ). We can make this covering length arbitrary small by making either δ larger or ε smaller. That is, in this measure theoretic, quantitative, sense, "almost all" reals are not in any particular denumerable set.</p>
        <p>Next we define a computable real by the property that it is produced -that is, it is the output of -some algorithm "running" on a (supposedly universal) computer. The set of computable reals is denumerable because we can find ways to enumerate all of them: for a sketch of the idea how to perform this task, imagine the numbers produced by successively generated (by their code lengths in lexicographic order) algorithms at successive times.</p>
        <p>As a consequence we find that "almost all" reals are incomputable. That is, if one considers the real unit interval as a "continuum urn" -one needs the axiom of choice in order to "draw" a general element of this urn, as no computable "handle" exists to fetch it -then with probability 1 it will be incomputable.</p>
        <p>It may appear amazing that all denumerable sets are so "meagre," and its members so "thinly distributed and embedded" in the real continuum. In particular, such sets -such as the rational numbers and also the computable ones which include "many more" irrational numbers -are dense in the sense that in-between two arbitrary numbers a and b with a &lt; b of a dense set there always lies another number c in that set, such that c is larger than a but smaller than b; that is, a &lt; c &lt; b.</p>
        <p>A real can be defined to be random if its (decimal) expansion cannot be algorithmically compressed [103,122,127,178,315,316,325,326,337]. In particular [122], "it may perhaps not appear entirely arbitrary to define a patternless or random finite binary sequence as a sequence which, in order to be calculated, requires, roughly speaking, at least as long a program as any other binary sequence of the same length."</p>
        <p>Randomness via algorithmic incompressibility implies that the respective random sequences or random reals [353] "possess all conceivable computable statistical properties of randomness"; that is, they pass all conceivable computable statistical tests of randomness. What is such a "conceivable computable statistical test?" It is based on all conceivable computable laws -that is, all algorithms. More precisely, a single conceivable computable statistical test is based on a single algorithm: it is the hypothesis that the random sequences or random real cannot be generated by this algorithm. Because if it were, it would be algorithmically compressible by that algorithm. Herein lies the connection between statistical test and algorithm: that any algorithm constitutes an algorithmic test against nonrandomness (algorithmic compressibility); and vice versa, every computable statistical test is representable by an algorithm.</p>
        <p>Let us be more precise and, for the sake of avoiding difficulties related to subadditivity [127], restrict ourselves to prefix or instantaneous program codes [126,335] which have the "prefix (free) property." This property requires that there is no whole code word in the system that is a prefix (initial segment) of any other code word in that same system.</p>
        <p>Define the algorithmic information (content), or, used synonymously, the (Kolmogorov) program-size complexity, or the information-theoretic complexity of an individual object is a measure or criterion how difficult it is to algorithmically specify (but not in terms of time it takes to produce) that object [127]. In particular, the algorithmic information content I (x) of binary string x as the size/length (encoded in bits, that is, binary digits) of the shortest/smallest program running on some (Turingtype) universal computer U to calculate x, plus the information content I (|s|) of the length |s| of this sequence (since this also contributes); that is, if |s| stands for the length of the binary sequence s n in bits, and the order O( f ) of f stands for a function whose absolute value is bounded by a constant times f [and thus O(1) just stands for a constant], then of length less than nk. Thus the number of strings of length n and algorithmic information content less than nk decreases exponentially as k increases, and increases exponentially as n increases."</p>
        <p>As a consequence, most of the sequences of length n are of algorithmic information content close to n, and, according to the definition earlier, appear random. Therefore, if one chooses some nonalgorithmic method generating such sequences -if they are operational, that is, they can be produced by some physically process; say, by tossing a fair coin and hoping for the best that this process is not deterministic or biased as alleged in Ref. [169] -then chances are high that the algorithmic information content of such a string will be as long as the length of that sequence. Pointedly stated: "grabbing and picking" a random real from the continuum with nonalgorithmic means, facilitated by the axiom of choice, will almost always yield a random real.</p>
        <p>By reduction to the halting problem it can be argued that the algorithmic information content I in general is incomputable. Because computability of the algorithmic information content I (s n ) would require that it would be possible to compute whether or not particular programs of length up to n + O(n) halt (after output of s n ). But this is clearly impossible for large enough (and even for small) n; see also the busy beaver function discussed later.</p>
        <p>It is therefore impossible that in general it is possible to prove (non)randomness or (in)computability of a particular individual infinite sequence. (Any particular finite sequence is provable computable, as by the earlier mentioned tabulation technique, an algorithm outputting it can be constructed by putting it directly into a program as a table. This is no contradiction to the earlier definition of randomness of a finite sequence because this is means relative and not absolute.) That is, all statements (e.g., Ref. [589]) such as "this string is irreducibly random," at least as far as they relate to ontology, are provably unprovable hypotheses. Epistemically they are inclinations at best, as expressed by Born's statement [68, p. 866] (English translation in [570, p. 54]) "I myself am inclined to give up determinism in the world of atoms." At worst they are ideologies which remain unfalsifiable.</p>
        <p>In a quantitative sense one could go beyond the Gödel-Turing reduction. Let us follow Chaitin [124] and employing Berry's paradox, as reported by Russell [273, p. 153, contradiction (4), footnote 3]: "But 'the least integer not nameable in fewer than nineteen syllables' is itself a name consisting of eighteen syllables; hence the least integer not nameable in fewer than nineteen syllables can be named in eighteen syllables, which is a contradiction."</p>
        <p>Chaitin's paradoxical construction which is based upon the Berry paradox can be expressed by the following sentence [124,127] which cannot be valid: "the program which yields the shortest proof that its algorithmic information content is much greater than its length, say, 1 billion bits."</p>
        <p>Because such a program, if it existed, purports to prove that its algorithmic information content is much greater than its length, say, 1 billion bits. And yet, this statement is quite short, and certainly less than a billion bits. This is contradictory; and as a consequence no such program can exist. Stated differently: For every formal system deriving statements of the form I (s) &gt; n, there is a number k such that no such statement is provable using the given rules of that formal system for any n &gt; k [161, pp. 265-266]. k can be called the "strength" of such a formal system. This strength is a limiting measure for the capacity of the formal system to prove statements about the algorithmic information content of sequences.</p>
        <p>One way of interpreting this result is in terms of independence: because more axioms specify more theorems, different axioms specify different theorems. That is, it is the choice of the formalist which deductive mathematical universe is created by the assumptions [270, p. 38].</p>
        <p>In view of the aforementioned incompleteness and independence result one may ask [132, p. 148]: "How common is incompleteness and unprovability? Is it a very bizarre pathological case, or is it pervasive and quite common?" Indeed, just as the set of computable reals is "meagre" in the set of reals, so is the set of provable (by constructive, algorithmic methods) theorems "meagre" with respect to all true theorems of mathematics. This has been proven in a topological sense of "meagre" in the context of Gödel-Turing type incompleteness [105] (and not in the sense of independence of, say, the continuum hypothesis).</p>
        <p>Rice's theorem [432] asserts that all non-trivial, (semantic) functional properties of programs are undecidable. A functional property is one (i) describing how some functions performs in terms of its input/output behavior, and (ii) which is non-trivial in the sense that some (of all perceivable) programs which have this input/output behavior, and other programs which don't. Rice's theorem can be algorithmically proven by reduction to the halting problem: Suppose there is an algorithm A deciding whether or not any given function or algorithm B has any functional property. Then we can define another program C which first solves the halting problem for some other arbitrary function D, clears the memory, and consecutively executes a program E with has the respective functional property decided by A. As long as D halts, all may go well. But if D does not halt, the program C never clears the memory, and can never execute a program E with the respective functional property. Now, if one inserts C into A, in order to be able to decide (positively) about the functional property of E -and thus of C -A would have to be able to solve the halting problem for D first; a task which is provable impossible by algorithmic means.</p>
        <p>As Yanofsky observes this bears some similarity to the downward Löwenheim-Skolem [580, footnote 20, p. 375], "stating that if there is a consistent way of using a language [[statements in mathematics . . . written with a finite set of symbols]] to talk about such a system [[with an uncountably infinite number of elements]], then that language might very well be talking about a system with only a countably infinite number of elements. That is, the axioms might be intended for discussing something uncountably infinite, but we really cannot show that it is has more than a countably infinite number of elements."</p>
        <p>The program lengths | p i | of q algorithms p i , i = 1, . . . , q encoded by binary prefix free codes on a universal computer satisfies the Kraft inequality</p>
        <p>These bounds are motivation to define Chaitin's halting probability [126,135,136] Ω by the sum of the weighted length of all binary prefix free encoded programs p on a given universal computer which halt:</p>
        <p>Ω is Borel normal in any base [123], and "highly incomputable" as it requires the solution to all halting problems. Conversely, knowledge of Ω, at least up to some degree, entails the solution of decision problems associated with halting problems: for instance, the Goldbach conjecture ("every even number greater than 2 can be represented as the sum of two primes") can be rephrased as a halting problem by parsing through all cases and halting if one of them fails.</p>
        <p>Despite this obvious computational hardness, the initial bits of Ω can be computed, or at least estimated up to some small degree [111,129,134]. This is related to the fact that very small-size programs still "converge fast" -that is, they soon halt -if ever; and, because of the exponentially decreasing weight with length, those programs contribute more to Ω as longer ones. But because of the recursive unsolvability of the halting problem there exists no computable rate of convergence. In particular, as we shall see next, halting times grow faster than every computable function of program length.</p>
        <p>Suppose one considers all programs (on a particular computer) up to length n. The busy beaver function Σ(n) of n is the largest number producible by such a programs forever. Hence, because of the recursive unsolvability of the halting problem the maximal halting time cannot be a computable function. As a consequence, the upper bounds for the recurrence of any kind of physical behaviour can be expressed in terms of the busy beaver function [499]. In particular, for deterministic systems representable by n bits the maximal recurrence time grows faster than any computable number of n. This maximal estimate related to possible behaviours may be interpreted quite generally as a measure of the impossibility to predict and forecast such behaviours by algorithmic means.</p>
        <p>Just as for Ω, knowledge of busy beaver function and thus the maximal halting time at least up to some degree, entails the solution of decision problems associated with halting problems. But these capacities, at least with computable means, are forever blocked by recursion theoretic incomputability.</p>
        <p>Chaitin's independence theorem discussed earlier imposes quantitative bounds on formal expressability: essentially [that is, up to O(1)] it is impossible to "squeeze out" (in terms of proofs of theorems, and with a caveat [325]) of a formal system much more than one has put in. If one wants more provable theorems one has to assume more. There appears to be no "pay once eat all scenario" envisaged by Hilbert. This, flamboyantly speaking, "garbage in, garbage out" situation fits well with means relativity and intrinsic perception of embedded observers: as there is no external point of view from which to execute omniscience the only possibility is to perform relative to the (intrinsic) means available. There is, however, another option not excluded by Chaitin's limiting theorems: the possibility to obtain a system of arbitrary algorithmic information content by considering subsequences of infinite sequences interpreted as formal systems, or as axioms thereof. There are two extreme scenarios which could be imagined: in the first scenario, a "primordial chaos" is taken as a resource. In the second scenario, the continuum of all infinite binary sequences 2 ω is approximated by a nonterminating process of generating it.</p>
        <p>In both cases partial sequences could in principle be taken as a basis representation for axiomatic systems. And in both cases the encoded axiomatic systems are potentially infinite, with an unlimited algorithmic information content. for the "+" and "-" states along θ and ϕ, respectively. By setting all the phases and angles to zero, one obtains the original orthonormalized basis {|-, |+ }.</p>
        <p>In order to evaluate Boole's classical conditions of possible experience, and check for quantum violations of them, the classical probabilities and correlations entering those classical conditions of possible experience have to be compared to, and substituted by, quantum probabilities and correlations derived earlier. For example, for n spin-q = = q ++ + q --= 1 2 1 + E q,2,2 = 1 2 {1 -[cos θ 1 cos θ 2sin θ 1 sin θ 2 cos(ϕ 1 -ϕ 2 )]} , q = = q +-+ q -+ = 1 2 1 -E q,2,2 = 1 2 {1 + [cos θ 1 cos θ 2 + sin θ 1 sin θ 2 cos(ϕ 1 -ϕ 2 )]} .</p>
        <p>(B.9)</p>
        <p>Finally, the quantum mechanical correlation is obtained by the difference q =q = ; i.e., E q,2,2 (θ 1 , ϕ 1 , θ 2 , ϕ 2 ) = = -[cos θ 1 cos θ 2 + cos(ϕ 1 -ϕ 2 ) sin θ 1 sin θ 2 ] .</p>
        <p>(B.10)</p>
        <p>By setting either the azimuthal angle differences equal to zero, or by assuming measurements in the plane perpendicular to the direction of particle propagation, i.e., with θ 1 = θ 2 = π 2 , one obtains</p>
        <p>The single particle spin one angular momentum observables in units of are given by [447]</p>
        <p>(B.12)</p>
        <p>Again, the angular momentum operator in arbitrary direction θ, ϕ is given by its spectral decomposition</p>
        <p>Consider the two spin-one particle singlet state</p>
        <p>The spin three-half angular momentum observables in units of are given by [447]</p>
        <p>terms of the sum decomposition (11.9) by P F = 5 i=1 |e i e i |+|e 6 e 7 |+|e 7 e 6 |+ |e 8 e 8 |.</p>
        <p>is a sequence of mutually orthogonal elements in L -in particular, this applies to atoms within the same context (block, Boolean subalgebra) -then the two-valued state is additive on those elements a i ; that is, additivity holds:</p>
        <p>u s λ 1 + λ 2 + λ 3 s λ 7 + λ 8 + λ 9 + λ 10 + λ 11 u s λ 4 + λ 5 + λ 6 s λ 1 + λ 3 + λ 9 + λ 10 + λ 11 u s λ 2 + λ 7 + λ 8 s λ</p>
        <p>German original: ewige Wiederkunft[373]. vii</p>
        <p>German original[225]: "Er wird leicht in die Versuchung geraten, was er in dumpfer Selbstwahrnehmung von den Eigentümlichkeiten seiner eigenen Person erkennt, als allgemeingültige Theorie in die Wissenschaft hinauszuprojizieren, er wird die psychoanalytische Methode in Misskredit bringen und Unerfahrene irreleiten."Preface ix</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_4</p>
        <p>Interesting though that if, instead of on a horse, Münchhausen would have ridden a loaded cannon, then by firing the cannon ball towards the ground might have helped.</p>
        <p>German original [10, Chap. 2, p. 15]: "Wenn man für alles eine Begründung verlangt, muss man auch für die Erkenntnisse, auf die man jeweils die zu begründende Auffassung -bzw. die betreffende Aussagen-Menge -zurück geführt hat, wieder eine Begründung verlangen". © The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_5</p>
        <p>On What Is Entirely Hopeless</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_6</p>
        <p>Forecasting and Unpredictability</p>
        <p>What if There Are No Laws? Emergence of Laws</p>
        <p>German original[225]: "Er wird leicht in die Versuchung geraten, was er in dumpfer Selbstwahrnehmung von den Eigentümlichkeiten seiner eigenen Person erkennt, als allgemeingültige Theorie in die Wissenschaft hinauszuprojizieren, er wird die psychoanalytische Methode in Misskredit bringen und Unerfahrene irreleiten".</p>
        <p>German original [550, p. 537]: ". . . die Hauptsache ist sozusagen durch Gelehrsamkeit verschüttet." © The Author(s)</p>
        <p>K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_10</p>
        <p>"Shut Up and Calculate"</p>
        <p>Evolution by Permutation</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_12</p>
        <p>Quantum Mechanics in a Nutshell</p>
        <p>German original: "Ist I im Zustande ϕ(q) und I I im Zustande ξ(r ), so ist I + I I im Zustande Φ(q, r ) = ϕ(q)ξ(r ). Ist dagegen I + I I in einem Zustande Φ(q, r ), der kein Produkt ϕ(q)ξ(r ) ist, so sind I und I I Gemische, aber Φ stiftet eine ein-eindeutige Zuordnung zwischen den möglichen Werten gewisser Größen in I und in I I . [554, Sect. VI.2, p. 232] . . . bei alleiniger Kenntnis . . . der getrennten Systeme I und I I , gehen alle "Wahrscheinlichkeitsabhängigkeiten", die zwischen denn beiden Systemen noch bestehen können, verloren. Wenn man aber sowohl den Zustand von I als auch denjenigen von I I genau kennt, kommen "Wahrscheinlichkeitsabhängigkeiten" nicht in Frage, und man kennt auch I + I I genau [554, Sect. VI.2, p. 227]".</p>
        <p>German original[452, Sect. 10, p. 827] "Das Ganze ist in einem bestimmten Zustand, die Teile für sich genommen nicht."</p>
        <p>German original[452, p. 811]: "Es gibt in keinem Augenblick ein Kollektiv klassischer Modellzustände, auf das die Gesamtheit der quantenmechanischen Aussagen dieses Augenblicks zutrifft."</p>
        <p>German original[554, Sect.</p>
        <p>6.3, p 233], ". . . das Resultat der Messung ist unbestimmt, weil der Zustand des Beobachters vor der Messung nicht genau bekannt ist. Es wäre denkbar, da ein solcher Mechanismus funktioniert, denn die Informiertheit des Beobachters über den eigenen Zustand könnte naturgesetzliche Schranken haben."</p>
        <p>German original [68, p. 866]: "Vom Standpunkt unserer Quantenmeehanik gibt es keine Größe, die im Einzelfalle den Effekt eines Stoßes kausal festlegt; aber auch in der Erfahrnng haben wir bisher keinen Anhaltspunkt dafür, daß es innere Eigensehaften der Atome gibt, die einen bestimmten Stoßerfolg bedingen. Sollen wir hoffen, später solche Eigenschaften (etwa Phasen der inneren Atombewegungen) zu entdecken und im Einzelfalle zu bestimmen? Oder sollen wir glauben, daß die Übereinstimmung von Theorie und Erfahrung in der Unfähigkeit, Bedingungen für den kausalen Ablauf anzugeben, eine prästabilierte Harmonie ist, die auf der Nichtexistenz solcher Bedingungen beruht? Ich selber neige dazu, die Determiniertheit in der atomaren Welt aufzugeben." © The Author(s)</p>
        <p>K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_13</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_14</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_15</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_16</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_17</p>
        <p>Classical (In)Determinism</p>
        <p>Partition Logics, Finite Automata and Generalized Urn Models</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_20</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_21</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7_22</p>
        <p>(De)briefing</p>
        <p>In German[410, p.</p>
        <p>26]: ". . . das Kausalgesetz ist weder richtig noch falsch, es ist vielmehr ein heuristisches Prinzip, ein Wegweiser, und zwar nach meiner Meinung der wertvollste Wegweiser, den wir besitzen, um uns in dem bunten Wirrwarr der Ereignisse zurechtzufinden und die Richtung anzuzeigen, in der die wissenschaftliche Forschung vorangehen muss, um zu fruchtbaren Ergebnissen zu gelangen. Wie das Kausalgesetz schon die erwachende Seele des Kindes sogleich in Beschlag nimmt und ihm die unermüdliche Frage "warum ?" in den Mund legt, so begleitet es den Forscher durch sein ganzes Leben und stellt ihm unaufhörlich neue Probleme. Denn die Wissenschaft bedeutet nicht beschauliches Ausruhen im Besitz gewonnener sicherer Erkenntnis, sondern sie bedeutet rastlose Arbeit und stets vorwärtsschreitende Entwicklung, nach einem Ziel, das wir wohl dichterisch zu ahnen, aber niemals verstandesmäßig voll zu erfassen vermögen". 2 German original: ". . . zum Abschluss, zum Endgültigen, werden wir nie kommen. Das wissenschaftliche Arbeiten wird nie aufhören -es wäre schlimm, wenn es aufhören würde. Denn wenn es keine Probleme mehr gäbe, dann würde man die Hände in den Schoß legen und den Kopf zur Ruhe und würde nicht mehr arbeiten. Und Ruhe ist Stillstand, und Ruhe ist Tod -in wissenschaftlicher Beziehung. Das Glück des Forschers besteht nicht darin, eine Wahrheit zu besitzen, sondern die Wahrheit zu erringen. Und in diesem fortschreitenden erfolgreichen Suchen nach der Wahrheit, da liegt die eigentliche Befriedigung. Das Suchen an sich befriedigt natürlich noch nicht. Es muss erfolgreich sein. Aber dieses erfolgreiche Arbeiten, das ist dasjenige, was den Quell jeder Anstrengung und auch den Quell eines jeden geistigen Genusses darstellt. Wenn der Quell versiegt, wenn die Wahrheit gefunden ist, dann ist es zu Ende, dann kann man sich geistig und körperlich schlafen legen. Aber dafür ist gesorgt, dass wir das nicht erleben, und darin besteht unser Glück".</p>
        <p>© The Author(s) 2018 K. Svozil, Physical (A)Causality, Fundamental Theories of Physics 192, https://doi.org/10.1007/978-3-319-70815-7</p>
        <p>particles in states (subscript i refers to the ith particle) "+ i " or "i " along the directions θ 1 , ϕ 1 , θ 2 , ϕ 2 , . . . , θ n , ϕ n , the classical-to-quantum substitutions are[212,448,513]:</p>
        <p>This work was supported in part by the European Union, Research Executive Agency (REA), Marie Curie FP7-PEOPLE-2010-IRSES-269151-RANPHYS grant. In particular, I kindly thank Pablo de Castro from the Open Access Project of LIBER -Ligue des Bibliothèques Européennes de Recherche for his kind guidance and help with regard to the open access rendition of this book. xii Contents</p>
        <p>Classical physics, in particular, classical Newtonian mechanics, can be perceived as being modelled by systems of simultaneous differential equations of second order, for which the initial values of the variables and their derivatives are known. It slowly dawned on the mathematical physicists that the solutions, even if they satisfied Lipschitz continuity and thus were unique, could have a huge variety of solutions; with huge structural differences. Some of these solutions turned out to be unstable [256]: not always "a small error in the data only introduces a small error in the result" [359, pp. 442] (see also [162]).</p>
        <p>What is presently known as deterministic chaos [387,458] -a term which is a contradictio in adjecto, an oxymoron of sorts -has a long and intriguing history, not without twists, raptures and surprises [170,171]. As has been mentioned earlier (see Sect. 17.4 on p. 137) already Maxwell hinted on physical situations in which very tiny variations or disturbances of the state could get attenuated tremendously, resulting in huge variations in the evolution of the system. In an epistemic sense, this might make prediction and forecasting an extremely difficult, if not impossible task.</p>
        <p>The idea is rather simple: the term "deterministic" refers to the state evolutionoften a first-order, nonlinear difference equation [360] -which is "deterministic" in the sense that the past state determines the future state uniquely. This state evolution is capable of "unfolding" the information contained in the initial state.</p>
        <p>The second term "chaos" or "chaotic" refers to a situation in which the algorithmic information of the initial value is "revealed" throughout evolution. Thereby, "true" irreducible chaos rests on the assumption of the continuum, and the possibility to "grab" or take (supposedly random with probability 1; cf. Sect. A.2 on p. 171) one element from the continuum, and recover the (in the limit algorithmically Appendix A</p>
        <p>Hilbert's dream in particular, and the formalistic axiomatic program in general, was to ground mathematics by a finite formal system -a set of axioms and deterministic rules of derivation, the latter (by the Curry-Howard correspondence) operating like an algorithm on the former like an input, which would proof all true theorems of mathematics. Gödel and Turing, among others, put an end to this formalistic dream [160,273], as vividly expressed by Gödel in a postscript, dated from June 3, 1964 [243, pp. 369-370]: "due to A. M. Turing's work, a precise and unquestionably adequate definition of the general concept of formal system can now be given, the existence of undecidable arithmetical propositions and the non-demonstrability of the consistency of a system in the same system can now be proved rigorously for every consistent formal system containing a certain amount of finitary number theory.</p>
        <p>Turing's work gives an analysis of the concept of "mechanical procedure" (alias "algorithm" or "computation procedure" or "finite combinatorial procedure"). This concept is shown to be equivalent with that of a "Turing machine." A formal system can simply be defined to be any mechanical procedure for producing formulas, called provable formulas."</p>
        <p>We shall present a very brief survey of the consequences of these findings, and first hint on the fact that almost all elements of the continuum, and, in particular, almost all reals, are incomputable. That is, they are inaccessibly to any computation.</p>
        <p>Then we head on to modern, algorithmic, definitions of randomness, and, in particular, of random reals. That is, random reals are algorithmically incompressible, and cannot be produced by any program whose code has much smaller length (as the original phenotype or number).</p>
        <p>Thereby we shall mention quantitative incompleteness theorems and introduce the busy beaver function. In a certain sense, those constructions give a glimpse on how fast a computation may diverge, and how difficult it is to compute or represent certain objects. We shall also speculate how primordial chaos may give rise to unbounded complexities.</p>
        <p>Finally we consider Chaitin's halting probability Omega, which serves as a sort of Rosetta stone for comprehending at least mild forms of random reals in perplexing ways.</p>
        <p>(A.1)</p>
        <p>Instead of delving into joint and mutual information content we head straight to a formal definition of randomness. A finite random binary sequence s n of length n is defined to be (nearly) algorithmically incompressible; that is, its algorithmic information content I (s n ) is not (much) less than n. An infinite binary sequence s is random if its initial segments s n are random finite binary sequences. That is, s is random if and only if there exists some constant c, such that, for all n ∈ N, the algorithmic information content of its initial segments s n is bounded from below by nc; that is,</p>
        <p>A random real (in arbitrary base notation) is one whose base 2 expansion of its fractional part (forgetting the integer part as long as it is finite) is a random infinite binary sequence.</p>
        <p>Let us go a little further and mention a bound from above on the algorithmic information content of a string of length n: it must be less than n + O (1). Because, to paraphrase Chaitin [124, p. 11</p>
        <p>Almost all reals of the continuum are not only incomputable, as we have argued previously by a measure theoretic argument, but they are also random. Rather than rephrasing the argument, Chaitin's argument [124, p. 11] can be paraphrased as follows: "the algorithmic information content of the great majority of strings of length n is approximately n, and very few strings of length n are of algorithmic information content much less than n. The reason is simply that there are much fewer programs of length appreciably less than n than strings of length n. More exactly, there are 2 n binary strings of length n, and less than 2 n-k binary encoded programs of length n before halting [71,168,426]. (Note that non-halting programs, possibly producing infinite numbers, for example by a non-terminating loop, do not apply.) Alternatively, in terms of algorithmic information content, the busy beaver function Σ(n) can be defined as the largest number (of bits) whose algorithmic information content is less than or equal to n [125,128]; that is,</p>
        <p>Σ(n) grows faster than any computable function of n and therefore is incomputable. Let us follow Chaitin [128] and suppose that n is greater than I ( f ) + O(1), the algorithmic information content of f (in terms of its binary code) plus a positive constant.</p>
        <p>For the computation of f (n) + 1 it suffices to know a minimal-size program to compute f , as well as the value of n, or, even more economically, the value of n -I ( f ). Thus,</p>
        <p>A related question is about the maximal execution or run-time of a halting algorithm of length smaller than or equal to n: what is minimum time S(n) -or, alternatively, recurrence time -such that all programs of length at most n bits which halt have done so; that is, have terminated or, alternatively, are recurring?</p>
        <p>An answer to this question will explain just how long it may take for the most time-consuming program of length n bits to halt. That, of course, is a worst-case scenario. Many programs of length n bits will have halted long before this maximal halting time [115]. S(n) can be estimated in terms of Σ(n) by two bounds; one from below and one from above. The bound from below is rather straightforward: Since the printout of any symbol requires at least a unit time step, Σ(n) can be interpreted as a sort of counter variable. Thus a first estimate is S(n) ≥ Σ(n); that is, S(n) grows faster than any computable function of n.</p>
        <p>A bound from above can be conceptualized in terms of an "inner dialogue" of an algorithm which can be published -that is, printed -with little algorithmic overhead. Stated differently, every "algorithmic contemplation" or "symbolic computation" could, with a little overhead [symbolized by "of the order of" O(•)], be transformed into a "printout" of this "monologue" directed towards the outside world, whose size in turn cannot exceed Σ(n + O( 1)) [125]. This yields a bound from above</p>
        <p>)) [101]. Thus the busy beaver function can serve as some sort of measure of what some algorithm can(not) do before it halts. In this sense "expressing something to the world" can be equated to "contemplating internally."</p>
        <p>A simulation of the original computation yields bounds Σ(3n + O( 1)) [51,52] for any program of size n bits to either halt, or else never to halt. Knowledge of the maximal halting time -in particular, some computable upper bound on Σ -would solve the halting problem quantitatively because if the maximal halting time were known and bounded by any computable function of the program size of n bits, one would have to wait just a little longer than the maximal halting time to make sure that every program of length n -also this particular program, if it is destined for termination -has terminated. Otherwise, the program would run</p>
        <p>As has already been pointed out earlier, due to the Einstein-Podolsky-Rosen explosion type setup [196] in certain (singlet) states allowing for uniqueness [448,508,514] through counterfactual reasoning, second order correlations appear feasible (subject to counterfactual existence).</p>
        <p>For dichotomic observables with two outcomes {0, 1} the classical expectations in the plane perpendicular to the direction connecting the two particles is a linear function of the measurement angle [388]. Assume the uniform distribution of (opposite but otherwise) identical "angular momenta" shared by the two particles and lying on the circumference of the unit circle, as depicted in Fig. B.1; and consider only the sign of these angular momenta. The arc lengths on the unit circle A + (θ 1 , θ 2 ) and A -(θ 1 , θ 2 ), normalized by the circumference of the unit circle, correspond to the frequency of occurrence of even ("++" and "--") and odd ("+-" and ("-+") parity pairs of events, respectively. Thus, A + (θ 1 , θ 2 ) and A -(θ 1 , θ 2 ) are proportional to the positive and negative contributions to the correlation coefficient. One obtains for 0 where the subscripts stand for the number of mutually exclusive measurement outcomes per particle, and for the number of particles, respectively. Note that</p>
        <p>The two spin one-half particle case is one of the standard quantum mechanical exercises, although it is seldom computed explicitly. For the sake of completeness and with the prospect to generalize the results to more particles of higher spin, this case will be enumerated explicitly. In what follows, we shall use the following notation: Let |+ denote the pure state corresponding to (1, 0) , and |denote the orthogonal pure state corresponding to (0, 1) .</p>
        <p>Let us start with the spin one-half angular momentum observables of a single particle along an arbitrary direction in spherical co-ordinates θ and ϕ in units of [447]; that is,</p>
        <p>The angular momentum operator in some direction specified by θ, ϕ is given by the spectral decomposition</p>
        <p>with σ(θ, ϕ) defined in Eq. (B.3).</p>
        <p>The two-partite quantum expectations corresponding to the classical expectation value E c,2,2 in Eq. (B.1) can be defined to be the difference between the probabilities to find the two particles in identical spin states (along arbitrary directions) minus the probabilities to find the two particles in different spin states (along those directions); that is, E q,2,2 = q ++ + q ---(q +-+ q -+ ), or q = q ++ + q --= 1 2 1 + E q,2,2 and q = = q +-+ q -+ = 1 2 1 -E q,2,2 . In what follows, singlet states |Ψ d,n,i will be labelled by three numbers d, n and i, denoting the number d of outcomes associated with the dimension of Hilbert space per particle, the number n of participating particles, and the state count i in an enumeration of all possible singlet states of n particles of spin j = (d -1)/2, respectively. For n = 2, there is only one singlet state (see Ref. [448] for more general cases).</p>
        <p>Consider the singlet "Bell" state of two spin-1 2 particles</p>
        <p>The density operator ρ Ψ 2,2,1 = |Ψ 2,2,1 Ψ 2,2,1 | is just the projector of the dyadic product of this vector. Singlet states are form invariant with respect to arbitrary unitary transformations in the single-particle Hilbert spaces and thus also rotationally invariant in configuration space, in particular under the rotations [29, Eqs. ( 7)-( 49</p>
        <p>The Bell singlet state satisfies the uniqueness property [508] in the sense that the outcome of a spin state measurement along a particular direction on one particle "fixes" also the opposite outcome of a spin state measurement along the same direction on its "partner" particle: (assuming lossless devices) whenever a "plus" or a "minus" is recorded on one side, a "minus" or a "plus" is recorded on the other side, and vice versa.</p>
        <p>We now turn to the calculation of quantum predictions. The joint probability to register the spins of the two particles in state ρ Ψ 2,2,1 aligned or anti-aligned along the directions defined by (θ 1 , ϕ 1 ) and (θ 2 , ϕ 2 ) can be evaluated by a straightforward calculation of</p>
        <p>(B.8)</p>
        <p>Since q = + q = = 1 and E q,2,2 = q =q = , the joint probabilities to find the two particles in an even or in an odd number of spin-"- 1 2 "-states when measured along (θ 1 , ϕ 1 ) and (θ 2 , ϕ 2 ) are in terms of the correlation coefficient given by</p>
        <p>The singlet state of two spin-3/2 observables can be found by the general methods developed in Ref. [448]. In this case, this amounts to summing all possible twopartite states yielding zero angular momentum, multiplied with the corresponding Clebsch-Gordan coefficients</p>
        <p>of mutually negative single particle states resulting in total angular momentum zero. More explicitly, for j 1 = j 2 = 3 2 , |ψ 4,2,1 can be written as</p>
        <p>Again, this two-partite singlet state satisfies the uniqueness property. The four different spin states can be identified with the Cartesian basis of 4-dimensional Hilbert space 3 2 = (1, 0, 0, 0) , 1 2 = (0, 1, 0, 0) , -1 2 = (0, 0, 1, 0) , and -3 2 = (0, 0, 0, 1) , respectively, so that |ψ 4,2,1 = (0, 0, 0, 1, 0, 0, -1, 0, 0, 0, 1, 0, -1, 0, 0, 0) .</p>
        <p>(B.22)</p>
        <p>The general case of spin correlation values of two particles with arbitrary spin j (see the Appendix of Ref. [321] for a group theoretic derivation) can be directly calculated in an analogous way, yielding</p>
        <p>Thus, the functional form of the two-particle correlation coefficients based on spin state observables is independent of the absolute spin value.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:48+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>The class imbalance problem is common in the credit scoring domain, as the number of defaulters is usually much less than the number of non-defaulters. To date, research on investigating the class imbalance problem has mainly focused on indicating and reducing the adverse effect of the class imbalance on the predictive accuracy of machine learning techniques, while the impact of that on machine learning interpretability has never been studied in the literature. This paper fills this gap by analysing how the stability of Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP), two popular interpretation methods, are affected by class imbalance. Our experiments use 2016-2020 UK residential mortgage data collected from European Datawarehouse. We evaluate the stability of LIME and SHAP on datasets of progressively increased class imbalance. The results show that interpretations generated from LIME and SHAP are less stable as the class imbalance increases, which indicates that the class imbalance does have an adverse effect on machine learning interpretability. To check the robustness of our outcomes, we also analyse two open-source credit scoring datasets and we obtain similar results.</p>
        <p>Financial institutions rely on credit scoring models to estimate the default probability of borrowers and decide whether or not to approve loan applications. With the boosted enthusiasm in the machine learning-based predictive techniques adopted in finance, applications such as credit scoring have gained substantial interest from both academia and industry ( Bank of England, 2019;Brown &amp; Mues, 2012;Chang et al., 2018;Lessmann et al., 2015 ). However, machine learning techniques such as Neural Networks and Extreme Gradient Boosting (XGBoost) are regarded as "black-box" methods since they are too complex to explain and validate their predictions.</p>
        <p>In academia, there has been a debate about the trade-off between the gain in accuracy and the loss in interpretability obtained with advanced credit scoring models (e.g., Bücker et al., 2021;Dumitrescu et al., 2022;Gunnarsson et al., 2021 ). Moreover, regulators have been committed to revealing new risks brought by machine learning techniques and emphasising the need for modelling transparency and interpretability in the lending sector. For example, in the United States, the Equal Credit Opportunity Act (ECOA) requires creditors to provide statements of specific reasons to applicants against whom adverse action is taken. There-fore the public can be protected from the risk of using "black-box" credit scoring models following this regulation ( Consumer Financial Protection Bureau, 2022 ). Similar regulation is also included in the General Data Protection Regulation (GDPR) in the European Union ( Voigt &amp; von dem Bussche, 2017 ). The European Union also proposed the Artificial Intelligence (AI) Act to identify risk categories for AI applications, and credit scoring is classified as a high-risk AI application ( European Commission, 2021 ). The European Banking Authority (EBA) recognises the necessity for financial institutions to take interpretability into account in their financial decisions ( European Banking Authority, 2020 ). It also insists on using machine learning interpretability techniques when building Internal ratings-based models ( European Banking Authority, 2021 ). Similarly, in a report on the governance of AI in Finance ( Laurent Dupont et al., 2020 ), the French Prudential Supervision and Resolution Authority (ACPR) discusses the requirements of interpretability and potential interpretability methods that could be used with "black-box" models in credit scoring. As mentioned by EBA and ACPR ( European Banking Authority, 2020;Laurent Dupont et al., 2020 ), model-agnostic interpretation methods such as Local Interpretable Model-agnostic Explanations (LIME) ( Ribeiro et al., 2016 ) and SHapley Additive exPlanations (SHAP) ( Lundberg &amp; Lee, 2017 ) could be used at a second stage to interpret the prediction results generated by the "black-box" credit scoring models. These interpretation methods can keep the high predictive accuracy of the machine learning models and make the prediction interpretable.</p>
        <p>Class imbalance is also a common issue in the credit scoring domain, as bad customers only account for a very small proportion of all customers. The proportion of defaults varies with financial products. For example, the default rate in mortgage portfolios is typically less than 0.5% ( Thomas et al., 2017 ), while loans to Small and Medium Enterprises (SMEs) have a higher percentage (around 5%) ( Andreeva et al., 2016;Gramegna &amp; Giudici, 2021 ). In the context of the class imbalance problem, current research has mainly focused on analysing the effects of class imbalance on the predictive ability of machine learning techniques. It can be concluded that the predictive performance decreases as the class imbalance level increases and the researchers have proposed various methods to improve it (e.g. Calabrese &amp; Osmetti, 2015;Chawla et al., 2011;Krawczyk, 2016;Li et al., 2019 ). However, based on our knowledge, the impact of class imbalance on the performance of interpretation methods has never been studied in the literature. Considering the growing popularity of using interpretation methods for highstakes decision-making such as credit scoring, this omission is a significant gap in the research.</p>
        <p>Since regulations like ECOA ( Consumer Financial Protection Bureau, 2022 ) and GDPR ( Voigt &amp; von dem Bussche, 2017 ) provide the right for individuals to receive an explanation of a decision made by "black-box" systems, customers could expect some guidance provided by interpretation methods on how to act to observe a desired outcome, such as an approval of a loan ( ICO &amp; The Alan Turing Institute, 2020; Singh et al., 2021 ). However, suppose the interpretive performance could be disturbed due to the class imbalance problem -customers may put effort into improving a specific feature (credit index) that may not be taken into account in their next loan application, hence getting disappointed and losing opportunities due to the misleading information. Moreover, financial institutions who provide unstable or misleading interpretations for similar situations seem to violate regulations (e.g., ECOA and GDPR) and could thus put themselves at risk. Customers will also lose faith in those financial institutions which may lead to a crisis of confidence and consequently bring substantial financial loss to the institutions.</p>
        <p>We make three methodological and three empirical contributions to the literature. From the methodological perspective, first, we propose an experimental framework including a controlled sampling process to investigate the stability of LIME and SHAP considering the effects of class imbalance, which has never been studied in the literature. Second, we apply two novel indexes, named Sequential Rank Agreement (SRA) ( Ekstrøm et al., 2019 ) and Coefficient of Variation (CV), to evaluate the interpretation stability in two ways. For both LIME and SHAP, key information sought by users is the importance ranking of the relevant features. Specifically, a ranking list of features is determined by the magnitude of the absolute SHAP values and the magnitude of the absolute LIME coefficient values. Therefore, in this paper, we measure the stability of LIME and SHAP based on their feature ranking lists and the corresponding feature importance values. We use SRA to measure the feature ranking stability, defined as the similarity of feature ranking lists generated to interpret the prediction results for a specific target at the same class imbalance level. For the feature importance value stability, we use CV to measure the similarity of the absolute SHAP values and absolute LIME coefficients corresponding to the same feature when interpreting the prediction results for a specific target at the same class imbalance level. Third, we extend the work of Visani et al. (2021) to measure and compare the "internal" and "external" stability of LIME by checking the coefficients' confidence intervals and the similarity of features during the feature selection process in LIME, using the Coefficients Stability Index (CSI) and the Variables Stability Index (VSI), respectively.</p>
        <p>From the empirical point of view, our first contribution is to measure the stability of LIME and SHAP when the level of class imbalance incrementally increases in the credit scoring context for the first time. To make our experiments as close to bank practice as possible, we use a dataset on residential mortgage defaults obtained from the European Datawarehouse, a centralised securitisation repository implemented by the European Central Bank that collects, validates and distributes standardised loan-level data for several European countries. We also use two additional open source credit scoring datasets, which are South German Credit Datasetfoot_0 and Taiwan Credit Card Dataset,foot_1 to enable reproducibility and verify the robustness of our experiments. Second, we use both XGBoost and Random Forestfoot_2 as the "black-box" machine learning models to make predictions, and we evaluate the interpretation stability based on them. We choose XGBoost and RF since they have become increasingly prevalent in the credit scoring domain over recent years for their superior predictive performance ( Barbaglia et al., 2021;Gunnarsson et al., 2021;Xia et al., 2017 ). Moreover, they perform relatively better with class imbalance than other "black-box" machine learning models ( Brown &amp; Mues, 2012;Fitzpatrick &amp; Mues, 2016 ). Third, we show that the stability of LIME and SHAP will be affected by the class imbalance, especially at extreme class imbalance levels (lower than 5% default rate). Specifically, the feature importance ranking becomes less stable as the class distribution becomes less balance. The variation of feature importance value deepens with the increase of the class imbalance level, and the "internal" stability of LIME does suffer from extremely imbalanced data.</p>
        <p>The rest of this paper is structured as follows. Section 2 reviews the work that has used the interpretation methods in the credit scoring domain, and we focus on how they deal with the class imbalance problem. A brief introduction to XGBoost, LIME and SHAP can be found in Section 3 . Section 4 thoroughly explains the proposed experimental framework from two aspects: data pre-processing and sampling procedure. Section 5 explains the stability indexes used in this paper. The results of the empirical study are then presented and discussed in Section 6 . Eventually, Section 7 gives the conclusions drawn from the study and discusses the possible directions of future research.</p>
        <p>Besides applying interpretable predictive models (e.g. logistic regression), using model-agnostic interpretation methods ( Molnar, 2021 ) to explain the prediction results separately after applying the machine learning "black-box" predictive models, has attracted more attention from both academia and industry. The great advantage of the model-agnostic interpretation methods over interpretable predictive models is their flexibility, including model flexibility, explanation flexibility and representation flexibility (See Ribeiro et al., 2016 for more details).</p>
        <p>Researchers have developed various model-agnostic interpretation methods focusing on different interpretation aspects. Some methods such as Partial Dependence Plots ( Friedman, 2001 ) and Accumulated Local Effects Plots ( Apley &amp; Zhu, 2020 ) interpret the "black-box" models by analysing the relationships among features or between features and dependent variables. Other methods try to interpret the prediction results of "black-box" models in various ways, including through identifying the features' contributions to the predictions like SHAP or through analysing the variance in feature values when disturbing the prediction results like Counterfactual Explanations ( Wachter et al., 2017 ). There are also methods ( Guidotti et al., 2018 ) using interpretable models such as logistic regression and decision trees to understand the prediction results or extract the IF-THEN rules directly. Table 1 summarises the existing literature on model-agnostic interpretation methods in the credit scoring domain. Martens et al. (2007) and Szwabe &amp; Misiorek (2018) used Support Vector Machine (SVM) and Tree-based ensembles, respectively, to predict the credit scoring behaviours. Both papers applied decision trees such as C4.5 and CART to approximate predictions provided by first stage "black-box" models, finally generating decision rules to interpret the prediction results. Bracke et al. (2019) at the Bank of England applied Shapley values and clustering algorithms to explain how much each feature of a scoring model contributes to the final prediction provided by a Gradient Tree Boosting (GB) model. The authors concluded that the important features found by Shapley values, loan-to-value ratio and current interest rate, are in line with the relevant literature. Barbaglia et al. (2021) from the European Commission similarly used GB classifiers to predict loan defaults. Using ALE plots, they observed a non-linear relationship between the current LTV and the probability of default. Ariza-Garzon et al. (2020) and Moscato et al. (2021) used the same loan default data from a P2P platform. They applied various interpretation methods such as LIME and SHAP to explain some classification models such as RF, 
            <rs type="software">XGBoost</rs> and Neural Networks (NN). Gramegna &amp; Giudici (2021) also used LIME and SHAP to generate feature weights after applying 
            <rs type="software">XGBoost</rs> to estimate the default probability of SMEs. To evaluate LIME and SHAP's ability to define distinct groups of observations, the authors further employed the feature weights generated by LIME and SHAP as input space for a K -means clustering and an RF model. The results showed that SHAP seems to have a clear advantage in terms of discriminative power compared with that of LIME. Bücker et al. (2021) tried to build the explanation framework for credit scoring by using different modelagnostic interpretation methods to provide either global-level or local-level interpretations at various stages of the credit scoring process to satisfy the requirements of stakeholders' interests. It can be seen from Table 1 that LIME and SHAP are the two most popular model-agnostic interpretation methods used in the credit scoring domain, and therefore we use both of them in this paper.
        </p>
        <p>We report in Table 1 the percentage of defaults in each work. Except Bücker et al. (2021) , all other datasets have different degrees of class imbalance, which is regarded as a common characteristic in the credit scoring domain ( Thomas et al., 2017 ).</p>
        <p>The class imbalance has an adverse effect on detecting rare events (e.g., the defaulters in credit scoring datasets) in the classification problem. To achieve better classification performance, several imbalanced learning techniques have been proposed. For a review of these methods, see Haixiang et al. (2017) ; He &amp; Garcia (2009) and Kaur et al. (2019) . Focusing on credit risk, Brown &amp; Mues (2012) and Fitzpatrick &amp; Mues (2016) performed a comparison of different classifiers on few credit scoring datasets, and concluded that the Tree-based ensembles such as GB and RF performed relatively well with pronounced class imbalance. Other research also indicated that balancing sample distribution such as using under-sampling or over-sampling techniques can improve classification accuracy ( Crone &amp; Finlay, 2012;Marqués et al., 2013;Moscato et al., 2021;Namvar et al., 2018 ). The main analyses in credit risk summarised in Table 1 are in line with the findings of the above literature that they either used Tree-based ensembles as classification models or employed sampling techniques to achieve better predictive power.</p>
        <p>Besides analysing the adverse effect of class imbalance on classification performance, some researchers also investigated its impact on the interpretations of the explanatory variables for interpretable models, such as Logistic Regression. King &amp; Zeng (2001a,b) theoretically and empirically showed that the estimation bias of coefficients in Logistic Regression could be greatly magnified by class imbalance. Owen (2007) also suggested that, in the case of extreme class imbalance, the minority class only contributes to the Logistic Regression estimation via its sample mean vector, and this issue cannot be solved by using penalisation or likelihood weighting ( Li et al., 2019 ).</p>
        <p>However, to the best of our knowledge, no research has considered the potential effect of class imbalance on the model-agnostic interpretation methods. For example, Patil et al. (2020) obtained a balanced dataset by oversampling, and they applied LIME and SHAP to the balanced dataset to pick out the important features. They concluded that oversampling does not alter the feature correlation since the important features for predictions of valid and fraud observations are consistent. Still, they did not compare the performance of LIME and SHAP on the imbalanced dataset with the balanced one. Bussmann et al. (2021) only mentioned in the conclusion and future research section that it would be interesting to analyse the effects of imbalanced datasets and sampling techniques on their proposed Shapley value context. Some researchers have already questioned the general robustness of the model-agnostic interpretation methods, such as LIME and SHAP, especially when interpreting the out-of-distribution samples, due to the permutation mechanism and the sampling procedure included in these methods ( Alvarez-Melis &amp; Jaakkola, 2018;Shaikhina et al., 2021;Sundararajan &amp; Najmi, 2020;Visani et al., 2021 ). Some possible remedies to solve this problem were proposed. For example, both Slack et al. (2021) and Zhao et al. (2021) exploited prior knowledge in Bayesian framework and developed Bayesian versions of LIME and SHAP to capture the uncertainty and improve the consistency in interpretations. Li et al. (2020) utilised a fixed reference distribution (e.g., training set) to control the uncertainty brought by permutation in repeated interpretations of SHAP. A similar method is also used by Shankaranarayana &amp; Runje (2019) and Zafar &amp; Khan (2019) for LIME. Although the above research evaluated the robustness of LIME and SHAP, none of them take into account the class imbalance problem. The intuitions could be that the class imbalance may be of no effect on the stability of the interpretations or even mitigate the uncertainty since the interpretations may come from the majority class, which may have more stable distributions, or it could have an adverse effect on the performance of interpretation methods since the rare events may be out-of-distribution and therefore it could be hard to interpret them. Therefore, this paper contributes to filling this gap by designing a novel framework based on feature ranking and value stability indexes to compare the stability of the two most used interpretation methods -LIME and SHAP -under different levels of class imbalance to see how the class imbalance affects the interpretations.</p>
        <p>In this section, we present an overview of XGBoost, LIME and SHAP. We start by setting the notation.</p>
        <p>be a dataset with N loans where x i is the feature vector, and y i be a binary response variable with y = 1 if default occurs and y = 0 otherwise. Let x j ( j = 1 , 2 , . . . , P ) be the value of jth feature of x i . In this section, we briefly introduce 
            <rs type="software">XGBoost</rs>, the "black-box" machine learning technique chosen to make predictions, and LIME and SHAP, the interpretation methods chosen to explain the predictions provided by 
            <rs type="software">XGBoost</rs>.
        </p>
        <p>
            <rs type="software">XGBoost</rs> proposed by Chen &amp; Guestrin (2016) is an advanced gradient tree boosting model in the machine learning literature. It shares the concept of gradient boosting algorithm which applies an additive form of weak base learners to minimise the loss function to measure how well the model fits the current data. The general gradient boosting runs a series of iterations m ( m = 1 , 2 , . . . , M), where at each iteration m , the base learner f m is sought by minimising the objective function expressed as follows:
        </p>
        <p>is the additive boosted model that represents the prediction on the m th iteration. Specifically, 
            <rs type="software">XGBoost</rs> uses CART decision tree algorithm as the base learner: , 110, 120,130,..., 500 the k th leaf and
        </p>
        <p>is a set of unknown parameters that needs to be optimised.</p>
        <p>In 
            <rs type="software">XGBoost</rs>, a regularisation term is added to Eq. ( 1) to avoid overfitting:
        </p>
        <p>where α &gt; 0 is a l 1 -penalty on the number of leaves in a CART as shown in Eq. ( 2) , λ &gt; 0 is a l 2 -penalty on the leaf nodes weights w k . Compared to the general gradient boosting algorithm, 
            <rs type="software">XGBoost</rs> quickly approximates Eq. ( 3) with a second-order Taylor expansion and it further speeds the convergence during the model training by applying an approximate greedy algorithm for finding the optimal tree structure. For more details about 
            <rs type="software">XGBoost</rs> see Chen &amp; Guestrin (2016) and Xia et al. (2017) .
        </p>
        <p>Implementation of 
            <rs type="software">XGBoost</rs> requires setting several hyperparameters. For example, the learning rate and the number of iterations are two hyper-parameters, which control the model convergence to make the boosting process more robust to over-fitting. A lower learning rate generally requires a larger number of iterations to ensure a sufficient convergence. Other hyper-parameters such as the penalty terms α and λ introduced in Eq. ( 3) , maximum tree depth, feature-based and sample-based subsampling rates, enable 
            <rs type="software">XGBoost</rs> to control the tree complexity, thereby avoiding overfitting and accelerating learning. This paper uses the Python package 
            <rs type="software">xgboost</rs> ( Chen &amp; Guestrin, 2016 ) to conduct the learning process.
        </p>
        <p>Considering a trade-off between the model performance and the computational cost, we employ a stepwise parameter tuning process, which is also used in Xia et al. (2017) . First, we follow the learning rate 0.1 as suggested by Friedman (2001) ; Xia et al. (2017) , and we fix the number of iterations at 200. The rest of the hyper-parameters are tuned using a grid search method. Afterwards, we keep the learning rate at 0.1, and tune the number of iterations with the rest of the hyper-parameters fixed at the values obtained from the first step. Table 2 reports the parameter tuning grid and other parameters that are not involved will be set to the default values in the 
            <rs type="software">xgboost</rs> Python package. The parameter values selected for the tuning grid are based on the preliminary exploration and similar research using gradient boosting algorithms in the credit scoring domain ( Barbaglia et al., 2021;Chang et al., 2018;Fitzpatrick &amp; Mues, 2016;Gunnarsson et al., 2021;Xia et al., 2017 ). In each step, we use five-fold cross-validation to find the best combination of the hyper-parameters with the highest Hmeasure obtained on the validation data.
        </p>
        <p>We use H-measure in Python package hmeasure to evaluate the predictive performance of 
            <rs type="software">XGBoost</rs> since it is a preferred metric for the imbalanced dataset ( Calabrese et al., 2016;Fitzpatrick &amp; Mues, 2016;Lessmann et al., 2015 ). As a coherent alternative to the Area Under the Curve (AUC), H-measure allows one to specify a severity ratio, measured by the cost of misclassifying a class 0 data point to the cost of misclassifying a class 1 data point ( Hand, 2009;2010 ). This paper uses the default setting of the severity ratio which equals to the reciprocal of relative class frequency, so
        </p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] that misclassifying the rare class (class 1 data points) is considered a more severe mistake, which exactly suits the credit scoring applications.</p>
        <p>LIME proposed by Ribeiro et al. (2016) aims to interpret the machine learning model prediction of a specific target x i by appropriating the "black-box" machine learning model ( f : R d → R ) with a local interpretable model g ∈ G , where G is a family of possible interpretable models. To fit a local surrogate centred on x i , LIME generates a new dataset (neighbourhood around x i ) by randomly perturbing features from the target x i and obtaining the corresponding predictions from the "black-box" model. The interpretable model g is then trained on the new dataset Z, which is weighted by the distances from the perturbed samples to the target x i . Therefore, the learned interpretable model g ensures local fidelity, which means it should be a good approximation of the "black-box" model predictions locally (focusing on the target x i ) but does not guarantee a good global approximation. Mathematically, the interpretation for the target x i can be obtained by optimising the following objective function:</p>
        <p>where π x i represents a proximity measure between a sample to the target x i , so we need to define the neighborhood around x i .</p>
        <p>( g ) is a regulation term which measures the complexity of the interpretable model g, L (•) is a loss function which is minimised to get an interpretable model g most similar to f in the neighbourhood defined by π x i , while the model complexity (g) is kept low:</p>
        <p>where z denotes the simplified inputs and z ∈ { 0 , 1 } P . 4 In this paper, we follow the default setting in the Python package 
            <rs type="software">lime</rs> for applying LIME to tabular data. Specifically, Ridge Regression is used as the interpretable model g.
        </p>
        <p>an exponential kernel, which smoothly assigns higher weights to samples closer to the target of interest. Here we use the default setting in the 
            <rs type="software">lime</rs> package, where D represents the Euclidean distance function and σ is the kernel width 0 . 75 × √ P . To control the model complexity (g) , a feature selection step is performed initially to select 10 most important features to be used in the Ridge regression.
        </p>
        <p>The Ridge regression estimates a linear relationship between the selected features and the approximated predictions, which provides an interpretation of the prediction through its coefficients: the larger the absolute coefficient values, the larger variation in the value of the response variable when the feature is changed. Therefore, the absolute coefficient values of features can represent the feature importance value. By sorting the absolute coefficient values in a decreasing order, we can get a feature ranking list, which will be used for the LIME stability measurement.</p>
        <p>SHAP ( Lundberg et al., 2020;Lundberg &amp; Lee, 2017 ) is based on the Shapley value ( Shapley, 1953 ), a concept from game theory that assigns fair payout to a player depending on its contribution to the total gain when coalitions are taken into account. In the machine learning field, a player is a certain feature value, coalitions are possible feature subsets S, and the fair payout represents the contribution of a certain feature value to the prediction. Thus, in our context, the Shapley value φ j of a feature value x j in the target x i can be calculated by averaging the prediction differences (contribution) generated between the model with and without j over all possible feature subsets S and considering all feature orderings:</p>
        <p>where x S denotes the values of the input features in the set S of target x i , f S (x S ) denotes the model trained with x j present in x S , and f S\ j (x S\ j ) denotes the model trained without x j in x S . It is proved that Shapley values is the only explanation method in the broad class of additive feature attribution methods that could simultaneously satisfy three properties -local accuracy, missingness and consistency ( Lundberg &amp; Lee, 2017 ).</p>
        <p>Unfortunately, as the number of the features increases, averaging over all possible feature subsets will be an intractable problem, hence sampling-based approximation methods are always applied to solve Eq. ( 4) ( Lundberg &amp; Lee, 2017;Štrumbelj &amp; Kononenko, 2014 ). However, those approximation methods reply on post hoc modelling of an arbitrary function and thus can still be slow and also suffer from sampling variability. Lundberg et al. (2020) therefore proposed Tree SHAP that could provide a fast and exact computation of Shapley values by leveraging the internal structure of tree-based models such as XGBoost. Shapley values require a summation of prediction differences over all possible feature subsets. Tree SHAP collapses this summation into a set of calculations specific to each leaf in a tree in the tree-based models, hence reducing the complexity of exact Shapley value computation from exponential to polynomial time.</p>
        <p>Specifically, to compute the impact of a specific feature subset ( f S (x S ) in Eq. ( 4) ) during the Shapley value calculation, Tree SHAP uses interventional expectations over a user-supplied background dataset:</p>
        <p>where X represents a sample in the background dataset, and the do-notation formulation emphasizes an intervention on X when we manually set the feature values in X S to the same values in x S of the target x i ( X S = x S ). The interventional Tree SHAP enforces independence between the set S and the set of remaining features based on the laws of causality ( Janzing et al., 2019 ). It should be noted that since the background dataset is fixed, Tree SHAP calculates Eq. ( 5) by iterating over each sample in the background dataset, hence there is no estimation variability in Tree SHAP like other sampling-based approximation methods (see Lundberg et al., 2020 for more algorithm details).</p>
        <p>In this paper, we use the training set as the background dataset and we use the Python 
            <rs type="software">package</rs> shap to apply Tree SHAP (abbreviated as SHAP in this paper) in our experiments. For each target x i , the absolute SHAP value of each feature measures the magnitude of contribution to the prediction and therefore can be regarded as the feature importance value. Features with larger absolute SHAP values contribute more to the prediction and therefore are more important. By sorting the absolute SHAP values in a decreasing order, we could get a ranking list of features, which will be used for SHAP stability measurement.
        </p>
        <p>Both LIME and SHAP measure feature contribution at the observation level (local explanation). Benefiting from the solid theoretical foundation in game theory, SHAP ensures that the prediction is fairly distributed among the features, and therefore could further</p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] provide global model interpretations such as global feature importance for a whole dataset, which are consistent with the local explanations ( Lundberg &amp; Lee, 2017 ). Based on the solid theory, SHAP also guarantees contractive explanations by comparing the prediction with the average prediction, which is not feasible for LIME. Therefore, SHAP is more suitable and compliant when people need full interpretations locally and globally based on a solid theoretical foundation.</p>
        <p>While LIME lacks the theoretical foundation as SHAP and tends to be "internal" unstable (this will be discussed in Section 5.3.3 ), LIME is time efficient and can make human-friendly explanations compared to SHAP. Tree SHAP is relatively faster compared to other types of SHAP by leveraging the internal structure of tree-based models. Machine learning models other than tree-based models can only rely on other SHAP algorithms, such as Kernel SHAP, which is slow and impractical to use when computing Shapley values for many observations ( Molnar, 2021 ). LIME provides short and clear explanations using the feature selection step. An interpretable model, such as Ridge regression in LIME, also enables LIME to make statements about changes in prediction for changes in the input, which is not applicable in SHAP. Besides, LIME is capable of providing interpretations for a new observation only based on properties of the training set (mean and standard deviation) and the machine learning prediction function, while SHAP needs to access the data (training set or other background datasets) to compute the Shapley values ( Lundberg &amp; Lee, 2017;Ribeiro et al., 2016 ). Therefore, LIME is more appropriate in applications when the recipient of interpretations is a lay person or with a limitation of time, or there is a restriction of access to data.</p>
        <p>To analyse the effects of the class imbalance on LIME and SHAP, this paper conducts an empirical study, which compares the performance of LIME and SHAP when explaining the predictions of XGBoost on credit scoring datasets with different default rates. The proposed experimental framework is provided in Fig. 1 . In this section, we use the European Datawarehouse data as an example to explain the experimental framework.</p>
        <p>In Step 1 , loan data for residential property purchased in the UK are collected and pre-processed for the following analysis. The detailed data preparation process will be introduced in Section 4.1 . Since LIME and SHAP interpret the predictions on individuals, to make the stability of the interpretive performance across varied class imbalance comparable, it is necessary to initially sample the target individuals of which the predictions from the "black-box" model will be interpreted. Therefore, we randomly select 100 defaultsfoot_5 and 100 non-defaults to be used as targets ( T argetset =</p>
        <p>, T = 200 ) for local interpretation in Step 2 . After the initial sampling, for the rest of the data, we build 12 training sets</p>
        <p>) with the same sample size Z but different default rates u ( u = 1% , 2 . 5% , 5% , 10% , . . . , 45% , 50% ) in Step 3 . By doing so, it is possible to identify whether the interpretations of machine learning predictions generated by LIME and SHAP are adversely affected when there is a substantially lower number of observations in one of the classes. The sampling procedure will be explained in Section 4.2 . In Step 4 , 
            <rs type="software">XGBoost</rs> is trained on the training sets generated from Step 3 with the best parameters selected using the grid search method introduced in Section 3.1.2 .
        </p>
        <p>The trained 
            <rs type="software">XGBoost</rs> with parameters selected with the highest Hmeasure value is then applied to get the predictions on each target x i obtained in Step 2. In Step 5 , the feature importance value, referring to the absolute LIME coefficient value LIME iu b (F p ) or the absolute SHAP value SHAP iu b (F p ) of each feature F p in the feature set F = { F 1 , . . . , F P } , is generated for each target x i at each default rate u . We repeat the process from Step 3 to Step 5 B = 100 times. After iterations, for each target x i at each default rate u , we obtain 100 feature importance values for each feature F p , denoted as
        </p>
        <p>. Then in Step 6 , for each target x i at each default rate u , we first measure the ranking and value stability for each feature F p , based on its 100 feature importance valuesfoot_7 , noted as RankStab iu (F p ) and V alueStab iu (F p ) . Then we calculate the final ranking stability for each target x i at each default rate u , denoted as RankStab iu d , by aggregating ranking stability indexes of features at each list depth d of feature ranking lists. Simultaneously, the final value stability for each target x i at each default rate u , denoted as V alueStab iu , is measured by the average of value stability indexes of all features in the feature set F = { F 1 , . . . , F P } . The details of stability indexes will be introduced in Section 5 . Note that we repeat the whole experiment 5 times to obtain the stability measurements for 5 sets of targets ( 5 × 200 targets) to avoid any potential bias results when only depending on 200 targets. The stability measurements results for 5 sets are reported in Section 6 .</p>
        <p>In this empirical study, we use UK residential mortgage data between January 2016 and December 2020, collected from European Datawarehouse. Here a mortgage default is defined as being in arrears for three months or more with mortgage payments. The loan-level data provides loan characteristics, borrower information, property information and loan performance for each loan. In particular, the loan characteristics include some static information such as loan original balance, and some dynamic information such as the current interest rate. The borrower information contains the employment status, age, annual income, etc., of the loan borrower collected at the origination of the loan. The underlying assets information provides the type, original value, current value (dynamic), location, etc., of the property. Loan performance information provides the status of the loan, whether it is performing or in arrears, and for how many months it has been in arrears, which is also dynamic. Note that the dynamic information is updated at least quarterly.</p>
        <p>To select the explanatory variables as close as possible to a real scenario, we refer to several published papers that used the same or similar database. Barbaglia et al. ( 2021) is a paper from the European Commission, which used the residential mortgage dataset from the European Datawarehouse as we did. Bracke et al. ( 2019) is a paper from the Bank of England, which similarly conducted a UK residential mortgage default analysis. Li et al. (2019) ; Sirignano et al. (2018) also predicted the mortgage loan default with similar features as we have, although they used mortgage data from the U.S.</p>
        <p>As Barbaglia et al. (2021) indicated, European Datawarehouse data is rich but unexplored. It comes with some flaws that need to be addressed before using it. The main flaws include: inactive or matured loans still exist in the database; some loans are inconsistent across time period analysed; and some loans have unex- pected attributes such as very high interest rate (e.g., over 20%). In order to address these flaws, we implemented an intensive data cleaning process based on our understanding of the data, combined with the data cleaning steps mainly introduced in Barbaglia et al. ( 2021) ; Bracke et al. (2019) . The data cleaning details can be found in Supplementary Materials (Part A). Table 3 reports the explanatory variables we used in the loan default predictive model. Since the database is updated at a quarter level and we need to predict the loan status (1 = default, 0 = non-default) one year ahead, here we identify the loan status (i.e. response variable) quarterly and collected the explanatory variables one year in advance to build our dataset. The detailed data collection time is shown in Table 4 . Note that we first exclude those loans that were already defaulted in 2016 Q1-Q4 from the whole dataset since we do not have data in 2015 to collect their explanatory variables. After this data collection process, we have a dataset in which for each loan there are loan records for a series of quarters. Based on this dataset, when a loan is first found to be in default in a certain quarter in the period 2017Q1 to 2020Q4, we remove its records in other quarters to ensure that there is no duplicated defaulted loan in the dataset and avoid the possibility that the defaulted loan is also shown as a non-defaulted one in a different period. Since we do not consider the impact of the time change on the prediction and interpretation results in this study, we also remove the duplicated non-defaulted loans, in other words, we randomly select one record for each non-defaulted loan in the dataset ( Calabrese et al., 2016 ). As a result of this selection, we obtain a dataset with all distinct loans, including 3229 defaulted loans. The default rate is 0.6%, which indicates the highly unbalanced nature of the mortgage default variable as stated in Thomas et al. (2017) .</p>
        <p>After initially sampling the targets, which will also be used as a test set to evaluate accuracy, we perform data resampling to create the training sets with various loan default rates ranging from 5% to 50% in increments of 5%. We also include two more extreme loan default rates which are 2.5% and 1%. To exclude the effect of sample size on the predictive and interpretive performance, we fix the sample size to Z = 6258 , which is the size of a balanced dataset (loan default rate = 50%) with all 3129 defaults included and 3129 randomly selected non-defaults. Note that we do not use any over-sampling method such as SMOTE ( Harald et al., 2016 ) to create more records of defaults and to artificially increase the sample size since this method involves randomness that might generate unseen features with a stochastic approach that would add an additional level of complexity to the problem ( Bueff et al., 2022 ). Therefore, with the sample size fixed at 6258, we under-sample the defaults as well as randomly select more non-defaults to obtain an increasing level of class imbalance. The number of the defaults and non-defaults for each default rate are shown in Table 5 .</p>
        <p>In this paper, we use Sequential Rank Agreement (SRA) proposed by Ekstrøm et al. (2019) to measure the ranking stability of feature lists generated by LIME and SHAP for each target x i at each default rate u ( RankStab iu in Fig. 1 ). Based on our knowledge, this paper is the first research that uses this method to compare the LIME and SHAP feature ranking lists. The details of SRA can be found in Section 5.1 . Besides evaluating the feature ranking stability, we also measure the feature importance value stability generated by LIME and SHAP for each target x i at each default rate u ( V alueStab iu in Fig. 1 ) using the Coefficient of Variation (CV), as explained in Section 5.2 .</p>
        <p>For the sake of completeness, we also include another two stability measures, namely Variables Stability Index (VSI) and Coefficients Stability Index (CSI), that have been used before in the literature only for LIME ( Visani et al., 2021 ). The VSI is proposed to check whether the selected features are the same or not among the repeated LIME interpretations. The CSI measures the LIME stability through the similarity of coefficients among the repeated LIME interpretations. The details of VSI and CSI are introduced in Section 5.3 .</p>
        <p>Both SRA and VSI are based on feature variation, but SRA takes into account the feature's position in the ranking lists. SRA can be used for both SHAP and LIME but VSI can only apply to LIME since VSI focuses on feature selection step which is not included in SHAP. Both CV and CSI focus on feature importance value stability. CV considers the feature coefficient value itself whereas CSI only checks whether the confidence intervals of coefficients for the same feature overlap or not in different LIME interpretations. Note that Visani et al. (2021) proposed VSI and CSI originally to check the "internal" stability of LIME, which refers to the stability of explanations derived from repeated LIME calls under the same conditions and the same distribution (law). As mentioned in Section 3.3 , there should be no "internal" instability (estimation variability) in Tree SHAP like LIME since the background dataset is fixed. Therefore, in this paper, we consider VSI and CSI specifically for LIME to examine the class imbalance effects on the "internal" stability and compare them with the "external" stability for LIME. More details are explained in Section 5.3.3 .</p>
        <p>We remind that b ( b = 1 , . . . , B ) represents the iteration with B = 100 and F = { F 1 , . . . , F P } the feature set in this section.</p>
        <p>The SRA value could provide the level of ranking agreement using a function of the depth in the lists. Following our experimental framework ( Fig. 1 ), for each target x i at each default rate u , we obtain 100 feature importance values for each feature after repeating</p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] Table 6 Example set of ranking lists shows: (a) three sets of feature importance values ( 1 , 2 , 3 ) with the CV values of features ( ValueStab(F p ) ), (b) three feature ranking lists ( L 1 , L 2 , L 3 ) based on Panel (a), (c) ranking R (F p ) obtained by each feature in each of three lists with the ranking agreement values of features ( RankStab(F p ) ), and (d) the cumulative set of features up to a given depth in the three ranking lists (i.e., a feature is added to S(d) whenever it appears in at least one list) with the SRA value RankStab d of each list depth d.</p>
        <p>Step 3 to Step 5 100 times. In other words, we obtain 100 sets of feature importance values of all features for each target x i at each default rate u . Therefore, by sorting the feature importance values in each set in decreasing order, the corresponding ordered feature names could form 100 feature ranking lists. Therefore, in what follows, we describe the process of obtaining SRA values based on 100 feature ranking lists for one target x i at a specific default rate u .</p>
        <p>We denote the feature importance value of a feature as Every feature ranking list L iu b contains the same number of features. We point out that in the SHAP feature ranking lists, the number of features equals to that used in the "black-box" machine learning model since SHAP uses all features involved in the predictive model to interpret the prediction result. While in LIME feature ranking lists, the number of features equals to the number of unique features selected during the feature selection step in all 100 LIME interpretable models. We further denote a ranking function of a feature ranking list L iu b as R iu b : { F 1 , . . . , F P } → { 1 , . . . , P } , such that R iu b (F p ) is the ranking of a feature F p in a feature ranking list L iu b , as illustrated in Table 6 . For example, 1 in Panel (a) is a set of feature importance values of 5 features, L 1 in Panel (b) is the feature ranking list based on 1 , 7 In our experimental framework ( Fig. 1 ), iu b (F p ) would be either</p>
        <p>and the value of R 1 in Panel (c) is the corresponding ranking of each feature in L 1 . Thus, for each feature F p , the ranking agreement value RankStab iu (F p ) across B feature ranking lists can be calculated as follows:</p>
        <p>where | S iu d | is the cardinality of the set S iu d . Therefore, the SRA value of list depth 1 to 5 in Panel (d) of Table 6 is 2.33, 2.6 6, 2.6 6, 3.32 and 3.32 respectively. The SRA value RankStab iu d in each list depth d is equivalent to the pooled variance of features found in S iu d . When comparing SRA values for one target under the same list depth among different class imbalance levels, a smaller SRA value suggests the feature ranking lists agree more on the rankings, which means the feature rankings are more stable. For example, the RankStab iu d of every list depth will be 0 when the ranking lists are identical.</p>
        <p>The CV is a statistical measure of relative variability, defined as the ratio of the standard deviation to the mean among a set of data points. The CV is particularly suitable for our study as it is dimensionless and thus, comparable among different sets of data points with various means or various units.</p>
        <p>We continue using the notations in Section 5.1 . The CV value for a feature F p ( V alueStab iu (F p ) ) across B sets of feature importance values can then be calculated as follows:</p>
        <p>where the numerator is the sample standard deviation of the feature importance values for the feature F p over B sets of feature importance values, and the denominator</p>
        <p>the expected feature importance value of the feature F p over B sets of feature importance values. For example, the CV values of 5 features in Table 6 are shown in Panel (a).</p>
        <p>After getting the CV value V alueStab iu (F p ) for each feature F p , the CV value for a target x i at a default rate u , which is defined as V alueStab iu , can be obtained by calculating the average of CV values V alueStab iu (F p ) of all the features whose importance values presenting at least 2 timesfoot_8 in the B sets of feature importance</p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] values, which can be expressed as follows:</p>
        <p>Since the CV value V alueStab iu measures the degree of variability in the feature importance values, when comparing CV values for one target among different class imbalance levels, a larger CV value indicates less stable interpretations of the prediction made for the chosen target.</p>
        <p>In this section, we introduce the VSI and CSI proposed by Visani et al. (2021) to measure the stability of LIME. We start by setting the notation. We consider g iu 1 , . . . , g iu M as M interpretable models (Ridge regressions) generated by LIME for a target x i at a default rate u .</p>
        <p>The VSI is proposed to check the stability of the feature selection step included in LIME -whether the selected features are same among M interpretable models. Let C iu = { C iu 1 , . . . , C iu K } be the set of all possible combinations of the M interpretable models, two by two. The generic element of the set C iu is the pair of interpretable models C iu k = (g iu α , g iu β ) and the number of pairs</p>
        <p>For each pair C iu k , we count the number of the same features used by both interpretable models, denoted by SAME(C iu k ) . Note that SAME(C iu k ) is an integer and 0 ≤ SAME(C iu k ) ≤ 10 9 hence the VSI value v si iu for a target x i at a default rate u can be calculated as follows:</p>
        <p>where the numerator calculates the average number of same features used by all pairs of interpretable models in the set C iu . We further normalise dividing by the number of selected features 10, and obtain the VSI value v si iu for one target ranging from 0 to 1. The more it approaches 1, the more features found in M interpretable models are the same.</p>
        <p>The CSI measures the LIME stability through the similarity of coefficients generated from M interpretable models for a target x i at a default rate u . Specifically, we calculate 95% confidence intervals of each coefficient in M interpretable models (see Visani et al., 2021 for more mathematical explanation), and consider the coefficients for a feature to be unstable when the calculated confidence intervals for this feature from different interpretable models are not overlapped at all. Instead, we consider the coefficients of a feature to be stable whenever the confidence intervals overlap to some extent. In what follows, we use equations to explain the CSI.</p>
        <p>The comparison among confidence intervals is carried out separately for each feature F p . Therefore, let CI iu (F p ) = { C I iu 1 (F p ) , . . . , C I iu D (F p ) } be the set of all 95% confidence intervals of the coefficients for a certain feature F p presented in the M interpretable models. Let A iu (F p ) = { A iu 1 (F p ) , . . . , A iu T (F p ) } be the set of all possible combinations of the D confidence intervals of the coefficients for the feature F p , two by two. The generic will have only one feature importance value and the CV value ValueStabF eat iu (F p )</p>
        <p>of this feature will be 0, which cannot truly represent a meaningful degree of variability. Therefore, P * in Eq. ( 9) will be less than or equal to the number of unique features in B interpretable models. 9 The maximum value of SAME(C iu k ) is the number of features selected in the feature selection step, which equals to 10.</p>
        <p>element of the set A iu (F p ) is the pair of confidence intervals</p>
        <p>) and the number of pairs T in the set A iu (F p ) equals to D ! 2!(D -2)! . Hence for each pair A iu t (F p ) , we consider a binary variable OV ERLAP (A iu t (F p )) , which equals to 1 if the pair of confidence intervals is overlapped and 0 otherwise:</p>
        <p>We calculate OV ERLAP (A iu t (F p )) for all pairs of confidence intervals in the set A iu (F p ) and add them up. The outcome is a count variable, which we normalise dividing by the number of pairs T , to obtain a Partial Index P I iu (F p ) for the feature F p considered:</p>
        <p>To obtain the CSI value csi iu for a target x i at a default rate u , we average the Partial Indices of all the features presenting at least 2 timesfoot_9 in the M interpretable models, which can be expressed as follows:</p>
        <p>Similar to the VSI value v si iu , the CSI value csi iu for one target also ranges from 0 to 1, and the more it approaches to 1, the more the LIME coefficients for the same feature in the M interpretable models can be considered stable for the chosen target.</p>
        <p>Consider performing LIME to explain a prediction made for a certain target for several times when the predictive model and the underlying training set stay unchanged. Hence LIME follows the same distribution (law) to generate new data points to build the interpretable models. However, due to the random nature of the sampling, LIME could still generate different data points among repeated calls and build different interpretable models, thus providing unstable interpretations for the chosen target. The internal stability described here is different from the stability measured by following our experimental framework introduced in Fig. 1 . In our experiments, we measure the ranking and value stability of LIME under the same class imbalance level using different training sets and therefore different predictive models, which means LIME generates data points and build interpretable models based on different training sets and therefore different distribution (law) to some extent. Therefore, we name the stability measured in our experiments the "external" stability.</p>
        <p>It is important to note that Tree SHAP used in our experiments does not include a sampling procedure, hence there is no need to check the internal stability for SHAP. To check the internal stability of LIME, we perform an experiment in which we use the same 200 initially sampled targets and adjust Step 5 of experimental framework introduced in Fig. 1 to fit the measurement of internal stability of LIME. We specifically repeat the Step 5 for 30 times, so that we could get 30 LIME interpretable models (Ridge regressions) based on the same conditions (generating the neighborhood from the same training set/distribution). Moreover, other than repeating the process from Step 3 to Step 5 for 100 times, here we only repeat for 10 times. Hence we could get 30 × 10 LIME interpretable models for each target at each default rate, in which every 30 LIME</p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] interpretation models are based on the same conditions. We then calculate the VSI value v si iu using Eq. ( 10) and the CSI value csi iu using Eq. ( 11) for every 30 LIME interpretable models based on the same conditions. Therefore, at every default rate u , we could obtain 10 VSI values and 10 CSI values for each target x i and we calculate the average of them respectively to get the average VSI v si iu internal and the average CSI csi iu internal for each target x . In this study, we also measure the external stability of LIME using CSI and VSI. Specifically, we follow our original experimental framework ( Fig. 1 ) to calculate the VSI value v si iu external using Eq. ( 10) and the CSI value csi iu external using Eq. ( 11) for 100 interpretable models (100 sets of feature importance values) based on different conditions (generating the neighborhood from different training set/distribution) for each target x i at the same default rate u .</p>
        <p>By doing this, we could compare v si iu internal and csi iu internal with v si iu external and csi iu external respectively, to gain a greater insight into the effects of class imbalance on the interpretation stability of LIME.</p>
        <p>Following the experimental framework described at the beginning of Section 4 , for LIME and SHAP respectively, we repeat Step 3 to Step 5100foot_10 times to obtain 100 sets of feature importance values, which can be transformed into 100 feature ranking lists, for each target x i at every class imbalance level u . We then calculate the SRA value RankStab iu d of each list depth d introduced in Section 5.1 to measure the feature ranking stability of LIME and SHAP for each target x i at every class imbalance level u , and the results are discussed in Section 6.1 . Similarly, the CV value V alueStab iu introduced in Section 5.2 are calculated to measure the feature importance value stability of LIME and SHAP for each target x i at every class imbalance level u , and the results are discussed in Section 6.2 . Moreover, for each target x i , we obtain the internal VSI value v si iu internal and CSI value csi iu internal , as well as the external VSI value v si iu external and CSI values csi iu external following the description in Section 5.3 to further evaluate the stability of LIME, and the results are discussed in Section 6.3 . Note that our results are consistent across all three datasets. In Sections 6.1 -6.3 , we describe the results for the European Datawarehouse data. In Section 6.4 , we summarise the results of two open-source credit scoring datasets, and the details of the results can be found in the Supplementary Materials (Parts F and G). The prediction results (H-measure) can also be found in Supplementary Materials (Part C). Note that the relative spread of H-measure values (dispersion levels of Hmeasure values) for fixed targets (regarded as the test set) over different levels of class imbalance is basically stable, which confirms that the prediction performance will not affect the stability of the interpretations generated by LIME and SHAP.</p>
        <p>Recall that we repeat the whole experiment framework 5 times and therefore we have stability measurement results for 10 0 0 ( 5 × 200 ) targets. To achieve a more general result, at each class imbalance level u , we calculate the average of the SRA value RankStab iu d of all 10 0 0 initially sampled targets for each list depth d. Panel (a) and Panel (b) of Fig. 2 shows the average SRA value for LIME and SHAP respectively. Here we use a line chart to show, for a particular list depth d, the trend of the average SRA value as the class imbalance level decreases. Specifically, on the x -axis is the class imbalance level, represented by the default rate, ranging from 1% (extreme imbalanced) to 50% (balanced), and on the y -axis is the average SRA value. Please note that the range of the average SRA value on the y -axis for LIME and SHAP is different in order to make the line chart more visible. Each line represents a list depth and therefore each point corresponding to a default rate on the line is the average SRA value at a certain class imbalance level. Note that for clarity of presentation, here we only show the average SRA value for the list depth from 1 to 5, which represents the ranking stability of the top most important 5 features, and the average SRA values for the complete list depths can be found in Supplementary Materials (Part D), which leads to similar conclusions.</p>
        <p>Every line in Fig. 2 show a distinct downward trend with slight fluctuations, which means the average SRA value of each list depth continues to decrease as the default rate gradually increases. Recall that the smaller the SRA value, the better the agreement achieves among the ranking lists. The results indicate that the feature ranking stability increases with the decrease of the class imbalance, thereby confirming the class imbalance does have an adverse effect on the interpretive performance of both LIME and SHAP.</p>
        <p>Other than comparing the average SRA value based on the variation of the class imbalance level, we could also observe a regular tendency when comparing within each class imbalance level (default rate). With the exception of 1% and 2.5% default rates in the line chart of LIME, in each default rates for LIME and SHAP, the average SRA value presents the minimum when the list depth equals to 1, indicating the best agreement, and then increases as the list depth increases. It indicates that the feature ranking lists generated by LIME and SHAP agree more on higher rankings and can achieve the most stable ranking for the most important feature, but the variability still exists as the average SRA value is not equal to 0. For the average SRA value within 1% and 2.5% default rates in the line chart of LIME, although not distinct, there is an opposite trend that they show higher disagreement (larger average SRA value) in the top of the lists followed by a decrease as the list depth increases. The reason behind this is rather subtle. Looking at the absolute value of the Ridge regression coefficients in LIME, we see that most of them are very close to zero and have almost equal absolute value at these two extreme class imbalance levels (1% and 2.5%). It implies that when features are ranked according to the magnitude of their absolute value of the coefficients, their order becomes more uncertain and more close to a random permutation. Hence the feature ranked in the top of the lists may obtain a larger ranking agreement value RankStab iu (F p ) according to Eq. ( 6) , which results in a larger average SRA value.</p>
        <p>When comparing the average SRA value between LIME and SHAP, it can be seen that for every default rate, the average SRA value of all list depths for SHAP are larger than those for LIME. This is reasonable since LIME performs a preliminary feature selection step and only use 10 selected features but not all 37 features used by SHAP to generate interpretations, which leads to a much smaller ranking range for LIME, and hence a smaller average SRA value.</p>
        <p>At each class imbalance level u , we obtain a total of 10 0 0 CV values V alueStab iu for all 10 0 0 initially sampled targets. For better illustration, for LIME and SHAP respectively, we draw a box-andwhisker plot (boxplot) for every set of 10 0 0 CV values at each class imbalance level, as shown in Panel (a) and Panel (b) in Fig. 3 .</p>
        <p>The x -axis represents the class imbalance level (default rate), and the y -axis represents the CV value. The range of the CV value on the y -axis for LIME and SHAP is different in order to make As demonstrated in Panel (a) of Fig. 3 , CV values for LIME under 1% and 2.5% default rates are evidently larger compared with the other sets of CV values. While there is a decreasing trend after 5%, there is no significant difference in the sets of CV values when the default rate is greater than 15%. Recall that the larger the CV value, the greater the (average) variability among the feature importance values. It proves that the absolute LIME coefficients generated are much less stable in the case of the extreme class imbalance. The same conclusion can also applies to SHAP based on the results presented in Panel (b) of Fig. 3 , with the distinction between the CV values at the 1%, 2.5% default rates and others are relatively more obvious.</p>
        <p>When comparing the CV values between LIME and SHAP, we can see that the CV values for SHAP at each default rate are larger than those for LIME. One possible reason behind this could be that the CV values of SHAP are calculated based on all 37 features, which may result in more variation among feature importance values especially for those relatively unimportant features.</p>
        <p>We now focus on the feature selection stability and the feature coefficients stability of LIME, which are measured by VSI and CSI respectively. [m5G;July 3, 2023;20:3 ] calls of LIME based on the same conditions tend to yield more stable interpretation results than those based on different conditions. This is reasonable since the neighbourhoods sampled from different training sets on which the interpretable models are built can increase the instability of the interpretations for a certain target. Moreover, the external stability of LIME is more affected by class imbalance than its internal stability. One possible reason behind this is that when we repeat the sampling procedure, due to the lack of information for defaulters in the (extreme) imbalanced training sets, we can only build the interpretable models for a certain target based on incomplete and distinct information and therefore increase the variability among the interpretation results.</p>
        <p>We apply the experiments above described also to two additional datasets to check the robustness of the stability measurement results. We use the South German Credit Dataset, which contains 10 0 0 observations and 21 predictors with personal credit risk information; and the Taiwan Credit Card Dataset, which contains 30,0 0 0 observations and 24 predictors with customers' credit card transaction information. Both datasets are open-sourced, differ with respect to the number of observations and predictors, and are used by many papers in the credit scoring literature (e.g., Gunnarsson et al., 2021;Lessmann et al., 2015 ), which complement the European Datawarehouse mortgage data in terms of feature and sample size variety.</p>
        <p>The details of the results are presented in Supplementary Materials (Parts F and G). Overall, the SRA, CV, CSI and VSI results using the two open-source datasets are consistent with those using European Datawarehouse data. This confirms the robustness of our results that the stability of interpretability results could be adversely affected by class imbalance. For feature importance ranking stability, the only difference is that for the open source datasets, the SRA values when the list depth is equal to 1 significantly differ from the SRA values at other list depths. This may be because the features ranked first are relatively stable, but there is greater randomness in the subsequent rankings. The feature importance values generated by LIME and SHAP are also less stable at extreme class imbalance based on open source datasets, while the difference between CV values at 1% and 2.5% default rates and CV values at larger default rates are less obvious for SHAP compared to that on the European Datawarehouse data. Similarly, the results of VSI and CSI for LIME on the open source data also agree with those on the European Datawarehouse data.</p>
        <p>In this paper, we consider two popular model-agnostic interpretation methods -LIME and SHAP, and study their interpretative performance over various class imbalance levels. We achieve this by proposing a controlled sampling process to produce a series of datasets with different default rates but the same sample size. We use residential mortgage data provided by the European Datawarehouse and two more open source credit scoring datasets to verify the robustness of our results. XGBoost and Random Forest are selected as the "black-box" machine learning models to generate prediction since they are widely used in the credit scoring literature for their excellent performance ( Barbaglia et al., 2021;Gunnarsson et al., 2021;Xia et al., 2017 ). We focus on the feature ranking lists and the corresponding feature importance values generated by LIME and SHAP. Sequential Rank Agreement (SRA) and Coefficient of Variation (CV) are then applied to measure the feature ranking stability and the feature importance value stability respectively. We further evaluate the "internal" stability and the "external" stabil-ity of LIME by using Variables Stability Index (VSI) and Coefficients Stability Index (CSI).</p>
        <p>The results of our experiments show that the class imbalance does have an adverse effect on the interpretive performance of both LIME and SHAP. Firstly, the feature importance rankings generated by LIME and SHAP are more stable as the class imbalance level decreases (from 1% default rate to 50% default rate). Secondly, there is greater variability among the absolute SHAP values corresponding to the same feature and also the absolute LIME coefficients corresponding to the same feature in the case of an extreme class imbalance (1%, 2.5% and 5% default rates). Finally, in LIME, the consistency of the selected features, as well as the similarity of the coefficients for the same feature, does increase as the class distribution becomes more balanced (from 1% default rate to 50% default rate). Even when we measure the "internal" stability of LIME, for which we perform repeated calls of LIME based on the same predictive model and the same training set, it appears that LIME does generate less stable interpretations at extreme class imbalance levels (1%, 2.5% and 5% default rates).</p>
        <p>To the best of our knowledge, this is the first study that measures the stability of LIME and SHAP in terms of class imbalance, which fills a key research gap in the literature. Although we focus on credit scoring in this paper, the proposed experimental framework can be used in other operational research applications that also suffer from the class imbalance problem, such as the medical industry. Our research has important implications for financial institutions and other adopters who have already or are willing to use the model-agnostic interpretation methods to interpret the "black-box" machine learning models, to measure the stability of the selected interpretation methods. Although the interpretation methods are flexible and easy to adopt, practitioners should be very careful when applying them to imbalanced datasets and making any decisions based on them, as the class imbalance can obviously exacerbate the instability of interpretations, especially at extreme class imbalance levels (under 5% for the proportion of the rare events).</p>
        <p>As mentioned in the Introduction, various imbalanced learning techniques have been introduced to improve classification performance (e.g. Calabrese &amp; Osmetti, 2015;Chawla et al., 2011;Krawczyk, 2016 ). Similarly, the potential effects of these imbalanced learning techniques on the performance of interpretation methods could be investigated in future research. It is worth noting that although the resampling methods could be used to tackle imbalanced data challenges, they could cause problems such as overfitting (over-sampling methods) or information loss (undersampling methods) ( Fernández et al., 2018;Haixiang et al., 2017;Kaur et al., 2019;Li et al., 2019 ). More importantly, resampling methods could increase randomness and add noise for the original input data, which is not desirable in financial applications and limits their prevalent in the corporate landscape ( Gunnarsson et al., 2021;Lessmann et al., 2015;Sanz et al., 2015 ). Therefore, it would be preferable to apply other imbalanced learning techniques to credit scoring, such as cost-sensitive or algorithm-based methods (e.g., Paleologo et al., 2010;Zhang et al., 2014 ) to help in generating unbiased interpretation results. More fundamentally, it would be valuable to investigate the effects of class imbalance on the stability of interpretation methods theoretically. For example, we could start using Logistic Regression as the predictive model and establishing the theoretical results of interpretations generated by SHAP or LIME in the context of class imbalance. This could provide further guidance for analysing the stability of interpretation methods when using more complicated "black-box" machine learning models.</p>
        <p>Besides the directions of future research mentioned above, another interesting extension would be to explore how sample size changes may affect the stability of the interpretation methods</p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] while the class imbalance remains the same. Finally, the novel interpretation method could be investigated to consider the class imbalance issue and the potential stability measurements could be embedded to provide insights of the interpretation stability.</p>
        <p>like LIME and Local Rule-Based Explanation</p>
        <p>https://archive.ics.uci.edu/ml/datasets/South+German+Credit .</p>
        <p>https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients .</p>
        <p>The results of XGBoost and Random Forest (RF) are very similar. Due to the page limits, we only present the results of XGBoost in the main text. The results of RF can be found in Supplementary Materials (Part H).</p>
        <p>Interpretation methods often use simplified inputs x ∈ { 0 , 1 } P that map to the original inputs through a mapping function x = h x ( x ) . Local methods try to ensure g( z ) ≈ f (h x ( z )) whenever z ≈ x .</p>
        <p>European Journal of Operational Research xxx (xxxx) xxx</p>
        <p>The reason why we choose 100 defaults is based on the number of defaults in the sample, which is 3229. When initially sampling 100 defaults, we have a sample size Z equals to (3229 -100) × 2 =</p>
        <p>, with a reasonable number of defaults in the dataset with 1% default rate, which equals to 63 ( 6258 × 1% ). A larger initially sampled number of defaults will lead to fewer defaults in the dataset with 1% default rates. Please refer to Section 4.2 for more details of the sampling process.</p>
        <p>We present some examples in the Supplementary Materials (Part B) that show how the distribution of absolute SHAP values for specific features over 100 iterations could change when the default rate increases from 1% to 50% for a specific target. This further demonstrates the importance of studying the effects of class imbalance on the interpretations</p>
        <p>When calculating the CV value for each target x i at a default rate u , we do not consider the feature which only presents once in all B explanation models since it</p>
        <p>Similar to the CV value, When calculating the CSI csi iu for a target x i at a default rate u , we do not consider the features which only presents once in all M interpretable models since there is no pair of confidence intervals can be compared. Therefore, P * in Eq. (11) will be less than or equal to the number of unique features P in M interpretable models.</p>
        <p>We take 100 iterations since we need to make sure the stability indexes converge for more accurate measurements. Based on our experiments, the results of measurements converge after around 60 iterations (See Supplementary Materials (Part E) for more details).</p>
        <p>Raffaella Calabrese acknowledges the support of the ESRC Project code ES/W010259/1.</p>
        <p>Data collection time. Collect explanatory variable 2016Q1 2016Q2 2016Q3 2016Q4 2017Q1 2017Q2 2017Q3 2017Q4 Collect response variable 2017Q1 2017Q2 2017Q3 2017Q4 2018Q1 2018Q2 2018Q3 2018Q4 Collect explanatory variable 2018Q1 2018Q2 2018Q3 2018Q4 2019Q1 2019Q2 2019Q3 2019Q4 Collect response variable 2019Q1 2019Q2 2019Q3 2019Q4 2020Q1 2020Q2 2020Q3 2020Q4</p>
        <p>JID: EOR [m5G;July 3, 2023;20:3 ] for all 10 0 0 initially sampled targets. The setting of the boxplot is the same as that described for Fig. 3 in Section 6.2 .</p>
        <p>Recall that for the chosen target, the more the VSI value approaches 1, the more the features found in different LIME interpretable models are the same. Similarly, the more the CSI value approaches 1, the more the LIME coefficient values for the same feature in different LIME interpretable models may be considered stable.</p>
        <p>As we can see in Panels (a) and (b) of Fig. 4 , internal and external VSI values show different behaviours. As shown in Panel (a), all the internal VSI values are above 0.8, most 0.9, which means that the selected features in LIME interpretable models generated from the same conditions are almost the same. Even for the extreme class imbalanced cases (1%, 2.5% and 5% default rates), internal VSI values are slightly lower than for the more balanced cases, but with an average still around 0.9. However, we can more clearly see the adverse effect of class imbalance on the stability of LIME interpretations from the boxplots of external VSI values in Panel (b). Specifically, the mean of each set of external VSI values starts from 0.7 at 1% default rate and has a distinct growing tendency with the increase of the default rate, which proves that the similarity of the selected features in LIME interpretable models increases as the class distribution becomes more balanced.</p>
        <p>When looking into the internal and external CSI values in Panels (c) and (d) of Fig. 4 , we can see both of them share the effect of class imbalance on stability, although for the external CSI values such an effect is more extreme. Similar to the internal VSI values, all the internal CSI values in Panels (c) are above 0.9, but the range of the set of internal CSI values for the extreme class imbalanced cases (1%, 2.5% and 5% default rates) are obviously larger than for the more balanced cases. The mean of the set of internal CSI values also increases from 1% default rate and remains basically unchanged after the default rate reaches 10%. It confirms that the coefficients are less stable in the case of an extreme class imbalance (1%, 2.5% and 5% default rates). As shown in Panel (d), the mean of each set of external CSI values starts from only around 0.3 at 1% default rate, then goes up with the default rate and shows a dramatic increase between 2.5% default rate and 5% default rate. It confirms that LIME generates more stable coefficients of the same feature based on more balanced datasets, and the concordance of coefficients to the same feature tends to be seriously affected at the extreme class imbalance level (1% and 2.5% default rates).</p>
        <p>When comparing the internal stability with the external stability of LIME, for both VSI and CSI values, the internal ones show higher values than the external ones at each class imbalance level. It indicates that although there are still inconsistencies, repeated</p>
        <p>Supplementary material associated with this article can be found, in the online version, at doi: 10.1016/j.ejor.2023.06.03 .</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T13:31+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Class imbalance is an active research area in the machine learning community.Class imbalance is an active research area in the machine learning community.</p>
        <p>However, existing and recent literature showed that class overlap had a higher negative impact on the performance of learning algorithms. This paper provides detailed critical discussion and objective evaluation of class overlap in the context of imbalanced data and its impact on classification accuracy. First, we present a thorough experimental comparison of class overlap and class imbalance. Unlike previous work, our experiment was carried out on the full scale of class overlap and an extreme range of class imbalance degrees. Second, we provide an in-depth critical technical review of existing approaches to handle imbalanced datasets. Existing solutions from selective literature are critically reviewed and categorised as class distribution-based and class overlap-based methods. Emerging techniques and the latest development in this area are also discussed in detail. Experimental results in this paper are consistent with existing literature and show clearly that the performance of the learning algorithm deteriorates across varying degrees of class overlap whereas class imbalance does not always have an effect. The review emphasises the need for further research towards handling class overlap in imbalanced datasets to effectively improve learning algorithms' performance.However, existing and recent literature showed that class overlap had a higher negative impact on the performance of learning algorithms. This paper provides detailed critical discussion and objective evaluation of class overlap in the context of imbalanced data and its impact on classification accuracy. First, we present a thorough experimental comparison of class overlap and class imbalance. Unlike previous work, our experiment was carried out on the full scale of class overlap and an extreme range of class imbalance degrees. Second, we provide an in-depth critical technical review of existing approaches to handle imbalanced datasets. Existing solutions from selective literature are critically reviewed and categorised as class distribution-based and class overlap-based methods. Emerging techniques and the latest development in this area are also discussed in detail. Experimental results in this paper are consistent with existing literature and show clearly that the performance of the learning algorithm deteriorates across varying degrees of class overlap whereas class imbalance does not always have an effect. The review emphasises the need for further research towards handling class overlap in imbalanced datasets to effectively improve learning algorithms' performance.</p>
        <p>Learning from datasets with skewed class distributions remains a challenge in machine learning. such datasets are realised as imbalanced datasets and widely seen in many applications, for example, anomaly detection [1], medical prediction [2,3], object recognition [4,5] and business management [6]. In these domains, the minority class is usually the class of interest and has a higher misclassification cost than the majority class. Standard learning algorithms generally build classification models based upon the maximum accuracy, which often leads to biased classification towards the majority class and misclassification of minority class instances [7,8]. However, such failure in classification of imbalanced datasets is not always caused by class imbalance solely. In fact, a linearly separable dataset can be perfectly classified by a typical classification algorithm no matter how skewed the class distribution is [9]. On the contrary, when class overlap is present, even a balanced dataset can be difficult for a learning task.Learning from datasets with skewed class distributions remains a challenge in machine learning. such datasets are realised as imbalanced datasets and widely seen in many applications, for example, anomaly detection [1], medical prediction [2,3], object recognition [4,5] and business management [6]. In these domains, the minority class is usually the class of interest and has a higher misclassification cost than the majority class. Standard learning algorithms generally build classification models based upon the maximum accuracy, which often leads to biased classification towards the majority class and misclassification of minority class instances [7,8]. However, such failure in classification of imbalanced datasets is not always caused by class imbalance solely. In fact, a linearly separable dataset can be perfectly classified by a typical classification algorithm no matter how skewed the class distribution is [9]. On the contrary, when class overlap is present, even a balanced dataset can be difficult for a learning task.</p>
        <p>When dealing with classification of imbalanced data, rebalancing class distribution is among the most common approaches that researchers consider. Many traditional and recent resampling methods only aim at getting a more balanced version of the training data and do not factor in the problem of class overlap [10,11,12]. Some methods deal with instances in the overlapping region, especially those near the borderline areas; however, their resampling rates are controlled by the degree of class imbalance [13,14]. Thus, in some scenarios, results can be highly influenced by class imbalance rather than class overlap.When dealing with classification of imbalanced data, rebalancing class distribution is among the most common approaches that researchers consider. Many traditional and recent resampling methods only aim at getting a more balanced version of the training data and do not factor in the problem of class overlap [10,11,12]. Some methods deal with instances in the overlapping region, especially those near the borderline areas; however, their resampling rates are controlled by the degree of class imbalance [13,14]. Thus, in some scenarios, results can be highly influenced by class imbalance rather than class overlap.</p>
        <p>For instance, when a dataset suffers from high class overlap but its classes are slightly imbalanced, insufficient resampling may result in class overlap not being properly addressed. On the other hand, with low class overlap and high class imbalance, excessive resampling may occur.For instance, when a dataset suffers from high class overlap but its classes are slightly imbalanced, insufficient resampling may result in class overlap not being properly addressed. On the other hand, with low class overlap and high class imbalance, excessive resampling may occur.</p>
        <p>The impacts of class imbalance, class overlap and other characteristics such as small disjunct and dataset size have been investigated [15,16,17,18]. Class overlap frequently shows the highest negative influence among potential factors including class imbalance [17,8]. This raises some important questions in handling classification of imbalanced datasets: 1) Are the solutions that mainly aim to rebalance the class distribution sufficiently effective? 2) Should the problem of class overlap be the main concern in developing new algorithms?The impacts of class imbalance, class overlap and other characteristics such as small disjunct and dataset size have been investigated [15,16,17,18]. Class overlap frequently shows the highest negative influence among potential factors including class imbalance [17,8]. This raises some important questions in handling classification of imbalanced datasets: 1) Are the solutions that mainly aim to rebalance the class distribution sufficiently effective? 2) Should the problem of class overlap be the main concern in developing new algorithms?</p>
        <p>Although several reviews on the problem of imbalanced data in classification exist [8,19,20], the problem of class overlap in imbalanced data was not emphasised as the main issue and the discussions often lacked a support of sufficient experimental evidence. Das et al. [8] proposed that the two key challenges for standard learning algorithms are class imbalance and class overlap.Although several reviews on the problem of imbalanced data in classification exist [8,19,20], the problem of class overlap in imbalanced data was not emphasised as the main issue and the discussions often lacked a support of sufficient experimental evidence. Das et al. [8] proposed that the two key challenges for standard learning algorithms are class imbalance and class overlap.</p>
        <p>Possible nature of learning outcomes in different scenarios of class imbalance and class overlap based on the dataset size were suggested; however, no experimental evidence was given. The authors also investigated other data irregularities such as small disjunct and missing features; thus, the discussion on the class overlap problem was limited. In [19], merely a brief description of other studies on the effect of class overlap in relation to class imbalance was given. The authors paid particular attention to the discussion of different techniques used in existing methods for handling imbalanced classification. Stefanowski [20] motivated the research community to develop new algorithms for imbalanced data that realise data factors, which included overlapping between classes. The author presented the analyses on characteristics of the minority class, which was divided into sub regions of safe, borderline, rare and outlier samples. This was studied along with the behaviours of different learning algorithms; however, this cannot yet be mathematically verified on real world datasets. Like in many other reviews [21,22], Kaur et al. [23] conducted a comparative analysis of methods, which was mainly organised as data preprocessing and algorithmic approaches, and the problem of class overlap was barely discussed. Some other reviews focused on the issue of imbalanced data classification in specific contexts such as big data [24,25], multi-class problem [24,26] and neural networks [27,28]. These clearly show that there is still a gap in the study of class overlap in the context of class imbalance.Possible nature of learning outcomes in different scenarios of class imbalance and class overlap based on the dataset size were suggested; however, no experimental evidence was given. The authors also investigated other data irregularities such as small disjunct and missing features; thus, the discussion on the class overlap problem was limited. In [19], merely a brief description of other studies on the effect of class overlap in relation to class imbalance was given. The authors paid particular attention to the discussion of different techniques used in existing methods for handling imbalanced classification. Stefanowski [20] motivated the research community to develop new algorithms for imbalanced data that realise data factors, which included overlapping between classes. The author presented the analyses on characteristics of the minority class, which was divided into sub regions of safe, borderline, rare and outlier samples. This was studied along with the behaviours of different learning algorithms; however, this cannot yet be mathematically verified on real world datasets. Like in many other reviews [21,22], Kaur et al. [23] conducted a comparative analysis of methods, which was mainly organised as data preprocessing and algorithmic approaches, and the problem of class overlap was barely discussed. Some other reviews focused on the issue of imbalanced data classification in specific contexts such as big data [24,25], multi-class problem [24,26] and neural networks [27,28]. These clearly show that there is still a gap in the study of class overlap in the context of class imbalance.</p>
        <p>In this paper, the importance of handling class overlap in imbalanced data classification is investigated. This was carried out through an extensive experiment and a critical review of solutions to imbalanced learning. The experiment provides an objective measurement of the impact of class overlap versus the impact of class imbalance. Unlike in previous studies [15,16,17,18], which were based on limited ranges of class imbalance and class overlap degrees, we carried out a full-scale experiment using over 1,000 synthetic datasets. The in-depth review of existing solutions to classification of imbalanced datasets is presented in an alternative perspective rather than data and algorithm levels, which was commonly arranged in other review papers [19,8,29,30,7]. We considered the main objective of the solutions and categorised them into class distribution-based and class overlap-based approaches for better comparing and contrasting the two approaches. Class distribution-based methods mainly concern and aim to suppress the problem of imbalanced class distribution. Class overlap-based methods focus on improving the visibility of instances, especially positive instances, in the overlapping region. In addition, recent and emerging methods that do not particularly deal with the class imbalance or class overlap problems are also discussed. These include, for example, the use of one of the latest techniques in machine learning, Generative Adversarial Networks (GANs) [31,32].In this paper, the importance of handling class overlap in imbalanced data classification is investigated. This was carried out through an extensive experiment and a critical review of solutions to imbalanced learning. The experiment provides an objective measurement of the impact of class overlap versus the impact of class imbalance. Unlike in previous studies [15,16,17,18], which were based on limited ranges of class imbalance and class overlap degrees, we carried out a full-scale experiment using over 1,000 synthetic datasets. The in-depth review of existing solutions to classification of imbalanced datasets is presented in an alternative perspective rather than data and algorithm levels, which was commonly arranged in other review papers [19,8,29,30,7]. We considered the main objective of the solutions and categorised them into class distribution-based and class overlap-based approaches for better comparing and contrasting the two approaches. Class distribution-based methods mainly concern and aim to suppress the problem of imbalanced class distribution. Class overlap-based methods focus on improving the visibility of instances, especially positive instances, in the overlapping region. In addition, recent and emerging methods that do not particularly deal with the class imbalance or class overlap problems are also discussed. These include, for example, the use of one of the latest techniques in machine learning, Generative Adversarial Networks (GANs) [31,32].</p>
        <p>The main contributions of this review are listed below.The main contributions of this review are listed below.</p>
        <p>1. A technical discussion with advantages and disadvantages of evaluation metrics including how some of them can be misleading in certain imbalanced contexts 2. An extensive experiment illustrating the scales of impact of class overlap and class imbalance on imbalanced dataset classification 3. A critical discussion of methods and literature selected from leading peerreviewed publications in the perspective of class overlap-based and class distribution-based approaches, as well as recent emerging technologies 4. An overview of benchmarking methods in the literature showing commonlyused ones that can be considered as good standards, but at the same time suggesting a need for comparing against recent and state-of-the-art methods for more convincing and reliable evaluation The remainder of this paper is organised as follows. In Section 2, we give the definitions of class imbalance and class overlap. Section 3 contains an in-depth discussion of evaluation metrics used in imbalanced learning. Section 4 provides the experimental results and discussion on the effects of class imbalance and class overlap on the learner's performance in an extensive range of scenarios. In section 5, we critically review existing approaches for handling classification of imbalanced datasets. Finally, the conclusion is delivered in Section 6.1. A technical discussion with advantages and disadvantages of evaluation metrics including how some of them can be misleading in certain imbalanced contexts 2. An extensive experiment illustrating the scales of impact of class overlap and class imbalance on imbalanced dataset classification 3. A critical discussion of methods and literature selected from leading peerreviewed publications in the perspective of class overlap-based and class distribution-based approaches, as well as recent emerging technologies 4. An overview of benchmarking methods in the literature showing commonlyused ones that can be considered as good standards, but at the same time suggesting a need for comparing against recent and state-of-the-art methods for more convincing and reliable evaluation The remainder of this paper is organised as follows. In Section 2, we give the definitions of class imbalance and class overlap. Section 3 contains an in-depth discussion of evaluation metrics used in imbalanced learning. Section 4 provides the experimental results and discussion on the effects of class imbalance and class overlap on the learner's performance in an extensive range of scenarios. In section 5, we critically review existing approaches for handling classification of imbalanced datasets. Finally, the conclusion is delivered in Section 6.</p>
        <p>An imbalanced dataset is a dataset with an unequal distribution of classes. This is depicted in Figure 1, where majority and minority class instances are represented by circles and triangles, respectively. In machine learning, class imbalance becomes an issue when the minority class is significantly smaller in size and is the primary class of interest with a relatively high misclassification cost. Thus, in a binary-class problem, the minority class is also realised as the positive class whereas the majority class is the negative class.An imbalanced dataset is a dataset with an unequal distribution of classes. This is depicted in Figure 1, where majority and minority class instances are represented by circles and triangles, respectively. In machine learning, class imbalance becomes an issue when the minority class is significantly smaller in size and is the primary class of interest with a relatively high misclassification cost. Thus, in a binary-class problem, the minority class is also realised as the positive class whereas the majority class is the negative class.</p>
        <p>The degree of class imbalance can be measured as the imbalance ratio (imb) as expressed in Eq. 1 or the percentage of the minority class (%minority) as shown in Eq.2, where M and m are the numbers of instances in the majority class and minority class, respectively. Since class overlap has not been mathematically well characterised [34], a standard measurement of the overlap degree is not yet defined. Several approaches have been formulated to estimate the overlap degree, however, with limitations.The degree of class imbalance can be measured as the imbalance ratio (imb) as expressed in Eq. 1 or the percentage of the minority class (%minority) as shown in Eq.2, where M and m are the numbers of instances in the majority class and minority class, respectively. Since class overlap has not been mathematically well characterised [34], a standard measurement of the overlap degree is not yet defined. Several approaches have been formulated to estimate the overlap degree, however, with limitations.</p>
        <p>For example, in [16], the overlap degree of a synthetic dataset was determined from the overlapping area with respect to the the total data space. In [33],For example, in [16], the overlap degree of a synthetic dataset was determined from the overlapping area with respect to the the total data space. In [33],</p>
        <p>the authors adapted such measurement so that class imbalance was also taken into account seeing that the minority class is relatively more overwhelmed by class overlap. The overlap degree was instead measured from the overlapping area with respect to the the total area of the positive class. Another common approach is using the classification error as the estimated overlap degree, e.g., the percentage of instances misclassified by the k-Nearest Neighbor rule [35] (kNN) with respect to the number of total instances [36,37]. However, in [34],the authors adapted such measurement so that class imbalance was also taken into account seeing that the minority class is relatively more overwhelmed by class overlap. The overlap degree was instead measured from the overlapping area with respect to the the total area of the positive class. Another common approach is using the classification error as the estimated overlap degree, e.g., the percentage of instances misclassified by the k-Nearest Neighbor rule [35] (kNN) with respect to the number of total instances [36,37]. However, in [34],</p>
        <p>the authors showed that such an approach was inaccurate and proposed a use of the ridge curves of the probabilistic density function to quantify class overlap.the authors showed that such an approach was inaccurate and proposed a use of the ridge curves of the probabilistic density function to quantify class overlap.</p>
        <p>The computation was based on the ratio of the saddle point to a smaller peak of the ridge curves of the two classes. This method is one of a few existing methods that measure overlap from the actual contour of data and can be extended to handle multi-class datasets. The main drawback of this approach is that it is only applicable to datasets with normal distributions of both data and features, which is impracticable to real-world datasets. In [38], the overlap degree was defined as the distance between the class centroids, which is likely to be inaccurate due to arbitrary shapes and non-uniformity of data in nature. Another approach [39] was based on Support Vector Data Description (SVDD) [40]. SVDD was used to locate approximated boundaries of each class in binary-class datasets, and the overlapping region was estimated based on the amount of the common instances found within both boundaries. Similar to the approach of [38], this method tends to introduce high errors in the overlap approximation since SVDD is only capable of discovering a spherical-shaped boundary of a class, which is not ideal for real-world datasets.The computation was based on the ratio of the saddle point to a smaller peak of the ridge curves of the two classes. This method is one of a few existing methods that measure overlap from the actual contour of data and can be extended to handle multi-class datasets. The main drawback of this approach is that it is only applicable to datasets with normal distributions of both data and features, which is impracticable to real-world datasets. In [38], the overlap degree was defined as the distance between the class centroids, which is likely to be inaccurate due to arbitrary shapes and non-uniformity of data in nature. Another approach [39] was based on Support Vector Data Description (SVDD) [40]. SVDD was used to locate approximated boundaries of each class in binary-class datasets, and the overlapping region was estimated based on the amount of the common instances found within both boundaries. Similar to the approach of [38], this method tends to introduce high errors in the overlap approximation since SVDD is only capable of discovering a spherical-shaped boundary of a class, which is not ideal for real-world datasets.</p>
        <p>For our experiment discussed in Section 4, we follow the measure of class overlap proposed in [33] (Eq. 3). Figure 1 illustrates how the regions in the equation are approximated.For our experiment discussed in Section 4, we follow the measure of class overlap proposed in [33] (Eq. 3). Figure 1 illustrates how the regions in the equation are approximated.</p>
        <p>overlap(%) = overlapping area minority class area * 100overlap(%) = overlapping area minority class area * 100</p>
        <p>(3)(3)</p>
        <p>Some typical evaluation metrics for classification are not affected by skewed class distributions while others can be misleading with biases towards the majority class. Common metrics for classification of imbalanced datasets such as sensitivity, specificity, balanced accuracy, G-mean, AUC and F1-score will be discussed in detail. For other assessment measures, the reader may refer to [41,42,43,44].Some typical evaluation metrics for classification are not affected by skewed class distributions while others can be misleading with biases towards the majority class. Common metrics for classification of imbalanced datasets such as sensitivity, specificity, balanced accuracy, G-mean, AUC and F1-score will be discussed in detail. For other assessment measures, the reader may refer to [41,42,43,44].</p>
        <p>In imbalanced problems, accurate detection of minority class instances is crucial. This is usually evaluated in terms of sensitivity, which is also known as true positive rate (TPR) or recall. The metric is formulated as in Eq. 4,In imbalanced problems, accurate detection of minority class instances is crucial. This is usually evaluated in terms of sensitivity, which is also known as true positive rate (TPR) or recall. The metric is formulated as in Eq. 4,</p>
        <p>where TP and FN denote true positive and false negative, respectively. As sensitivity only reflects the performance over one class, it is often reported in conjunction with another metric, such as specificity (i.e. true negative rate -Eq. 5, where TN and FP denote true negative and false positive, respectively), balanced accuracy, G-mean and AUC, to also explore the overall performance or the trade-off between the classes [45,46,47].where TP and FN denote true positive and false negative, respectively. As sensitivity only reflects the performance over one class, it is often reported in conjunction with another metric, such as specificity (i.e. true negative rate -Eq. 5, where TN and FP denote true negative and false positive, respectively), balanced accuracy, G-mean and AUC, to also explore the overall performance or the trade-off between the classes [45,46,47].</p>
        <p>Balanced accuracy is the arithmetic mean of the accuracy over each class (Eq. 6) [45,48,49,22]. It is also referred to as balanced mean accuracy [50],Balanced accuracy is the arithmetic mean of the accuracy over each class (Eq. 6) [45,48,49,22]. It is also referred to as balanced mean accuracy [50],</p>
        <p>average accuracy [51,11,52], macro-accuracy [53], etc. The traditional accuracy (Eq. 7) can be significantly misleading when class imbalance is high and the negative class accuracies (T N and T N + F P ) are highly dominant. For instance, a perfectly classified negative class of 1000 instances with an entirely misclassified positive class with 10 instances result in over 99% accuracy, which could be misleading as a good classification model. In fact, this same case yields 50% balanced accuracy, which more reflects the actual performance of the model.average accuracy [51,11,52], macro-accuracy [53], etc. The traditional accuracy (Eq. 7) can be significantly misleading when class imbalance is high and the negative class accuracies (T N and T N + F P ) are highly dominant. For instance, a perfectly classified negative class of 1000 instances with an entirely misclassified positive class with 10 instances result in over 99% accuracy, which could be misleading as a good classification model. In fact, this same case yields 50% balanced accuracy, which more reflects the actual performance of the model.</p>
        <p>Thus, balanced accuracy often replaces the traditional accuracy, and it is among the most common measures used for imbalanced problems [54].Thus, balanced accuracy often replaces the traditional accuracy, and it is among the most common measures used for imbalanced problems [54].</p>
        <p>balanced accuracy = sensitivity + specif icity 2 ( 6)balanced accuracy = sensitivity + specif icity 2 ( 6)</p>
        <p>Another metric for evaluating the overall performance is G-mean [55]. It is the geometric mean of sensitivity and specificity (Eq. 8). Since both G-mean and balanced accuracy give the average values of sensitivity and specificity, they are often used interchangeably. Based on the literature reviewed in this paper, G-mean was more frequently used. This could be attributed to the fact that G-mean is also a widely-known metric for datasets with non-skewed class distributions whereas balanced accuracy roughly reduces to the traditional overall accuracy in such scenarios.Another metric for evaluating the overall performance is G-mean [55]. It is the geometric mean of sensitivity and specificity (Eq. 8). Since both G-mean and balanced accuracy give the average values of sensitivity and specificity, they are often used interchangeably. Based on the literature reviewed in this paper, G-mean was more frequently used. This could be attributed to the fact that G-mean is also a widely-known metric for datasets with non-skewed class distributions whereas balanced accuracy roughly reduces to the traditional overall accuracy in such scenarios.</p>
        <p>G -mean = specif icity * sensitivityG -mean = specif icity * sensitivity</p>
        <p>AM -GM inequality :AM -GM inequality :</p>
        <p>balanced accuracy ≥ G -meanbalanced accuracy ≥ G -mean</p>
        <p>According to the Arithmetic Mean-Geometric Mean Inequality theory (AM-GM inequality) (9), it can be said that balanced accuracy is always greater than or equal to G-mean (10). The equality holds when sensitivity and specificity are equal. For further analysis, consider Fig. 2, which presents values of G-mean and balanced accuracy across varying scenarios in terms of the difference between sensitivity and specificity values. On the x-axis, all possible combinations of sensitivity and specificity at a step of 10% are shown. It can be seen that the difference between G-mean and balanced accuracy becomes greater when the difference between sensitivity and specificity increases. This is due to the fact that the geometric mean is affected more by the lower value.According to the Arithmetic Mean-Geometric Mean Inequality theory (AM-GM inequality) (9), it can be said that balanced accuracy is always greater than or equal to G-mean (10). The equality holds when sensitivity and specificity are equal. For further analysis, consider Fig. 2, which presents values of G-mean and balanced accuracy across varying scenarios in terms of the difference between sensitivity and specificity values. On the x-axis, all possible combinations of sensitivity and specificity at a step of 10% are shown. It can be seen that the difference between G-mean and balanced accuracy becomes greater when the difference between sensitivity and specificity increases. This is due to the fact that the geometric mean is affected more by the lower value.</p>
        <p>Here are some examples to illustrate the differences in G-mean and balanced accuracy in difference scenarios. In Fig. 2, at specif icity = 90% and sensitivity = 60%, G-mean is 73.48% and balanced accuracy is 75%. The difference between G-mean and balanced accuracy in this case is not significant. In an extreme case where specif icity = 100% and sensitivity = 10%, the resulting G-mean is 31.62% while balanced accuracy is 55%. It is clearly seen that G-mean is affected more by sensitivity. For another extreme case when there is zero accuracy of any class, G-mean= 0. This suggests that G-mean is able to reflect these unfavourable scenarios where balanced accuracy only provides average values. Thus, to determine a more suitable metric between G-mean and balanced accuracy, the user will need to carefully make a selection based on the application domain and the main objective of the classification task. Another common metric, F1-score, is the harmonic mean of sensitivity and precision as expressed in Eq.11. It is also a widely-used metric for imbalanced problems [37,53,56]. However, unlike G-mean and balanced accuracy, F1-score takes into account of precision instead of specificity. As shown in Eq. 12, precision is dependent of FP and TP. Since FP and TP are not normalised with respect to the class size, FP can be excessively higher than TP in an extremely imbalanced case. This high FP can be deceptive when in fact the true false positive rate (FPR) is insignificant. In such case, precision is strongly influenced by FP and does not reflect the actual performance on the positive class. As a consequence, F1-score will be misleading.Here are some examples to illustrate the differences in G-mean and balanced accuracy in difference scenarios. In Fig. 2, at specif icity = 90% and sensitivity = 60%, G-mean is 73.48% and balanced accuracy is 75%. The difference between G-mean and balanced accuracy in this case is not significant. In an extreme case where specif icity = 100% and sensitivity = 10%, the resulting G-mean is 31.62% while balanced accuracy is 55%. It is clearly seen that G-mean is affected more by sensitivity. For another extreme case when there is zero accuracy of any class, G-mean= 0. This suggests that G-mean is able to reflect these unfavourable scenarios where balanced accuracy only provides average values. Thus, to determine a more suitable metric between G-mean and balanced accuracy, the user will need to carefully make a selection based on the application domain and the main objective of the classification task. Another common metric, F1-score, is the harmonic mean of sensitivity and precision as expressed in Eq.11. It is also a widely-used metric for imbalanced problems [37,53,56]. However, unlike G-mean and balanced accuracy, F1-score takes into account of precision instead of specificity. As shown in Eq. 12, precision is dependent of FP and TP. Since FP and TP are not normalised with respect to the class size, FP can be excessively higher than TP in an extremely imbalanced case. This high FP can be deceptive when in fact the true false positive rate (FPR) is insignificant. In such case, precision is strongly influenced by FP and does not reflect the actual performance on the positive class. As a consequence, F1-score will be misleading.</p>
        <p>To demonstrate such an issue, consider an example of a dataset with 10:1000 positive to negative class instances and the classification result of 10 true positives and 10 false positives. This indicates 100% sensitivity and 1% FPR, which is generally highly desirable. Yet, the precision turns out to be 50% leading to a 67% F1-score, which very much underestimates and deviates from the actual performance.To demonstrate such an issue, consider an example of a dataset with 10:1000 positive to negative class instances and the classification result of 10 true positives and 10 false positives. This indicates 100% sensitivity and 1% FPR, which is generally highly desirable. Yet, the precision turns out to be 50% leading to a 67% F1-score, which very much underestimates and deviates from the actual performance.</p>
        <p>It is also worth pointing out is that using F1-score alone may not be sufficient to compare models. In other words, any two models that yield similar FP, TP and sensitivity, will have similar F1-score regardless of their difference in FPRs.It is also worth pointing out is that using F1-score alone may not be sufficient to compare models. In other words, any two models that yield similar FP, TP and sensitivity, will have similar F1-score regardless of their difference in FPRs.</p>
        <p>Consider an example of two models predicting on datasets with 10:100 and 10:10000 positive to negative class instances where the models achieve 10% and 0.1% FPR, respectively. Given the same sensitivity gained, the models have the same value of F1-score accordingly, which is 67%. In fact, the former case with 10% FPR is less favourable than the latter case with 0.1% FPR, but F1-score does not convey that. Thus, the use of F1-score alone may not be sufficient to indicate the quality of a classification model in imbalanced domains. Yet, it can be meaningful when carefully considered along with other measures.Consider an example of two models predicting on datasets with 10:100 and 10:10000 positive to negative class instances where the models achieve 10% and 0.1% FPR, respectively. Given the same sensitivity gained, the models have the same value of F1-score accordingly, which is 67%. In fact, the former case with 10% FPR is less favourable than the latter case with 0.1% FPR, but F1-score does not convey that. Thus, the use of F1-score alone may not be sufficient to indicate the quality of a classification model in imbalanced domains. Yet, it can be meaningful when carefully considered along with other measures.</p>
        <p>Another commonly-used metric is the area under the receiver operating characteristic curve (AUC). A receiver operating characteristic curve (ROC)Another commonly-used metric is the area under the receiver operating characteristic curve (AUC). A receiver operating characteristic curve (ROC)</p>
        <p>visualises the values of TPR against FPR at varying probability thresholds.visualises the values of TPR against FPR at varying probability thresholds.</p>
        <p>AUC gives a summary of the ROC curve as a single value. AUC is often used to compare the performance among classifiers; however, there have been someAUC gives a summary of the ROC curve as a single value. AUC is often used to compare the performance among classifiers; however, there have been some</p>
        <p>arguments raised against its usage [57]. Firstly, ROC curves are useful when misclassification costs and class distributions are not specified [44]; so is AUC [58].arguments raised against its usage [57]. Firstly, ROC curves are useful when misclassification costs and class distributions are not specified [44]; so is AUC [58].</p>
        <p>This suggests that ROC and AUC can be used for inspecting and summarising the general performance of a classifier. However, in a real-life application, the error costs are known and a model can be fine-tuned for the optimal results, which eventually falls onto a single point on the RUC curve. Thus, a classifier with a higher AUC does not necessarily give a better result. This leads to the second argument that visual inspection of ROC curves should be carried out instead of considering only AUC values [57]. However, often there is no clear winning between the two ROC curves making it difficult to compare [58]. Last but foremost, AUC weights the positive and negative class errors equally while in many application domains, misclassification costs are unequal. In this case, summarising over all possible threshold values is unconvincing [59].This suggests that ROC and AUC can be used for inspecting and summarising the general performance of a classifier. However, in a real-life application, the error costs are known and a model can be fine-tuned for the optimal results, which eventually falls onto a single point on the RUC curve. Thus, a classifier with a higher AUC does not necessarily give a better result. This leads to the second argument that visual inspection of ROC curves should be carried out instead of considering only AUC values [57]. However, often there is no clear winning between the two ROC curves making it difficult to compare [58]. Last but foremost, AUC weights the positive and negative class errors equally while in many application domains, misclassification costs are unequal. In this case, summarising over all possible threshold values is unconvincing [59].</p>
        <p>In summary, it is recommended that for evaluation of imbalanced dataset classification, individual class accuracy, especially sensitivity, is considered along with an overall performance measure such as balanced accuracy or G-mean. F1score and AUC can also be assessed; however, they should be carefully discussed due to the constraints addressed above.In summary, it is recommended that for evaluation of imbalanced dataset classification, individual class accuracy, especially sensitivity, is considered along with an overall performance measure such as balanced accuracy or G-mean. F1score and AUC can also be assessed; however, they should be carefully discussed due to the constraints addressed above.</p>
        <p>When handling classification of imbalanced data, rebalancing the class distribution is often an approach that researchers take. However, it should also be realised that class overlap is another common issue in classification tasks, which becomes more serious when it occurs in an imbalanced context. Many traditional and recent resampling methods for handling imbalanced datasets only aim at making the class distribution balanced and do not factor in the problem of class overlap [10,11,12]. On the other hand, some resampling methods deal with instances in the overlapping region, especially those near the borderline areas, without concerning the resulting class distribution [33,56,60]. There also exist methods that address both of the class overlap and class imbalance problems [13,14]. In the last type of methods, problematic instances in the overlapping region are resampled until the class distribution becomes balance. This means that the problem of class overlap is handled according to the class imbalance degree and regardless of the class overlap degree. As a result, insufficient resampling may occur when class imbalance is low. On the contrary, when class imbalance is high, these methods may lead to excessive resampling. All of these approaches have shown their potentials in improving classification results in different ways. Countless variations of existing methods make it impossible to compare and find out which approach is better. Instead, we can consider the scale of effect of class imbalance in comparison to class overlap. This will advise which of the problems should be more concerned.When handling classification of imbalanced data, rebalancing the class distribution is often an approach that researchers take. However, it should also be realised that class overlap is another common issue in classification tasks, which becomes more serious when it occurs in an imbalanced context. Many traditional and recent resampling methods for handling imbalanced datasets only aim at making the class distribution balanced and do not factor in the problem of class overlap [10,11,12]. On the other hand, some resampling methods deal with instances in the overlapping region, especially those near the borderline areas, without concerning the resulting class distribution [33,56,60]. There also exist methods that address both of the class overlap and class imbalance problems [13,14]. In the last type of methods, problematic instances in the overlapping region are resampled until the class distribution becomes balance. This means that the problem of class overlap is handled according to the class imbalance degree and regardless of the class overlap degree. As a result, insufficient resampling may occur when class imbalance is low. On the contrary, when class imbalance is high, these methods may lead to excessive resampling. All of these approaches have shown their potentials in improving classification results in different ways. Countless variations of existing methods make it impossible to compare and find out which approach is better. Instead, we can consider the scale of effect of class imbalance in comparison to class overlap. This will advise which of the problems should be more concerned.</p>
        <p>Previous literature suggested that class overlap had a higher negative effect on the learner's performance than class imbalance [16,17,18]. In [17], the authors showed that imbalanced datasets with no presence of class overlap could be perfectly classified using fuzzy sets. Moreover, when the class overlap degree was low, class imbalance had no significant effects on the classification results. It has to be pointed out that these observations were based on the maximum overlap degree of 64% (see [17] for their measurement of the overlap degree) although a wide range of class imbalance degrees was used in the experiment. Similar findigns were reported in [18] when using decision tree (DT), rule based and k-NN classifiers. Interestingly, the authors of [61] showed that when sufficient training data was available, class imbalance did not degrade the performance of support vector machine (SVM). Unlike in the case of class overlap, SVM performance significantly degraded. These experiments however were carried out with only a few variations of class imbalance and class overlap. For instance, in [16], some datasets were simulated such that the positive class became dominant in the overlapping region. This created inconsistencies in the overall imbalance degree as defined in Eq. 2, ranged from 10% to 100% with a step of 10. In each dataset, there were 1,000 majority class instances and the number of minority class instances was based on the imbalance degree. Datasets were uniformly distributed, which means that the data densities of the majority class and the minority class were equal. The rationale behind this was two-folded. First, there was no class imbalance in the overlapping region to ensure that each of the components, e.g. class imbalance and class overlap, was solely investigated with no interfering effect of the other. Second, there would be no effects on the learning algorithm caused by differences in the data density. An example of such synthetic datasets is illustrated in Figure 3.Previous literature suggested that class overlap had a higher negative effect on the learner's performance than class imbalance [16,17,18]. In [17], the authors showed that imbalanced datasets with no presence of class overlap could be perfectly classified using fuzzy sets. Moreover, when the class overlap degree was low, class imbalance had no significant effects on the classification results. It has to be pointed out that these observations were based on the maximum overlap degree of 64% (see [17] for their measurement of the overlap degree) although a wide range of class imbalance degrees was used in the experiment. Similar findigns were reported in [18] when using decision tree (DT), rule based and k-NN classifiers. Interestingly, the authors of [61] showed that when sufficient training data was available, class imbalance did not degrade the performance of support vector machine (SVM). Unlike in the case of class overlap, SVM performance significantly degraded. These experiments however were carried out with only a few variations of class imbalance and class overlap. For instance, in [16], some datasets were simulated such that the positive class became dominant in the overlapping region. This created inconsistencies in the overall imbalance degree as defined in Eq. 2, ranged from 10% to 100% with a step of 10. In each dataset, there were 1,000 majority class instances and the number of minority class instances was based on the imbalance degree. Datasets were uniformly distributed, which means that the data densities of the majority class and the minority class were equal. The rationale behind this was two-folded. First, there was no class imbalance in the overlapping region to ensure that each of the components, e.g. class imbalance and class overlap, was solely investigated with no interfering effect of the other. Second, there would be no effects on the learning algorithm caused by differences in the data density. An example of such synthetic datasets is illustrated in Figure 3.</p>
        <p>Random Forests (RF) was chosen as the learning algorithm for the following reasons. First, it is a representative of standard learning algorithms that aim at maximising the overall classification accuracy such as DT, SVM, neural net, naive bayes, etc. Without an appropriate adjustment, these learning algorithms tend to be influenced more by the dominating class, which will result in biased classification. Second, RF is robust to overfitting [62], which helps minimise the effect of different sample sizes. Lastly, though RF is one of the most widely-used learning algorithms for classification of imbalanced datasets [22], it was not experimented in previous studies [16,17,18].Random Forests (RF) was chosen as the learning algorithm for the following reasons. First, it is a representative of standard learning algorithms that aim at maximising the overall classification accuracy such as DT, SVM, neural net, naive bayes, etc. Without an appropriate adjustment, these learning algorithms tend to be influenced more by the dominating class, which will result in biased classification. Second, RF is robust to overfitting [62], which helps minimise the effect of different sample sizes. Lastly, though RF is one of the most widely-used learning algorithms for classification of imbalanced datasets [22], it was not experimented in previous studies [16,17,18].</p>
        <p>The default parameter settings of RF in 
            <rs type="software">caret</rs>
            <rs type="version">1</rs> package in R were used. That i.e %minority = 10 and 20. This can be attributed to the effect of sample size.
        </p>
        <p>That is, at lower imbalance degrees, the sample sizes were larger, which might be sufficient to suppress the effect of class imbalance [61,18]. Finally, the effect of class imbalance was more obvious when the overlap degree was higher. These results suggest that the impact of class imbalance on sensitivity highly depended on the level of class overlap as well as the sample size.That is, at lower imbalance degrees, the sample sizes were larger, which might be sufficient to suppress the effect of class imbalance [61,18]. Finally, the effect of class imbalance was more obvious when the overlap degree was higher. These results suggest that the impact of class imbalance on sensitivity highly depended on the level of class overlap as well as the sample size.</p>
        <p>On the other hand, class overlap clearly degraded sensitivity at all degrees.On the other hand, class overlap clearly degraded sensitivity at all degrees.</p>
        <p>A higher decrease in sensitivity per change in %overlap can be seen when class imbalance was high enough. This was the symmetrical effect that class imbalance and class overlap had on each other. That is, the presence of one element can strengthen the scale of impact of the other element. However, this only applied in some certain scenarios, e.g. when class imbalance was sufficiently high. It is worth pointing out that even with no presence of class imbalance (%minority = 100), the influence of class overlap on sensitivity was apparent. In contrast, when there was no class overlap, the ideal sensitivity value was achievable regardless of imbalance degrees. Thus, it can be said that the effect of class imbalance is dependent of the presence of class overlap, but not the other way around.A higher decrease in sensitivity per change in %overlap can be seen when class imbalance was high enough. This was the symmetrical effect that class imbalance and class overlap had on each other. That is, the presence of one element can strengthen the scale of impact of the other element. However, this only applied in some certain scenarios, e.g. when class imbalance was sufficiently high. It is worth pointing out that even with no presence of class imbalance (%minority = 100), the influence of class overlap on sensitivity was apparent. In contrast, when there was no class overlap, the ideal sensitivity value was achievable regardless of imbalance degrees. Thus, it can be said that the effect of class imbalance is dependent of the presence of class overlap, but not the other way around.</p>
        <p>Finally, all of these results suggest that class overlap hurts sensitivity more than class imbalance.Finally, all of these results suggest that class overlap hurts sensitivity more than class imbalance.</p>
        <p>Figure 4 also shows that specificity increased as class imbalance increased. This is expected because the increase in size of the dominating class was in favour of specificity. On the other hand, specificity was negatively affected by class overlap due to the decrease in visibility of majority class instances. It can be observed that class overlap had a higher impact on sensitivity than on specificity.Figure 4 also shows that specificity increased as class imbalance increased. This is expected because the increase in size of the dominating class was in favour of specificity. On the other hand, specificity was negatively affected by class overlap due to the decrease in visibility of majority class instances. It can be observed that class overlap had a higher impact on sensitivity than on specificity.</p>
        <p>This was because class overlap was measured with respect to the data space of the minority class. Thus, the overlapping region occupied larger data space of the minority class than that of the majority class, relatively to the class size. In an extreme case, the overlapping region occupied the entire minority class but only some part of the majority class.This was because class overlap was measured with respect to the data space of the minority class. Thus, the overlapping region occupied larger data space of the minority class than that of the majority class, relatively to the class size. In an extreme case, the overlapping region occupied the entire minority class but only some part of the majority class.</p>
        <p>Interestingly, it can be seen in Figure 4 that class imbalance had no apparent impact on BA and AUC. In contrast, it is clear that BA and AUC decreased as class overlap was higher. This was due to the fact that when class overlap increases, the number of hard-to-classify samples in both class is higher. This is another evidence that researchers should pay more attention to the problem class overlap.Interestingly, it can be seen in Figure 4 that class imbalance had no apparent impact on BA and AUC. In contrast, it is clear that BA and AUC decreased as class overlap was higher. This was due to the fact that when class overlap increases, the number of hard-to-classify samples in both class is higher. This is another evidence that researchers should pay more attention to the problem class overlap.</p>
        <p>The experiment clearly shows that class overlap hurt the learning algorithm's performance more than class imbalance. This is evidenced by the results in sensitivity, balanced accuracy and AUC. While class overlap always degraded the results, class imbalance had an impact only in the presence of class overlap.The experiment clearly shows that class overlap hurt the learning algorithm's performance more than class imbalance. This is evidenced by the results in sensitivity, balanced accuracy and AUC. While class overlap always degraded the results, class imbalance had an impact only in the presence of class overlap.</p>
        <p>Moreover, the scale of impact of class imbalance on sensitivity highly depended on the degree of class overlap. That is class imbalance was more impactful when class overlap was high and it seemed insignificant when class overlap was low. Lastly, class overlap showed apparent influence on the trade-off between sensitivity and specificity, i.e. BA and AUC, whereas class imbalance did not.Moreover, the scale of impact of class imbalance on sensitivity highly depended on the degree of class overlap. That is class imbalance was more impactful when class overlap was high and it seemed insignificant when class overlap was low. Lastly, class overlap showed apparent influence on the trade-off between sensitivity and specificity, i.e. BA and AUC, whereas class imbalance did not.</p>
        <p>Existing literature often discussed solutions to imbalanced datasets as data- 1.Existing literature often discussed solutions to imbalanced datasets as data- 1.</p>
        <p>We categorised methods that are designed to reduce the bias in class distribution as class distribution-based methods. Random resampling, the simplest and most common approach, is the process of either randomly eliminating majority class instances (undersampling) or [47]; k-means SMOTE [12]; k-means undersampling [11]; Sensitivity-based undersampling [64];We categorised methods that are designed to reduce the bias in class distribution as class distribution-based methods. Random resampling, the simplest and most common approach, is the process of either randomly eliminating majority class instances (undersampling) or [47]; k-means SMOTE [12]; k-means undersampling [11]; Sensitivity-based undersampling [64];</p>
        <p>density peak-based undersampling [67]; DBMUTE [56] neighbourhood search varied by settings SLSMOTE [68]; Borderline SMOTE [13] balanced weightsdensity peak-based undersampling [67]; DBMUTE [56] neighbourhood search varied by settings SLSMOTE [68]; Borderline SMOTE [13] balanced weights</p>
        <p>Adaptive kNN [69] inversed k-INOS [70] neural networks balanced GRSOM [71]; Radial-based oversampling [72]; SMOTE-CSELM [73]; UBRKWELM [74]; UBKELM [75] ensemble balanced RUSBagging [55,76];Adaptive kNN [69] inversed k-INOS [70] neural networks balanced GRSOM [71]; Radial-based oversampling [72]; SMOTE-CSELM [73]; UBRKWELM [74]; UBKELM [75] ensemble balanced RUSBagging [55,76];</p>
        <p>RUSBoost [77];RUSBoost [77];</p>
        <p>SMOTEBagging [78];SMOTEBagging [78];</p>
        <p>BalancedEnsemble [79];BalancedEnsemble [79];</p>
        <p>GRSOM [71]; UBRKWELM [74]; UBKELM [75] varied by settings SMOTEBoost [80] inversed Inverse-imbalance Bagging [81] class overlap-based clustering not balanced OBU [45]; BoostOBU [3]; DBMUTE [56] not balanced MWMOTE [60] balanced A-SUWO [14]; NI-MWMOTE [82] neighbourhood search balanced ADASYN [83]; LR-SMOTE [84] not balanced NCL [85]; PNN [86]; NB-based undersampling [33] noise removal not balanced SMOTE-IPF [87];GRSOM [71]; UBRKWELM [74]; UBKELM [75] varied by settings SMOTEBoost [80] inversed Inverse-imbalance Bagging [81] class overlap-based clustering not balanced OBU [45]; BoostOBU [3]; DBMUTE [56] not balanced MWMOTE [60] balanced A-SUWO [14]; NI-MWMOTE [82] neighbourhood search balanced ADASYN [83]; LR-SMOTE [84] not balanced NCL [85]; PNN [86]; NB-based undersampling [33] noise removal not balanced SMOTE-IPF [87];</p>
        <p>Redency-driven Tomek-link undersampling [51] SVM not balanced VIRTUAL [88]; OSM [37]; ACFSVM [89]; IEFSVM [90] balanced DCS [91] ensemble not balanced HardEnsemble [30]; EVINCI [92] region splitting not balanced Soft-Hybrid [93] Others clustering balanced Hierachical decomposition [94] not balanced PSS [95] ensemble not balanced PT-bagging [53]; Random Balance [96] Evolutionary algorithm balanced EBUS [97]; EUSBoost [98]; EGIS-CHC [99] not balanced EUSCM [97]; EPRENNID [100] neural networks balanced LMLE-kNN [52]; cGAN oversampling [31]; MFC-GAN [32] balanced error weights DNN-MFE [101] not balanced Attention Aggregation [50]; CoSen [102]; CSDNN [103];Redency-driven Tomek-link undersampling [51] SVM not balanced VIRTUAL [88]; OSM [37]; ACFSVM [89]; IEFSVM [90] balanced DCS [91] ensemble not balanced HardEnsemble [30]; EVINCI [92] region splitting not balanced Soft-Hybrid [93] Others clustering balanced Hierachical decomposition [94] not balanced PSS [95] ensemble not balanced PT-bagging [53]; Random Balance [96] Evolutionary algorithm balanced EBUS [97]; EUSBoost [98]; EGIS-CHC [99] not balanced EUSCM [97]; EPRENNID [100] neural networks balanced LMLE-kNN [52]; cGAN oversampling [31]; MFC-GAN [32] balanced error weights DNN-MFE [101] not balanced Attention Aggregation [50]; CoSen [102]; CSDNN [103];</p>
        <p>Focal Loss [104]; DQNimb [105] SVM not balancedFocal Loss [104]; DQNimb [105] SVM not balanced</p>
        <p>Adaptive FH-SVM [106] synthesising new minority class instances (oversampling) to achieve the balanced class distribution. Although it is simple to employ, random undersampling can potentially lead to a loss of important information while random oversampling is prone to overfitting [7]. Moreover, it was shown that randomly rebalancing class distribution did not guarantee better results [107].Adaptive FH-SVM [106] synthesising new minority class instances (oversampling) to achieve the balanced class distribution. Although it is simple to employ, random undersampling can potentially lead to a loss of important information while random oversampling is prone to overfitting [7]. Moreover, it was shown that randomly rebalancing class distribution did not guarantee better results [107].</p>
        <p>One of the most well-established methods, Synthetic Minority Over-sampling Technique (SMOTE), was designed to create new instances using linear interpolation between minority class neighbouring points [10]. The authors suggested that the method could expand the decision regions of the minority class and as a results caused less overfitting than random oversampling. Due to its simplicity yet decent performance, SMOTE has been widely applied to real-world problems [108,109,110]. However, its weaknesses has been presented. In [99], it was shown that by applying SMOTE, their classification results were not improved.One of the most well-established methods, Synthetic Minority Over-sampling Technique (SMOTE), was designed to create new instances using linear interpolation between minority class neighbouring points [10]. The authors suggested that the method could expand the decision regions of the minority class and as a results caused less overfitting than random oversampling. Due to its simplicity yet decent performance, SMOTE has been widely applied to real-world problems [108,109,110]. However, its weaknesses has been presented. In [99], it was shown that by applying SMOTE, their classification results were not improved.</p>
        <p>This could have been because the method does not include any selection criteria for linear interpolation; hence, synthesised instances may not be useful unless they are near the decision boundary. For more detailed discussion on drawbacks of SMOTE, the reader is referred to [111]. These disadvantages have led to many extensions of SMOTE such as DBSMOTE [47], DBMUTE [56], Borderline-SMOTE [13], Safe-Level-SMOTE [68] (SLSMOTE) and others [12,80,78].This could have been because the method does not include any selection criteria for linear interpolation; hence, synthesised instances may not be useful unless they are near the decision boundary. For more detailed discussion on drawbacks of SMOTE, the reader is referred to [111]. These disadvantages have led to many extensions of SMOTE such as DBSMOTE [47], DBMUTE [56], Borderline-SMOTE [13], Safe-Level-SMOTE [68] (SLSMOTE) and others [12,80,78].</p>
        <p>DBSMOTE [47] is an oversampling method relying on Density-Based Spatial Clustering of Applications with Noise (DBSCAN) [112] to locate instances in different areas. SLSMOTE [68] is another oversampling method based on neighbourhood searching. The main objective of both methods is to synthesise more minority class instances in the non-overlapping region and minimise the synthesis in the overlapping and borderline areas. Although both 
            <rs type="software">DBSMOTE</rs> and 
            <rs type="software">SLSMOTE</rs> often achieved improvement over SMOTE, other extensions of SMOTE showed superior performance. In particular, these were 
            <rs type="software">DBMUTE</rs> [56] and Borderline-SMOTE [13], which also utilize 
            <rs type="software">DBSCAN</rs> and neighbourhood searching, respectively. It is worth noting that, however, these two methods synthesise more minority class instances near the borderline regions, which q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 q q q q q q q q q q q q q q q 0 10 20 In [12], the authors proposed a method to account for possible amplification of noise created by SMOTE. They applied k-means clustering to discover clusters dominated by the positive class. This was followed by oversampling in such clusters with the oversampling amount inversely proportional to the number of positive instances. A similar approach was presented in [70]. Both methods however led to significant decreases in the minority class accuracy. This could have been attributed to the exclusion of essential positive instances that were sparse and overlapped with dominating negative instances, especially those near the borderline.
        </p>
        <p>Although undersampling has an advantage of reducing the training set size, which results in lower computational costs [113], this could lead to information loss at the same time. To address this issue, clustering is among the common techniques employed during undersampling to ensure the diversity of the remaining instances. In [64] and [11], the authors applied k-means clustering on the majority class and selected representative instances from each cluster. Similar approach was proposed by Di et al. [114]. The authors used a more recent clustering algorithm, density peak-based clustering [67], which not only considers the distance but also the local density. These clustering-based methods resulted in reduced training sets with diversified samples. However, since balanced class distribution was aimed, when applying these method on a dataset with a relatively very small minority class size, they nonetheless resulted a significant loss of information.Although undersampling has an advantage of reducing the training set size, which results in lower computational costs [113], this could lead to information loss at the same time. To address this issue, clustering is among the common techniques employed during undersampling to ensure the diversity of the remaining instances. In [64] and [11], the authors applied k-means clustering on the majority class and selected representative instances from each cluster. Similar approach was proposed by Di et al. [114]. The authors used a more recent clustering algorithm, density peak-based clustering [67], which not only considers the distance but also the local density. These clustering-based methods resulted in reduced training sets with diversified samples. However, since balanced class distribution was aimed, when applying these method on a dataset with a relatively very small minority class size, they nonetheless resulted a significant loss of information.</p>
        <p>Several solutions based on neural networks have also been recently proposed [71,72,73,75]. In [71] [73,75,115,74,116]. ELM is a single-layer feed-forward neural network that uses a random approach to generate the hidden layer weights. This enables its training speed to be faster than other gradient-based algorithms [74].Several solutions based on neural networks have also been recently proposed [71,72,73,75]. In [71] [73,75,115,74,116]. ELM is a single-layer feed-forward neural network that uses a random approach to generate the hidden layer weights. This enables its training speed to be faster than other gradient-based algorithms [74].</p>
        <p>The authors exploited this benefit of ELM, and since the traditional ELM was not designed for imbalanced data, they proposed to use many techniques to rebalance the data such as class-specific regularization parameters computed based on the class distribution [115], SMOTE [73] and UnderBagging [74,75].The authors exploited this benefit of ELM, and since the traditional ELM was not designed for imbalanced data, they proposed to use many techniques to rebalance the data such as class-specific regularization parameters computed based on the class distribution [115], SMOTE [73] and UnderBagging [74,75].</p>
        <p>Another neural network-based method was introduced in [72]. The authors used radial basis functions to locate overlapping and non-overlapping regions and avoided synthesising new minority class instances in the overlapping region.Another neural network-based method was introduced in [72]. The authors used radial basis functions to locate overlapping and non-overlapping regions and avoided synthesising new minority class instances in the overlapping region.</p>
        <p>However, by doing so, the density of the minority class instances in the overlapping region became relatively sparser. As a consequence, they had a higher tendency to appear as noise to the learning algorithm. Results showed that the method improved specificity but led to lower sensitivity, which is undesired in imbalanced problems. This was consistent with the results obtained with DBSMOTE [47] and SLSMOTE [68] discussed earlier, and underlines the need of improving the visibility of the minority class instances in the overlapping region.However, by doing so, the density of the minority class instances in the overlapping region became relatively sparser. As a consequence, they had a higher tendency to appear as noise to the learning algorithm. Results showed that the method improved specificity but led to lower sensitivity, which is undesired in imbalanced problems. This was consistent with the results obtained with DBSMOTE [47] and SLSMOTE [68] discussed earlier, and underlines the need of improving the visibility of the minority class instances in the overlapping region.</p>
        <p>Ensemble-based classifiers, which are known to often outperform single classifiers [22], have been extensively adopted to handle imbalanced datasets.Ensemble-based classifiers, which are known to often outperform single classifiers [22], have been extensively adopted to handle imbalanced datasets.</p>
        <p>In [79], the authors proposed an approach to preserve all available information in building an ensemble-based classifier. This was achieved by subsetting the majority class and combining with the minority class instances with equal class distribution. Other than preventing information loss, another advantage of this method is that every base classifier is trained with no bias in class distribution.In [79], the authors proposed an approach to preserve all available information in building an ensemble-based classifier. This was achieved by subsetting the majority class and combining with the minority class instances with equal class distribution. Other than preventing information loss, another advantage of this method is that every base classifier is trained with no bias in class distribution.</p>
        <p>Several widely-known ensemble-based methods are the integrations of ensemble algorithms, such as Bagging (i.e. Bootstrap aggregating) [117] and Boosting [118], and common class distribution-based methods. These are, for example, the combinations of random undersampling and Bagging [55,76], random undersampling and Boosting (RUSBoost) [77], SMOTE and Boosting [80], and SMOTE and Bagging [78]. These methods provided promising results, however, at the cost of higher computational complexity.Several widely-known ensemble-based methods are the integrations of ensemble algorithms, such as Bagging (i.e. Bootstrap aggregating) [117] and Boosting [118], and common class distribution-based methods. These are, for example, the combinations of random undersampling and Bagging [55,76], random undersampling and Boosting (RUSBoost) [77], SMOTE and Boosting [80], and SMOTE and Bagging [78]. These methods provided promising results, however, at the cost of higher computational complexity.</p>
        <p>Unlike typical class distribution-based methods, which attempt to rebalance the class distribution, an inversion of class imbalance was proposed in [81].Unlike typical class distribution-based methods, which attempt to rebalance the class distribution, an inversion of class imbalance was proposed in [81].</p>
        <p>This was done by randomly undersampling the negative class until the positive class became over-represented. As a result, higher positive class accuracy was obtained. At the same time, this caused lower negative class accuracy. The authors addressed this issue by combining the approach with Bagging. ResultsThis was done by randomly undersampling the negative class until the positive class became over-represented. As a result, higher positive class accuracy was obtained. At the same time, this caused lower negative class accuracy. The authors addressed this issue by combining the approach with Bagging. Results</p>
        <p>showed that by doing so, the trade-off between TPR and FPR was improved.showed that by doing so, the trade-off between TPR and FPR was improved.</p>
        <p>Class overlap-based methods mainly address the class overlap problem in classification of imbalanced datasets. Methods in this category deal with either overlapped instances near the borderline or instances in the entire overlapping region. Folllowing [93], we define borderline instances as those along the borderline region between the two classes whereas overlapped instances may reside further from the border. Therefore, we can say that borderline instances are a subset of overlapped instances. The common objective of overlap-based approaches is to emphasise the presence of the minority class in the overlapping region. This is depicted in Figure 6, which shows the resulting datasets after applying simple class overlap-based resampling methods. In Figure 6(b), (c) and (d), q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 30 40 50 0 20 40 60 x1 x2 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 30 40 50 0 20 40 60 x1 x2 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 30 40 50 0 20 40 60 x1 x2 q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0 10 20 SMOTEBagging [78] and 
            <rs type="software">RUSBoost</rs> [77]. The ability of the methods to provide outstanding results in sensitivity makes them suitable for many real-world problems that require high predication accuracy on the minority class such as in the medical domain and security-related issues, where the accuracy of the majority class can be more compromised.
        </p>
        <p>A neighbourhood-based (NB-based) undersampling framework was proposed in [33] aiming at accurately removing problematic majority class instances from the overlapping region to prevent excessive elimination while enhancing the presence of the minority class. Four methods based on neighbourhood searching to locate overlapped majority class instances were used. Competitive results with other state-of-the-art methods were achieved. Also, its superior results over OBU [45] suggesting that under the same objective to maximise the presence of minority class instances, this NB-based framework, which uses a more local technique, was more efficient. Its successful application in the medical domain was also demonstrated [3]. One of the NB-based methods was selected to handled public medical-related datasets and showed the highest sensitivity on average, which is usually preferable in a medical problem, while often obtaining higher trade-offs between sensitivity and specificity than other methods. However, these NB-based methods are solely based on Euclidean distance. There may be some variations in results on real-world problems due to other data characteristics such as density and class densities, which was not considered in the work. As an alternative to the above methods, Adaptive Synthetic sampling (ADASYN) was introduced to enhance the presence of the minority class by selectively oversampling in the overlapping region [83]. Instance generation was based on the neighbouring condition. That is, the amount of new instances generated from each minority class instance was proportional to the number of its majority class nearest neighbours. Consequently, more instances were created in the overlapping region while unnecessary syntheses outside such a region were avoided. However, a major drawback of ADASYN is that sparse minority class instances that are highly overlapped with majority class instances will be excluded from oversampling.A neighbourhood-based (NB-based) undersampling framework was proposed in [33] aiming at accurately removing problematic majority class instances from the overlapping region to prevent excessive elimination while enhancing the presence of the minority class. Four methods based on neighbourhood searching to locate overlapped majority class instances were used. Competitive results with other state-of-the-art methods were achieved. Also, its superior results over OBU [45] suggesting that under the same objective to maximise the presence of minority class instances, this NB-based framework, which uses a more local technique, was more efficient. Its successful application in the medical domain was also demonstrated [3]. One of the NB-based methods was selected to handled public medical-related datasets and showed the highest sensitivity on average, which is usually preferable in a medical problem, while often obtaining higher trade-offs between sensitivity and specificity than other methods. However, these NB-based methods are solely based on Euclidean distance. There may be some variations in results on real-world problems due to other data characteristics such as density and class densities, which was not considered in the work. As an alternative to the above methods, Adaptive Synthetic sampling (ADASYN) was introduced to enhance the presence of the minority class by selectively oversampling in the overlapping region [83]. Instance generation was based on the neighbouring condition. That is, the amount of new instances generated from each minority class instance was proportional to the number of its majority class nearest neighbours. Consequently, more instances were created in the overlapping region while unnecessary syntheses outside such a region were avoided. However, a major drawback of ADASYN is that sparse minority class instances that are highly overlapped with majority class instances will be excluded from oversampling.</p>
        <p>An ensemble-based method, 
            <rs type="software">HardEnsemble</rs>, incorporating both oversampling and undersampling to address overlapped instances of both classes was proposed in [30]. Undersampling was performed based on the contribution to the classification accuracy of instances, which potentially facilitated removal of majority class instances in the overlapping region. Using the same criterion, oversampling was done particularly on minority class instances in the overlapping area. These two resampling processes were carried out in parallel and the resulting datasets were used to train 
            <rs type="software">RUSBoost</rs> [77]. 
            <rs type="software">HardEnsemble</rs> showed comparable performance with other solutions. Moreover, it has a benefit over many other existing solutions of no parameter tuning required.
        </p>
        <p>Another method based on ensemble and an Evolutionary Algorithm (EA) was proposed in [92]. An EA was employed so that negative instances were selectively removed from the overlapping region and relatively more minority class instances were present. The method was applied to multi-class imbalanced problems and outperformed other state-of-the-art ensemble-based methods. However, by utilising both EA and emsemble techniques, this method requires high computation costs.Another method based on ensemble and an Evolutionary Algorithm (EA) was proposed in [92]. An EA was employed so that negative instances were selectively removed from the overlapping region and relatively more minority class instances were present. The method was applied to multi-class imbalanced problems and outperformed other state-of-the-art ensemble-based methods. However, by utilising both EA and emsemble techniques, this method requires high computation costs.</p>
        <p>In [93], the authors proposed to use different learning algorithms for classifying different regions of a dataset. Non-overlapping, overlapping, and borderline regions were identified using information based on the data characteristics such as the maximum Fisher's discriminant ratio, probability distributions of the two classes, and the distance between the centers of the two classes. This was followed by using different learning algorithms in the different regions. 
            <rs type="software">DBSCAN</rs> was selected to learn the borderline region due to its ability in discovering arbitrary-shaped clusters. At the same time, Radial Basis Function Network (RBFN) was used to classified instances in the other regions. This approach showed improvement in classification results. However, it is only applicable to datasets with Gaussian distribution, which is not ideal for handling real-world problems.
        </p>
        <p>With a lower risk of information loss, several methods only focus on overlapped instances that reside near the decision boundary, which we realise as borderlineWith a lower risk of information loss, several methods only focus on overlapped instances that reside near the decision boundary, which we realise as borderline</p>
        <p>instances. An early and well-known method, Neighbourhood Cleaning Rule (NCL) [85], was adapted from the Edited Nearest Neighbor algorithm (ENN) [119]. NCL is based on removing negative instances that are either misclassified or cause misclassification of positive instances using the 3-NN classifier. Since NCL only considers three nearest neighbours, it is likely that many negative instances would still remain in the overlapping region, especially in a highly imbalanced and overlapped case. Similar approach was developed by Liang et al. [84], where negative nearest neighbours of misclassified positive samples by SVM were all removed. Further to that, SMOTE was applied to positive instances near the class center to avoid enlarging the effect of noise, which is the drawback caused by the random process of SMOTE. Both of these process contributed to the improvement in the visibility of the positive class, and this was reflected by higher sensitivity achieved. Moreover, since information loss of the negative class was minimised, good trade-offs between sensitivity and specificity were obtained.instances. An early and well-known method, Neighbourhood Cleaning Rule (NCL) [85], was adapted from the Edited Nearest Neighbor algorithm (ENN) [119]. NCL is based on removing negative instances that are either misclassified or cause misclassification of positive instances using the 3-NN classifier. Since NCL only considers three nearest neighbours, it is likely that many negative instances would still remain in the overlapping region, especially in a highly imbalanced and overlapped case. Similar approach was developed by Liang et al. [84], where negative nearest neighbours of misclassified positive samples by SVM were all removed. Further to that, SMOTE was applied to positive instances near the class center to avoid enlarging the effect of noise, which is the drawback caused by the random process of SMOTE. Both of these process contributed to the improvement in the visibility of the positive class, and this was reflected by higher sensitivity achieved. Moreover, since information loss of the negative class was minimised, good trade-offs between sensitivity and specificity were obtained.</p>
        <p>In [51], aiming at minimising information loss, only negative instances with high similarities and low contribution to classification were removed. However, no thresholds were defined as a stopping criterion for undersampling, and instead negative instances were progressively eliminated according to the similarity and contribution factors until a balanced class distribution was obtained. Applying this method on a highly imbalanced datasets could anyway result in excessive elimination of negative instances. SMOTE-IPF [87] was proposed in an attempt to remove noisy instances in the original data as well as those generated by SMOTE. This was done by simply applying a noise filter after SMOTE. The authors suggested that this approach had the following advantages over other methods by removing noise prior to oversampling. Firstly, sparse positive instances near the borderline mistaken as noise would no longer appear as noise after applying oversampling and hence would not be filtered out. This would preserve highly important information, e.g. rare cases, as well as expand the decision boundary of the positive class.In [51], aiming at minimising information loss, only negative instances with high similarities and low contribution to classification were removed. However, no thresholds were defined as a stopping criterion for undersampling, and instead negative instances were progressively eliminated according to the similarity and contribution factors until a balanced class distribution was obtained. Applying this method on a highly imbalanced datasets could anyway result in excessive elimination of negative instances. SMOTE-IPF [87] was proposed in an attempt to remove noisy instances in the original data as well as those generated by SMOTE. This was done by simply applying a noise filter after SMOTE. The authors suggested that this approach had the following advantages over other methods by removing noise prior to oversampling. Firstly, sparse positive instances near the borderline mistaken as noise would no longer appear as noise after applying oversampling and hence would not be filtered out. This would preserve highly important information, e.g. rare cases, as well as expand the decision boundary of the positive class.</p>
        <p>Secondly, having more positive instances in the overlapping region could result in some negative instances being filtered out, hence enhancing the visibility of the positive class in such a region to the learning algorithm.Secondly, having more positive instances in the overlapping region could result in some negative instances being filtered out, hence enhancing the visibility of the positive class in such a region to the learning algorithm.</p>
        <p>A modification of kNN to improve the classification of imbalanced datasets, Positive-biased Nearest Neighbour (PNN), was presented in [86]. The classification decision was adjusted to be biased towards the positive class, particularly in the regions where positive instances were found under-represented. This benefited the positive class especially in the overlapping region. The method showed superior performance over other neighbourhood-based algorithms with significantly lower computational cost.A modification of kNN to improve the classification of imbalanced datasets, Positive-biased Nearest Neighbour (PNN), was presented in [86]. The classification decision was adjusted to be biased towards the positive class, particularly in the regions where positive instances were found under-represented. This benefited the positive class especially in the overlapping region. The method showed superior performance over other neighbourhood-based algorithms with significantly lower computational cost.</p>
        <p>In addition to class overlap, the problems of small sub-clusters and withinclass imbalance were also addressed in [60,14,82]. Majority Weighted Minority Oversampling Technique (MWMOTE) [60] and Adaptive Semi-Unsupervised Weighted Oversampling (A-SUWO) [14] were proposed. In both methods, borderline minority class instances were discovered using kNN and assigned higher weights for oversampling. In addition to kNN, a semi-unsupervised hierarchical clustering was applied to improve the identification of such instances in A-SUWO.In addition to class overlap, the problems of small sub-clusters and withinclass imbalance were also addressed in [60,14,82]. Majority Weighted Minority Oversampling Technique (MWMOTE) [60] and Adaptive Semi-Unsupervised Weighted Oversampling (A-SUWO) [14] were proposed. In both methods, borderline minority class instances were discovered using kNN and assigned higher weights for oversampling. In addition to kNN, a semi-unsupervised hierarchical clustering was applied to improve the identification of such instances in A-SUWO.</p>
        <p>Subsequently, new instances were synthesised within each sub-cluster. 
            <rs type="software">MW-MOTE</rs> created more new instances in sparser sub-clusters whereas A-SUWO focused on oversampling more instances in sub-clusters with higher misclassification errors. Both methods showed improvement in classification results, however, with many parameters needed to be fine-tuned. Moreover, A-SUWO uses complex techniques that may cause poor sampling when it overcomplicates sub-clusters [82]. Wei et al [82] further improved these approaches and proposed 
            <rs type="software">NI-MWMOTE</rs>, which was developed based on 
            <rs type="software">MWMOTE</rs> [60]. They introduced adaptive noise removal based on distance and neighbour density before considering borderline instances for oversampling to avoid generation of new noise.
        </p>
        <p>Support Vector Machine (SVM) is one of the most frequently-used classifiers with imbalanced problems [22]. It has also been adapted in several methods for handling imbalanced datasets [88,91,120]. This includes the use of support vectors to identify and resample potential borderline instances [88,91] considering that support vectors are mostly composed of such instances [91]. In [88], an SVM-based active learning algorithm was combined with SMOTE to adaptively synthesise instances between positive support vectors in each active learning.Support Vector Machine (SVM) is one of the most frequently-used classifiers with imbalanced problems [22]. It has also been adapted in several methods for handling imbalanced datasets [88,91,120]. This includes the use of support vectors to identify and resample potential borderline instances [88,91] considering that support vectors are mostly composed of such instances [91]. In [88], an SVM-based active learning algorithm was combined with SMOTE to adaptively synthesise instances between positive support vectors in each active learning.</p>
        <p>Unlike typical data resampling, this oversampling was repeatedly performed during the training process. Similarly, Jian et al. [91] resampled instances based on support vectors. They made use of Biased SVM [121], which is a learning algorithm implemented specifically to handle imbalanced datasets, to identify support and non-support vectors in the training data. Oversampling and undersampling were then applied to support and non-support vectors, respectively. By doing so, more informative instances were emphasised and information loss was minimised. Cho et al. [120] developed IEFSVM based on EFSVM [90] with a modified entropy for the fuzzy SVM algorithm (FSVM) [122].Unlike typical data resampling, this oversampling was repeatedly performed during the training process. Similarly, Jian et al. [91] resampled instances based on support vectors. They made use of Biased SVM [121], which is a learning algorithm implemented specifically to handle imbalanced datasets, to identify support and non-support vectors in the training data. Oversampling and undersampling were then applied to support and non-support vectors, respectively. By doing so, more informative instances were emphasised and information loss was minimised. Cho et al. [120] developed IEFSVM based on EFSVM [90] with a modified entropy for the fuzzy SVM algorithm (FSVM) [122].</p>
        <p>IEFSVM reduced the importance of majority class instances that were detected close to minority ones. This was considered from the changes in the nearest neighbours' classes when the number of nearest neighbours (k) was increased.IEFSVM reduced the importance of majority class instances that were detected close to minority ones. This was considered from the changes in the nearest neighbours' classes when the number of nearest neighbours (k) was increased.</p>
        <p>However, this technique may not be sufficiently effective when the majority class highly dominates in the overlapping region unless k is set high enough.However, this technique may not be sufficiently effective when the majority class highly dominates in the overlapping region unless k is set high enough.</p>
        <p>An algorithmic solution based on SVM, an overlap-sensitive margin classifier (OSM), was proposed in [37]. It began with instance weighting that was proportional to the degrees of class imbalance and class overlap, and locating authors employed such a technique to allow parallel sampling in large datasets.An algorithmic solution based on SVM, an overlap-sensitive margin classifier (OSM), was proposed in [37]. It began with instance weighting that was proportional to the degrees of class imbalance and class overlap, and locating authors employed such a technique to allow parallel sampling in large datasets.</p>
        <p>All discovered clusters of the majority class were simultaneously undersampled to speed up the learning process. Undersampling was carried out in a way that minimum negative class instances were remained for effective training of an SVM classifier. That is, only negative instances near the class boundary were kept in the training set. The method proved a substantial reduction in the computational complexity while comparable results to other existing methods were achieved.All discovered clusters of the majority class were simultaneously undersampled to speed up the learning process. Undersampling was carried out in a way that minimum negative class instances were remained for effective training of an SVM classifier. That is, only negative instances near the class boundary were kept in the training set. The method proved a substantial reduction in the computational complexity while comparable results to other existing methods were achieved.</p>
        <p>As distinct from typical algorithm-level methods, PT-bagging [53] The variety of the training subsets resulted in diversified weak classifiers, which is beneficial for constructing a good ensemble-based model [123]. Despite its simplicity, results showed that this method performed better than some other state-of-the-art ensembles that are more complex.As distinct from typical algorithm-level methods, PT-bagging [53] The variety of the training subsets resulted in diversified weak classifiers, which is beneficial for constructing a good ensemble-based model [123]. Despite its simplicity, results showed that this method performed better than some other state-of-the-art ensembles that are more complex.</p>
        <p>The application of Evolutionary Algorithms (EAs) has been extensively seen in recent solutions to imbalanced problems [99,97,98,100]. An undersampling framework based on evolutionary prototype selection algorithms was introduced in [97]. The framework aimed at maximising classification results while minimising the training set size. Many variations of methods under this framework were proposed. Both balanced and imbalanced training sets were obtained using the proposed variations, and unlike most undersampling methods, removing minority class instances was allowed. Substantial reductions in sizes of both positive and negative classes were reported while comparable results with well-established methods were achieved. An ensemble-based extension of this evolutionary-based undersampling approach, 
            <rs type="software">EUSBoost</rs>, was presented in [98]. 
            <rs type="software">EUSBoost</rs> is the integration of Boosting and the evolutionary-based undersampling with a modified fitness function to obtain diversified weak classifiers. The extension showed better performance over many state-of-the-art ensembles.
        </p>
        <p>EPRENNID is an integratation of ensemble, undersampling and oversampling based on evoluationary algorithms [100]. In particular, evolutionary prototype selection and prototype generation were used as undersampling and oversampling techniques, respectively. By employing evolutionary prototype selection on both positive and negative instances, several reduced subsets were obtained. Then, only well-performing subsets were selected for subsequently applying prototype generation on. To avoid overfitting, which may be introduced by prototype generation, combinations of several resampled subsets were used for ensemblebased classification. EPRENNID produced relatively robust results on different densities of the minority class compared to some existing solutions while reducing instances of both classes. The method showed better performance than many wellknown methods; however, its training time was far higher than those methods.EPRENNID is an integratation of ensemble, undersampling and oversampling based on evoluationary algorithms [100]. In particular, evolutionary prototype selection and prototype generation were used as undersampling and oversampling techniques, respectively. By employing evolutionary prototype selection on both positive and negative instances, several reduced subsets were obtained. Then, only well-performing subsets were selected for subsequently applying prototype generation on. To avoid overfitting, which may be introduced by prototype generation, combinations of several resampled subsets were used for ensemblebased classification. EPRENNID produced relatively robust results on different densities of the minority class compared to some existing solutions while reducing instances of both classes. The method showed better performance than many wellknown methods; however, its training time was far higher than those methods.</p>
        <p>This was attributed to the use of an EA together with an ensemble technique, which are both computationally expensive.This was attributed to the use of an EA together with an ensemble technique, which are both computationally expensive.</p>
        <p>Another evolutionary-based method was proposed in [99]. The authors applied an EA for selecting the generalised exemplars, i.e. representative instances, that maximised classification results, particularly in AUC. Classification decisions of new instances were made based on their distances to these generalised exemplars. Experiments showed that the method performed better than other exemplar-based learning algorithms.Another evolutionary-based method was proposed in [99]. The authors applied an EA for selecting the generalised exemplars, i.e. representative instances, that maximised classification results, particularly in AUC. Classification decisions of new instances were made based on their distances to these generalised exemplars. Experiments showed that the method performed better than other exemplar-based learning algorithms.</p>
        <p>One of the most recent approaches for handling imbalanced datasets is the use of neural network algorithms. Like other learning algorithms, deep NeuralOne of the most recent approaches for handling imbalanced datasets is the use of neural network algorithms. Like other learning algorithms, deep Neural</p>
        <p>Networks have been used to learn imbalanced datasets, and to improve performance, data resampling and cost-sensitive learning methods were applied [52,124,103]. A great number of new loss functions for handling class imbalance have been introduced recently. In [101,50], new loss functions were formulated to reduce the bias in imbalanced class distribution. The authors of [101] proposed to use loss functions that considered the error rates of individual classes; however, results showed trivial improvement over the mean square error (MSE), a commonly-used loss function in deep learning. In [103] and [102], novel loss ants [125] in object detection. Due to its simplicity and effectiveness, many later methods have been designed based on Focal Loss. This includes its adaptation in the loss function of standard SVM to handle imbalanced data [106].Networks have been used to learn imbalanced datasets, and to improve performance, data resampling and cost-sensitive learning methods were applied [52,124,103]. A great number of new loss functions for handling class imbalance have been introduced recently. In [101,50], new loss functions were formulated to reduce the bias in imbalanced class distribution. The authors of [101] proposed to use loss functions that considered the error rates of individual classes; however, results showed trivial improvement over the mean square error (MSE), a commonly-used loss function in deep learning. In [103] and [102], novel loss ants [125] in object detection. Due to its simplicity and effectiveness, many later methods have been designed based on Focal Loss. This includes its adaptation in the loss function of standard SVM to handle imbalanced data [106].</p>
        <p>In [69], two novel adaptive kNN algorithms for imbalanced classification were proposed. Neural networks were applied in the first proposed algorithm to find the minimum value of k that correctly classified each instance in the training set. In the second algorithm, the value of k was inversely proportional to the local density. This allowed a relatively smaller k value to be used in the overlapping region, which was suggested to be more effective in classifying overlapped instances than a high value of k [37,16].In [69], two novel adaptive kNN algorithms for imbalanced classification were proposed. Neural networks were applied in the first proposed algorithm to find the minimum value of k that correctly classified each instance in the training set. In the second algorithm, the value of k was inversely proportional to the local density. This allowed a relatively smaller k value to be used in the overlapping region, which was suggested to be more effective in classifying overlapped instances than a high value of k [37,16].</p>
        <p>Over the past few years, extensions of a state-of-the-art data augmentation algorithm, Generative Adversarial Net (GAN) [126], have been used as oversampling methods for imbalanced datasets [31,32,127]. GAN consists of two models -the generative model, which generates new samples as similar to the original data as possible, and the discriminative model, which attempts to distinguish between the original data and the generated data. The objective of GAN is to simultaneously optimise the two models so that the overall distance between the original and the generated distribution is minimised. This ability of GAN was employed as an oversampling technique in [31] and [32] to synthesise minority class instances. In [31], Conditional GAN (cGAN) [128] was directly applied as an oversampling method. Since GAN is an unsupervised learning algorithm, the authors included class labels as an additional learning condition required in cGAN. Results showed that the method outperformed common resampling methods such as Borderline SMOTE [13], ADASYN [83].Over the past few years, extensions of a state-of-the-art data augmentation algorithm, Generative Adversarial Net (GAN) [126], have been used as oversampling methods for imbalanced datasets [31,32,127]. GAN consists of two models -the generative model, which generates new samples as similar to the original data as possible, and the discriminative model, which attempts to distinguish between the original data and the generated data. The objective of GAN is to simultaneously optimise the two models so that the overall distance between the original and the generated distribution is minimised. This ability of GAN was employed as an oversampling technique in [31] and [32] to synthesise minority class instances. In [31], Conditional GAN (cGAN) [128] was directly applied as an oversampling method. Since GAN is an unsupervised learning algorithm, the authors included class labels as an additional learning condition required in cGAN. Results showed that the method outperformed common resampling methods such as Borderline SMOTE [13], ADASYN [83].</p>
        <p>However, there was inconsistency in the results, which migh have been attributedHowever, there was inconsistency in the results, which migh have been attributed</p>
        <p>to insufficient numbers of training data [129,130]. In [32], Multiple Fake Class GAN (MFC-GAN) was proposed specifically as an oversampling technique to rebalance class distribution. Unlike common GAN extensions, MFC-GAN was designed to created multiple fake classes to improve the classification accuracy of the minority class. This method was evaluated on multi-class image datasets and results showed that it outperformed SMOTE and other GAN extensions [131,132]. Despite promising results achieved using these GAN-based methods, a limitation on the size of training data when applying a deep learning model is a concern.to insufficient numbers of training data [129,130]. In [32], Multiple Fake Class GAN (MFC-GAN) was proposed specifically as an oversampling technique to rebalance class distribution. Unlike common GAN extensions, MFC-GAN was designed to created multiple fake classes to improve the classification accuracy of the minority class. This method was evaluated on multi-class image datasets and results showed that it outperformed SMOTE and other GAN extensions [131,132]. Despite promising results achieved using these GAN-based methods, a limitation on the size of training data when applying a deep learning model is a concern.</p>
        <p>One of the latest technologies, deep reinforcement learning (DRL), has been recently used to handle imbalanced classification tasks [105]. DRL is a combination of deep learning and reinforcement learning. It has recently gained significant interests by the research community due to its ability to successfully learn complex decision-making tasks, which may not be achievable by other standard learning algorithms [133]. In [105], the authors formulated the classification problem as a sequential decision-making process and solve it using DRL, which followed the approach of Wiering et al. [134] to apply reinforcement learning in classification tasks. This approach is considered relatively new in this research topic and need to be further investigated. Although it is a powerful method, DRL has a major has a major drawback on extreme complexity and computational performance [135]. Moreover, it is limited to only very large dataset as DRL is known to be data-hungry. Despite these advantages, this DRL-based method has revealed a new alternative for handling imbalanced data classification and paved the way for researchers to develop new emerging approaches in this field.One of the latest technologies, deep reinforcement learning (DRL), has been recently used to handle imbalanced classification tasks [105]. DRL is a combination of deep learning and reinforcement learning. It has recently gained significant interests by the research community due to its ability to successfully learn complex decision-making tasks, which may not be achievable by other standard learning algorithms [133]. In [105], the authors formulated the classification problem as a sequential decision-making process and solve it using DRL, which followed the approach of Wiering et al. [134] to apply reinforcement learning in classification tasks. This approach is considered relatively new in this research topic and need to be further investigated. Although it is a powerful method, DRL has a major has a major drawback on extreme complexity and computational performance [135]. Moreover, it is limited to only very large dataset as DRL is known to be data-hungry. Despite these advantages, this DRL-based method has revealed a new alternative for handling imbalanced data classification and paved the way for researchers to develop new emerging approaches in this field.</p>
        <p>An overview of common and well-known methods that were used in the reviewed literature for evaluation and comparison purposes is presented in this subsection. Table 2 outlines these benchmarking methods mapped with their compared methods and listed in the order of publishing year. Table 3, The information provided in methods that can be considered as good standards for evaluating purposes.An overview of common and well-known methods that were used in the reviewed literature for evaluation and comparison purposes is presented in this subsection. Table 2 outlines these benchmarking methods mapped with their compared methods and listed in the order of publishing year. Table 3, The information provided in methods that can be considered as good standards for evaluating purposes.</p>
        <p>885 However, it is worth pointing out that some of these methods such as SMOTE and Borderline SMOTE are long-established and have been outperformed by a number of more recent methods. This suggests that there is a need for benchmarking new algorithms against recent and state-of-the-art methods for more convincing and reliable evaluation.885 However, it is worth pointing out that some of these methods such as SMOTE and Borderline SMOTE are long-established and have been outperformed by a number of more recent methods. This suggests that there is a need for benchmarking new algorithms against recent and state-of-the-art methods for more convincing and reliable evaluation.</p>
        <p>In this paper, we provided a comprehensive review on the impact of class overlap in classification of imbalanced datasets. This was presented through an extensive experiment, an in-depth discussion on existing solutions, a technical [96] real slightly highly ensembles(DT,kNN): PT-bagging [53] discussion on evaluation metrics, and an overview of benchmarking methods.In this paper, we provided a comprehensive review on the impact of class overlap in classification of imbalanced datasets. This was presented through an extensive experiment, an in-depth discussion on existing solutions, a technical [96] real slightly highly ensembles(DT,kNN): PT-bagging [53] discussion on evaluation metrics, and an overview of benchmarking methods.</p>
        <p>The experiment was carried out at the full scale of class overlap and extreme degrees of class imbalance. Results showed that classification errors increased with the degree of class overlap regardless of imbalance degree. Moreover, the effect of class imbalance highly depended on the presence of class overlap. We also critically discussed related literature and methods for handling imbalanced dataset classification selected from leading peer-reviewed publications. The methods were categorised into class distribution-based approach, class overlap-based approach and other emerging techniques for the discussion. Our experimental results and literature review highlighted the importance of the class overlap problem. In general, the choice of suitable methods will vary across problems due to different misclassification costs and variations in objectives or requirements of the users. However, based on the experimental results, the problem of class overlap should be addressed in all cases whereas handling class imbalance may not be necessary. Suggested approaches are as follows:The experiment was carried out at the full scale of class overlap and extreme degrees of class imbalance. Results showed that classification errors increased with the degree of class overlap regardless of imbalance degree. Moreover, the effect of class imbalance highly depended on the presence of class overlap. We also critically discussed related literature and methods for handling imbalanced dataset classification selected from leading peer-reviewed publications. The methods were categorised into class distribution-based approach, class overlap-based approach and other emerging techniques for the discussion. Our experimental results and literature review highlighted the importance of the class overlap problem. In general, the choice of suitable methods will vary across problems due to different misclassification costs and variations in objectives or requirements of the users. However, based on the experimental results, the problem of class overlap should be addressed in all cases whereas handling class imbalance may not be necessary. Suggested approaches are as follows:</p>
        <p>When there is no class overlap, classification tasks can be handled using standard learning algorithms regardless of the imbalance degree. Thus, no application of additional methods is needed.When there is no class overlap, classification tasks can be handled using standard learning algorithms regardless of the imbalance degree. Thus, no application of additional methods is needed.</p>
        <p>For datasets with slight to moderate imbalance degrees, overlap-based methods are likely to be a better approach when improvement in sensitivity is expected. Also, those methods with no concern of rebalancing the class distribution may be preferable since results show clearly that in such scenarios, class imbalance barely has an impact on the learner's performance. Finally, our review also showed that emerging techniques such as deep learning algorithms and evolutionary algorithms have constantly gained the community's attention. This is because they are self-learning and capable of providing optimal results. Although the use of such algorithms have been widely proposed to address the class imbalance problem [32,98,100], class overlap was rarely discussed. Also, these techniques have some well-known limitations. Besides highFor datasets with slight to moderate imbalance degrees, overlap-based methods are likely to be a better approach when improvement in sensitivity is expected. Also, those methods with no concern of rebalancing the class distribution may be preferable since results show clearly that in such scenarios, class imbalance barely has an impact on the learner's performance. Finally, our review also showed that emerging techniques such as deep learning algorithms and evolutionary algorithms have constantly gained the community's attention. This is because they are self-learning and capable of providing optimal results. Although the use of such algorithms have been widely proposed to address the class imbalance problem [32,98,100], class overlap was rarely discussed. Also, these techniques have some well-known limitations. Besides high</p>
        <p>synthesising instances of both majority and minority classes. When majority class undersampling is needed, an entirely new majority class instances are created to replace the original minority class instances. Raghuwanshi and Shukla have recently proposed many variants of methods based on extreme learning machine (ELM)synthesising instances of both majority and minority classes. When majority class undersampling is needed, an entirely new majority class instances are created to replace the original minority class instances. Raghuwanshi and Shukla have recently proposed many variants of methods based on extreme learning machine (ELM)</p>
        <p>-5 suggests common and reliable-5 suggests common and reliable</p>
        <p>https://CRAN.R-project.org/package=carethttps://CRAN.R-project.org/package=caret</p>
        <p>different regions, i.e. highly overlapping and low overlapping, using the FSVM.different regions, i.e. highly overlapping and low overlapping, using the FSVM.</p>
        <p>Then, different learning algorithms were employed in different regions. In the low overlapping region, the classification was carried out using fuzzy SVM. An extreme local search algorithm, 1-NN, which had shown better results than other classifiers for highly imbalanced and overlapped data [16], was used in the highly overlapping region. Results showed that OSM outperformed other well-known SVM-based classifiers while consuming lower training time. In [89], FSVM was employed with modified membership values to give lower importance to borderline majority class instances. Such instances as well as potential majority-class outliers were identified using techniques based on SVDD [40] and the kernel kNN. This allowed the classification boundary to shift toward the minority class.Then, different learning algorithms were employed in different regions. In the low overlapping region, the classification was carried out using fuzzy SVM. An extreme local search algorithm, 1-NN, which had shown better results than other classifiers for highly imbalanced and overlapped data [16], was used in the highly overlapping region. Results showed that OSM outperformed other well-known SVM-based classifiers while consuming lower training time. In [89], FSVM was employed with modified membership values to give lower importance to borderline majority class instances. Such instances as well as potential majority-class outliers were identified using techniques based on SVDD [40] and the kernel kNN. This allowed the classification boundary to shift toward the minority class.</p>
        <p>The method outperformed other SVM-based techniques for imbalanced data.The method outperformed other SVM-based techniques for imbalanced data.</p>
        <p>However, with infeasibility of SVM on large datasets due to the huge memory requirement [106], these SVM-based methods face the same limitation.However, with infeasibility of SVM on large datasets due to the huge memory requirement [106], these SVM-based methods face the same limitation.</p>
        <p>Rather than focusing on the class overlap and class imbalance problems, many recent solutions use alternative approaches in handling classification of imbalanced datasets. These include the use of emerging techniques such as deep neural networks-based algorithms, genetic algorithms and one of the latest technologies, deep reinforcement learning. Unlike traditional solutions, some of these methods have the main objective to preserve the topology of the original data and in some methods, undersampling is not limited to majority class instances but removal of minority class instances is also allowed.Rather than focusing on the class overlap and class imbalance problems, many recent solutions use alternative approaches in handling classification of imbalanced datasets. These include the use of emerging techniques such as deep neural networks-based algorithms, genetic algorithms and one of the latest technologies, deep reinforcement learning. Unlike traditional solutions, some of these methods have the main objective to preserve the topology of the original data and in some methods, undersampling is not limited to majority class instances but removal of minority class instances is also allowed.</p>
        <p>A hierarchical classification method that is an integration of basic methods, e.g. clustering, outlier detection and feature selection, was proposed in [94]. The authors pointed out that clustering of outliers and minority class instances may provide similar results. Thus, they employed an outlier detection method to detect minority class instances in each level of the hierarchy. The method was able to effectively handle highly imbalanced and highly overlapped datasets.A hierarchical classification method that is an integration of basic methods, e.g. clustering, outlier detection and feature selection, was proposed in [94]. The authors pointed out that clustering of outliers and minority class instances may provide similar results. Thus, they employed an outlier detection method to detect minority class instances in each level of the hierarchy. The method was able to effectively handle highly imbalanced and highly overlapped datasets.</p>
        <p>Data clustering was also used in [95], however for a different purpose. The computational complexity, neural network-based techniques generally require large training data, which is not often available in certain imbalanced domains, e.g, medical-related fields. Thus, another possible future direction may include the development of methods using emerging techniques, for example, GAN-based methods to deal with overlapped instances of small-sized datasets.Data clustering was also used in [95], however for a different purpose. The computational complexity, neural network-based techniques generally require large training data, which is not often available in certain imbalanced domains, e.g, medical-related fields. Thus, another possible future direction may include the development of methods using emerging techniques, for example, GAN-based methods to deal with overlapped instances of small-sized datasets.</p>
    </text>
</tei>

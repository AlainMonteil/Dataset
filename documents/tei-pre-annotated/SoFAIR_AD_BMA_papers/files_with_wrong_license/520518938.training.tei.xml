<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T09:40+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>We study the incentives of a digital business to collect and protect users' data. The users' data the business collects improve the service it provides to consumers, but they may also be accessed, at a cost, by strategic third parties in a way that harms users, imposing endogenous users' privacy costs. We characterize how the revenue model of the business shapes its optimal data strategy: collection and protection of users' data. A business with a more data-driven revenue model will collect more users' data and provide more data protection than a similar business that is more usage-driven. Consequently, if users have small direct benefit from data collection, then more usage-driven businesses generate larger consumer surplus than their more data-driven counterparts (the reverse holds if users have large direct benefit from data collection). Relative to the socially desired data strategy, the business may over-or under-collect users' data and may over-or under-protect it. Restoring e ciency requires a two-pronged regulatory policy, covering both data collection and data protection; one such policy combines a minimal data protection requirement with a tax proportional to the amount of collected data. We finally show that existing regulation in the US, which focuses only on data protection, may even harm consumer surplus and overall welfare.</p>
        <p>"The problem with data protection laws is that it presumes the data collection was ok.</p>
        <p>The growing social and economic activity conducted online -from sharing location data on Uber to searching for medications or diagnoses on Google -generates extensive amounts of data. These data are used to improve products and services o ered to consumers, but there are also undesirable consequences. From firms like Cambridge Analytica using Facebook data to sway election outcomes and manipulate public opinion to health insurance companies anticipating medical needs of potential insurees based on undisclosed personal data -opportunities for user data exploitation are rife. 1 These and other numerous privacy scandals and data breaches of recent years have further raised consumer privacy concerns and data privacy has been singled out as one of the biggest challenges faced by the digital economy. 2 Governments responded with new data privacy laws and regulations, which, as argued by many, are only partially e ective at best. 3 This paper addresses four fundamental questions: (1) What are the trade-o s that people face when using online services? (2) What are the determinants of harmful use of users' data? (3) What are the incentives of digital businesses to collect and protect users' data? and (4) What actions should be taken by regulatory authorities in order to protect consumer privacy and maximize social welfare?</p>
        <p>We develop an economic model of the creation, collection, protection, and potential misuse of users' data (Section 2). We focus on activity data: information on users' interactions with the digital business's services such as users ' posts, messages, clicks, etc. 4 A digital business chooses a data strategy which comprises a choice of data collection and data protection. The data collection strategy specifies the proportion of users' activity that is collected, stored, and processed by the business. Examples include the decision by Whatsapp to encrypt users' text messages (thus reducing the amount of information it collects) and Facebook's practices (at least until August 2019) of 1 A few articles that illustrate these undesirable consequences are "How Trump Consultants Exploited the Facebook Data of Millions", New York Times, 17 March 2018 (see also Papanastasiou 2020 and Candogan and Drakopoulos 2020),and " 'We've Been Breached': Inside the Equifax Hack", Wall Street Journal, 18 September 2017</p>
        <p>2 "Americans and Privacy: Concerned, Confused and Feeling Lack of Control Over Their Personal Information", dating services, are some of the numerous examples of such businesses. Between these two extremes, lie ad-driven companies like Facebook and Google, whose main source of revenue is o ering targeted advertisements and, therefore, they capitalize directly on the users' information they collects. At the same time, these businesses also attaches a direct value to user activity because they need users to be active for them to view and click on the ads. 6 We first characterize, in Section 3, how di erent data strategies shape users' demand for the business's services and the adversaries' demand for users' data (Proposition 1). We show that, as the business's data collection strategy becomes more expansive, users' activity first increases and then decreases. The resulting amount of users' data that is actually collected by the business, follows a similar pattern. Such non-monotonicity reflects the endogenous reaction of adversarial activity to the change in the data collection strategy that, in turn, shapes users' privacy costs.</p>
        <p>When the data collection strategy is restricted, users' privacy costs are low because only a handful of adversaries are active. Hence, an increase in data collection improves the service to users at a little privacy cost. As the business collects data on a larger fraction of users' activity, adversaries' activity also increases, thereby imposing larger privacy costs on users, who, in turn, reduce their activity. With respect to the data protection strategy, we show that an increase in data protection reduces adversarial activity and, therefore, lowers privacy costs and increases both average users' activity and total users' data collected.</p>
        <p>Our second set of results, in Section 4, provides a characterization of the optimal business data strategy (Theorem 1). We show that data-collection and data-protection are complementary instruments in generating profit for the business. This implies that, all else equal, a more data-driven digital business sets higher levels of data collection and data protection. Consequently, if users have small direct benefit from data collection, then usage-driven businesses generate larger consumer surplus than their more data-driven counterparts. The opposite is true if users' direct benefit from data collection are significant (Proposition 2)</p>
        <p>Section 5 compares the business's data strategy with the data strategy that maximizes social welfare (a weighted sum of consumer surplus and the business's profit). Theorem 2 shows that a business may, relative to the social optimal policy, over-collect and over-protect users' data, over-collect and under-protect, or under-collect and under-protect, depending on the parameters of the model and, in particular, the business's revenue model. To complement this result, our numerical analysis shows that, typically, businesses with significant usage-driven revenue components tend to under-collect and under-protect users' data, whereas businesses with revenue models that include significant data-driven components tend to over-collect users' data and often also over-protect it.</p>
        <p>There are three implications of this comparison. First, a data strategy policy must take into considerations the underlying business model. For example, a policy that regulates only one dimension of the data strategy, say data protection, should incentivize data protection investments for usage-driven businesses, and may need to disincentivize such investments for data-driven businesses. Second, it is di cult to restore e ciency with a regulation that focuses only on one aspect of the data policy. For example, the practice of the US Federal Trade Commission (FTC), which has a mandate to enforce a minimum level of data protection, is not su cient to restore e ciency. 7 In fact, Proposition 3 shows that such regulations may even decrease consumer surplus and overall welfare, especially when a business's revenue model is intermediate, putting similar weights on data and usage. Third, e ciency can be restored by a two-pronged regulatory policy that combines a requirement of a minimum level of data protection together with either a liability policy or a tax on stored data (Proposition 4). The challenge of implementing this policy is how to measure data collection in order to properly calibrate the tax/liability rate.</p>
        <p>We discuss broader policy implications in Section 6; Section 7 o ers a review of related literature and provides concluding remarks. All proofs are given in Appendix A.</p>
        <p>A digital business (it) chooses a data strategy that specifies: a) what proportion of users' activity the business records and stores -a data collection strategy, and b) a costly investment in data protection -a data protection strategy. 8 Each user (she) decides how much to use the services provided by 7 See, for example, the case of FTC vs D-Link: "D-Link agrees to 10 years of security audits to settle FTC case", The Verge, 4 Jul 2019; https://www.ftc.gov/enforcement/cases-proceedings/132-3157/d-link. 8 We focus on a monopolistic digital business. However, we note that even if the business is not in direct competition with other platforms it still faces pressures in choosing a data policy that provides users su cient value for them to the business -her activity level. By analyzing the stored users' activity data, the business learns about relevant users' characteristics and provides higher quality services to them. Third parties (henceforth, adversaries) can, at a cost that is increasing in the business's data protection investment, attempt to access and use the data for purposes that are not in line with users' preferences. Thus, if successful, an adversary (he) can harm users. We next introduce formally these elements.</p>
        <p>There is a unit mass of users of a digital business. Each user i chooses activity level a i oe R + , which can be thought of as the amount of time that the user spends using the business's services. A proportion › oe [0, 1] of each user's activity is recorded and stored by the business, and therefore, a i › is the total amount of stored data on user i's activity (here, › is the business's data collection strategy, to be specified in Section 2.3). Denote by ā = s j a j dj, the average user activity. We assume that users are homogeneous, and that the utility of user i is as follows:</p>
        <p>The first term summarizes user i's standalone benefits and costs of using the service. The second term introduces classical positive network e ects that are parameterized byoe [0, 1). 10 The last term captures positive and negative information externalities imposed on user i due to user's profiling by the business and third parties. Informally, the data collected on user's activity, a i ›, allow an entity in possession of these data to learn with probability f (a i ›) some of user i's traits.</p>
        <p>Upon learning the user's traits, the business can o er her better services, as captured by the positive externalities flf (a i ›), where fl oe [0, 1). However, profiling by individuals, firms, and governments who exploit users' data and pursue goals that are in conflict with users' preferences, imposes negative externalities on the user. These are captured by ≠Êf (a i ›), where Ê Ø 0 is the expected number of engage with the services o ered by the business. Adding multiple digital businesses competing for users' attention would allow to understand how competitive forces may discipline further the choice of the data policy and alleviate ine ciencies. 9 In Online Appendix C.1 we extend the analysis to allow for multiple dimensions of users' heterogeneity. Furthermore, the linear-quadratic specification of users' preferences allows us to derive closed-form solutions, but the qualitative results generalize beyond this specification; see the formal discussion in Online Appendix C.2. 10 The specification of network e ects is widely used in network economics literature (see, e.g., Bloch and Quérou 2013, Candogan et al. 2012, Fainmesser and Galeotti 2016, to name a few). The assumption that -&lt; 1 guarantees that there is a unique activity level in equilibrium.</p>
        <p>such adversarial activities and we refer to it as the demand for user information from adversaries;</p>
        <p>we explain how it is derived in Section 2.2. In Online Appendix C.3 we provide a formal example of profiling. 11</p>
        <p>Remark 1 Some digital businesses collect data about users primarily during registration. These data often include demographic and financial information (e.g., age, address, credit card details). In this case, when it comes to privacy concerns, the most important choice faced by users is whether to register to the service. That is, users' choices, instead of reflecting engagement, are discrete: users decide whether to register or not on the platform. The business, in turn, can determine what information is required to complete registration to the service (the parameter ›). For example, the business may require storing credit card information on file, which will facilitate seamless future transactions and be beneficial for consumers. However, storing credit card information may also introduce concerns of credit card fraud by third party adversaries. In Online Appendix C.4 we re-formulate the model and show that all of our results carry over to these settings with no change.</p>
        <p>To simplify our presentation and obtain closed-form solutions, we assume for the remainder of the paper that f (a i ›) = a i › and therefore user i's utility becomes:</p>
        <p>(1)</p>
        <p>There is a large number M of potential adversaries, who are heterogeneous in their cost to access information collected by the digital business. This heterogeneity is captured by the parameter ", which is drawn for each adversary from a uniform distribution over [0, M] (see Online Appendix C.2 for a generalization to non-uniform distributions). An adversary observes his own " and chooses whether to be active (action 1) or not (action 0). The gain to an inactive adversary is his outside option, which we normalize to zero (fi(0|") = 0). An active adversary with characteristic " incurs a</p>
        <p>11 Information is modeled as a one-dimensional variable, which means that the information about users that is valuable to the business is the same as the one that is valuable to the adversaries. This assumption is partly justified by the correlation across information types that are exploited by AI. However, by taking into account the multidimensional structure of information, one could explore how di erent businesses may choose to collect di erent attributes and to what extent it is possible for the business to collect data without attracting adversarial actions. This question is related to the field of di erential privacy and additional research in computer science, operations research, and related fields.</p>
        <p>fixed cost "C to access the data collected by the business on all users and gains ā›. C describes how well data are protected against misuse by third parties and it is a choice of the business. The total payo expected by an active adversary with " is then:</p>
        <p>To simplify the presentation of the results, we let M ae OE. This assumption does not have an e ect on our qualitative insights, and the only role it plays is to ensure that the number of adversaries who are active and the number of those who are inactive, are both strictly positive.</p>
        <p>The digital business chooses a data strategy that consists of:</p>
        <p>(a) A data collection strategy: a proportion › oe [0, 1] of each user i's activity a i that is collected, stored, and processed by the business;</p>
        <p>(b) A data protection strategy: a protection level C Ø 0 at a cost ÂC for some Â Ø 0.</p>
        <p>The objective of the business is a function which is increasing in users' activity, ā, and in the amount of users' activity that the business records -collected data, ›ā. Where not stated otherwise, we focus on a linear profit function that takes the following form:</p>
        <p>where P oe [0, 1]. That is, 1 ≠ P is the additional profit to the business from a marginal increase in users' activity, ceteris-paribus, and P is the additional profit to the business from a marginal increase in data collected by the business. In that sense, 1 ≠ P and P are, respectively, the "prices" that the business extracts for each unit of user activity and for each unit of user data collected, respectively.</p>
        <p>A purely data-driven business's sole source of revenue is the sale of data or data-based services to third parties. For such businesses, P = 1. A purely usage-driven business's source of revenue are the payments made by users in the form of subscription fees or commissions. For such businesses, P = 0. Between these two extremes, we have advertisement-driven companies, like Facebook and Google, who capitalize directly on users' data that they collect by selling ads, but that also attach a direct value to users' activity, since users have to be active to view and click on the ads.</p>
        <p>The linear formulation of the business's profit function is chosen for analytical tractability and allows us to develop clear intuitions. However, our main results extend to a model in which the business's profit function is much more general and takes the following form:</p>
        <p>The analysis for this formulation imposes standard smoothness/concavity/convexity assumptions on the function (•, •) and has the linear case as a special case; the details are available in Appendix C.5.</p>
        <p>We consider the following sequential game. In the first stage, the digital business chooses its data strategy and the choice is observed by users and adversaries. Then, users choose their activity levels and, simultaneously, adversaries decide whether or not to be active. 13 The strategy of a digital business is its data strategy: › oe [0, 1] and C oe R + . User i's strategy is a function a i : [0, 1] ◊ R + ae R + that specifies user i's activity for every possible data strategy, (›, C). The strategy of an adversary is a function v j : [0, M] ◊ [0, 1] ◊ R + ae {0, 1} that specifies, for every possible " oe [0, M] and data strategy (›, C) whether adversary j is active and attempts to access the business's database. We use a and v to denote the strategy profiles of users and adversaries respectively. We focus on perfect Bayesian equilibria of the game: a data strategy choice (› ú , C ú ) and a strategy profile (a ú , v ú ) such that: (a) the digital business maximizes its profit given (a ú , v ú );</p>
        <p>and (b) for every › and C, (a ú , v ú ) is a Bayesian equilibrium in the ensuing subgame. 14 13 The assumption that users know the data strategy of the digital business can be easily relaxed. We clarify this in Section 6.1 where we discuss policies that attempt to increase users' awareness like the EU's GDPR regulation.</p>
        <p>14 In each subgame, users and adversaries have a common prior that " is uniformly distributed between 0 and M .</p>
        <p>Our first result clarifies how the equilibrium users' and adversaries' activities depend on the business data strategy.</p>
        <p>Proposition 1 Fix the business's data strategy (›, C). The ensuing subgame has a unique equilibrium in which users' and adversaries' activities are:</p>
        <p>and the resulting consumer surplus is:</p>
        <p>Furthermore, equilibrium users' activity, āú (›, C) and users' data collected, ›ā ú (›, C) both: (a)</p>
        <p>Increase in data protection C; (b) First increase and then decrease in data collection ›.foot_3</p>
        <p>Average Activity, Average Information Collected, An increase in data protection decreases the adversaries' demand for stored data, thereby decreasing users' privacy concerns. This, in turn, increases users' activity in the business. More subtle is the e ect of a change in the data collection strategy on users' activity and information collected; this is illustrated in Figure 1. By collecting larger proportions of users' data (increasing ›), the digital business creates new benefits for users since it can then o er tailored services based on users' information. Consequently, users' demand for the business's service increases. At the same time, increasing › also boosts adversaries' demand for information. This creates a negative externality on users' participation and o sets the increase in users' demand for the business. Which e ect dominates depends on the level of ›.</p>
        <p>Adversaries impose small costs on users when › is small and, therefore, an increase in › will increase users' activity. With increasing › further, the stored dataset becomes more valuable to adversaries and thus they have more to gain from the attack; this leads to further increase not only in adversaries' demand for users' data but also in the loss that users su er from each adversarial action. At some point, these negative e ects outweigh the benefits to users from their information being used for tailored services. When this happens, users' average activity starts declining in ›.</p>
        <p>Even though users' average activity declines in › for every › &gt; ›(C), total information collected by the business keeps increasing when › oe [›(C), ›(C)]. In this region, negative externalities imposed by adversaries on users are su ciently significant to lead to a decrease in usage, but not severe enough to make this decrease large. It is only when › &gt; ›(C) that any additional increase in › leads to a decrease in users' activity that is steep enough to reduce total information collected, notwithstanding an increase in the fraction of information stored.</p>
        <p>We characterize the business's strategy, and show that a more data-driven business will collect a higher fraction of the data generated by users' activity and, at the same time, provide more data protection. How this increase in data collection and data protection shapes consumer surplus depends on the direct benefit to consumer from data collection.</p>
        <p>The digital business's data collection, › ı and data protection, C ı are:</p>
        <p>(5)</p>
        <p>All else equal, a more data-driven digital business (i.e., higher P ) sets weakly higher levels of data Figure 2: Equilibrium data collection and data protection strategies as a function of the business's revenue model, P (higher P implies a more data-driven business model).</p>
        <p>We next show that depending on the direct benefits to consumers from data collection, consumers may be better or worse o from the business having a more data-driven revenue model.</p>
        <p>Proposition 2 If P &lt; fl/(1 + fl), then a more data-driven digital business (i.e., higher P ) generates weakly higher consumer surplus, CS(› ı , C ı ). Otherwise, CS(› ı , C ı ) weakly decreases with P .</p>
        <p>Proposition 2 tells us that there is a best revenue model from users' prospective, that is, consumer surplus is the highest when the business's revenue model is set at an intermediate level P = fl/(1 + fl).</p>
        <p>To understand the intuition behind the result, we first note that consumer surplus is proportional to the square of the average users' activity level. Hence businesses that lead to high consumer surplus are those who choose a data policy that generates a lot of users' activity. Second, as we move on the spectrum from usage-to data-driven revenue models, both data protection and data collection increase (see Theorem 1). This generates two conflicting e ects. On the one hand, for a given data protection level, the increase in data collection leads to a decrease in users' activity.</p>
        <p>On the other hand, for a given data collection level, the increase in data protection increases users'</p>
        <p>activity. Which e ect dominates depends on the the revenue model, P and on the direct utility users have from data collection, fl. When fl is small, average activity is very sensitive to changes in data collection. This implies that for a large range of values of P , a shift towards a more data-driven revenue model has an overall negative e ect on average activity and subsequently on consumer surplus. The opposite intuition holds when fl is large.</p>
        <p>In this section we compare the business's choice of data strategy with the choice of a benevolent social planner. This highlights the sources of ine ciencies in data collection and data protection, and allows us to evaluate the strengths and weaknesses of existing regulations and to propose e cient regulations. The social planner chooses a data strategy to maximize the weighted average of consumer surplus and the business's profit. Hence, the planner's prescribed data strategy, › W , C W (here the superscript W stands for welfare) solves the following problem:</p>
        <p>whereoe [0, 1]. Note that the business's data strategy given by (› ı , C ı ) is the solution to problem 7 under the special case of -= 0.</p>
        <p>Theorem 2 Relative to the socially optimal strategy, the equilibrium data strategy of a purely usage-driven business (P = 0) prescribes under-collection and under-protection of users' data, i.e.,</p>
        <p>The equilibrium data strategy of a business with any data-driven component in its revenue model (P &gt; 0) never prescribes under-collection and over-protection of users' data i.e., it is never the case that › ı AE › W and C ı Ø C W with at least one of the inequalities being strict.</p>
        <p>Any other direction in which the optimal business's data strategy deviates from the socially optimal 13</p>
        <p>Electronic copy available at: https://ssrn.com/abstract=3459274</p>
        <p>strategy is the outcome for some specification of the business's revenue model and values of other model parameters.</p>
        <p>Theorem 2 implies that while the business never simultaneously under-collects and over-protects users' data (as compared to the socially optimal strategy), any of the following relationships between the business's data strategy and the socially optimal strategy is possible:</p>
        <p>We obtain more definite characterizations for special cases. 16 But we were not able to do so generally. We can however rely on numerical analysis to get a better picture of how the business's revenue model, P , and the direct benefit for consumers from data-collection, fl, jointly determine the misalignment between the socially optimal and the business's data strategies. Figure 4 summarizes this numerical analysis. When the business is su ciently usage-driven and when the value of data to users is low (region A in Figure 4) the benefits from data collection are too low to justify the cost of data protection, both from the business's and the socially optimal perspective. In that case, the business collects no data and this is e cient. There are then three main regions.</p>
        <p>When the business is su ciently usage-driven and the value of data to users is su ciently high (region B), the business under-protects the information relative to the socially optimal and, because of the complementary in the business's data strategy, it also under-collects data. The reverse occurs in region C where the business is su ciently data-driven and the value of data to users is low. In this case the platform over-collects data, and, to keep users active, it also over-protects the data.</p>
        <p>In region D the value of users' data is high enough for both the social planner and the business to optimally collect all available information. Yet, despite the e cient data collection, the digital business under-protects the data.</p>
        <p>These four regions are the typical regions that we have observed through numerical analysis.</p>
        <p>The shaded region E in Figure 4 illustrates that there is another possibility, which is less prevalent. This is when the business revenue model is su ciently data-driven and the users' value of data is moderate. In this case the platform over-collects and under-protects relative to the planner. We stress that the intricacies of the possible misalignment of social and business incentives illustrated in Theorem 2 and Figure 4 result from the fact that the data policy is multi-dimensional: it prescribes both collection and protection of data. 17 An important implication of these results is that regulation that aims at reconciling the private incentives of a business with the social planner's objective must regulate both components of the data strategy, i.e., both data protection and data collection. We expand on this point in the next two sections.</p>
        <p>The US Federal Trade Commission (FTC) has a mandate to enforce a minimal level of data protection. 18 In our model, such requirement maps into guaranteeing a level of data protection</p>
        <p>C not lower than a certain threshold. We now show that a minimal data protection requirement is insu cient to achieve social e ciency and it may, in fact, decrease consumer surplus, profits, and overall welfare. In particular, we show that whether a minimal data protection requirement is beneficial to consumers depends on the business's revenue model, P , the direct benefit to consumers</p>
        <p>17 Indeed, when either data-collection or data-protection are held fixed we get straightforward results and these are available in Proposition 5 in appendix C.5.</p>
        <p>18 For instance, in recent settlements with Zoom and D-Link, FTC required these companies to enhance their data protection: "FTC Requires Zoom to Enhance its Security Practices as Part of Settlement", 9 November 2020, https: //www.ftc.gov/news-events/press-releases/2020/11/ftc-requires-zoom-enhance-its-security-practices-part-settlement, and "D-Link Agrees to Make Security Enhancements to Settle FTC Litigation", 2 July 2019, https://www.ftc.gov/ news-events/press-releases/2019/07/d-link-agrees-make-security-enhancements-settle-ftc-litigation.</p>
        <p>from data collection, fl, and the cost of data protection, Â.</p>
        <p>Consider a regulatory imposed minimal data protection level C min such that the business is obligated to choose a protection level C &gt; C min . We focus on the relevant case for which the regulation is binding with respect to the business's incentives for data protection investment (i.e., C min Ø C ı ). The next proposition shows that even when C min is only slightly larger than C ı , it may lead to a decrease in consumer surplus. Because regulations that impose a minimum level of data protection always reduced the business's profits, we get that whenever the regulation reduces consumer surplus it also reduces total welfare.</p>
        <p>Proposition 3 Consider a regulation that imposes a minimum level of data protection C min .</p>
        <p>Then, there exists Ĉ &gt; C ı such that for all C min oe (C ı , Ĉ) consumer surplus decreases as compared to when no regulation is imposed if and only if the business is moderately data-driven (i.e., As our discussion surrounding Theorem 2 suggests, if a business is mostly usage-driven, it is likely to be under-protecting and under-collecting user data. In this case, forcing the business to increase it's protection level means that the business will also collect more data and, in this case, this increases consumer surplus. On the other hand, if a business is heavily data-driven, it is likely to be already collecting all (or almost all) of the data generated by users' activity. In this case, forcing the business to provide additional protection will not come at the cost of an increase in data collection. In between these extremes, the business may already be over-collecting users' data and 19 The interval</p>
        <p>] is non-empty if and only if the direct benefit for consumers from data collection is small and the cost of data protection is large (i.e., Â &gt; fl 2 /(1 + fl))</p>
        <p>any regulation that forces the business to provide more data protection will lead to an undesirable increase in data collection. Whether the latter overwhelms the benefits of the added protection depends on the cost of data protection and the direct benefits to users from data collection.</p>
        <p>In this section, we show that to achieve the socially optimal outcome regulatory authorities could supplement the minimal level of data protection requirement with either (i) liability fines proportional to the damage inflicted on users by adversarial activity or (ii) a tax on data collected. Our prescribed policies are not unique, and, as such, could be substituted with alternative policies, as long as such policies address both the data collection and the data protection aspects of the business's data strategy. We discuss some alternative policies below. Formally,</p>
        <p>Definition 2 A data collection tax policy imposes a tax rate t on the business for each unit of users' data collected. The expected amount of tax that the business pays is therefore t ◊ ›ā ú (›, C) Remark 3 In our model the amount of data collected, ›ā ú (›, C), enters directly into the users' utility function, adversaries' payo function, and the business's profit function. That is, data "quantity" is measured by the usability of the information contained within the data rather than by physical measures related to the data itself (such as server storage units). Getting the measurement right may be important, because distortion in taxation may a ect the business's incentives to collect one type of data versus another.</p>
        <p>The next proposition characterizes a simple two-pronged policy that restores e ciency regardless of the business's revenue model.</p>
        <p>d› be the ratio between the elasticity of the total amount of user's data collected, ›ā ú (›, C) with respect to the data collection strategy › and the elasticity of users' activity, āú (›, C) with respect to ›. At › = ›(C) this ratio of elasticities is equal to +OE, it then decreases in › and it equals zero at › = ›(C).</p>
        <p>The following two-pronged policy induces an equilibrium in which the business chooses the socially optimal data collection and data protection strategies: a) a required minimum level of data protection C min = C W (where C W is the socially-optimal data protection level) combined with either b 1 ) a liability fine proportional to the expected damage from adversarial activity with the rate</p>
        <p>b 2 ) or a data collection tax policy with a tax rate</p>
        <p>Furthermore, if a business is purely usage-driven (i.e., if P = 0), then imposing only a minimum level of data protection requirement is su cient.</p>
        <p>Intuitively, imposing a liability fine or a tax on information collected guarantees that the sociallyoptimal data collection level › W coincides with the business's optimal choice of data collection under socially-optimal data protection level C W . At the same time, by reducing the level of data collection, a liability fine or data tax also eliminate any incentive the business may have to over-protect the data. A minimal protection level is then su cient to guarantee that the business protects the collected data appropriately from a welfare standpoint.</p>
        <p>To establish the liability rate that leads to e cient data collection, note that under an arbitrary liability policy ¸and an arbitrary data collection strategy ›, the average fine that the business expects to pay is</p>
        <p>where the third equality follows by noticing that, from Proposition 1, Ê ú (›) = ›ā ú (›, C)/C. Hence, the objective function of the business becomes</p>
        <p>The fine F (›, C, ¸) is an increasing function of total information collected, and therefore it reduces the benefit that the business obtains by capitalizing on the information. The magnitude of the reduction increases with the level of ¸. Because it is exactly the direct capitalization on user data that drives the over-collection of information by the business in the first place, the introduction of such policy, if rightly calibrated, eliminates the misalignment with the social objective.</p>
        <p>In practice, despite the FTC's mandate to enforce a minimal level of data protection, the vast majority of the FTC's actions against firms come in response to documented data breaches. Once such breaches have been exposed and verified, the FTC imposes heavy fines on the businesses involved. Our analysis suggests that the FTC's practice of imposing fines on businesses based on the documented data breaches has the potential to be welfare enhancing if calibrated correctly. We do note, however, that imposing liability fines requires a litigation process to establish and quantify</p>
        <p>damages. An alternative policy that takes a more legislative and bureaucratic path is imposing a tax on collected user information. The tax rate that aligns the incentive of the business with the regulator's incentives, is derived in a similar way of the liability rate. Alternative policies, such as regulating directly the amount of data collected, subsidies for investment in data protection, etc., may also replace some of the suggested policies. The important practical lesson is that both aspects of the data strategy need to be regulated in a way that acknowledges their interdependence. Whether a tax, liability fines, or alternative policies are recommended depends on feasibility constraints, both technological and political. For example, whether it is easier to measure damages or data collection.</p>
        <p>We conclude by discussing broad policy issues that our framework sheds light on.foot_5</p>
        <p>We have assumed that users know the data collection strategy of the digital business, which captures the common practice where users accept terms and conditions when opening an account in a digital business. However, in reality, some users may not read those terms. This could be because they have no privacy concerns or because they lack awareness and do not internalize the privacy costs.</p>
        <p>The model can easily accommodate heterogeneity across users in their sophistication or awareness levels and the qualitative results do not change. In fact, in our model, the reaction of users to the data policy disciplines, to some extent, the incentive of the platform to collect too much information and to protect it too little. Less sophisticated users will not react to the business's data policy, which implies that the presence of such users makes the business's incentives even less aligned with users' preferences.</p>
        <p>More recently, regulations in the EU, California, and to some extent China, focused on increasing users' awareness (and control) of the data collected by digital businesses. For example, many users can now request their information from digital businesses under the European GDPR law (see Art. 15 GDPR -Right of access by the data subject). The GDPR, therefore, allows users to have accurate knowledge of what data is collected about them</p>
        <p>Studying the consequences of the rollout of the EU's General Data Protection Regulation (GDPR), Goldberg et al. 2021 show that it increased the businesses' costs of collecting consumer data. On the other hand, Aridor et al. 2020 show that despite the increase in opt-outs that followed the GDPR, the ability of firms to target consumers did not decrease, suggesting that the e ective amount of information did not decrease. 21 These results by Aridor et al. 2020 suggest that e ective regulation of data collection may require enforcement at a collective level, addressing directly the usable information contained in the data collected. This is in line with our analysis in Section 5.</p>
        <p>The last two decades have seen significant consolidation of digital businesses, with many mergers and acquisitions led by the Big Five Tech Giants or GAFAM (Google, Amazon, Facebook, Apple, and Microsoft). 22 Many of those acquisitions were vertical. That is, the acquired business operated in a separate market and/or provided a distinct service from the acquiring business. However, in privacy terms, even seemingly unrelated mergers may have an important e ect. This is so because data may be shared between di erent subsidiaries of the same parent company.</p>
        <p>In online appendix C.7 we provide a simple example that demonstrates how our model can incorporate such considerations. We shows that if following the mergers the businesses make joint decisions regarding data collection, consumers will be worse o . This negative e ect is not present if the businesses merge their databases but keep the data collection decisions (and revenue from data collections) separate.</p>
        <p>Whether users should be paid for their data is an interesting question that pertains to the property rights over individual-level data and transcends the analysis in this paper.foot_7 However, our framework could be useful in evaluating the e ect that pay-for-data schemes may have on data collection and data protection, and subsequently on consumer surplus and total welfare.</p>
        <p>In Online Appendix C.8, we illustrate that when a regulator requires businesses to pay users for their data, users' direct incentives to exert activity increase (since they are now paid for their data) while business's marginal returns from each unit of user information decrease. The interplay between the two defines whether the business collects a higher fraction of information or not and whether users benefit from such "pay for data" policy. We show that there exist regimes in which the incentives of users to increase activity are so strong that the business can gain from an imposed payment by increasing data collection, ›, despite of the lowered returns from user information.</p>
        <p>When the price per data is su ciently large, we find that consumer surplus can be higher than without the "pay for data" policy, and it can actually reach the socially optimal level.</p>
        <p>This paper contributes to an active interdisciplinary area of research that studies the consequences for market outcomes of the ability of digital institutions to amass large data sets. The main issues discussed in the literature are the collection and management of consumer information, consumer privacy, and possible policy intervention.</p>
        <p>Recent work has focused on understanding how user information is either voluntarily disclosed (Ali et al. 2020) or inferred from users' actions such as purchasing behavior over time (Conitzer et al. 2012, Fudenberg and Villas-Boas 2006), ratings (Bonatti and Cisternas 2017), formation of social links (Acemoglu et al. 2017), platform usage (Ichihashi 2020a), or gathered through monetary transfers (Bergemann et al. 2020). We rely on this line of work and assume that there is a one-to-one mapping between any user's actions (usage of the platform) and the information that is revealed about this user to the platform. 26 28 The business's tradeo is driven by users' tradeo between providing data (via usage) and potential privacy infringements. Cong et al. 2020 show the implications of this tradeo for the growth of the data economy.</p>
        <p>of privacy regulation (such as requiring customers' consent or prohibiting data tracking) in scenarios in which customers interact with firms sequentially and the customers' data can be shared among these firms (so-called, data linkages). Markovich and Yehezkel 2021 investigate whether users or the platform should have control over users' data.</p>
        <p>Notably, information generated by di erent users might be interdependent, so that information generated by one user can reveal information on another. Such considerations could be introduced into our model by: (1) replacing a i › in user's i utility with a function of other users' usage levels, and (2) replacing everywhere ā with a di erent function aggregating the usage levels of all users. We defer this analysis for future work and refer the interested reader to work on the topic by Acemoglu Our contribution is to formulate a model that captures the di erent components of a business's data strategy and in which privacy costs are endogenous and therefore change with the data strategy of the business. This allows us to assess positive and normative implications of data policy design and to compare conclusions across di erent domains-in particular, between data-driven and usage-driven revenue models. We demonstrate how our framework is useful to study broader policy issues such as the motivations and welfare e ect of vertical integration in digital markets and the potential welfare benefits of introducing a carefully calibrated payment to users for their data.</p>
        <p>Given users' expectation of the adversaries' demand for information Ê, there exists a unique response of the users. Existense (su cient condition) follows from Glaeser and Scheinkman 2000:</p>
        <p>, which can only be satisfied if -&lt; 1. Condition for the uniqueness of the users' response also follows from Glaeser and Scheinkman 2000:</p>
        <p>Next, we derive our characterization of the unique response. Denote a ≠i the activity choice that i conjecture about the other users. Then user i's best reply is a i = 1 + -ā ≠ Ê› + fl› where ā = s j a j dj. In equilibrium users' expectation are correct and so</p>
        <p>the sign of which is defined by the sign of flC(1 ≠ -) ≠ 2› ≠ fl› 2 . For any C &gt; 0, this expression is first positive, then negative. The change of sign happens at › 1 (C)</p>
        <p>Similarly, derivative of ›ā ú (›, C) can be derived as</p>
        <p>For any C &gt; 0, the latter expression is first positive, then negative. The change of sign happens at › 2 (C) (largest solution to</p>
        <p>, taking square of the both sides and rearranging, we can show that this inequality holds. Finally, it always holds that › 1 (C) Ø 0 and › 2 (C) Ø 0. We define</p>
        <p>Lemma 2 (Optimal Data Collection › ı (C) as a Function of Data Protection) Given a data protection level C, the business stores:</p>
        <p>-part of the information</p>
        <p>Here Ÿ(C) = 1≠P ≠P flC(1≠-)</p>
        <p>Case Â &lt; P fl: We first show that C inv (›) &lt; C h (›) (where C h (›) is defined in Lemma 1) and they 29 C h (›) &gt; 0 if either (i) Â &lt; P fl or (ii) Â &gt; P fl and › &lt; min{1,</p>
        <p>only intersect at › = 0 under this condition. That is, we need to show:</p>
        <p>The LHS can be rewritten as 2(1+fl›)(1≠P +P ›) P +fl≠P fl+2›P fl . Developing, we need to show that 4Â(1 + fl›)(1 ≠ P + P ›) &lt; (P + (1 ≠ P )fl + 2›P fl) 2 . The LHS of the latter inequality is the highest under Â = P fl and the RHS is the lowest under Â = P fl (since we consider case Â &lt; P fl). It is easy to check that inequality holds in this case and hence C inv (›) &lt; C h (›), '› &gt; 0. Thus, the only two points of intersection of best-responses C ı (›) and › ı (C) are › = 0, C = 0 and › = 1, C = C h (1). We evaluate company's profit at both points. The first point gives fi 0 = 1≠P 1≠-, while the second gives</p>
        <p>for the case that Â &lt; P fl (it is also easy to verify that fi 1 (P fl) &gt; fi 0 ). Figure 5 (a) illustrates this case.</p>
        <p>We get two non-negative roots: › = 0 and</p>
        <p>). We will establish first conditions when › i oe [0, 1]. We will consider case of P &gt; (1 ≠ P )fl (case P &lt; (1 ≠ P )fl can be considered similarly and leads to the same result).</p>
        <p>On the interval Â oe [P fl, Â L ), › i &gt; 1, hence on › oe [0, 1] we have C inv (›) &lt; C h (›). Thus, the two points of intersection of the best-response functions are › = 0, C = 0 and › = 1, C = C h (1). fi 1 (Â) decreases with Â on [P fl, Â L ) (derivative fi Õ 1 (Â) changes sign only once and at Â L it is negative).</p>
        <p>Plugging Â L into fi 1 (Â) defined above, we get fi 1 (Â L ) &gt; fi 0 . We thus conclude that fi 1 (Â) &gt; fi 0 on Finally, on the interval Â oe [Â L , Â H ], there are two points of intersection of the best-responses:</p>
        <p>Where z(Â) = Ò Â Â≠P fl -decreasing function of Â. Taking derivative fi Õ › i (Â) we obtain:</p>
        <p>Here, inequality follows from noticing that the expression in the second brackets is ≠› i &lt; 0. To check</p>
        <p>and Summarizing the above, we have two thresholds</p>
        <p>Here function C h (›) is defined in Lemma 1. The three cases can then be written succinctly as in the statement of the Theorem. In particular, the expression for › ı is just › i bounded within [0, 1] interval and with an additional max{0, Â ≠ P fl} which does not change › i when Â &gt; P fl, but turns › i into OE (and, thus guarantees that › ı = 1 is chosen) when Â &lt; P fl &lt; Â L . Expression for C ı is just the one for</p>
        <p>Finally, in order to show that both › ı and C ı weakly increase with P , it is necessary to consider the behavior of Â L , Â H as functions of P and then analyze the behavior of › ı , C ı in the three regions defined by these functions. We delegate this proof to the last part of the following Appendix A.3.</p>
        <p>From Theorem 1, we know that the business sets: (a)</p>
        <p>First, notice that āa and āc do not depend on P and āa &gt; āc when Â &lt; fl 2 1+fl .</p>
        <p>After algebraic simplification we obtain: P (1+z)+(1≠P )(z≠1)fl</p>
        <p>. Given that z Ø 1, we conclude that this expression is non-negative. Hence, consumer surplus weakly increases in the sub-region (b) P &lt; fl 1+fl .</p>
        <p>Furthermore, one can verify that āc = āb (P H ) where P H solves Â = Â H and that āa = āb (P L ) where</p>
        <p>Taken together with the behavior of consumer surplus in regions (a) and (c), we conclude that consumer surplus weakly increases with P when P &lt; fl 1+fl . The other case can be derived similarly.</p>
        <p>Finally, we can verify that › ı , C ı weakly increase with P using the techniques from above. First, clearly both protection and collection are higher in region (a) than in region (c). Both do not change with P in these regions. We are left to show that in region (b) both protection and collection also increase with P (there are no jumps on the borders of regions (c)-( b) and ( b)-(a) as per Theorem 1).</p>
        <p>Consider first behavior of › ı in the sub-region (b) P &lt; fl 1+fl (the other sub-region can be considered in similar fashion). Signs of the derivative is defined by the following function:</p>
        <p>The sign of the numerator of this expression is defined by z 2 (fl</p>
        <p>-an increasing function for any z &gt; 0. Given that Â oe [Â L , Â H ] in this region, we have z oe We will first prove that relative to the socially optimal strategy, the business's data strategy never prescribes a combination of higher level of data protection and lower level of data collection. That all other combinations are possible is shown numerically with the linear example on page 16 (see Figure 4).</p>
        <p>Taking into account the users and adversaries' equilibrium strategies, the social welfare function 7</p>
        <p>Consumer Surplus, CS Business's Revenue Model, P 1.0 0 Consumer Surplus, CS Business's Revenue Model, P</p>
        <p>Figure 7: Consumer surplus, CS(› ı , C ı ), as a function of the business's revenue model, P , and the marginal benefit for consumers from data collection and usage, fl.</p>
        <p>can be rewritten as follows in order to capture dependence on āú and āú ›:</p>
        <p>where CS is increasing in āú and is decreasing in C and increasing in āú and āú ›.</p>
        <p>Assume by contradiction that › ı AE › W and C ı Ø C W , and let</p>
        <p>Given our contradiction assumption that › ı AE › W this equation implies that</p>
        <p>Then, given our contradiction assumption that C ı Ø C W this equation implies that W Ø ı . It can never be the case that › ı = › W and C ı = C W (unless -= 0). Therefore, for our contradiction assumption to hold it must be that either</p>
        <p>contradiction to the optimality of the business's strategy (that is, contradicting that by definition</p>
        <p>Finally, for purely usage-driven business (P = 0): assume that the business over-collects (i.e., › ı &gt; › W ). Then, necessarily C ı &gt; C W from the first-order conditions on the data collection.</p>
        <p>The argument then follows the one from above: we have by definition ı Ø W and W ı AE W W .</p>
        <p>Then, it has to be that āú (› ı , C ı ) AE āú (› W , C W ). But then it also has to be that C ı AE C W , which is a contradiction. Therefore, purely usage-driven business under-collects and under-protects (under-collection and over-protection is prohibited due to the argument in the first part of the proof).</p>
        <p>Without loss of generality, we set -= 0 (-&gt; 0 case can be considered in the same way). First, it is easy to show that, when regulation C min &gt; C ı is imposed, the firm sets C = C min and › = › ı (C min )</p>
        <p>(where › ı (C) is derived from the first-order condition on data collection and is given by technical Lemma 2). There exists C l = 2+fl≠P (1+fl)</p>
        <p>otherwise. Consider three cases:</p>
        <p>increases with introduction of C min (as users' activity is an increasing function of data protection,</p>
        <p>Case B:</p>
        <p>. Furthermore, we can write down users' activity to which the firm shifts upon introduction of the regulation: C min (1+fl› ı (C min )) C min +› ı (C min ) 2 . Plugging the expression for › ı (C min ) and taking the derivative wrt to C min , we obtain after simplification:</p>
        <p>(P ≠ 1)((P ≠ 1)fl + P ) 4(P (C min P + P ≠ 2) + 1)</p>
        <p>This expression is positive when fl≠P fl≠P (1≠P ) 2 +C min P 2 &gt; 0 which is true when P &lt; fl 1+fl . This is the condition for CS to increase on</p>
        <p>decreases on this interval.</p>
        <p>Case C: C ı &lt; C l and C min &gt; C l If P &lt; fl 1+fl then CS is increasing for C min &lt; C l as per Case B above. For C min &gt; C l we have › ı (C min ) = 1 and an increase in C min leads to even higher CS.</p>
        <p>In summary: when C min &gt; C ı we have three regimes. First, if P &gt;</p>
        <p>and regulation leads to higher CS. If, P &lt; fl 1+fl then C ı &lt; C l and regulation also leads to higher CS. In all other cases, regulation hurts if C min oe (C ı , Ĉ) and improves CS otherwise (here Ĉ &gt; C l ).</p>
        <p>The proof is a particular case of a more general proof in Online Appendix C.5 (Page 9) which considers business's general profit function.</p>
        <p>Profit function is concave in C on C Ø 0 (by verifying the second-order derivative). Taking the derivative wrt C and solving the FOC, we obtain:</p>
        <p>(the only positive root). Taking the derivative of C h (›) wrt ›, we get that it's sign is defined by:</p>
        <p>It is non-negative at › = 0. Developing equality dC h (›) d› = 0, we get that it is equivalent to:</p>
        <p>Consider two cases. First, let P fl &gt; Â then also 3P fl &gt; 2Â. Note also that 34P fl ≠ 16Â &gt; 0.</p>
        <p>Then coe cients of the polynomial don't change sign, hence there are no positive roots and hence</p>
        <p>Now let Â &gt; P fl. In case 3P fl &lt; 2Â, irrespective of the sign of the coe cient in front of › 2 , coe cients of the polynomial change sign only once, hence there is one positive root and thus dC h (›) d› changes sign only once for › &gt; 0. If 3P fl &gt; 2Â then 34P fl &gt; 16Â even in the worst case of Â = 3 2 P fl, hence coe cient in front of › 2 is positive. Thus, there is only one change of sign and hence also one root in this case. Thus, we conclude that when Â &gt; P fl, derivative dC h (›) d› changes sign only once for</p>
        <p>It is easy to see that C h (0) = 0. Also, solving C h (›) = 0 for › &gt; 0 we obtain:</p>
        <p>&gt; 0 when Â &gt; P fl. Knowing that C Õ h (›) changes sign only once if Â &gt; P fl and that it increases at › = 0, we conclude that C h (›) &gt; 0 for low › if Â &gt; P fl. The result of lemma now follows.</p>
        <p>Expression for › ı (C) follows from solving the first-order condition which has one positive root. Also</p>
        <p>is increasing and concave in C. Indeed, the sign of it's derivative wrt C is defined by:</p>
        <p>Finally, second-order derivative of the profit function wrt › has the sign of</p>
        <p>We can consider the following three specifications of user's utility:</p>
        <p>The three utility specifications capture (i) heterogeneity in standalone benefit, as captured by b i ; (ii) heterogeneity in sensitivity to positive information externalities, as captured by fl i ; (iii) heterogeneity in sensitivity to negative information externalities, i.e., privacy concerns, as captured by " i . In the remainder of this analysis we will focus on the first type of heterogeneity. The other two utility speicfications can be analyzed in similar fashion.</p>
        <p>The analysis for utility specification with heterogeneous standalone benefit, b i follows the proof</p>
        <p>Assume that user i's utility is generalized to the following form:</p>
        <p>The first argument is the user's activity, while the second argument is the user's expectation on the benefit/downside of exerting each unit of activity. We assume that function U is concave in its two arguments and the arguments are complements. We will denote U Õ x , U Õ y -partial derivatives of U wrt the first and the second arguments respectively. The first-order condition for the user is thus</p>
        <p>We assume that adversaries' abilities " are distributed with cdf G and pdf g. We also assume that distribution of " satisfies increasing generalized failure rate (IGFR) property (see Lariviere 2006).</p>
        <p>In other words, we have that xg(x)/(1 ≠ G(x)) increases in x or alternatively g Õ (z) Ø ≠ g(z) 2 1≠G(z) .</p>
        <p>The main driving force of the results of the paper was the fact that the equilibrium average activity was non-monotone in the level of data collection strategy ›. In particular, activity increases with › for low › and it decreases in › when › is large. This non-monotonicity in user activity is the result of the interplay between positive and negative externalities that information imposes on users along with the adversaries' endogenous demand for information. When › is low, adversaries' demand for information is small, hence information produces more positive than negative externality to users -thus, an increase in › incentivises users to be more active. When › is large, the adversaries' demand for information is strong, and so negative externalities dominate, and users decrease their activity as › grows. We can show that the behavior of user activity inherits such traits in this more general model.</p>
        <p>Similar to the result of Proposition 1 we can solve for user's equilibrium activity. In particular, it is such a ú (›) solves: U Õ x (a, ›fl ≠ ›G(a›/C)) = 0. Derivative of user activity wrt ›:</p>
        <p>Thus, the sign of this expression is defined by µ</p>
        <p>In order to show that a ú (›) first increases and then decreases with ›, we need to show that µ(›) either always remains positive (then a ú (›) always increases) or crosses 0 only once.</p>
        <p>Assume that there exists ›, such that µ( ›) = 0, then also da ú (›) d› | › = 0. Using IGFR property of distribution of ", we conclude that sign of µ Õ ( ›) is defined by ≠2 + 2G(a ú ( ›) ›/C) + g(a ú ( ›) ›/C) which is negative given that we know that at ›: fl = G(a ú ( ›) ›/C) + g(a ú ( ›) ›/C). Clearly a ú (›)› increases with › when a ú (›) increases. Behavior of a ú (›)› when a ú (›) decreases is defined by the sign of ›U ÕÕ x,y (. . .)(fl ≠ G(a ú (›)›/C)) ≠ U ÕÕ x,x (. . .)a ú (›) and depends on the higher-order derivatives of function U . Notice that for a ú (›)› to decrease, necessarily it must be that fl &lt; Ê = G(a ú (›)›/C) or a ú (›)› is high enough. a ú (›)› increases first and then decreases if ›(fl ≠ Ê)/a ú (›) single-crosses</p>
        <p>Our specification of the users' utility function is motivated by the following simple example. Consider a business -a digital platform that collects information about users' activity for the purpose of user profiling. A user has a characteristic ◊ i oe {0, 1} (each realized with probability 1/2) that represents, for example, her political ideology, consumption patterns, dating preferences, etc. Ex ante, the platform has a uniform prior about this characteristic, but can learn it by collecting, storing, and processing what the user writes, shares, and likes. In particular, if the platform stores and analyzes a proportion › of user's activity a i , then it learns the true characteristic ◊ i of user i with probability f (a i ›), and learns nothing about the user with probability 1 ≠ f (a i ›).</p>
        <p>The business recommends services/products/matches to the user (e.g., a movie, a book, a post, or a date). Recommendation r = ◊ creates value of V to a user of type ◊ and no value to a user of the opposite type. Without learning the user's characteristic, the platform chooses a recommendation at random, and the expected value to the user is V /2. On the other hand, if the platform learns the user's characteristic, the user obtains a perfect recommendation of value V . Hence, in this simple example fl = V /2. The exact magnitude of fl depends on the specific domain the platform operates in. One could expect fl to be high in the case of dating platforms, search engines like Google, and marketplaces similar to Amazon. At the same time, one could expect that fl is lower for platforms o ering more impersonal services: e.g., weather forecast services.</p>
        <p>Once a user's activity is stored in the platform's database, there is a risk that the very same data will be accessed by individuals and organizations that may try to sway election outcomes, manipulate users' perceptions, personalize health insurance o erings based on confidential data, or commit identity theft (see the introduction for specific examples and a discussion). Such adversarial activities impose a cost on the user, which is normalized to 1. Then, the expected harm to the user is the probability that a third party correctly infers their type, f (a i ›), multiplied by the expected number of adversarial activities, Ê.</p>
        <p>Some digital businesses collect data about users primarily during their registration. These data often include demographic and financial information (e.g., age, address, credit card details). In this case, when it comes to privacy concerns, the most important choice faced by users is whether to register or not to the service. To a ect users' decisions, the business can determine what and how much information is required to complete registration to the service. The trade-o s we study in previous sections are still present: for example, the business may require storing credit card information on file, which will facilitate seamless future transactions and be beneficial for consumers. However, storing credit card information may also introduce concerns of credit card fraud by adversaries who may get a hold of the data.</p>
        <p>In this section we formalize the adoption model and show that all of our results carry over to this model with no change. The models of the adversaries and of the business carry over as-is. Our model of users requires accounting for the binary adoption decisions and the introduction of user heterogeneity to facilitate the study of the extensive (adoption) margins.</p>
        <p>Let a i oe {0, 1} be the decision ({not register, register}) of user i and consider the following utility function: and they guarantee that the business's optimal choice of data protection is always positive and lower than C, which in turn assures that › is interior.</p>
        <p>We will show that under this general profit function, our main results are structurally equivalent to that under the linear model. In particular, business's data strategy is ine cient, in general, there are three types of ine ciencies, two-pronged regulatory policy is enough to fix these ine ciencies;</p>
        <p>and finally, we identify conditions when data protection and data collection are complements.</p>
        <p>First-order conditions for profit maximization take the following form:</p>
        <p>Here, function q(›, C) = ≠ d(›ā ú (›,C)) d› / dā ú (›,C) d› , and by Õ ā, Õ ›ā we denote first-order partial derivatives of function with respect to the first (activity) and second (information) arguments, respectively.</p>
        <p>We will first verify that solution to the above system of equations is indeed maximum.</p>
        <p>Lemma 3 Hessian of (›, C) is negative semi-definite when evaluated at › ı , C ı .</p>
        <p>Proof. First, solve first-order condition ( 14) and ( 15) wrt Õ ā and Õ ›ā . We get:</p>
        <p>where C = C(1 ≠ -). As before, we will use shorthand notation ÕÕ ā,ā , ÕÕ ›ā,›ā , ÕÕ ›ā,ā to denote second-order derivatives of function . For the rest of the proof, all functions are evaluated at optimal › ı , C ı , yet we will write ›, C for simplicity. We will also use the following notation:</p>
        <p>Substituting Õ ā, Õ ›ā found above, we get:</p>
        <p>It can be easily shown that ˜ ÕÕ ›› (›, C) AE 0 and ˜ ÕÕ CC (›, C) AE 0 using concavity of (e.g., see, proof of Proposition 6). Finally, ˜ ›› (›, C) ˜ CC (›, C) ≠ ˜ 2 ›C (›, C) after simple algebraic manipulations can be rewritten as:</p>
        <p>The first term is non-negative due to concavity of . The second and the third terms can be combined and simplified (substitute the expressions for ⁄)</p>
        <p>2 which is non-negative. Thus, we are left to determine the sign of the last three terms. We can rewrite those as follows:</p>
        <p>where µ = b› ≠ Cfl and ÷ = b(› 2 ≠ C) ≠ 2fl› C. Notice that sign of µ can be either positive or negative, while ÷ &lt; 0 since › &lt; ›(C), where › solves ÷ = 0. The multiplier of expression ( 16) is positive. The last term is also positive since ÕÕ ›ā,›ā AE 0. Finally, irrespective of the sign of µ The first three terms can also be shown positive using concavity of (applying the same technique as in the proof of Proposition 6). Hence, we showed that at optimal › ı , C ı Hessian of ˜ (›, C) is negative semi-definite. Given that K ÕÕ (C) Ø 0, we have Hessian of the profit function (›, C) is also negative semi-definite at › ı , C ı . Proposition 5 (i) Fix the level of data protection, C. The data collection strategy of a purelyusage driven business is socially optimal. For all other types of businesses (i.e., not purely usage-driven), their optimal data collection level as well as total users' data collected are too large, while users' activity (and consequently consumer surplus) are too low, as compared to socially optimal outcome.</p>
        <p>(ii) Fix the level of data collection, ›. Relative to the socially optimal outcome, the business's data protection level, total users' data collected and users' activity are too low.</p>
        <p>Proof. Fix C, the derivative of the welfare function W (›, C) (Eq. 7) wrt › and divided by 1 ≠ -is</p>
        <p>For brevity here we denote Õ ā and Õ ›ā partial derivatives of the profit function wrt activity and information respectively (first and second arguments). Both derivatives are evaluated at (ā ú (›, C), ›ā ú (›, C)). Foroe (0, 1) and non purely usage-driven business (i.e., Õ ›ā &gt; 0), expression ( 17) is positive at ›(C) and negative at ›(C). The first term of Eq. ( 17) is negative for any › oe (›(C), ›(C)). When solving the first-order-condition W Õ › (›, C) = 0 to obtain socially-optimal › W (C), only solutions, which are such that W Õ › (›, C) intersects horizontal axis from above, can be local maxima. It is clear then that when comparing those solutions to profit-maximizing › ı (C) (i.e., -= 0), we get › W (C) Ø › ı (C). The ordering of total users' data collected and users' activity then follow. When the business is purely usage-driven, the last term of Eq. ( 17) is zero. Then the first-order condition for socially-optimal data collection level › W (C) is such that āú (›, C) is maximized. This holds for any level of -, including -= 0, which implies › ı (C) = › W (C). Now, fix ›. The derivative of W (›, C) wrt C is:</p>
        <p>The derivative is positive at C = 0 and is ≠OE at C. The first term of Eq. 18 is positive. Any local maximum is such that Eq. 18 intersects horizontal axis from above. Hence, an increase inshifts local maxima to the right. Hence C ı (›) AE C W (›) and the result follows.</p>
        <p>Theorem 2 and its proof are the same for the case of the general profit function.</p>
        <p>The statement of Proposition 4 is not changed. Below we provide its proof for the case of the general profit function.</p>
        <p>Proof of Proposition 4 (Page 17) with General Profit Function.</p>
        <p>Proof. We will prove statement of the proposition for the case when liability policy ¸ı is used. The proof when tax rate t ı is used is similar. The profit function of the business under liability policy is Now, evaluate these conditions at socially-optimal › W , C W . The first-order condition for sociallyoptimal data collection is ˆ ˆ› (1 ≠ -) + -ˆā ú (›,C) ˆ› = 0 (by derivation of Eq. 7). Substitute ˆ ˆ› into Eq. ( 19) and solve for ¸. We will get expression (8) for optimal liability rate ¸ı. After simplification (use ˆ ˆC from the first-order condition for socially-optimal data protection), the first-order conditions for business's profit at › W , C W , ¸ı become:</p>
        <p>Where the second inequality follows from the fact that › W , C W are such that › W &lt; ›(C) (To see that, notice that ≠› 2 + C(1 ≠ -) + 2fl›C(1 ≠ -) defines the sign of ˆā›/ˆ›. In equilibrium, both for the business and for the social planner it must be that ˆā›/ˆ› &gt; 0 and ˆā/ˆ› &lt; 0, otherwise, they would be able to increase their objectives by either increasing or decreasing › depending on the signs of the derivatives). Thus, business under minimum data protection C Ø C min requirement and liability policy ¸ı, sets › W , C W . In case if Õ ›ā = 0, welfare is maximized when activity āú (›, C) is maximized, hence condition ˆ (›,C) ˆ› | › W ,C W ,¸= 0 is satisfied with ¸= 0.</p>
        <p>Finally, we will show that under the general profit function, the business sees its data collection and data protection strategies as complements for a large and economically relevant family of revenue models.</p>
        <p>Proposition 6 Suppose that for any ā and ›, ÕÕ ›ā,ā (ā, ›ā) Ø ≠› ÕÕ ›ā,›ā (ā, ›ā), then at the business's equilibrium strategy, data collection › and data protection C are complements. That is,</p>
        <p>Intuitively, the condition of Proposition 6 guarantees that, for the business's profit, users' activity and users' information are su ciently strong complements, relative to the second-order e ects of users' activity data. This condition is satisfied in many formulations often used in economics, such as in the case that is linear as well as if is a Cobb-Douglas or a CES function. It is also satisfied for a business that is purely usage-driven, regardless of the specific functional form of .</p>
        <p>Proof. First-order conditions for profit-maximizing › ı , C ı can be rewritten as:</p>
        <p>where C = C(1≠-) and as in the proofs above we use Õ ā, Õ ›ā for brevity. We will also use shorthand notation ÕÕ ā,ā , ÕÕ ›ā,›ā , ÕÕ ›ā,ā to denote second-order derivatives of function . For the rest of the proof, all functions are evaluated at optimal › ı , C ı , yet we will write ›, C for brevity. We will also The first term is non-negative due to concavity of function . Indeed, we have ›ā,›ā to the first term to find its lower bound which can be shown to be non-negative. The second term of Eq. ( 22) is non-negative by assumption of the Proposition, which finishes the proof.</p>
        <p>Set fl = 0. One can use similar techniques to those in the proof of Theorem 1 to derive welfare maximizing › W , C W (i.e., those that maximize Eq. ( 7)). In particular, there exist Â L (-) = P ≠P Ô 1≠2P -(1≠-)</p>
        <p>and Â H (-) = P 2 (1≠-) 4(1≠P (1≠-)) such that › W = 1 when Â &lt; Â L (-); › W = 0 when Â &gt; Â H (-); and › W (-) = P (4Â≠P (1≠-)(P +4Â))</p>
        <p>4Â(2-Â≠P 2 (1≠-))</p>
        <p>otherwise. Trivially, Â L (-) &lt; lim -ae0 Â L (-), '-&gt; 0.</p>
        <p>Also, Â H (-) &lt; Â H (0), '-&gt; 0. Finally, › W (-) &lt; › W (0) = › ı , where › ı is profit-maximizing data</p>
        <p>In our narrative throughout the paper, we included the targeting of ads as a positive factor in users' utility functions. That is, users prefer relevant ads and information revealed by users through their activity improves the match between them and the ads they observe. However, in practice, targeting of certain ads may also lead to a reduction in users' utility. Consider, for instance, targeting adds of addictive products to vulnerable users (e.g., an AA member can be targeted with an ad for alcoholic beverages), similarly some ads may be misleading or even manipulative. 3</p>
        <p>Notably, harmful targeting relies on data collected and quality targeting technology in order to be e ective. Moreover, if, as digital businesses often argue, they prefer not to advertise harmful ads, they may invest in the better screening of the advertising content that they post. Circumventing the additional screening is costly to marketers who seek to post harmful ads, and thus the extra screening can be thought of as a form of data protection (or protection of the ability to target using the data). Our analysis of optimal regulation thus follows with this alternative interpretation.</p>
        <p>3 See, e.g., Liu et al. 2020 and references therein; see also "Facebook Says It Won't Back Down From Allowing Lies in Political Ads", The New York Times, 9 Jan 2020, on Facebook's policy regarding misleading campaign ads.</p>
        <p>22</p>
        <p>See also https://www.visualcapitalist.com/the-big-five-largest-acquisitions-by-tech-company/ 23 For one of many examples, see https://www.e .org/deeplinks/2020/04/google-fitbit-merger-would-cement-googles-data-empire about the Google-Fitbit merger. For a recent paper that analyzes how combining datasets could feed back into user behavior see</p>
        <p>Liang and Madsen 2020.</p>
        <p>Electronic copy available at: https://ssrn.com/abstract=3459274</p>
        <p>The exact way that an advertisement-driven company weighs users' activity vs. collected data may depend, among other things, on the life cycle of the business: at the startup phase, the objective is, generally, to maximize activity as this is the metric that allows raising capital through investors. As the platform matures, the weight is often shifted towards monetizing collected data.</p>
        <p>Another interpretation of this model of the adversary is that there is a mass of adversaries, who upon gaining access to the digital business's data, attack one user chosen uniformly at random.</p>
        <p>In particular, there exist 0 AE ›(C) AE ›(C) such that (i) āú (›, C) increases with › for › oe [0, ›(C)] and decreases otherwise; (ii) ›ā ú (›, C) increases with › for › oe [0, ›(C)] and decreases otherwise.</p>
        <p>For example, if users do not receive benefit from data collection (i.e., if fl = 0), then the equilibrium data strategy always prescribes over-collection of users' data (i.e., › ı Ø › W ), see appendix C.6.</p>
        <p>In addition to the discussion below, Online Appendix C.9 discusses how the endogenous reactions of users a ect the welfare costs of adversarial activities, and Online Appendix C.10 discusses how our model can address the implications of harmful ads.</p>
        <p>See, e.g., https://www.propublica.org/article/google-has-quietly-dropped-ban-on-personally-identifiable-web-tracking</p>
        <p>In related work, Arrieta-Ibarra et al. 2018 make the case that users should be paid for their data as if that data were labor, whereas Ichihashi 2020a explores a scenario in which competing data brokers compensate users for their data, and Bergemann and Bonatti 2019 and Acemoglu et al. 2019 study a setting where a data intermediary extracts users' information by o ering monetary transfers. Emerging work in the marketing literature seeks to evaluate users' valuation of privacy via empirical and experimental approaches (see Lin 2019 and references therein).</p>
        <p>For simplicity, we omit network e ects and assume that data protection strategy C is fixed.</p>
        <p>In particular, ā(›, C) no adversaries = 1+fl› 1≠-and CS no adversaries (›, C) = 1 2 ā(›, C) 2 no adversaries .</p>
        <p>Seminar (VFTS), École Polytechnique CREST; and the Federal Trade Commission for helpful comments and discussions. Andrea Galeotti gratefully acknowledges financial support from European Research Council through the ERC-consolidator grant (award no. 724356). Ruslan Momot gratefully acknowledges financial support from HEC Paris Foundation and a grant of the French National Research Agency (ANR), "Investissements d'Avenir" (LabEx Ecodec/ANR-11-LABX-0047).</p>
        <p>collection strategy. Thus, we conclude that the business always weakly over-collects data when fl = 0.</p>
        <p>Consider the linear model and two digital businesses j oe {1, 2}. The profit function of business j has now an added term " j merger › ≠j āú ≠j , where " j &gt; 0 and merger is the indicator function, taking a value of 1 if the businesses allow for reciprocal access to their databases and 0 otherwise (i.e., merger = 1 if business 1 can access database of business 2 and vice versa). Users' utilities and adversaries' cost-benefit structure remain the same as before. Now consider the following three scenarios: Scenario 0 is the pre-merger scenario, the two businesses act independently without sharing information. In Scenario DM (Data Merger), the businesses allow for reciprocal access to their databases but maintain autonomy over setting their data collection policies. Finally, in Scenario SM (Strategy Merger), in addition to the reciprocal access to data, businesses decide jointly on their data collection policies.</p>
        <p>It is easy to see that in Scenario DM, businesses' data collection policies, user activity, privacy costs, and consumer surplus will remain the same as in Scenario 0, but profits will increase. In contrast, in Scenario SM the businesses will internalize the positive externalities they impose on each other by collecting information. As a result, data collection, profits, and privacy costs will increase, whereas user activity and consumer surplus will decrease relative to Scenarios 0 and DM.</p>
        <p>Intuitively, the move to Scenario SM is akin to a business becoming more data-driven, leading to increased data collection and a decrease in user activity.</p>
        <p>Users' utility is modified by adding tā›, which, in the case of exogenous t is equivalent to setting f = fl + t. Thus, āú (›, C) is modified by increasing fl to f. Denote modified users' response as ãú (›, C). Then business's profit without paying for data is (›) = (1 ≠ P )ā ú (›, C) + P ›ā ú (›, C) and when paying for data it is ˜ = (1 ≠ P )ã ú (›, C) + P ›ã ú (›, C) ≠ tã ú (›, C)›. Without loss of generality let -= 0.</p>
        <p>Fix C. Equilibrium data collection strategy of the business which pays for data › ı (t) has the following form: (1 ≠ P ) ˆã ú (›,C) ˆ› + (P ≠ t) ˆ(›ã ú (›,C)) ˆ› = 0. Solving for equilibrium › ı (t, C) we obtain:</p>
        <p>. The sign of the derivative of Ÿ(t, C) wrt t is defined by ≠CP t 2 + 2tC(P + fl(1 ≠ P )) + (1 ≠ P )P + C((1 ≠ P )fl 2 ≠ P 2 ). Equilibrium data collection strategy of the business which doesn't pay for data is › ı = › ı (0, C). If P (1 ≠ P ) + C((1 ≠ P )fl 2 ≠ P 2 ) &lt; 0 then at t ae 0, we will get › ı (t, C) &gt; › ı -at least for small t business collects higher fraction of user information than with t = 0. Now, consider t = P , then business sets ›(t = P ) (s.t. dã ú (›) d› = 0) -data collection strategy maximizing consumer surplus.</p>
        <p>Even under the socially optimal data strategy, users experience negative externalities caused by adversarial activity. In fact, it turns out that the direct disutility or damage inflicted on users by adversaries capture only a small part of the loss of welfare due to adversarial activity, and that the total welfare loss from the presence of adversaries can be arbitrarily large.</p>
        <p>. Then we have:</p>
        <p>Proposition 7 suggests that governments may want to take a more comprehensive approach that supplements the aforementioned policies with steps to minimize the government's share of adversarial activities as well as outlawing a wider range of private adversarial activities and establishing corresponding independent enforcement authorities.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T13:36+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>In late 2018, the Journal of Operations Management published an invited methods article by Lonati et al. (2018) to provide guidance to authors on how to design behavioral experiments to achieve the rigor required for consideration in the journal. That article was written as a response to a number of behavioral research submissions to JOM, each dealing with interesting topics but viewed by the editors to possess poor design choices at inception. While the Lonati et al. ( 2018) piece provides experimental guidance fitting to certain research agendas, questions have arisen concerning whether and how exactly to implement some of the points that it makes, and how to best address trade-offs in the design of behavioral experiments. Questions have also arisen concerning how to apply these concepts in operations management (OM) research. This technical note seeks to address these questions, by diving into the details of research risks and trade-offs regarding demand effects, incentives, deception, sample selection, and context-rich vignettes. The authors would like to recognize the input of a large number of senior scholars in the JOM community who have provided support and feedback as we have sought to help authors tease out what can reasonably be done in designing strong behavioral experiments that fit various research agendas.In late 2018, the Journal of Operations Management published an invited methods article by Lonati et al. (2018) to provide guidance to authors on how to design behavioral experiments to achieve the rigor required for consideration in the journal. That article was written as a response to a number of behavioral research submissions to JOM, each dealing with interesting topics but viewed by the editors to possess poor design choices at inception. While the Lonati et al. ( 2018) piece provides experimental guidance fitting to certain research agendas, questions have arisen concerning whether and how exactly to implement some of the points that it makes, and how to best address trade-offs in the design of behavioral experiments. Questions have also arisen concerning how to apply these concepts in operations management (OM) research. This technical note seeks to address these questions, by diving into the details of research risks and trade-offs regarding demand effects, incentives, deception, sample selection, and context-rich vignettes. The authors would like to recognize the input of a large number of senior scholars in the JOM community who have provided support and feedback as we have sought to help authors tease out what can reasonably be done in designing strong behavioral experiments that fit various research agendas.</p>
        <p>Behavioral research in the Journal of Operations Management (JOM) and the Operations Management (OM) field draws from a wide variety of disciplines and methodological approaches.Behavioral research in the Journal of Operations Management (JOM) and the Operations Management (OM) field draws from a wide variety of disciplines and methodological approaches.</p>
        <p>As the scope of the field broadens (Croson, Schultz, Siemsen, &amp; Yeo, 2013), it requires researchers to be more knowledgeable in multiple reference disciplines (Bendoly, Croson, Goncalves, &amp; Schultz, 2010). The benefits in expanding OM's foundational focus are, of course, numerous; foremost is that this allows researchers to ask interesting research questions and apply the most appropriate methodologies for a specific research question. Yet, the combination of such diverse methodologies also requires trade-offs in research design, and can be challenging for authors and reviewers trying to successfully navigate such a wide range of methods using the highest standards.As the scope of the field broadens (Croson, Schultz, Siemsen, &amp; Yeo, 2013), it requires researchers to be more knowledgeable in multiple reference disciplines (Bendoly, Croson, Goncalves, &amp; Schultz, 2010). The benefits in expanding OM's foundational focus are, of course, numerous; foremost is that this allows researchers to ask interesting research questions and apply the most appropriate methodologies for a specific research question. Yet, the combination of such diverse methodologies also requires trade-offs in research design, and can be challenging for authors and reviewers trying to successfully navigate such a wide range of methods using the highest standards.</p>
        <p>The recent invited article by Lonati, Quiroga, Zehnder, and Antonakis (2018) presents "the 'ten commandments' of experimental research" (p. 20) with the end goal of providing "a synthesis of best practices into a uniform methodological paradigm that can help guide future experimental work" (p. 20). These authors raise critical concerns about many of the challenges associated with experimental work, identifying several pitfalls that should be avoided. However, given the way their recommendations are presented in the article (particularly Table 1), there is a risk of OM researchers potentially overlooking the nuance implicit to their methodological recommendations and interpreting them as more restrictive than perhaps intended. While Lonati et al. (2018) acknowledge some of these nuances, and provide reasons to employ different experimental design choices beyond exclusive adherence to the "ten commandments", there is a need to further flesh out guidance and compensate for misunderstandings that have been observed by many experienced authors in the JOM community. In this technical note, we therefore extend the discussion started by Lonati et al. (2018), offering further perspectives on ways that researchers and reviewers can carefully navigate research design choices. The primary design decisions that have raised questions in the OM community concern demand effects, incentive alignment, deception, sample issues, and context-rich vignette experiments.The recent invited article by Lonati, Quiroga, Zehnder, and Antonakis (2018) presents "the 'ten commandments' of experimental research" (p. 20) with the end goal of providing "a synthesis of best practices into a uniform methodological paradigm that can help guide future experimental work" (p. 20). These authors raise critical concerns about many of the challenges associated with experimental work, identifying several pitfalls that should be avoided. However, given the way their recommendations are presented in the article (particularly Table 1), there is a risk of OM researchers potentially overlooking the nuance implicit to their methodological recommendations and interpreting them as more restrictive than perhaps intended. While Lonati et al. (2018) acknowledge some of these nuances, and provide reasons to employ different experimental design choices beyond exclusive adherence to the "ten commandments", there is a need to further flesh out guidance and compensate for misunderstandings that have been observed by many experienced authors in the JOM community. In this technical note, we therefore extend the discussion started by Lonati et al. (2018), offering further perspectives on ways that researchers and reviewers can carefully navigate research design choices. The primary design decisions that have raised questions in the OM community concern demand effects, incentive alignment, deception, sample issues, and context-rich vignette experiments.</p>
        <p>In this note, we provide a roadmap, departing from the Lonati et al. (2018) discussion, that guides OM researchers as they explore a breadth of important research questions, rigorously drawing from a variety of theoretical foundations (Cook, Campbell, &amp; Shadish, 2002;Campbell &amp; Stanley, 2015). We explore trade-offs arising in designing and administering experimental research in OM (McGrath, 1982;Scandura &amp; Williams, 2000;Bickman &amp; Rog, 2008).In this note, we provide a roadmap, departing from the Lonati et al. (2018) discussion, that guides OM researchers as they explore a breadth of important research questions, rigorously drawing from a variety of theoretical foundations (Cook, Campbell, &amp; Shadish, 2002;Campbell &amp; Stanley, 2015). We explore trade-offs arising in designing and administering experimental research in OM (McGrath, 1982;Scandura &amp; Williams, 2000;Bickman &amp; Rog, 2008).</p>
        <p>Consideration of trade-offs does not imply a lack of standards or rigor; rather it enforces explicitness relating to potentially incompatible goals, the "strengths and weaknesses of alternative means for pursuing goals", and justification of research design choices (Collier, Brady, &amp; Seawright, 2004, p. 223). We provide insights regarding these experimental trade-offs in consideration of evidence in Section 2. In Section 3, we consider the interdependence of these design decisions, and provide examples of how these trade-offs may be made across the entire array of design decisions with published papers from the OM field.Consideration of trade-offs does not imply a lack of standards or rigor; rather it enforces explicitness relating to potentially incompatible goals, the "strengths and weaknesses of alternative means for pursuing goals", and justification of research design choices (Collier, Brady, &amp; Seawright, 2004, p. 223). We provide insights regarding these experimental trade-offs in consideration of evidence in Section 2. In Section 3, we consider the interdependence of these design decisions, and provide examples of how these trade-offs may be made across the entire array of design decisions with published papers from the OM field.</p>
        <p>All research methods have limitations, and as such no one method is the "best" or "correct" or "true" method (McGrath, 1982). Trade-offs are necessary in all experimental work. In this section, we develop the discussion around five issues raised by Lonati et al. (2018), exploring the nuances that are likely to arise in the OM context when designing and administering experiments: demand effects, incentive alignment, deception, sample issues, and context-rich vignette experiments. The guidance offered in this section is summarized in Table 1.All research methods have limitations, and as such no one method is the "best" or "correct" or "true" method (McGrath, 1982). Trade-offs are necessary in all experimental work. In this section, we develop the discussion around five issues raised by Lonati et al. (2018), exploring the nuances that are likely to arise in the OM context when designing and administering experiments: demand effects, incentive alignment, deception, sample issues, and context-rich vignette experiments. The guidance offered in this section is summarized in Table 1.</p>
        <p>• Decision-based incentives would bias respondents, increase demand effect risks, or reduce external validity. • The behaviors in the experiment are not well-defined choices between different economic outcomes or are otherwise unrelated to economic choices. • The experiment involves deception or withholding of information that could be perceived as influencing compensation.• Decision-based incentives would bias respondents, increase demand effect risks, or reduce external validity. • The behaviors in the experiment are not well-defined choices between different economic outcomes or are otherwise unrelated to economic choices. • The experiment involves deception or withholding of information that could be perceived as influencing compensation.</p>
        <p>• Make the experiment as close as possible to the research context being analyzed such as rewarding respondents like real-world reward scenarios.• Make the experiment as close as possible to the research context being analyzed such as rewarding respondents like real-world reward scenarios.</p>
        <p>• Non-deceptive alternatives could be used instead of deception. • The potential harms outweigh the benefits.• Non-deceptive alternatives could be used instead of deception. • The potential harms outweigh the benefits.</p>
        <p>• The sample population is frequently used for experiments such as a laboratory. • The deception would influence or could be perceived as influencing respondent compensation.• The sample population is frequently used for experiments such as a laboratory. • The deception would influence or could be perceived as influencing respondent compensation.</p>
        <p>• Use deception only when necessary for the research question and the benefits outweigh the potential harm. • Consider respondent harm, sampling norms of laboratories, available alternatives, and the fit between the research question and experimental design before using deception.• Use deception only when necessary for the research question and the benefits outweigh the potential harm. • Consider respondent harm, sampling norms of laboratories, available alternatives, and the fit between the research question and experimental design before using deception.</p>
        <p>• Be transparent about the usage of deception, and follow IRB guidelines (in the U.S.) and appropriate ethical guidelines.• Be transparent about the usage of deception, and follow IRB guidelines (in the U.S.) and appropriate ethical guidelines.</p>
        <p>General samples allowable when:General samples allowable when:</p>
        <p>• The research question emphasizes individual behaviors that are generalizable to many contexts. • The experiment context does not require indepth knowledge of any particular context.• The research question emphasizes individual behaviors that are generalizable to many contexts. • The experiment context does not require indepth knowledge of any particular context.</p>
        <p>• The research question is dependent on context.• The research question is dependent on context.</p>
        <p>• The level of analysis is at a higher level such as organizational or interfirm contexts. • The samples should have adequate experience to fully understand the context.• The level of analysis is at a higher level such as organizational or interfirm contexts. • The samples should have adequate experience to fully understand the context.</p>
        <p>• Match the unit of analysis and context to the sample population.• Match the unit of analysis and context to the sample population.</p>
        <p>• The manipulation lacks meaningful realism to the participants. • Participants lack context to make informed and realistic decisions. • Determining effect sizes.• The manipulation lacks meaningful realism to the participants. • Participants lack context to make informed and realistic decisions. • Determining effect sizes.</p>
        <p>• Match the research design to the research question and embrace the appropriate epistemological foundation for the research question. • Be transparent in the type of research and the foundations used in the research. • When using manipulations in all experimental designs, make such interventions as consequential and realistic as possible.• Match the research design to the research question and embrace the appropriate epistemological foundation for the research question. • Be transparent in the type of research and the foundations used in the research. • When using manipulations in all experimental designs, make such interventions as consequential and realistic as possible.</p>
        <p>One of the many criticisms levied against experimental research involving human subjects concerns the threat of experimental demand effects (also referred to as demand characteristics).One of the many criticisms levied against experimental research involving human subjects concerns the threat of experimental demand effects (also referred to as demand characteristics).</p>
        <p>These effects represent "changes in behavior by experimental subjects due to cues about what constitutes appropriate behavior (behavior 'demanded' from them)" (Zizzo, 2010, p. 75). For demand effects to pose a risk to experimental research, participants need to change their behavior based on their belief of the researcher's desired outcomes, and this change needs to be correlated with the objectives of the research (see Zizzo, 2010). Lonati et al. (2018) It is important to qualify the difference between these findings. The "weak" manipulation informs participants of the experimental hypothesis and the expected direction of their findings, with these instructions varying by treatment. As de Quidt et al. (2018) noted regarding the weak effect condition, "We believe that these treatments are likely to be more informative than implicit signals about demand in typical studies, so in our view these bounds will be sufficient for most applications" (p. 3267). The "strong" manipulation included asking participants to specifically answer questions in one way ("You will do us a favor if…") and demonstrated that it is possible to create demand effects by asking participants to answer in that specific way. Overall, these resultsThese effects represent "changes in behavior by experimental subjects due to cues about what constitutes appropriate behavior (behavior 'demanded' from them)" (Zizzo, 2010, p. 75). For demand effects to pose a risk to experimental research, participants need to change their behavior based on their belief of the researcher's desired outcomes, and this change needs to be correlated with the objectives of the research (see Zizzo, 2010). Lonati et al. (2018) It is important to qualify the difference between these findings. The "weak" manipulation informs participants of the experimental hypothesis and the expected direction of their findings, with these instructions varying by treatment. As de Quidt et al. (2018) noted regarding the weak effect condition, "We believe that these treatments are likely to be more informative than implicit signals about demand in typical studies, so in our view these bounds will be sufficient for most applications" (p. 3267). The "strong" manipulation included asking participants to specifically answer questions in one way ("You will do us a favor if…") and demonstrated that it is possible to create demand effects by asking participants to answer in that specific way. Overall, these results</p>
        <p>suggest that similar, well-designed experiments are likely not to be impacted by demand effects, but that participants will respond in certain ways if asked.suggest that similar, well-designed experiments are likely not to be impacted by demand effects, but that participants will respond in certain ways if asked.</p>
        <p>Mummolo and Peterson (2019) investigated demand effects using five different hypothetical vignettes grounded in various political science contexts. The manipulations used in Mummolo and Peterson (2019) were highly similar to the "weak" manipulation of de Quidt et al.Mummolo and Peterson (2019) investigated demand effects using five different hypothetical vignettes grounded in various political science contexts. The manipulations used in Mummolo and Peterson (2019) were highly similar to the "weak" manipulation of de Quidt et al.</p>
        <p>(2018), as they included only a hint regarding the research hypotheses in their instructions. 1 Mummolo and Peterson concluded that "across five surveys that involve more than 12,000 respondents and over 28,000 responses to these experiments, we fail to find evidence for the existence of [experimental demand effects] EDEs in online survey experiments" (2019, p. 518).(2018), as they included only a hint regarding the research hypotheses in their instructions. 1 Mummolo and Peterson concluded that "across five surveys that involve more than 12,000 respondents and over 28,000 responses to these experiments, we fail to find evidence for the existence of [experimental demand effects] EDEs in online survey experiments" (2019, p. 518).</p>
        <p>Similar to de Quidt et al. ( 2018), Mummolo and Peterson (2019) were able to create a demandSimilar to de Quidt et al. ( 2018), Mummolo and Peterson (2019) were able to create a demand</p>
        <p>1 For example, in the Partisan News Experiment, participants in the "weak" condition receive the following instructions: "You will now be asked to consider some hypothetical (not real) online news items and to indicate which news item you would most prefer to read. The purpose of this exercise is so we can measure whether the news outlet offering an article influences how likely people are to read the article." In the "strong" (explicit) condition, the last sentence is modified to: "The purpose of this exercise is so we can measure whether people are more likely to choose a news item if it is offered by a news outlet with a reputation of being friendly toward their preferred political party." (Mummolo and Peterson,Table 2,p. 522).1 For example, in the Partisan News Experiment, participants in the "weak" condition receive the following instructions: "You will now be asked to consider some hypothetical (not real) online news items and to indicate which news item you would most prefer to read. The purpose of this exercise is so we can measure whether the news outlet offering an article influences how likely people are to read the article." In the "strong" (explicit) condition, the last sentence is modified to: "The purpose of this exercise is so we can measure whether people are more likely to choose a news item if it is offered by a news outlet with a reputation of being friendly toward their preferred political party." (Mummolo and Peterson,Table 2,p. 522).</p>
        <p>effect in some extreme conditions, such as when respondents were given financial incentives to follow explicit demand effects: "When this added incentive is present, we are sometimes able to detect differences in observed treatment effects that are consistent with the presence of [experimental demand effects] EDEs. But on average, pooling across all our experiments, we still see no detectable differences in treatment effects even when financial incentives are offered" (2019, p. 518).effect in some extreme conditions, such as when respondents were given financial incentives to follow explicit demand effects: "When this added incentive is present, we are sometimes able to detect differences in observed treatment effects that are consistent with the presence of [experimental demand effects] EDEs. But on average, pooling across all our experiments, we still see no detectable differences in treatment effects even when financial incentives are offered" (2019, p. 518).</p>
        <p>Comparing findings across these studies, the evidence suggests demand effects were smaller in the vignette manipulations in Mummolo and Peterson (2019) than in the economic games in de Quidt et al. ( 2018), although for both groups the total impact was low. It is possible that the difference is due to the different demand effect manipulations between the two studies, the specific experiments selected, or the different contexts. For example, economic games tend to be more abstract in comparison to vignettes, which typically include a richer context (Camerer, 1997).Comparing findings across these studies, the evidence suggests demand effects were smaller in the vignette manipulations in Mummolo and Peterson (2019) than in the economic games in de Quidt et al. ( 2018), although for both groups the total impact was low. It is possible that the difference is due to the different demand effect manipulations between the two studies, the specific experiments selected, or the different contexts. For example, economic games tend to be more abstract in comparison to vignettes, which typically include a richer context (Camerer, 1997).</p>
        <p>When considering choices in an abstract thought experiment (i.e., the dictator game or trust game), it may be that participants are likely to rely more on situational cues (i.e., hints from the researcher) in selecting their behaviors, leading to heightened demand effects relative to vignette experiments where the contextual information provided is used to make behavioral decisions. Furthermore, although the vignettes studies in Mummolo and Peterson (2019) were conducted in political science, and not OM, their "inability to uncover evidence of hypothesis-confirming behavior across multiple samples, survey platforms, research questions and experimental designs suggests that longstanding concerns over demand effects in survey experiments may be largely exaggerated" (2019, p. 528). Some of the issues addressed in their vignettes in fact directly correlate to questions studied in an OM context, for example the implications of race on resumé treatment is remarkably similar to the context of consumer attitudes of crowdsourced delivery drivers based on ethnicity (Ta, Esper, and Hofer, 2018). Moreover, some of the underlying theories explaining behavior in these studies, for example framing effects, are the same as those used in OM research (e.g., Abbey et al., 2019;Wuttke et al., 2018). Taken together, these results suggest that experimental research can be carried out in the OM context in a way that minimizes demand effects. Further research into the demand effects of strong manipulations is warranted.When considering choices in an abstract thought experiment (i.e., the dictator game or trust game), it may be that participants are likely to rely more on situational cues (i.e., hints from the researcher) in selecting their behaviors, leading to heightened demand effects relative to vignette experiments where the contextual information provided is used to make behavioral decisions. Furthermore, although the vignettes studies in Mummolo and Peterson (2019) were conducted in political science, and not OM, their "inability to uncover evidence of hypothesis-confirming behavior across multiple samples, survey platforms, research questions and experimental designs suggests that longstanding concerns over demand effects in survey experiments may be largely exaggerated" (2019, p. 528). Some of the issues addressed in their vignettes in fact directly correlate to questions studied in an OM context, for example the implications of race on resumé treatment is remarkably similar to the context of consumer attitudes of crowdsourced delivery drivers based on ethnicity (Ta, Esper, and Hofer, 2018). Moreover, some of the underlying theories explaining behavior in these studies, for example framing effects, are the same as those used in OM research (e.g., Abbey et al., 2019;Wuttke et al., 2018). Taken together, these results suggest that experimental research can be carried out in the OM context in a way that minimizes demand effects. Further research into the demand effects of strong manipulations is warranted.</p>
        <p>Although the empirical evidence that is available suggests that the risk posed by demand effects is expected to be limited in the OM context, it is worth considering, as Lonati et al. (2018) did, the kinds of strong manipulations that might occur in this context, and whether such strong manipulations would yield demand effects that would be sufficiently large to cause concern. Even in these cases, examples of remedies can be found from the literature. For example, Dhar, Jain, and Jayachandran ( 2018), concerned that socially desirable responses might interact with their treatment in a field experiment, incorporated a short-form Marlow-Crowne module to measure respondents' propensity to offer socially desirable baselines.2 Similarly, researchers concerned about demand effects (for example, when the manipulations might be particularly salient to participants or the researcher interacts with the participants directly in some way) could benefit through direct measurement of potential demand effects, and correlating those biases to the treatments. De Quidt et al. (2018, p. 3288-3289) describe a within-subject approach that can be added at the end of a study to construct bounds for demand effects for each participant.Although the empirical evidence that is available suggests that the risk posed by demand effects is expected to be limited in the OM context, it is worth considering, as Lonati et al. (2018) did, the kinds of strong manipulations that might occur in this context, and whether such strong manipulations would yield demand effects that would be sufficiently large to cause concern. Even in these cases, examples of remedies can be found from the literature. For example, Dhar, Jain, and Jayachandran ( 2018), concerned that socially desirable responses might interact with their treatment in a field experiment, incorporated a short-form Marlow-Crowne module to measure respondents' propensity to offer socially desirable baselines.2 Similarly, researchers concerned about demand effects (for example, when the manipulations might be particularly salient to participants or the researcher interacts with the participants directly in some way) could benefit through direct measurement of potential demand effects, and correlating those biases to the treatments. De Quidt et al. (2018, p. 3288-3289) describe a within-subject approach that can be added at the end of a study to construct bounds for demand effects for each participant.</p>
        <p>Another key consideration in experimental design involves if and how participants are to be financially rewarded for their participation. Lonati et al. (2018) (Slater, 1980).Another key consideration in experimental design involves if and how participants are to be financially rewarded for their participation. Lonati et al. (2018) (Slater, 1980).</p>
        <p>In OM research, it is not atypical to encounter phenomena with no explicit extrinsic motivation, such as employees enacting organization-level decisions, where there is no expectation of direct, explicitly related rewards. For example, research investigating the decision to initiate a recall (Ball, Shah, &amp; Donohue, 2018) Even in contexts in which incentives are appropriate, there is ample empirical evidence that financial rewards meaningfully change behavior, and this can happen in unintended, non-trivial ways. Not surprisingly, these effects depend on a number of factors: the nature of the task (Eckartz, Kirchkamp, &amp; Schunk, 2012), the amount or type of incentive (Holt, 1986;Holt &amp; Laury, 2005), the value placed on incentives by participants (Ariely, Bracha, &amp; Meier, 2009), as well as a host of additional framing issues. For example, incentive alignment fails to improve performance when participants are risk averse, yet risk averse participants improve their performance under flat-rate schemes (Cadsby, Song, &amp; Tapon, 2016). There are also important cultural differences in responses to incentive framing (Lee, Ribbink, &amp; Eckerd, 2018). Financial incentives can even be harmful to decision-making efforts. In Camerer and Hogarth's (1999) evaluation of 74 studies comparing the use of incentives, they identified overlearning effects, overexertion effects, and selfconscious behaviors (i.e., "choking") as potentially adverse consequences of incentivization. Meloy, Russo, and Miller (2006) found that incentives can impact participants' mood, leading to biased information processing and overconfidence.In OM research, it is not atypical to encounter phenomena with no explicit extrinsic motivation, such as employees enacting organization-level decisions, where there is no expectation of direct, explicitly related rewards. For example, research investigating the decision to initiate a recall (Ball, Shah, &amp; Donohue, 2018) Even in contexts in which incentives are appropriate, there is ample empirical evidence that financial rewards meaningfully change behavior, and this can happen in unintended, non-trivial ways. Not surprisingly, these effects depend on a number of factors: the nature of the task (Eckartz, Kirchkamp, &amp; Schunk, 2012), the amount or type of incentive (Holt, 1986;Holt &amp; Laury, 2005), the value placed on incentives by participants (Ariely, Bracha, &amp; Meier, 2009), as well as a host of additional framing issues. For example, incentive alignment fails to improve performance when participants are risk averse, yet risk averse participants improve their performance under flat-rate schemes (Cadsby, Song, &amp; Tapon, 2016). There are also important cultural differences in responses to incentive framing (Lee, Ribbink, &amp; Eckerd, 2018). Financial incentives can even be harmful to decision-making efforts. In Camerer and Hogarth's (1999) evaluation of 74 studies comparing the use of incentives, they identified overlearning effects, overexertion effects, and selfconscious behaviors (i.e., "choking") as potentially adverse consequences of incentivization. Meloy, Russo, and Miller (2006) found that incentives can impact participants' mood, leading to biased information processing and overconfidence.</p>
        <p>Our point throughout this section is not to suggest that research should or should not use decision-based incentives. Rather, we illustrate that the use of financial incentives in experiments carries its own risks. Researchers and reviewers should carefully consider how incentives match the research question and the research context to maximize the internal validity of the experiment.Our point throughout this section is not to suggest that research should or should not use decision-based incentives. Rather, we illustrate that the use of financial incentives in experiments carries its own risks. Researchers and reviewers should carefully consider how incentives match the research question and the research context to maximize the internal validity of the experiment.</p>
        <p>A fast reading of illustrates that the degree of deception can be described on a continuum. For example, consider the use of a computer confederate in an experimental setting. Should it be considered deception to instruct participants, "You will be playing with a partner" when the partner is a computer?A fast reading of illustrates that the degree of deception can be described on a continuum. For example, consider the use of a computer confederate in an experimental setting. Should it be considered deception to instruct participants, "You will be playing with a partner" when the partner is a computer?</p>
        <p>Consider further if the instructions said, "You will be playing against another player in the room" when the computer player was locally hosted on the machine in the room. While most might agree that "an explicit misstatement of fact" (Nicks, Korn, &amp; Mainieri, 1997) qualifies as deception, a recent survey of experimental economists found there is considerable variability as to what constitutes deception and obfuscation (Samek, 2019). Thus, the problem is more difficult than asking when one can -or must -use deception; fundamentally, the concept is too ill-defined to answer such a query. Clearly, it is inconsistent with a scientific approach to draw an arbitrary line on deception; researchers need to carefully consider the benefits and risks of operating anywhere along the continuum.Consider further if the instructions said, "You will be playing against another player in the room" when the computer player was locally hosted on the machine in the room. While most might agree that "an explicit misstatement of fact" (Nicks, Korn, &amp; Mainieri, 1997) qualifies as deception, a recent survey of experimental economists found there is considerable variability as to what constitutes deception and obfuscation (Samek, 2019). Thus, the problem is more difficult than asking when one can -or must -use deception; fundamentally, the concept is too ill-defined to answer such a query. Clearly, it is inconsistent with a scientific approach to draw an arbitrary line on deception; researchers need to carefully consider the benefits and risks of operating anywhere along the continuum.</p>
        <p>A key issue regarding deception is its potential to generate confounding effects, i.e. that the act of deceiving a participant will cause the participant to alter their behavior. To date, while there are very few empirical evaluations of the methodological costs of deception, there are a few from which we can draw some conclusions. Ortmann and Hertwig (2002;Hertwig &amp; Ortmann, 2008a;2008b) conducted several evaluations across psychology studies using deception. They reported that when participants have direct and specific knowledge that deception was used, this impacted their behavior; yet, if that knowledge was indirect or general, targeted behaviors were not affected. Another set of studies (also reported on by Hertwig &amp; Ortmann, 2008a) finds mixed results regarding whether deceived participants experience resentment (ascertained after debriefing), but that telling participants they will be deceived beforehand can alter performance, and that deception that arouses suspicion in participants may reduce conformity behaviors. The take-aways are that researchers using deception would be wise to wait until all data for the research effort has been conducted before debriefing participants, and that it may be worthwhile to posttest participants for suspicion.A key issue regarding deception is its potential to generate confounding effects, i.e. that the act of deceiving a participant will cause the participant to alter their behavior. To date, while there are very few empirical evaluations of the methodological costs of deception, there are a few from which we can draw some conclusions. Ortmann and Hertwig (2002;Hertwig &amp; Ortmann, 2008a;2008b) conducted several evaluations across psychology studies using deception. They reported that when participants have direct and specific knowledge that deception was used, this impacted their behavior; yet, if that knowledge was indirect or general, targeted behaviors were not affected. Another set of studies (also reported on by Hertwig &amp; Ortmann, 2008a) finds mixed results regarding whether deceived participants experience resentment (ascertained after debriefing), but that telling participants they will be deceived beforehand can alter performance, and that deception that arouses suspicion in participants may reduce conformity behaviors. The take-aways are that researchers using deception would be wise to wait until all data for the research effort has been conducted before debriefing participants, and that it may be worthwhile to posttest participants for suspicion.</p>
        <p>A second issue concerns the use of repeated participant pools, as is the case with many undergraduate student laboratories. The potential risk posed is that a participant, once deceived in a study, will in the future be more suspicious of experiments, and that ambient distrust will lead to confounding behavior. There is indeed some evidence that deception can lead to less willingness to participate in future studies, particularly when the participant is both deceived and receives smaller compensation (Jamison, Karlan, &amp; Schechter, 2008). Note this affects recruiting efforts, not confound effects. Yet, for labs using repeat sampling pools it is a risk worth considering. Jamison et al. (2008) suggested that, rather than banning deception altogether, participant pools could be maintained separately. Avoidance of deception may most profitably be thought of as a lab rule, as opposed to a research rule.A second issue concerns the use of repeated participant pools, as is the case with many undergraduate student laboratories. The potential risk posed is that a participant, once deceived in a study, will in the future be more suspicious of experiments, and that ambient distrust will lead to confounding behavior. There is indeed some evidence that deception can lead to less willingness to participate in future studies, particularly when the participant is both deceived and receives smaller compensation (Jamison, Karlan, &amp; Schechter, 2008). Note this affects recruiting efforts, not confound effects. Yet, for labs using repeat sampling pools it is a risk worth considering. Jamison et al. (2008) suggested that, rather than banning deception altogether, participant pools could be maintained separately. Avoidance of deception may most profitably be thought of as a lab rule, as opposed to a research rule.</p>
        <p>A third issue involves the ethical considerations underlying deception and obfuscation. We would like to add to the discussion the role of review boards. Researchers in the U.S. must have their research designs vetted by Institutional Review Boards, all of which have special sections dedicated to the issues of deception or withholding of information. Similar Ethical Review Boards exist in many research universities in Europe, as well. These guidelines and gatekeepers help to uphold the standards of ethical research practice, although a systematic understanding of their breadth and applicability to issues of deception and obfuscation could be beneficial for the OM community.A third issue involves the ethical considerations underlying deception and obfuscation. We would like to add to the discussion the role of review boards. Researchers in the U.S. must have their research designs vetted by Institutional Review Boards, all of which have special sections dedicated to the issues of deception or withholding of information. Similar Ethical Review Boards exist in many research universities in Europe, as well. These guidelines and gatekeepers help to uphold the standards of ethical research practice, although a systematic understanding of their breadth and applicability to issues of deception and obfuscation could be beneficial for the OM community.</p>
        <p>Use of deception requires an evaluation of trade-offs and consideration of the research question. For some research questions, deception works in opposition to the goals of the experiment, thus detracting from internal validity. For other research questions, the trade-off involves the ability to model important contextual cues in a controlled way, versus not representing those factors in the experiment at all (Ariely &amp; Norton, 2007). For example, in studies of unethical behavior, confederates may be used to provide an initial modeling of the unethical behavior observed by participants so that ensuing contagion effects can be assessed (e.g., Gino, Gu, &amp; Zhong, 2009). Although few experiments using deception have been conducted in OM (Lonati et al., 2018), those that do tend also to make use of confederates in order to control the treatments participants are exposed to or to provide important contextual knowledge (e.g., Eckerd et al., 2013;Sommer et al., 2020 and described in Section 3 below). Per Jamison et al. (2008), researchers should carefully consider its role, and use deception or information withholding only when the research is not otherwise practicable, risks are minimal, and potential benefits outweigh potential risks; as a reviewer, consider the trade-offs presented.Use of deception requires an evaluation of trade-offs and consideration of the research question. For some research questions, deception works in opposition to the goals of the experiment, thus detracting from internal validity. For other research questions, the trade-off involves the ability to model important contextual cues in a controlled way, versus not representing those factors in the experiment at all (Ariely &amp; Norton, 2007). For example, in studies of unethical behavior, confederates may be used to provide an initial modeling of the unethical behavior observed by participants so that ensuing contagion effects can be assessed (e.g., Gino, Gu, &amp; Zhong, 2009). Although few experiments using deception have been conducted in OM (Lonati et al., 2018), those that do tend also to make use of confederates in order to control the treatments participants are exposed to or to provide important contextual knowledge (e.g., Eckerd et al., 2013;Sommer et al., 2020 and described in Section 3 below). Per Jamison et al. (2008), researchers should carefully consider its role, and use deception or information withholding only when the research is not otherwise practicable, risks are minimal, and potential benefits outweigh potential risks; as a reviewer, consider the trade-offs presented.</p>
        <p>Two issues often are relevant when it comes to experiment samples: sample size and the sample population. The issue of sample size is addressed by Lonati et al. (2018) in their eighth "commandment"; the advice provided in the table itself is that to "ensure an appropriate sample size per experimental cell for covariate balance (n &gt; 50 per cell)" (p. 20). One must reference the footnote for the caveat that advanced planning of an experimental study should, if possible, include a test of power which would indicate an appropriate sample size. We agree with this well-established advice (Verma &amp; Goodale, 1995), and would add to it that it is important to clearly differentiate between the number of participants versus the number of observations. Often in experiment research, multiple observations are collected from a single participant (see the streams of literature evaluating the bullwhip effect and newsvendor decision-making, as examples). Where this experimental design is used, it is the number of independent observations that typically matters. For a more in-depth review of sample size considerations, we refer readers to Lenth (2001), who pointed out additional practical and ethical concerns relating to sample size, beyond statistical considerations.Two issues often are relevant when it comes to experiment samples: sample size and the sample population. The issue of sample size is addressed by Lonati et al. (2018) in their eighth "commandment"; the advice provided in the table itself is that to "ensure an appropriate sample size per experimental cell for covariate balance (n &gt; 50 per cell)" (p. 20). One must reference the footnote for the caveat that advanced planning of an experimental study should, if possible, include a test of power which would indicate an appropriate sample size. We agree with this well-established advice (Verma &amp; Goodale, 1995), and would add to it that it is important to clearly differentiate between the number of participants versus the number of observations. Often in experiment research, multiple observations are collected from a single participant (see the streams of literature evaluating the bullwhip effect and newsvendor decision-making, as examples). Where this experimental design is used, it is the number of independent observations that typically matters. For a more in-depth review of sample size considerations, we refer readers to Lenth (2001), who pointed out additional practical and ethical concerns relating to sample size, beyond statistical considerations.</p>
        <p>The issue of sample population tends to be a trickier concern in our field. Laboratory-based studies often use students as participants. This is advocated for in Lonati et al. (2018) in their ninth "commandment", and we agree that students are convenient participants because it is easy to get them into university labs, and they are relatively cost-effective participants. However, theoretically, an argument needs to be made for the appropriateness of a sample, which means matching the sample to the focal research question (Thomas, 2011). For research evaluating universalistic theories, student research participants are generally "safe" (Stevens, 2011).The issue of sample population tends to be a trickier concern in our field. Laboratory-based studies often use students as participants. This is advocated for in Lonati et al. (2018) in their ninth "commandment", and we agree that students are convenient participants because it is easy to get them into university labs, and they are relatively cost-effective participants. However, theoretically, an argument needs to be made for the appropriateness of a sample, which means matching the sample to the focal research question (Thomas, 2011). For research evaluating universalistic theories, student research participants are generally "safe" (Stevens, 2011).</p>
        <p>However, even when the arguments for using students are sound, it should still be acknowledged in all studies employing student samples that their use represents a convenience sample, and the convenience of student samples comes at a cost. Research has described student participant pools as "WEIRD", meaning from cultures that are Western, educated, industrialized, rich, and democratic (Henrich, Heine, &amp; Norenzayan, 2010a;2010b). These restrictions can be quite meaningful when interpreting problems in the context of OM, where focal research questions orbit global problems. We are not suggesting this de-legitimizes the use of student participant pools, only that researchers need to be transparent regarding limitations and trade-offs inherent in their use. One effective mechanism to help reduce the concerns regarding the bias of a specific sample is to replicate the results using additional samples that are drawn from a different pool (McGrath, 1982). Replication of results across samples strengthens the triangulation and robustness of any research.However, even when the arguments for using students are sound, it should still be acknowledged in all studies employing student samples that their use represents a convenience sample, and the convenience of student samples comes at a cost. Research has described student participant pools as "WEIRD", meaning from cultures that are Western, educated, industrialized, rich, and democratic (Henrich, Heine, &amp; Norenzayan, 2010a;2010b). These restrictions can be quite meaningful when interpreting problems in the context of OM, where focal research questions orbit global problems. We are not suggesting this de-legitimizes the use of student participant pools, only that researchers need to be transparent regarding limitations and trade-offs inherent in their use. One effective mechanism to help reduce the concerns regarding the bias of a specific sample is to replicate the results using additional samples that are drawn from a different pool (McGrath, 1982). Replication of results across samples strengthens the triangulation and robustness of any research.</p>
        <p>Increasingly, studies in the OM space are adopting online platforms (e.g., AmazonIncreasingly, studies in the OM space are adopting online platforms (e.g., Amazon</p>
        <p>
            <rs type="software">Mechanical Turk [MTurk]</rs>, Qualtrics panels) to administer experiments (see Lee, Seo, &amp; Siemsen, 2018). These online platforms greatly expand potential participant pools, but also are subject to trade-offs (Aguinis &amp; Lawal, 2012;2013). A key initial issue is whether the sample is appropriate for the research question; for example, online panels can be particularly useful when there are confidentiality concerns, such as when asking about abusive supervision (Porter, Outlaw, Gale, &amp; Cho, 2019) or supply chain fraud (DuHadway, Talluri, Ho, &amp; Buckoff, 2020). A thorough exposition of the strengths and weaknesses of crowdsourcing platforms is offered by Goodman and Paolacci (2017). Among the strengths are reduced costs, participant diversity, and a wellreferenced section purporting the strong data quality achieved through 
            <rs type="software">MTurk</rs>. Hauser and colleagues (2018) provided evidence and solutions for some of the more common concerns relating to the use of MTurk samples, including insufficient effort, language comprehension issues, and misrepresentativeness. While some studies have reported increased attentiveness of MTurk samples as compared to student samples (Klein et al., 2014;Hauser &amp; Schwarz, 2016), the use of attention checks with any sample is an important experimental protocol (Abbey &amp; Meloy, 2017;Kane &amp; Barabas, 2019). Finally, it is possible through certain online platforms (e.g., 
            <rs type="software">MTurk</rs>) to build a personal panel of participants who can be properly screened and tracked over time (Peer, Paolacci, Chandler, &amp; Mueller, 2012;Litman, Robinson, &amp; Abberbock, 2017;Sharpe Wessling, Huber, &amp; Netzer, 2017).
        </p>
        <p>While much of this guidance is specific to 
            <rs type="software">MTurk</rs>, by and large the lessons conveyed are applicable to many of the other commonly used platforms (
            <rs type="software">Google</rs> Surveys, Prolific, etc.).
        </p>
        <p>Importantly, the concerns regarding online participant pools are not unique to OM. The guidance here originates from a variety of disciplines, in particular management and marketing, which have learned to successfully navigate this potentially rich resource.Importantly, the concerns regarding online participant pools are not unique to OM. The guidance here originates from a variety of disciplines, in particular management and marketing, which have learned to successfully navigate this potentially rich resource.</p>
        <p>Behavioral OM has tended to maintain a focus on actual behaviors, or "in-task" behaviors (Bachrach &amp; Bendoly, 2011;Croson et al., 2013). However, while actions represent one plausible dependent variable, intentions, attitudes, and affect also are important outcomes amenable to experimental study. These "out-of-task" perceptual measurements often are the underlying drivers of observed behaviors (Ajzen, 1991;Gino &amp; Pisano, 2008), and can be evaluated using vignette study designs (or non-consequential decision making, in Lonati et al., 2018, and addressed in their fourth "commandment"). Ultimately, the appropriateness of such design decisions comes down to the purpose of the research; for research intended to develop theory, it is reasonable to use "more artificial, stylized scenarios", but where the research purpose involves testing theory to better understand real behavior, then enhanced realism undeniably aids in that effort (Morales, Amir, &amp; Lee, 2017, p. 474). A quick perusal of Lonati et al. (2018) may leave the reader with the impression that they advocate a categorical ban on vignette studies; while the authors do offer some nuance on the topic, given the increasing prevalence of vignette studies in OM and the value they stand to offer, we find it useful to expand on this discussion. As with any other research method, there are best practices to be adhered to when conducting a vignette study (see Weber, 1992;Rungtusanatham, Wallin, &amp; Eckerd, 2011;Aguinis &amp; Bradley, 2014).Behavioral OM has tended to maintain a focus on actual behaviors, or "in-task" behaviors (Bachrach &amp; Bendoly, 2011;Croson et al., 2013). However, while actions represent one plausible dependent variable, intentions, attitudes, and affect also are important outcomes amenable to experimental study. These "out-of-task" perceptual measurements often are the underlying drivers of observed behaviors (Ajzen, 1991;Gino &amp; Pisano, 2008), and can be evaluated using vignette study designs (or non-consequential decision making, in Lonati et al., 2018, and addressed in their fourth "commandment"). Ultimately, the appropriateness of such design decisions comes down to the purpose of the research; for research intended to develop theory, it is reasonable to use "more artificial, stylized scenarios", but where the research purpose involves testing theory to better understand real behavior, then enhanced realism undeniably aids in that effort (Morales, Amir, &amp; Lee, 2017, p. 474). A quick perusal of Lonati et al. (2018) may leave the reader with the impression that they advocate a categorical ban on vignette studies; while the authors do offer some nuance on the topic, given the increasing prevalence of vignette studies in OM and the value they stand to offer, we find it useful to expand on this discussion. As with any other research method, there are best practices to be adhered to when conducting a vignette study (see Weber, 1992;Rungtusanatham, Wallin, &amp; Eckerd, 2011;Aguinis &amp; Bradley, 2014).</p>
        <p>Correctly designed vignette experiments situate participants in an operational scenario, or a storyline, that is carefully crafted to realistically depict the problem setting. The vignette consists of baseline information about the setting that is consistent across all treatments (a common module), and manipulations of the independent variable conveyed by different versions of the scenario (experimental cues modules) that are randomly distributed to participants by treatment (Rungtusanatham et al., 2011). 3 Vignette studies have a strong history of use in other disciplines, where they are used for their ability to balance the challenges regarding internal versus external validity (Aguinis &amp; Bradley, 2014). Benefits of vignette studies are that they overcome weak external validity of traditional experiments, cover a broad range of relevant factors, and increase "the generalizability of context specific results" (Atzmüller &amp; Steiner, 2010, p. 137).Correctly designed vignette experiments situate participants in an operational scenario, or a storyline, that is carefully crafted to realistically depict the problem setting. The vignette consists of baseline information about the setting that is consistent across all treatments (a common module), and manipulations of the independent variable conveyed by different versions of the scenario (experimental cues modules) that are randomly distributed to participants by treatment (Rungtusanatham et al., 2011). 3 Vignette studies have a strong history of use in other disciplines, where they are used for their ability to balance the challenges regarding internal versus external validity (Aguinis &amp; Bradley, 2014). Benefits of vignette studies are that they overcome weak external validity of traditional experiments, cover a broad range of relevant factors, and increase "the generalizability of context specific results" (Atzmüller &amp; Steiner, 2010, p. 137).</p>
        <p>Despite these benefits, there are also trade-offs to consider. First, while one benefit is in introduction of context comparable to what one might experience in "real life", there may be situations in which the context is too dense or broad in scope for a vignette study to adequately capture (Lohrke, Holloway, &amp; Woolley, 2010). A vignette needs to contain the information essential to understanding the context, or it may lead to a situation where the participant projects their own experiences or knowledge to fill in the gaps (Wason, Polonsky, &amp; Hyman, 2002). A second challenge involves the effective manipulation of variables. Manipulations must be salient to the participant, yet in their exposition it is important to protect against framing effects (Wason et al., 2002). Different treatments should be as similar as possible, to avoid potential confounds in the experiment (Rungtusanatham et al., 2011).Despite these benefits, there are also trade-offs to consider. First, while one benefit is in introduction of context comparable to what one might experience in "real life", there may be situations in which the context is too dense or broad in scope for a vignette study to adequately capture (Lohrke, Holloway, &amp; Woolley, 2010). A vignette needs to contain the information essential to understanding the context, or it may lead to a situation where the participant projects their own experiences or knowledge to fill in the gaps (Wason, Polonsky, &amp; Hyman, 2002). A second challenge involves the effective manipulation of variables. Manipulations must be salient to the participant, yet in their exposition it is important to protect against framing effects (Wason et al., 2002). Different treatments should be as similar as possible, to avoid potential confounds in the experiment (Rungtusanatham et al., 2011).</p>
        <p>In some fields, such as marketing, there is a current and concerted effort to improve the realism of vignette studies where it is a fit to the research question (Morales et al., 2017). Efforts to enhance realism can be particularly useful when testing theory, for example, but may be less pertinent when research is developing new theory (Morales et al., 2017). Mechanisms for increasing realism can involve the use of video or audio clips in vignette studies, as briefly pointed to in Lonati et al. (2018). These techniques can help increase participant engagement (Caro et al., 2012) and the ecological validity of vignette studies (Bateson &amp; Hui, 1992). For example, Victorino, Verma, and Wardell (2013) enhanced the realism of the scenarios used in their OM research examining service scripting by creating videos of service encounters (see also Seawright &amp; Sampson, 2007, for an example of a video vignette methodology applied to waiting lines).In some fields, such as marketing, there is a current and concerted effort to improve the realism of vignette studies where it is a fit to the research question (Morales et al., 2017). Efforts to enhance realism can be particularly useful when testing theory, for example, but may be less pertinent when research is developing new theory (Morales et al., 2017). Mechanisms for increasing realism can involve the use of video or audio clips in vignette studies, as briefly pointed to in Lonati et al. (2018). These techniques can help increase participant engagement (Caro et al., 2012) and the ecological validity of vignette studies (Bateson &amp; Hui, 1992). For example, Victorino, Verma, and Wardell (2013) enhanced the realism of the scenarios used in their OM research examining service scripting by creating videos of service encounters (see also Seawright &amp; Sampson, 2007, for an example of a video vignette methodology applied to waiting lines).</p>
        <p>Technological advancements are even facilitating the use of virtual reality studies for fullimmersion, multi-sensory experiences (Aguinis &amp; Edwards, 2014). Another design approach for enhancing participants' engagement involves opportunities to seek out or probe for additional information, such as conducting internet searches (Caro et al., 2012). Yet, even these design choices are not free of trade-offs; as studies become more immersive, there are more opportunities for confounding effects, increased costs, and logistical challenges. Victorino and Dixon (2016) provide excellent methodological guidance for the development of video experiments.Technological advancements are even facilitating the use of virtual reality studies for fullimmersion, multi-sensory experiences (Aguinis &amp; Edwards, 2014). Another design approach for enhancing participants' engagement involves opportunities to seek out or probe for additional information, such as conducting internet searches (Caro et al., 2012). Yet, even these design choices are not free of trade-offs; as studies become more immersive, there are more opportunities for confounding effects, increased costs, and logistical challenges. Victorino and Dixon (2016) provide excellent methodological guidance for the development of video experiments.</p>
        <p>As Croson and colleagues (2013, p. 4) pointed out: "Using [out-of-task psychometric measures] as correlates in the context of experiments or surveys is a technique that will enrich the field of behavioral operations". There is much to be learned from studies more deeply exploring the processes of judgement and decision-making, and one of the prominent techniques for doing so is through vignette experiments. Through the guidance offered in this section, we hope to continue to see high-quality, well-designed vignette studies in the OM space.As Croson and colleagues (2013, p. 4) pointed out: "Using [out-of-task psychometric measures] as correlates in the context of experiments or surveys is a technique that will enrich the field of behavioral operations". There is much to be learned from studies more deeply exploring the processes of judgement and decision-making, and one of the prominent techniques for doing so is through vignette experiments. Through the guidance offered in this section, we hope to continue to see high-quality, well-designed vignette studies in the OM space.</p>
        <p>While in Section 2 we focused on each discussion point (demand effects, incentive alignment, deception, sample issues, and context-rich vignette experiments) independent of one another, it is also useful to think about how trade-offs occur across designs. Because OM is an applied field, our research questions tend to depend heavily on context, which implies that participants in experiments may need to have work experience. It makes little sense to incentivize managers with money, as the quantities are generally not sufficiently motivating to get them to leave work and travel to a lab. Such participants are more likely to be motivated with access to study results through a white paper. They may also be willing to invest in the experiment even if they perceive it as non-consequential if they are convinced that it will result in the generation of useful knowledge. Vignette studies may thus yield important insights in the OM context even in the absence of perfectly aligned incentives.While in Section 2 we focused on each discussion point (demand effects, incentive alignment, deception, sample issues, and context-rich vignette experiments) independent of one another, it is also useful to think about how trade-offs occur across designs. Because OM is an applied field, our research questions tend to depend heavily on context, which implies that participants in experiments may need to have work experience. It makes little sense to incentivize managers with money, as the quantities are generally not sufficiently motivating to get them to leave work and travel to a lab. Such participants are more likely to be motivated with access to study results through a white paper. They may also be willing to invest in the experiment even if they perceive it as non-consequential if they are convinced that it will result in the generation of useful knowledge. Vignette studies may thus yield important insights in the OM context even in the absence of perfectly aligned incentives.</p>
        <p>As a further, more detailed example of how these research design considerations trade off in practice, we consider two experiments in our field that on the surface appear to be highly related, yet rely on fundamentally different design choices to explore their research question. Each of the following experiments explore individuals' perceptions of value for products. However, a more detailed exploration of these experiments shows that they rely on different foundations in their analysis to appropriately match their research design to their research question.As a further, more detailed example of how these research design considerations trade off in practice, we consider two experiments in our field that on the surface appear to be highly related, yet rely on fundamentally different design choices to explore their research question. Each of the following experiments explore individuals' perceptions of value for products. However, a more detailed exploration of these experiments shows that they rely on different foundations in their analysis to appropriately match their research design to their research question.</p>
        <p>Agrawal, Atasu, and van Ittersum (2015) investigated the typical customer's perceived value of original manufactured goods when refurbished versions are available from either the original manufacturer or a third party. The research context is a purchasing decision where a general consumer pays for the product. Agrawal et al. (2015) matched the context to the research question by using a sampling of data from MTurk, presenting simplified choices, and paying each respondent $1 and a chance to receive a decision-based incentive of $200 based on their specific choices in the experiment. This is an appropriate research sample, as their question focuses on general population purchasing behaviors, and MTurk is shown to be reasonably representative of the general population (Goodman, Cryder, &amp; Cheema, 2013). Had they used a more targeted sample (e.g., CEOs or students), their findings would be weaker due to decreased generalizability.Agrawal, Atasu, and van Ittersum (2015) investigated the typical customer's perceived value of original manufactured goods when refurbished versions are available from either the original manufacturer or a third party. The research context is a purchasing decision where a general consumer pays for the product. Agrawal et al. (2015) matched the context to the research question by using a sampling of data from MTurk, presenting simplified choices, and paying each respondent $1 and a chance to receive a decision-based incentive of $200 based on their specific choices in the experiment. This is an appropriate research sample, as their question focuses on general population purchasing behaviors, and MTurk is shown to be reasonably representative of the general population (Goodman, Cryder, &amp; Cheema, 2013). Had they used a more targeted sample (e.g., CEOs or students), their findings would be weaker due to decreased generalizability.</p>
        <p>The Agrawal et al. (2015) experiment presents limited context, describing primarily the choice between two products. Again, this is the correct approach because the research question is not highly dependent on the context and the choice is easy for participants to understand. Adding additional superfluous details in the experiment would detract from the strength of their manipulation. The use of a decision-based incentive is correctly matched in the experiment to the incentives in reality, where consumers benefit directly from purchases in the form of the goods they select. Had they selected a flat-rate incentive for participation, the findings would be weaker. given that for the research context, "students are expected to be fairly homogeneous with respect to task-specific competencies and knowledge" (Sommer et al., 2019, p. 10). The ability to make research design choices across the multiple epistemological backgrounds of operations research enhances experimental validity.The Agrawal et al. (2015) experiment presents limited context, describing primarily the choice between two products. Again, this is the correct approach because the research question is not highly dependent on the context and the choice is easy for participants to understand. Adding additional superfluous details in the experiment would detract from the strength of their manipulation. The use of a decision-based incentive is correctly matched in the experiment to the incentives in reality, where consumers benefit directly from purchases in the form of the goods they select. Had they selected a flat-rate incentive for participation, the findings would be weaker. given that for the research context, "students are expected to be fairly homogeneous with respect to task-specific competencies and knowledge" (Sommer et al., 2019, p. 10). The ability to make research design choices across the multiple epistemological backgrounds of operations research enhances experimental validity.</p>
        <p>The OM field draws on multiple foundations, and those foundations do not always agree on methodological "best practices." Lonati et al. (2018) started an important discussion in our field by provocatively presenting a set of "ten commandments" for experimental research (Table 1, p. 20). While the Lonati et al. (2018) piece provides experimental guidance fitting to certain research agendas, questions have arisen concerning whether and how exactly to implement some of the points that it makes, and how to best address trade-offs in the design of behavioral experiments.The OM field draws on multiple foundations, and those foundations do not always agree on methodological "best practices." Lonati et al. (2018) started an important discussion in our field by provocatively presenting a set of "ten commandments" for experimental research (Table 1, p. 20). While the Lonati et al. (2018) piece provides experimental guidance fitting to certain research agendas, questions have arisen concerning whether and how exactly to implement some of the points that it makes, and how to best address trade-offs in the design of behavioral experiments.</p>
        <p>Questions have also arisen concerning how to apply these concepts in operations management (OM). In this note, we have elaborated on the design choices surrounding demand effects, incentives, deception, sample issues, and the use of vignettes. We have carefully depicted the tradeoffs inherent in making design decisions in experimental research. Consistent with the longstanding perspective of McGrath (1982), there is no single "best" method; we agree that it is very difficult to lay down a set of guidelines that fairly and adequately addresses different research foundations. We suggest that the path forward is an acknowledgement and appreciation of these differences. As eloquently stated by Croson (2005, p. 145) "Each researcher needs to make their own methodological decisions based on the objectives of the experiment, the methods currently used in their field, and the audience they wish to address." We can all likely agree that OM is a cross-disciplinary field; we build on the foundations of other disciplines to inform our own work (Gino &amp; Pisano, 2008;Bendoly et al., 2010). Weaving the foundations of these disciplines into rigorous research requires that we as a community invest together to explore and understand the nuances so that we can produce actionable research efficiently, and so that we can provide guidance to JOM authors and review teams that maximizes the value of their investments.Questions have also arisen concerning how to apply these concepts in operations management (OM). In this note, we have elaborated on the design choices surrounding demand effects, incentives, deception, sample issues, and the use of vignettes. We have carefully depicted the tradeoffs inherent in making design decisions in experimental research. Consistent with the longstanding perspective of McGrath (1982), there is no single "best" method; we agree that it is very difficult to lay down a set of guidelines that fairly and adequately addresses different research foundations. We suggest that the path forward is an acknowledgement and appreciation of these differences. As eloquently stated by Croson (2005, p. 145) "Each researcher needs to make their own methodological decisions based on the objectives of the experiment, the methods currently used in their field, and the audience they wish to address." We can all likely agree that OM is a cross-disciplinary field; we build on the foundations of other disciplines to inform our own work (Gino &amp; Pisano, 2008;Bendoly et al., 2010). Weaving the foundations of these disciplines into rigorous research requires that we as a community invest together to explore and understand the nuances so that we can produce actionable research efficiently, and so that we can provide guidance to JOM authors and review teams that maximizes the value of their investments.</p>
        <p>Both you and your supplier bring a formal and contract-governed orientation to this relationship. Exchange of information in this relation-ship takes place infrequently, formally, and in accordance to the terms of a prespecified agreement. Even if you do know of an event or change that might affect the other party, you do not divulge this information to them. Strict adherence to the terms of the original agreement characterizes your relationship with this supplier. Even in the face of unexpected situations, rather than modifying the contract, you adhere to the original terms. You have an arm's-length relationship with your supplier. You do not think that the supplier is committed to your organization-in fact, you think that if you did not carefully monitor this supplier's performance, they would slack off from the original terms. Above all, you see your supplier as an external economic agent with whom you have to bargain in order to get the best deal for yourself.Both you and your supplier bring a formal and contract-governed orientation to this relationship. Exchange of information in this relation-ship takes place infrequently, formally, and in accordance to the terms of a prespecified agreement. Even if you do know of an event or change that might affect the other party, you do not divulge this information to them. Strict adherence to the terms of the original agreement characterizes your relationship with this supplier. Even in the face of unexpected situations, rather than modifying the contract, you adhere to the original terms. You have an arm's-length relationship with your supplier. You do not think that the supplier is committed to your organization-in fact, you think that if you did not carefully monitor this supplier's performance, they would slack off from the original terms. Above all, you see your supplier as an external economic agent with whom you have to bargain in order to get the best deal for yourself.</p>
        <p>Both you and your supplier bring an open and frank orientation to the relationship. Exchange of information in this relationship takes place frequently, informally, and not always according to a prespecified agreement. You keep each other informed of any event or change that might affect the other party. Flexibility is a key characteristic of this relationship. Both sides make ongoing adjustments to cope with the changing circumstances. When some unexpected situation arises, the parties would rather work out a new deal than hold each other responsible to the original terms.Both you and your supplier bring an open and frank orientation to the relationship. Exchange of information in this relationship takes place frequently, informally, and not always according to a prespecified agreement. You keep each other informed of any event or change that might affect the other party. Flexibility is a key characteristic of this relationship. Both sides make ongoing adjustments to cope with the changing circumstances. When some unexpected situation arises, the parties would rather work out a new deal than hold each other responsible to the original terms.</p>
        <p>You tend to help each other out in case of unexpected crises. If your supplier is unable to fulfill an order, they recommend an alternative source of supply for the same. Above all, you have a sense that your supplier is committed to your organization and that they work with you keeping your best interests in mind. You see each other as partners, not rivals.You tend to help each other out in case of unexpected crises. If your supplier is unable to fulfill an order, they recommend an alternative source of supply for the same. Above all, you have a sense that your supplier is committed to your organization and that they work with you keeping your best interests in mind. You see each other as partners, not rivals.</p>
        <p>Recently, the supplier informed you that they are involved in a labor dispute. Consequently, they are temporarily unable to guarantee on-schedule delivery. This creates some uncertainty for your organization. Delayed delivery of microchips, may, for example, cause problems for your organization in meeting delivery schedules to customers. The supplier has called to get your regular order. Drawing from experience, how would you be most likely to react in this situation? Please rate each of these statements to the extent that they match with your expectation of your reaction.Recently, the supplier informed you that they are involved in a labor dispute. Consequently, they are temporarily unable to guarantee on-schedule delivery. This creates some uncertainty for your organization. Delayed delivery of microchips, may, for example, cause problems for your organization in meeting delivery schedules to customers. The supplier has called to get your regular order. Drawing from experience, how would you be most likely to react in this situation? Please rate each of these statements to the extent that they match with your expectation of your reaction.</p>
        <p>across experimental manipulations, minimizing experimenter-participant interaction, not informing participants regarding study hypotheses, specific conditions, and expected outcomes, and efforts to mask researcher intent. Most of these remedies are relatively low-cost and, while they do not guarantee complete immunity from demand effects, they do reduce these risks and should be used whenever possible. Effectively managing demand effects through research design is a good example of how to off-set potential trade-offs in conducting rigorous and relevant experimental research.across experimental manipulations, minimizing experimenter-participant interaction, not informing participants regarding study hypotheses, specific conditions, and expected outcomes, and efforts to mask researcher intent. Most of these remedies are relatively low-cost and, while they do not guarantee complete immunity from demand effects, they do reduce these risks and should be used whenever possible. Effectively managing demand effects through research design is a good example of how to off-set potential trade-offs in conducting rigorous and relevant experimental research.</p>
        <p>It is worth clarifying here that social desirability bias and demand effects are separate risks to experimental validity. Social desirability bias is to answer surveys in a way that is viewed favorably by others, while demand effects represent changes in experiment behavior associated with treatment effects.It is worth clarifying here that social desirability bias and demand effects are separate risks to experimental validity. Social desirability bias is to answer surveys in a way that is viewed favorably by others, while demand effects represent changes in experiment behavior associated with treatment effects.</p>
        <p>An example vignette fromJoshi and Arnold (1997) is provided in Appendix A to clearly show baseline and experimental cue modules. Although having originated in marketing, this baseline scenario and context has been replicated across numerous disciplines, including OM(Ganesan et al., 2010;Ro et al., 2016;Su et al., 2017).An example vignette fromJoshi and Arnold (1997) is provided in Appendix A to clearly show baseline and experimental cue modules. Although having originated in marketing, this baseline scenario and context has been replicated across numerous disciplines, including OM(Ganesan et al., 2010;Ro et al., 2016;Su et al., 2017).</p>
        <p>. The research context was intentionally simple, as the research question was on the search process, not the actual decisions being made. To enhance the realism of the search process, the authors provided live feedback from a market analyst. Rather than being an individual providing feedback as it is presented in the simulation, the feedback was done using artificial intelligence (i.e., a confederate) that provided market analysis based on actual experimental performance and decisions made. This design choice was well justified in the research article. Even though the experimental task was a highly simplified context, the authors also incorporated the use of vignettes as part of the pretraining exercise to verify the efficacy of the student sample. The student sample was also justified,. The research context was intentionally simple, as the research question was on the search process, not the actual decisions being made. To enhance the realism of the search process, the authors provided live feedback from a market analyst. Rather than being an individual providing feedback as it is presented in the simulation, the feedback was done using artificial intelligence (i.e., a confederate) that provided market analysis based on actual experimental performance and decisions made. This design choice was well justified in the research article. Even though the experimental task was a highly simplified context, the authors also incorporated the use of vignettes as part of the pretraining exercise to verify the efficacy of the student sample. The student sample was also justified,</p>
        <p>The following vignette is reproduced from Joshi and Arnold (1997). The headings are not visible to the participant. We added the notations in brackets differentiating baseline modules and experimental cues modules. The vignette begins and concludes with baseline (i.e., common) modules. Each participant randomly received one of the two dependence experimental cues modules (low or high), and one of the two relational norms experimental cues modules (low or high).The following vignette is reproduced from Joshi and Arnold (1997). The headings are not visible to the participant. We added the notations in brackets differentiating baseline modules and experimental cues modules. The vignette begins and concludes with baseline (i.e., common) modules. Each participant randomly received one of the two dependence experimental cues modules (low or high), and one of the two relational norms experimental cues modules (low or high).</p>
        <p>You are a purchasing manager responsible for the purchase of microchips for a midsize electronic equipment manufacturer. Microchips are an important component for the equipment that you manufacture; therefore, they need to be purchased on a regular basis. You have one existing supplier for this component.You are a purchasing manager responsible for the purchase of microchips for a midsize electronic equipment manufacturer. Microchips are an important component for the equipment that you manufacture; therefore, they need to be purchased on a regular basis. You have one existing supplier for this component.</p>
        <p>As purchasing manager responsible for microchips, you find yourself in a situation wherein it is not difficult for you to find a suitable replacement for the existing supplier. If you decide to stop purchasing from this supplier, you could easily replace their volume with purchases from alternative suppliers. There are many competitive suppliers for microchips and you can switch to them without incurring any search costs. Switching suppliers is not going to have any negative effects on the quality or design of the equipment that you manufacture. Your production system can be easily adapted to use components from a new supplier. The procedures and routines that you have developed are standard and they are equally applicable with any supplier of this component. The skills that your people have acquired in the process of working with the supplier can easily be changed to fit another supplier's situation. You can therefore terminate your relationship with your present supplier without incurring any costs.As purchasing manager responsible for microchips, you find yourself in a situation wherein it is not difficult for you to find a suitable replacement for the existing supplier. If you decide to stop purchasing from this supplier, you could easily replace their volume with purchases from alternative suppliers. There are many competitive suppliers for microchips and you can switch to them without incurring any search costs. Switching suppliers is not going to have any negative effects on the quality or design of the equipment that you manufacture. Your production system can be easily adapted to use components from a new supplier. The procedures and routines that you have developed are standard and they are equally applicable with any supplier of this component. The skills that your people have acquired in the process of working with the supplier can easily be changed to fit another supplier's situation. You can therefore terminate your relationship with your present supplier without incurring any costs.</p>
        <p>As purchasing manager responsible for microchips, you find yourself in a situation wherein it is difficult for you to find a suitable replacement for the existing supplier. If you decide to stop purchasing from this supplier, you could not easily replace their volume with purchases from alternative suppliers. There are very few, if any, competitive suppliers for microchips and you cannot switch to them without incurring significant search and verification costs. Switching suppliers is also going to have negative effects on the quality or design of the equipment that you manufacture. Your production system cannot be easily adapted to use components from a new supplier. The procedures and routines that you have developed are unique and hence they are not applicable with any other supplier of this component. The skills that your people have acquired in the process of working with the supplier cannot easily be changed to fit another supplier's situation. You cannot therefore terminate your relationship with your present supplier without incurring significant costs.As purchasing manager responsible for microchips, you find yourself in a situation wherein it is difficult for you to find a suitable replacement for the existing supplier. If you decide to stop purchasing from this supplier, you could not easily replace their volume with purchases from alternative suppliers. There are very few, if any, competitive suppliers for microchips and you cannot switch to them without incurring significant search and verification costs. Switching suppliers is also going to have negative effects on the quality or design of the equipment that you manufacture. Your production system cannot be easily adapted to use components from a new supplier. The procedures and routines that you have developed are unique and hence they are not applicable with any other supplier of this component. The skills that your people have acquired in the process of working with the supplier cannot easily be changed to fit another supplier's situation. You cannot therefore terminate your relationship with your present supplier without incurring significant costs.</p>
    </text>
</tei>

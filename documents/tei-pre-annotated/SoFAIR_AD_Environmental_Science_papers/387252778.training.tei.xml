<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:14+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Front-line remote sensing tools, coupled with machine learning (ML), have a significant role in crop monitoring and disease surveillance. Crop type classification and a disease early warning system are some of these remote sensing applications that provide precise, timely, and cost-effective information at different spatial, temporal, and spectral resolutions. To our knowledge, most disease surveillance systems focus on a single-sensor based solutions and lagging the integration of multiple information sources. Moreover, monitoring larger landscapes using unmanned aerial vehicles (UAV) are challenging, and, therefore combining high resolution satellite imagery data with advanced machine learning (ML) models through the use of mobile apps could help detect and classify banana plants and provide more information on its overall health status. In this study, we classified banana under mixed-complex African landscapes through pixel-based classifications and ML models derived from multi-level satellite images (Sentinel 2, PlanetScope and WorldView-2) and UAV (
            <rs type="software">MicaSense</rs> RedEdge) platforms. Our pixel-based classification from random forest (RF) model using combined features of vegetation indices (VIs) and principal component analysis (PCA) showed up to 97% overall accuracy (OA) with less than
        </p>
        <p>Bananas and plantains serve as a primary food source for around 20 million people in East Africa and 70 million people in West and Central Africa, and given that Sub-Saharan Africa is highly dependent on banana/plantain cultivation for food, income and job security (De Buck and Swennen, 2016). In East and Central Africa (ECA), bananas are mainly grown on small-scale farms with a size ranging from 0.5 to 4 ha (FAOSTAT, 2017). Biotic stresses (bacterial, viral and fungal diseases, and pests) continue to affect the banana production systems in this region, causing partial damage or even the destruction of entire fields (De Buck and Swennen, 2016). The arrival and spread of major devastating banana diseases such as bunchy top disease (BBTD) and Xanthomonas wilt of banana (BXW) pose a considerable threat to food security in the ECA region (Blomme et al., 2017;Niyongere et al., 2012;Niyongere et al., 2015;Ocimati et al., 2019).</p>
        <p>Disease surveillance for BXW and BBTD carried out through field visits, or human scouting are often complemented by diagnostic tools such as using growth media and polymerase chain reaction (PCR) (Blomme et al., 2014;Bouwmeester et al., 2016;Ocimati et al., 2019). The inspection of early disease detection is assessed as prevalence (present or not) at plot/field, farm, village, or landscape level, and the procedure by itself at a large scale is challenging and time-consuming (Johansen et al., 2014). These limitations have led scientists to investigate on advanced and novel techniques that could obtain the crop health information rapidly and economically (Heim et al., 2019;Johansen et al., 2014;Steward et al., 2019).</p>
        <p>UAVs and satellites with the capability of capturing a large number of high-quality spectral-temporal aerial images are becoming an ultimate technology for assessing yield, health, and economic valuation of the crop (Ji et al., 2018). Conventional classification methods such as support vector machine (SVM), K-nearest neighbor (KNN), maximum likelihood classification (MLC) are successfully applied for crop type classification (Blanzieri and Melgani, 2008;Murthy et al., 2003). However, these methods are slower and involve supervised labeling to get precise data (Ji et al., 2018). Therefore, it is essential to develop crop classification technologies that are pragmatic and can be widely applied in extensive operational settings. Crop classification by remote sensing is an intricate task, especially if different crops have a similar spectral response and growth pattern. In such cases, classification tasks could be enhanced by combining object-based image analysis and advanced machine learning methods (Peña et al., 2014). In the last few years, high resolution UAV images coupled with machine learning (ML) classification techniques are emerging for crop classification (Steward et al., 2019). Recent research (Pourazar et al., 2019;Steward et al., 2019) also demonstrated the potential of deep learning-based instance segmentation approach on field-based disease classification using UAVs, but the efforts towards banana remain limited. Neupane et al. (2019) reported deep learning-based banana classification and height estimation in wellplanned commercial fields using UAV-RGB images. However, these models may be more complicated in an African scenario, where mixedcomplex agriculture systems bound the area with trees, intercrops, and bananas. Therefore, an automatic classification method to locate or detect banana plants/mats in the mixed-complex landscapes is in urgent need. Since 2010, UAV-based imaging sensors have been used for providing high operational flexibility in crop monitoring (Sankaran et al. 2015;Shi et al. 2016) but comes with their restrictions in terms of limited aerial coverage, operational skills, country regulations, and permissions, especially in developing countries. Another important advancement in crop monitoring is the availability of high-resolution images from satellites with sub-meter accuracy. With the high spatial and temporal resolution of satellite imagery, multispectral (MS) data can be used for crop classification similar to UAV images with the added advantage of larger area coverage.</p>
        <p>Remote sensing permits the non-invasive measurement of biophysical and biochemical parameters of crops, and thus for the nondestructive monitoring of plant health status (Lu et al., 2015;Mahlein et al., 2010;Ramcharan et al., 2017). The rapid technology development in UAV systems and the initiation of low-cost UAVs carrying visible and multispectral sensors, provide the opening to capture high spatial and spectral resolution data, especially for disease detection for production fields. The ability to capture the crop phenotypic differences in this complex multi-dimensional system is necessary to understand the host-pathogen interactions better and develop disease-resistant varieties (Steward et al., 2019). In wheat, the mobile and real-time plant disease (MARPLE) integrated diagnostics system already proved to be an early warning system in Ethiopia to directly inform diseases risk forecasting (Carvajal-Yepes et al., 2019). However, the benefits of integrated remote sensingbased diagnostic technologies have not been assessed in low-income countries, where emerging diseases in banana can be particularly devastating (e.g. Fusarium TR4 outbreak in Colombia; BXW in eastern DR Congo).</p>
        <p>The combination of aerial image information and AI-based approaches have the potential to provide an accurate, high-throughput method for plant disease detection under real-world conditions (Boulent et al., 2019). More recently, Selvaraj et al., (2019) demonstrated the deep learning potential on banana disease detection, which led to the release of a smartphone-based AI app (Tumaini), which could detect six major banana diseases (https://play.google.com/store/apps/details?id = ciat.cgiar.org.tumaini&amp;hl = en). The use of these AI apps for disease surveillance on a system-level approach to integrate multi-level sensing systems (satellites, UAVs AI apps, and ground-truthing) is necessary to monitor crop health at different scales. Continuous efforts to leverage advanced technologies for crop disease surveillance and management in low-income countries (LICs) must occur to effectively limit the impact of crop diseases spreading locally, and globally (Carvajal-Yepes et al., 2019). In this framework, the paper aims to investigate the potential of aerial imaging and ML techniques on banana classification and disease detection in the African mixed-landscape. To this aim, we used MS, UAVs (MicaSense RedEdge) and muti-level satellite images (Sentinel 2, PlanetScope and WorldView-2) (1) to classify and localize banana under mixed-complex African landscapes through pixel-based classifications and ML models. (2) Also, developed a object based deep learning mixed models for simultaneous banana detection and classification of two of their major diseases (BXW and BBTD), using UAV-RGB images.</p>
        <p>Typical banana growing regions were selected from Kabare district, South Kivu province of the Eastern Democratic Republic of Congo (DR Congo), and the Oueme, Plateau, Mono, and Atlantique departments of the southern Republic of Benin were selected as specific study regions (Fig. 1). The Eastern DR Congolese highlands (1,400-1,800 m asl) are dominated by the East African highland banana (Musa AAA-EAH genome) types. In contrast, plantain (Musa AAB genome) is the most widely grown Musa type in the Southern Republic of Benin. Both the regions are characterized by small-scale subsistence type of agriculture with mixed-complex landscapes where banana plants are mixed with shrubs, trees, annual food crops, pastures, buildings, roads, etc. Kabare district, DR Congo, is characterized by a mean annual rainfall of 1656 ± 235 mm (2015-2018) distributed over two rainy seasons (February to May and September to December) and a mean annual temperature of 18 • C. The study area in southern Republic of Benin is a coastal lowland area with an average altitude of 11 m in the coastal Atlantique to 250 m in the Oueme region. The mean annual temperature in the southern Republic of Benin is 28 • C, while annual rainfall ranges from 1,000 to 1,300 mm.</p>
        <p>In DR Congo, UAV images of BXW infected fields were collected from 15 different locations of the Kabare district, eastern DR Congo during the years of 2017, 2018, and 2019 (Supplementary Table 1). BXWaffected landscapes were selected based on the presence of apparent BXW symptoms in banana fields. Besides, pixel-based banana classification datasets of multispectral UAV and satellite images were collected from three different locations in the Kabare district of DR Congo (Supplementary Table 1). UAV-RGB images of BBTD infected banana fields were collected in Republic of Benin (Oueme Plateau, Mono, and Atlantique departments), West Africa, in 2019. The detailed location descriptions of two experimental sites are shown in Supplementary Table 1.</p>
        <p>Ground truth data of disease information was obtained during field surveys where experts walk through banana fields and assess the presence and incidence of the diseases. The specific size for each sample point covered one banana plant, and these samples are classified into two categories: healthy and diseased plants (BXW and BBTD), as reflected by the typical external characteristics and the symptoms expressed by the infected plants. Based on these ground truth information, the UAV-based aerial images were annotated by plant pathologists on the presence of BXW and BBTD (Supplementary Figure 1). The symptoms used to pinpoint a plant affected by BXW are leaf yellowing and wilting, whereas a bunchy top/rosette appearance was the typical symptom of BBTD-affected fields. Using large sets of annotated images, the datasets were prepared to localize and detect banana and their major diseases using machine learning models. The sensors and ML system used in this study are described below.</p>
        <p>Our ML detection system (banana and their major diseases) is based on multispectral and RGB images captured from satellites and UAVs, as presented in Fig. 2. In this study, we used multispectral (MS) satellite and UAV images for pixel-based banana classification in the field landscape (Fig. 2a) and high-resolution UAV-RGB images for both objectbased banana localization and disease detection (BXW and BBTD) purposes (Fig. 2b). An overview of the sensor and ML-based banana classification and disease detection system are shown in Table 1.The software and hardware used in this study are presented in Supplementary Table 2.</p>
        <p>UAV-MS images were taken (Fig. 2a) with a MicaSense RedEdge multispectral camera attached to a multicopter UAV Phantom 4 Pro (P4P) with automatic flight plan using 
            <rs type="software">Pix4Dcapture app</rs> (
            <rs type="url">https://www. pix4d.com/es/producto/pix4dcapture</rs>). In order to collect the aerial multispectral imaging data, the drone flew autonomously along a preset flight path at a speed of 2.5 m/s and a height of 100 m above ground level, achieving ~8.0 cm ground sample distance. The Micasense RedEdge can acquire a 16-bit raw image in five narrow bands from blue, green, red, red edge to near-infrared (NIR), and more details are listed in the Supplementary Table 3.
        </p>
        <p>MS orthomosaics (converting large numbers of individual images into a single geo-referenced mosaicked image) for each flight were constructed using 
            <rs type="software">Agisoft Metashape</rs> software (
            <rs type="creator">Agisoft</rs>, 2020) with the protocol described in (Selvaraj et al., 2020). We ended up with three high-quality MS orthomosaics from DR Congo (see Supplementary Table 1) where drone and satellite image dates coincide. We used three different satellite information that acquires low to high spatial resolution reflectance images (geometrically and radiometrically corrected products): 
            <rs type="software">WorldView</rs>
            <rs type="version">2</rs> (WV
            <rs type="creator">-2</rs>), (Digital Globe, 2016); PlanetScope (PS), (Planet, 2017); Sentinel 2(S2), (ESA, 2015) (Table 1). The descriptions of collected locations, spectral and spatial characteristics of each satellite are listed in Supplementary Tables 1 and3.
        </p>
        <p>To extract UAV-MS features, we annotated all banana plants (individual and clusters, i.e., mats) located in three MicaSense-derived high-quality MS orthomosaics from the Kabare district (Supplementary Table 1). To extract reflectance data, we drew 30 polygons, and a total of 21,036 reference points for the three classes (banana, pastures, and trees) were obtained. The data description is shown in Table 2. Simple Ratio (SR) and non-normalized Vegetation Indices (VIs) (Supplementary Table 4) were extracted from the data points to compare the structural part of the plant (referred to NIR band) over the visible region of chlorophyll absorption (Chuvieco, 1991).</p>
        <p>To extract satellite image features, the same technique of polygon feature extraction of banana plantations, as described in the above section, was used. Since the three satellite sensors (WV-2, PS, and S2) have different spatial resolutions, the chosen locations were not enough to extract the data points and train the models effectively. Therefore, we selected other banana plantations in the same UAV flown region and derived the same three classes (banana, pastures, and trees), however for PS and S2, we created a fourth class with non-banana features (i.e., the sum of pastures and trees), and this was created to test the performance of the model (Table 2). The amount of reference samples for each class type are shown in Table 2.</p>
        <p>We used the classification package from 
            <rs type="software">google earth engine (GEE)</rs> (Gorelick et al., 2017) that contains traditional ML algorithms for random forest (RF) (Breiman, 2001) and support vector machine (SVM) classifications (Hsu et al., 2003).
        </p>
        <p>The workflow of the classifications is shown below in Fig. 3, and the steps are described as follows:</p>
        <p>1) VIs calculation, masking of buildings, and soil: The thresholding mask using G-R, and VIs was created. 2) Principal component analysis (PCA) of bands and VIs: This technique is used to extract the best information from all the data and test if the model has higher precision, and avoid the autocorrelation of the features. 3) Training data collection and labeling: The reference data collected from all the sensors were alienated into 70% and 30% for training and testing, respectively. The feature collection function from GEE was used to store the features. Also, the following accuracy metrics were computed: Overall Accuracy (OA), Kappa coefficient (Kappa), per-class User's (UA) and Producer's (PA) Accuracy, as well as Commission Error (CE) and Omission Error (OE) (Campbell and Wynne, 2011;Lucas et al., 1994).</p>
        <p>Initially, UAV-RGB images of BXW and BBTD were taken using a 3DR SOLO multicopter UAV with a Sony QX1 RGB camera and subsequently using a DJI Phantom 4 Pro (P4P) UAV with an integrated RGB camera.</p>
        <p>The flight paths of 3DR Solo UAV and P4P was programmed using 
            <rs type="software">Tower app</rs> software (Tower, 2017) and 
            <rs type="software">Pix4Dcapture</rs> software (Pix
            <rs type="software">4D</rs>, 2018).
        </p>
        <p>To check the sensitivity of UAV captured images at spatial resolution, the flying heights were set at the altitudes of 50 and 100 m above the soil level. The location details are listed in Supplementary Table 1. The undulating terrain and tall trees (e.g., Ficus and Eucalyptus trees) made the UAVs challenging to fly at a low altitude; however, these flying heights were translated to a spatial resolution of 1 and 3 cm based on UAVs altitudes. These UAV-RGB images were also used to develop a DLbased banana detection system (Fig. 2 a, b).</p>
        <p>To detect all possible banana plants in the field (either healthy or diseased), we trained the object detection model to represent only one general class called banana. The annotation labels were drawn on the RGB images by plant pathologists using the 
            <rs type="software">Labellmg</rs> software (Tzutalin, 2015) (Fig. 4 a,b) in the PASCAL VOC format implemented in the ImageNet database (Deng et al., 2009) ending up with 6195 annotations from the RGB orthomosaics. To avoid the colossal processing and memory usage, orthomosaics having more than 10,000 pixels were split into several tiles, based on the original image size, with an overlap of 50 pixels between them. It was considered ideal for reconstructing the orthomosaics after prediction, thereforenot to lose samples in the splitting process.
        </p>
        <p>To classify the banana diseases, we used four classes, healthy individual plant, healthy banana cluster, BXW infected, and BBTD infected plants (Fig. 4 c,d,e,f). Using the 
            <rs type="software">Labellmg</rs> software (Tzutalin, 2015), the total number of 2753 annotations (filtered from the 6195 initial annotations) in the real field landscapes were categorized as 599, 705, 922, and 583 for healthy plants, healthy banana clusters, BXW infected plants and BBTD infected plants, respectively (Supplementary Fig. 1). The healthy individual plant was annotated at different ages, especially that are similar to the age of BBTD infected plants. This is performed to differentiate the BBTD infected plants from healthy young plants, and all the annotated images were confirmed by the plant pathologists. A python 
            <rs type="software">script</rs> was used to extract the images inside the annotation for each class and create a new dataset with desired classes based on our region of interest.
        </p>
        <p>As mentioned earlier, a disease classification model was placed on top of an object (banana) detection algorithm to classify healthy and diseases classes (Fig. 4). We used two architectures to evaluate the disease classification model; namely, a Visual Geometry Group 16 (VGG-16) pre-trained model and a custom model (Fig. 5), which are mentioned below.</p>
        <p>For banana localization, we used RetinaNet (Lin et al., 2017) with a backbone of ResNet50 developed by 
            <rs type="software">Fizyr</rs> (Fizyr, 2019) to localize and detect banana plants. To train the model, we implemented a transfer learning approach with the pre-trained COCO (Common objects in context) data set (Lin et al., 2017), which is available in Tensor Flow object detection API model Zoo (Huang et al., 2017). Finally, we used a mixed model (localizer + classifier) to detect plant status (healthy or diseased) (Fig. 5).
        </p>
        <p>For diseases (BXW and BBTD) recognition, we used two architectures to evaluate the disease classification model; namely, a visual geometry group 16 (VGG-16) pre-trained model and a custom model briefly mentioned below.</p>
        <p>VGG-16: VGG-16 was developed by the visual geometry group (VGG) of Oxford University. Due to its decent generalization performance, VGG-16 can increase the classification accuracy by using its pre-trained model on the ImageNet dataset (Deng et al., 2009). To adapt VGG-based architecture used in this study, we altered the model adding a dropout layer of 2% before the soft-max to reduce the overfitting.</p>
        <p>Custom Model: As observed in Supplementary Fig. 2, our proposed architecture starts with a fixed size input layer of established image dimensions (64x64x3), followed by four feature extraction blocks with two convolutional layers and one max-pooling layer, which extracts the image features. Then, to keep only the relevant features by class and reduce overfitting issues, a dropout layer of 0.2 was installed. When the image passes through the model, the resulting feature map is transferred to the next fully connected layer, where the inference is made.</p>
        <p>Training We trained RetinaNet with RetinaNet50 backbone to develop an object detection (banana) model. The minimum object size allowed is 32 pixels. We used the original architecture without any modifications and divided the dataset into 90% and 10% for training (6287 annotations) and testing (726 annotations), respectively. This proportion was mainly selected to have as many training samples as possible to improve the accuracy. Moreover, this chosen proportion will not affect the model importance on features (Fig. 6).</p>
        <p>In this paper, we applied different metrics such as loss function, IoU, precision and recall to evaluate the performance of the banana localization. The loss function provides a statistical representation of the training process and diagnoses a good-fit of the model. Loss function data was extracted using (Mané, 2015) the 
            <rs type="software">TensorFlow</rs>'s visualization toolkit (
            <rs type="software">TensorBoard</rs>), which helps to track and visualize metrics such as loss and accuracy in ML process. Once we train the model, the Intersection over Union (IoU) was implemented to extract true positives, if B 1 is the ground truth bounding box and B 2 the predicted bounding box, IoU is calculated as in the equation (1). In this case, to determine if a prediction is a real positive (TP), we used a value of 0.3 as an acceptable box overlap, so if IoU greater than 0.3, then the current prediction is a TP.
        </p>
        <p>For disease classification, precision, recall, and F1-score metrics were computed to evaluate the performance of the classification model. Recall, precision, and F1-scores were computed using the same formula reported by Mao et al. (2017). Additionally, we also computed the confusion matrix to evaluate the accuracy per class.</p>
        <p>The normalized VIs are sensitive to environmental factors and causes a high level of saturation (Yang et al., 2008) (Supplementary Fig. 3) and distortion (Azar et al., 2016). To address these limitations, several other derivatives and alternatives to NDVI have been reported in the scientific literature (Yang et al., 2008). We exploited non-normalized VIs for our banana classification task, which is listed in Supplementary Table 4.</p>
        <p>A variety of bands and VIs are produced from each sensor (Supplementary Table 4) 8), were showed the difference between the banana and non-banana classes (Fig. 8). However, in the case of S2, EVI and TGI are the only VIs that differentiate banana and non-banana classes. In contrast, other VIs was not much useful to separate those classes (Fig. 8 d).</p>
        <p>Fig. 9 clearly shows the correlation of VIs derived from MicaSense (UAV) and WV-2, where, a highly significant correlation was achieved in VIs of CIG and RVI-G (r = 0.73). Also, ARVI, IPVI, and OSAVI exhibited a high correlation of r = 0.66. Our correlation analysis between other sensors like PS vs. UAV, S2 vs. UAV combination is listed in Supplementary Figs. 4 and5. As expected, the correlation between VIs derived from UAV and other satellite sensors (PS and S2) also exhibited higher positive and significant correlation (Supplementary Figs. 4 and5), indicating the potential of those sensors on banana classification in mixed-complex African landscapes.</p>
        <p>MicaSense versus WV-2 based ML model : To evaluate the banana classification performance of the ML models (RF and SVM), we used the confusion matrix. The overall results of RF classifier model with combined (Bands + VIs + PCA) (Fig. 11) features demonstrated a maximum OA of 97% and 93% with ≤ 9% OE and CE, kappa of 0.96 and 0.90 for MicaSense and WV-2, respectively (Fig. 10; Supplementary Table 5). Likewise, the SVM model showed 82% and 76% with &lt; 28% OE and CE, kappa of 0.73 and 0.64 for MicaSense and WV-2, respectively (Supplementary Table 6). In both ML models, combined (Bands + VIs + PCA) features were identified as best performers compared to other features (Fig. 10).</p>
        <p>PS versus S2 based ML model :</p>
        <p>The models (RF and SVM) trained with the three classes (banana, pasture, and tree) showed low accuracy in the PS, S2 sensors. The results of confusion matrix between the ground-truth and the corresponding classification results of ML model are shown in Supplementary Tables 5 and6. However, the models trained with the banana classes and nonbanana (using PS and S2) showed similar good results compared to MicaSense and WV-2 models, in which the features extracted from Bands + VIs + PCA were identified as the best to train both models. The RF classifier model exhibited an OA of 88% and 76% with ≤ 25% of OE and CE, kappa of 0.81 and 0.53 for PS and S2, respectively (See Supplementary Table 5). Similarly, the SVM showed an OA of 75% and 69% with ≤ 32% errors, kappa of 0.50 and 0.37 for PS and S2, respectively (Fig. 10; See Supplementary Table 6).</p>
        <p>Fig. 11 shows the visual comparison of classification map results of Muhanhu Kabare district of eastern DR Congo generated using the RF model with different features for sensors (Fig. 11). It's evident that UAV MicaSense and WV-2 derived images showed higher accuracy and more detailed classification map results obtained with combined (B + VIs + PCA) features (Fig. 11). Also, we derived banana classification maps of two other regions (Inera and Kas) of Kabare district in addition to Mahanhu using RF model with best (B + VIs + PCA) features (Fig. 12). Banana classification maps of three regions of Kabare district using the best studied RF model are depicted in Fig. 12.</p>
        <p>To recognize the robust model to classify the major banana diseases (BXW, BBTD, we tested two models (VGG and Custom) and healthy plants (individual and banana clusters / mats). Reviewing the learning curves (Figs. 13 and 14) of the models during training, a continuous improvement was observed, showing that the developed models may benefit from further training epochs. However, the VGG model showed more variation in the loss function of the data validation set (Fig. 13), and the same was also noticed in the accuracy trend (Fig. 14). Our analysis revealed that the custom model was the best with higher accuracy (0.92) than VGG (0.85); therefore, the custom model can be the right candidate for banana disease classification using UAV-RGB images (Fig. 14). Our results on the trained models suggested that both dropout and data augmentation have positive effects on disease classification. The further combinations showed that our trained model has good control over the rate of training without overfitting, confirming that further improvements such as regularization, aggressive dropouts in later layers, the addition of weight decay could help improve the model. So far, we have not tuned the hyperparameters of the training algorithm, such as the learning rate, perhaps the most essential factor to achieve more accuracy.</p>
        <p>The confusion matrix (CM) of the custom model revealed that the classification model developed from this study exhibited more than 90% accuracy (Fig. 15) in all the classes studied. High classification accuracy was observed in the disease classes of BXW (92.8%) and BBTD (99.4%).</p>
        <p>In addition to CM, we have also computed the performance of other metrics, such as recall, precision, and F1 score. Recall was found around 0.99, 0.95, 0.92, 0.90, for the BBTD, BXW, healthy plant and cluster/mat classes, respectively. Precision metrics was found 0.98 for the BBTD class, 0.87 for healthy plant cluster/mat, 0.93 for healthy individual plant class, and 0.96 for BXW. The F1-score reported was: 0.92 for the BBTV class, 0.89 for cluster, 0.94 for healthy, and 0.90 for BXW.</p>
        <p>Monitoring the 50 epochs with the loss function shown in (Fig. 16), we found that after 30 epoch, the loss was stable and achieved a minimum value of 0.16 (Fig. 16). Recent literature also reported that training on higher accuracy allowed RetinaNet to detect pneumonia and road scenes analysis around 15 to 100 epochs (Blin et al., 2019;Liu et al., 2019). These results explain that the training model using more epochs will not necessarily decrease the loss significantly.</p>
        <p>We extracted the precision and recall data using 
            <rs type="software">IoU</rs> to pull out the true positives (TPs) and false positives (FPs), and the results indicate that our model can detect 74% of the testing dataset (Table 3). This is an essential task when considering the predictions to the classifier for disease detection. In both the training and testing datasets, FPs are found to be higher than TPs, and this might be due to the reason, that it is nearly impossible to label all the individual and clustered banana in the orthomosaics, that the machine also recognized the non-labelled predictions as datasets and eventually resulted in high FPs.
        </p>
        <p>Banana production worldwide is profoundly affected by many pests and diseases. Rapid and early disease diagnosis are crucial for precision crop management, allowing targeted interventions. Timely and precise diagnosis is a critical initial step in mitigating losses caused by crop diseases. Before performing disease surveillance at a regional or country level, the first step is to detect/locate the target crop remotely with higher accuracy. Once the target crop is classified accurately, then deep learning disease detection models can be applied for disease surveillance.</p>
        <p>In this study, the potential use of different VIs derived from UAV and different resolution satellite imagery (S2, PS, and WV-2) on pixel-based banana classification under mixed-complex landscape as well as the result of each VIs on classification accuracy were explored. For medium resolution sensors (S2), EVI and TGI are the only VIs that differentiate banana and non-banana classes. The VIs of CIG, and RVI-G derived from high (PS, WV-2), and very high resolution (MicaSense) senors are found to be more promising and these VIs use NIR and Green bands to calculate the total chlorophyll content in the leaves (Ahamed et al., 2011). Also, other promising VIs such as ARVI, IPVI, and OSAVI exhibited a high correlation, which might be due to the nature of these VIs having intrinsic atmospheric and soil corrections (Huete et al., 1999). Most of the other VIs also showed an adequate correlation (more than 50%) between MicaSense and WV-2 (Fig. 9). These average correlations are mainly due to the spectral band difference between both sensors, atmospheric correction, spatial, and temporal resolution. It is expected that this correlation could be significantly improved in the future by establishing ground control points (GCPs) in the field to permit overlap between sensors.</p>
        <p>The results of the ML-based banana classification model revealed the maximum overall accuracy (OA) with fewer errors (OE and CE), achieved by RF classifier using scenarios of B + VIs + PCA reached a level of 76% to 97% and kappa ranged between 0.53 and 0.96 based on the resolution of the sensors used (Fig. 10 a; Supplementary Table 5). The SVM classifier achieved lower OA (69% to 82%) and kappa (0.37 to 0.73) using scenarios of B + VIs + PCA but was lower than the RF classifier (Fig. 10b; Supplementary Table 6). RF proved to be a suitable model for multi-temporal crop classification due to its capacity to generate multiple paths with different variables to classify the same class (Zhong et al., 2019). RF model for crop classification with time-series MODIS data attained an accuracy of 88%, which confirms that the RF algorithm is suitable for selecting features and classifying crops when large volumes of data are used (Hao et al., 2015). The PCA trained model showed a better performance than the trained models with raw bands, and this is because the PCA values have no linear correlation between the components (Wold et al., 1987). When all features (bands, VIs, and PCA) were combined, the overall accuracy improved, showing that the combined information gives better features to classify banana and other classes accurately, even with low and medium resolution satellite images (Fig. 10). The best performed RF model with combined features (Bands + VIs + PCA) showed higher accuracy was used to map banana under mixed-complex African landscapes (Fig. Simple Ratio (SR) VIs showed differentiable features and have been proven to be high potential to achieve a useful classification (Chuvieco, 1991). Crop classification from Sentinel-2-derived VIs using ensemble learning and RF model exhibited more than 90% OA (Sonobe et al., 2018). In this study, S2-based banana mapping does not show high accuracy (76% OA), which is mainly due to small size of the data points and mixedcomplex setting of African landscapes and. SinceAfrican landscapes have mostly small-scale banana farms and often mixed with other classes (e.g., buildings, pastures, trees), the use of low and medium resolution satellite images to classify banana is highly challenging. The lowresolution pixels combined with sparse banana plantations would provide only one spectral signature from different classes resulting in similar VIs responses and bad classification models. The high-resolution sensor-based RF models (UAVs, WV-2 and PS) are found to be more promising and accurate to map banana in mixed African landscapes.</p>
        <p>Merging very high-resolution data (UAVs &amp; WV-2) with open-source medium resolution satellite information to increment the effectiveness in classification and decision-making seems to be the way forward (Johansen et al., 2020). Improving the spatial resolution of satellites could help replace skill and regulation oriented UAV technologies and monitoring of larger areas with high spatial and temporal resolution (Mayes et al., 2016) in developing countries. Moreover, the use of UAVs to monitor larger landscapes are challenging, so satellite-based ML models could help to classify banana and UAVs and smartphone-based sensors can be utilized to detect more definite problems, such as the disease and health status.</p>
        <p>The robust object detection (banana) and custom disease classification mixed model (Fig. 5) developed from this study demonstrate the capability of low-cost UAV-RGB images, and deep learning approaches of disease surveillance tool in Africa. To detect diseases accurately in mixed-complex African landscapes, it is necessary to detect banana plants irrespective of is phenological stages (young to mature) in an individual or cluster banana planting distribution. As showed in Fig. 5, our RetinaNet object detection model accurately detected banana plants regardless of their age and size automatically through orthomosaics, then the developed custom model classified the diseases (BXW and BBTD) using the detector (Fig. 5; Fig. 17). To monitor larger landscapes with good accuracy, we identified the images range from 50 to 100 m height to be sufficient and the orthomosaics are well georeferenced to classify the diseased plants in the mixed landscape without any problems. However, training a RetinaNet model needs intensive labeling to annotate huge orthomosaics, which is time consuming and it took around one month for 50 epochs. Currently, we are in the process of training the developed RetinaNet model with more epochs to improve the model performance.</p>
        <p>To check the robustness of the developed custom model, metrics such as loss function, accuracy, and confusion matrix were computed . Even though CM of the custom classification model exhibited more than 90% accuracy (Fig. 15) in all classes studied, the major confusion was noticed between healthy individual banana, banana clusters, and BXW (Fig. 15). This is because the big individual healthy banana and small healthy banana clusters in the mixed landscape looked similar and the model confused little bit (5.6%) and healthy individual banana classes are confused with BXW classes (4.6%) since normal yellow streaks confused with BXW symptoms, but the percentage of confusion was very low. These inter-class confusions can be avoided by collecting and training more distinct features of those classes. As expected, the confusion between BBTD and BXW was very low since these two symptoms are very contrasting with each other (Fig. 4). However, the major challenge of this developed RGB based custom model is to differentiate BXW and closely related wilt such as fusarium wilt. Since the UAV imagery obtained by the RGB sensor has only visual bands, the differences in spectral characteristics between healthy and diseased plants cannot be explained well. So, multi-and hyperspectral data should be further investigated to study the sensitivity of certain bands of BXW. Besides, changes in the spectral characteristic between BXW, Fusarium wilt, and other physiological yellowing phenomena caused by different abiotic stresses such as heat, drought, and nutrition disorders should also be scrutinized. Even though this study discussed different laborious DL approaches, the mixed model framework developed in this study have a simple front-end such a way the user can upload orthomosaics and receive the classified result with a bounding labels for the major banana diseases (Fig. 17).</p>
        <p>The use of artificial intelligence (AI) to predict crop health and related environmental impacts are developing rapidly. Here we propose a system-level AI-based banana disease alert system (Fig. 18) by integrating high &amp; low-resolution aerial (UAV and satellite) imagery coupled with advanced computer vision algorithms (Smart phone based AI apps). Additional ground-truth data can be achieved through expert field observations and the use of validated AI-powered disease symptom assessment apps (e.g., the Tumaini app). The validated Tumaini AIpowered mobile app (
            <rs type="url">https://play.google.com/store/apps/details?id = ciat.cgiar.org.tumaini&amp;hl</rs> = es_419) can also serve as the groundtruth for the BXW and BBTD classification model since these diseases could be easily detected (more than 90% accuracy) under real field conditions (Selvaraj et al., 2019). Moreover, GPS tagged mobile app images will also help to reconfirm RF-based banana classification models developed from this study (Fig. 18). It is obvious that a combination of robust AI detection models with medium to high spatiotemporal aerial image data, and our validated AI-powered mobile app (
            <rs type="software">Tumaini</rs>), together with local weather data, can lead to a robust early warning alert systems across banana production landscapes in Africa (Fig. 18). Outputs and models developed from this study can also be integrated into the banana mapper (http://www.crop-mapper. org/banana/index.html) and PestDisPlace (https://pestdisplace.org/) platforms developed by the Alliance of Bioversity and 
            <rs type="software">CIAT</rs> to enhance banana mapping and diseases surveillance at a global scale (Fig. 18b).
        </p>
        <p>Our pixel-based banana classification from random forest (RF) model using combined features of vegetation indices (VIs) along with principal component analysis (PCA) showed promising option to map banana under mixed-complex African landscapes. The high resolution sensors (UAVs, PS and WV-2) used in this study were found to be more accurate to map banana than medium resolution satellite images (S2). Accurate banana mapping using open-source medium resolution satellites (S2) under mixed complex system is still challenging, but integrating informations from different (medium and high resolution) sensor sources is way forward. Through UAV-based RGB imagery systems, we were able to detect banana and their major diseases with higher accuracy and less errors.The low-cost UAV-RGB based mixed-model disease classification pipeline developed from this study is feasible and could be further extended to other crop diseases. This mixed-model pipeline based banana disease classification system developed from this study can be further strengthened by training the CNN model using a larger dataset with multiple crops/diseases, which will be our impending work. The work is underway to collect more ground data points and ground-truth measurements from our global banana partners across Latin America, Africa, and India to enhance existing datasets and validate the developed ML algorithms. The output of this research paper are being integrated into other banana disease surveillance platforms of the CGIAR Research Program on Roots, Tubers and Bananas (RTB) to enhance the digital disease monitoring system at a global scale.</p>
        <p>The authors would like to thank the Alliance of Bioversity International and International Center for Tropical Agriculture (CIAT) Information Technology unit for providing facilities and logistics support. The authors would like to thank Joe Tohme, from the International Center for Tropical Agriculture (CIAT) for his support in this research. The authors would also like to acknowledge Milton Valencia, Manuel Valderrama, Jorge Casas, and Maria Montoya for their help in image annotation. Thanks to Nancy Safari, Jules Ntamwira and Jean-Pierre Mafuta of the Bioversity Bakavu,DR Congo office Innocent Nduwimana of Bioversity Burundi, and Deo Kantungeko of IITA, Burundi for their immense support to collect smartphone images. Thanks to Maxar globe Inc. for providing high resolution images through the CGIAR-Bigdata collaboration. We would also like to thank Dr. Sindhuja Sankaran of Washington State University for providing PS satellite imagery, as well as Angela Fernando, CIAT consultant for formatting and technical editing of the MS. Bioversity International provided funding for field UAV image collection in the framework of the RTB-CC3.1. and RTB-BA3.3, RTB-BA 3.4 clusters; Bioversity International and crop nutrition and health research area, CIAT to carry out the image processing and Machine learning model development work (AGBIO1). We thank the RTB Program Management Unit that supported this study and the CGIAR Fund Donors who support RTB (www.cgiar.org/who-we-are/ cgiar-fund/fund-donors-2).</p>
        <p>The authors declared that there is no conflict of interest.</p>
        <p>Supplementary data to this article can be found online at https://doi. org/10.1016/j.isprsjprs.2020.08.025.</p>
    </text>
</tei>

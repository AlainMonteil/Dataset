<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:11+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Species distribution models (SDMs) encompass a variety of methods used to predict the occurrence of a species from the environmental conditions at a given site, thus providing a proxy of habitat suitability (Warren &amp; Seifert, 2011). These methods are increasingly used in various fields of ecology (Guisan &amp; Thuiller, 2005), often with the aim of guiding decision-making in species conservation management and planning (Guisan et al., 2013). Indeed, SDMs represent a crucial and cost-effective tool to identify current important areas for threatened species, and to forecast ecosystem impacts of rapid human-induced environmental change (</p>
        <p>availability of high computational power, and due to their ability to fit complex nonlinear relationships without requiring an a priori definition of a data model (Breiman, 2001). However, there still are many decisions to be made at various steps of the model building process that can influence the final output (Guisan &amp; Thuiller, 2005). For example, the amount of complexity should be cautiously controlled to avoid models that underfit or overfit the underlying data (Merow et al., 2014;Warren &amp; Seifert, 2011).</p>
        <p>In general, the amount of complexity of a model depends on the number of chosen predictors and their transformations (Merow et al., 2014). Moreover, each machine learning algorithm has a series of parameters, known as hyperparameters. In contrast to model parameters, which are estimated from the data during model training, hyperparameters have a fixed value that must be defined before model training. Even if most machine learning algorithms have predefined default values, the optimal value of each hyperparameter is unknown, as it is specific to the modeling problem and the dataset.</p>
        <p>However, its choice affects model complexity and/or performance.</p>
        <p>For example, in a neural network, the maximum number of iterations controls the amount of iterations executed by its optimization algorithm. This value does not affect model complexity but if it is too low the algorithm might not converge, thus generating a model with lower performance. On the other hand, increasing the size of the hidden layer increases the number of parameters of the model and consequently its complexity, which in turn might affect its performance. In a Maxent model (Phillips, Anderson, &amp; Schapire, 2006), the amount of regularization controls overfitting by shrinking some parameters toward zero which consequently penalizes model complexity. Although several authors have stressed the importance of inspecting the hyperparameters because default settings did not always yield an optimal performance (Elith et al., 2010;Merow, Smith, &amp; Silander, 2013;Warren &amp; Seifert, 2011;Warren, Wright, Seifert, &amp; Shaffer, 2014), the time-consuming task of comparing models trained with a multitude of possible combinations of hyperparameters' values (e.g., Zeng, Low, &amp; Yeo, 2016) may discourage many researchers from doing so in practice.</p>
        <p>In order to optimize model complexity and performance, both the predictors used to build the model and the values of hyperparameters should be carefully selected which represents a challenge given the often numerous possible options. The new package 
            <rs type="software">SDMtune</rs> described here offers a framework to build and systematically tune SDMs. The package includes utilities that help 
            <rs type="software">R</rs> users (
            <rs type="creator">R Core Team</rs>, 2019) all along the analysis process, from data preparation to graphical representation of the results and reporting. In particular, it contains dedicated functions to perform variable selection and hyperparameter tuning. Hyperparameter tuning, also called hyperparameter optimization, is a process usually based on a trial and error experiment during which several models with different values of the hyperparameters are trained and evaluated in order to identify which combination yields the best performance. The simplest algorithm for hyperparameter tuning, grid search, trains and compares models with all possible combinations of the defined hyperparameters' values and can thus be a very time-consuming process.
        </p>
        <p>While other available 
            <rs type="software">R</rs> packages contain functions for tuning one (e.g., 
            <rs type="software">ENMeval</rs> (Muscarella et al., 2014), 
            <rs type="software">wallace</rs> (Kass et al., 2018)), 
            <rs type="software">kuenm</rs> (Cobos, Townsend Peterson, Barve, &amp; Osorio-Olvera, 2019) or several statistical model types (e.g., biomod2 (Thuiller, Georges, &amp; Breiner, 2019), 
            <rs type="software">sdm</rs> (Naimi &amp; Araújo, 2016), zoon (Golding et al., 2018) and 
            <rs type="software">caret</rs> (Kuhn et al., 2019)), functions for data-driven variable selection are not always included and the hyperparameter tuning is always based on grid search or random search algorithms. 
            <rs type="software">SDMtune</rs> offers an alternative that relies on a genetic algorithm for exploring the hyperparameter configuration space (Lessmann, Stahlbock, &amp; Crone, 2005;Young, Rose, Karnowski, Lim, &amp; Patton, 2015), applicable to the most commonly used SDM algorithms. This method significantly reduces the time required to find a near-optimal or the optimal model configuration. As an additional advantage, all functions for selecting the variables and tuning the hyperparameters are supported by an interactive real-time displayed chart that shows the change in model performance during the different steps of function execution. The chart is created in the 
            <rs type="software">RStudio</rs> (
            <rs type="creator">RStudio Team</rs>, 2018) viewer pane using the open source library 
            <rs type="software">Chart.js</rs> (
            <rs type="url">https://www. chart js.org</rs>), thus facilitating the understanding of the underlying algorithm action through a graphical representation of the output and avoiding the user's feeling of handling a black box that usually comes up when dealing with complex methods.
        </p>
        <p>In this section, we present a possible use of the 
            <rs type="software">SDMtune</rs> package that covers a complete analysis in seven steps (Figure 1 generating an output report. Users can combine the available functions in a way that best suits them. For example, step 4 could be repeated after step 5 to further fine-tune model hyperparameters.
        </p>
        <p>
            <rs type="software">SDMtune</rs> uses a special object to compile the data for the analysis. This object, called SWD (samples with data, a format similar to the one used by the 
            <rs type="software">Maxent</rs> software), bundles all the information re- The meaning of each hyperparameter can be found in the respective package documentation and default values, when available, are provided in the last column. a (l) linear, (q) quadratic, (p) product, and (h) hinge.
        </p>
        <p>Overview of the hyperparameters that can be tuned per statistical method and underlying package is inevitably transferred into the trained model, even if the validation data are not directly used to train the model (Chollet &amp; Allaire, 2018;Müller &amp; Guido, 2016). It is therefore advisable to hold apart an independent partition of the data, that is, the testing dataset, to obtain an unbiased evaluation of the final model (Hastie, Tibshirani, &amp; Friedman, 2009;Merow et al., 2014).</p>
        <p>The selection of a metric and a validation strategy should therefore be done early in the model tuning process, because it has implications on how the data should be split before training the first model. Note that the AICc score is computed using all the observation locations (Warren &amp; Seifert, 2011) and does not require to partition the observation data into training and validation.</p>
        <p>Currently, four machine learning methods are available (Table 1):</p>
        <p>artificial neural networks (ANN), boosted regression trees (BRT), maximum entropy (ME), and random forest (RF). Two different implementations of the ME method can be selected: "Maxent" to use the Java implementation (version 
            <rs type="version">3.4.1</rs> or higher) and "
            <rs type="software">Maxnet</rs>" for the 
            <rs type="software">R</rs> implementation using the 
            <rs type="software">maxnet</rs> package (Phillips, Anderson, Dudík, Schapire, &amp; Blair, 2017;Phillips et al., 2006). There are specific arguments of the train function that can be used to set the model hyperparameters. By default, these arguments are set to the same values as implemented in the dependent packages.
        </p>
        <p>A trained model can be evaluated using one of the three implemented metrics: (1) the area under the receiver operating characteristic (ROC) curve (AUC) (Fielding &amp; Bell, 1997), (2) the true skill statistic (TSS) (Allouche, Tsoar, &amp; Kadmon, 2006), and (3) Akaike's information criterion corrected for small sample sizes (AICc, only for ME method) (Burnham &amp; Anderson, 2004;Warren &amp; Seifert, 2011). It should be noted that AICc is a relative measure describing the model fit in relation to complexity (parsimony) but holds no information on predictive performance. It can thus only be used to compare competing models trained using the same data but not for final model evaluation.</p>
        <p>When the environmental variables used to train the model are highly correlated, it is difficult to interpret the model output, especially the relative importance of the variables and their response curves. A common practice is thus to select a subset of variables among which collinearity falls below a predefined threshold. A reasonable approach to remove highly correlated variables is to base the selection on expert knowledge, that is, retaining the environmental variable that is most likely to be ecologically meaningful for the target species. When this is unknown, an alternative approach is a "data-driven" variable selection that uses the information contained in the data to select the variable with the highest explanatory value among those that are highly corre-</p>
        <p>Tuning the model hyperparameters is a long process, as it requires testing many combinations of the hyperparameters in order to identify the best performing model. The simplest tuning method, known as "grid search," is implemented in the function 
            <rs type="software">gridSearch</rs>. The user has the possibility to define a set of possible values for one or several hyperparameters, out of which the function will create all possible combinations. The function also returns the value of the chosen evaluation metric so that the user can see the effect of varying the hyperparameters on the model performance.
        </p>
        <p>Grid search is based on a brute force method that results in a very time-consuming process with high computational costs. A possible alternative is to randomly select some hyperparameters' combinations among the user-defined values (Bergstra &amp; Bengio, 2012). This approach is implemented in the 
            <rs type="software">randomSearch</rs> function that usually finds a better performing model compared with the starting one. However, the disadvantage of the grid search and random search methods is that they do not use any information acquired during the iteration through the hyperparameter configuration space in order to improve the model performance. The function optimizeModel applies a genetic algorithm (Holland, 1992) instead, to more quickly optimize the combination of the hyperparameters (an example of a genetic algorithm used to define hyperparameters and architecture of a deep neural network is presented by Miikkulainen et al. (2018)). The algorithm (Figure 2) starts by generating a random initial "population" of models (using the 
            <rs type="software">randomSearch</rs> algorithm), with a given "population size". The "fitness" of the population is measured with the chosen evaluation metric computed on the validation dataset and models are ranked accordingly. During the evaluation of the "fitness," underfitting is controlled by ensuring that models for which the evaluation metric computed for the validation dataset is higher than the one computed for the training dataset are ranked in the last positions. At this point starts, the selection process during which some models ("individuals") are selected according to their "fitness" from the initial "population" to create the first "generation." There are two selection criteria. At first, a predefined proportion of the "fittest" models (i.e., models ranked in the first positions) is retained. Afterward, a small portion of the poor performing models (i.e., those not selected as "fittest") is randomly retained in order to keep more variation in the population and reduce the possibility that the algorithm falls in a local optimum. The retained models are then submitted to the optimization process: they are "bred" (i.e., combined) to create other "individuals" and to reach again the predefined "population" size. In this process, two models, called "parents," are randomly selected from the retained models ("selected individuals") to "breed" and generate a "child." This new model will randomly inherit a value for each hyperparameter from one of the "parents," a process called "crossover." A "mutation" chance with a predefined probability is added to increase the variation in the population. When the "mutation" is triggered one of the model's hyperparameter is randomly selected and its value is randomly sampled from those available but not included in the "parents." Once the population reaches the defined size, the "fitness" is calculated again, and the process is repeated for the number of generations specified in the function. The user can set all the arguments: population size, number of generations, fractions of both best and worst performing models to be retained at each generation as well as the probability of mutation during crossover episodes, but default values-that will work in most cases-are also defined. All the functions described in this section come with a real-time chart showing the model performance while the algorithm is running in the background.
        </p>
        <p>As soon as an optimal hyperparameter combination has been selected, we may want to reduce model complexity by removing some environmental variables ranked as less important.</p>
        <p>Flowchart illustrating the steps of the genetic algorithm implemented in the function optimizeModel, with orange ovals representing the begin and the end of the algorithm, blue boxes the main operations executed by the algorithm, and the green hexagons the iteration loops. In gray are provided the default values used by the function, with "size" indicating the initial population size; "keep best" the proportion of best models retained; "keep random" the proportion of less performing models retained; "mutation chance" the probability that a mutation event occurs. Keep best and keep random are provided as proportion of the initial population size. The dotted box shows an example of crossover during which two models, randomly selected from the selected "individuals", are combined to generate a child model that inherits the first and third hyperparameters' values from Model A real-time chart showing the removed variable together with its relative effect on model performance is generated during the execution of the function.</p>
        <p>At this point, after the variable set has been optimized (varSel and reduceVar) and the hyperparameters of the model have been tuned</p>
        <p>(
            <rs type="software">gridSearch</rs>, 
            <rs type="software">randomSearch</rs>, or 
            <rs type="software">optimizeModel</rs>) the model can be evaluated on the held apart testing dataset, which was never used during the tuning procedure, using one of the functions that compute the chosen metric (i.e., AUC or TSS). Another possibility would be to train a new model using the selected variables and hyperparameter combinations with the full dataset (i.e., without applying cross-validation or data partitioning) and evaluate it on the held apart testing dataset (Chollet &amp; Allaire, 2018). This way the model can avail of a greater amount of information and might thus be able to generalize better.
        </p>
        <p>There are several functions for visualizing the model results and predictions. The user can plot the response curves, the variable importance, the ROC curve, project the predicted relative probability of species occurrence to the extent of the environmental variables,</p>
        <p>We evaluated the performance of the genetic algorithm in terms of time-saving and model accuracy for the four SDM-methods available in SDMtune by comparing the output of the 
            <rs type="software">optimizeModel</rs> and 
            <rs type="software">gridSearch</rs> functions. We used the virtualSp dataset provided with the package. This dataset contains simulated presence, absence, and background locations generated with the package 
            <rs type="software">virtualspecies</rs> (Leroy, Meynard, Bellard, &amp; Courchamp, 2016). For artificial neural network, boosted regression trees, and random forest we used the presence and absence datasets, while for the maximum entropy method we used the presence and background datasets.
        </p>
        <p>The maximum entropy method was performed with the "Maxnet"</p>
        <p>implementation. In all cases, a 10-fold cross-validation was used as validation strategy and the AUC was used as evaluation metric. As first step, we trained a model with default hyperparameters' values (for artificial neural network we used an inner layer of a size equal to the number of environmental variables), and then executed the two functions testing 1200 possible hyperparameters' combinations (Table A1, for the optimizeModel function we used default arguments). The results of the analysis are presented in Table 2. In all cases, the optimizeModel functions found a near-optimal solution in a significantly reduced amount of time. Note: Models were trained using the virtualSp dataset available with the package and 1200 possible hyperparameters' combinations. Presence and background locations were used for the Maxnet method, presence and absence locations for the other methods.</p>
        <p>To We randomly split the observations into two partitions and used 80% (1363 observations) as training dataset and the remaining 20%</p>
        <p>(584) as testing dataset. A set of 39 environmental predictors that might be relevant for the species was prepared for the analysis, as using numerous predictors together with a large amount of species observations allows for a better illustration of the advantages and time-saving functionalities provided by our package. The variables included information on topography, climate, geology, anthropogenic infrastructure, land cover, and food availability, referring to Hirzel et al. (2004). All predictors were prepared as raster maps with a resolution of 100 × 100 m, with each cell containing the average value of the respective variable within a 1 km 2 circular moving window (a list of the variables is provided in Appendix A, Table A2). The whole analysis was conducted using 
            <rs type="software">R</rs> version 
            <rs type="version">3.6.0</rs> (
            <rs type="creator">R Core Team</rs>,
        </p>
        <p>We performed the data-driven variable selection using the function varSel on the initial set of 39 predictors. As a first step, we trained a model using the "Maxent" method with default settings (i.e., linear, quadratic, product and hinge as feature class combinations, regularization multiplier equal to 1, 10,000 background locations and 500 iterations) and the 39 environmental variables. We then used the varSel function to execute the variable selection using the percent contribution to rank variable importance and the AUC as performance metric. The function arguments were set to check for Spearman's correlation coefficients |r s | greater than or equal to 0.7, based on 30,000 random background locations (Table A3).</p>
        <p>Starting with the model trained using the 28 selected variables (i.e., the output of the varSel function, Table A4), we conducted a simple experiment to investigate the performance of the optimize-Model compared to the 
            <rs type="software">gridSearch</rs> function in terms of execution time and best hyperparameter combination. We selected the AUC as the performance metric running a fourfold cross-validation. The folds were created by randomly splitting the training dataset with the function randomFolds. For the optimizeModel function, we used the default arguments: a population size of 20 models, five generations, kept 40% of the best performing models, randomly retained 20% of the less performing ones and used a mutation chance of 40%. We tested different sets of hyperparameters (Table A5 and Figure A1), varying the feature class combinations, the regularization multiplier and the number of iterations. The results illustrate how using the optimizeModel function tremendously reduces computation time while providing a near-optimal solution when the number of hyperparameter combinations increases (Table 3). In our experiment, with 1200 possible hyperparameter combinations, the execution time dropped from 21 hr 14 min and 45 s using 
            <rs type="software">gridSearch</rs> to 1 hr 6 min and 58 s using optimizeModel with a similar predictive performance of the resulting models (mean validation AUC across the fourfold of 0.8588 and 0.8550, respectively).
        </p>
        <p>In a next step, we investigated whether the final evaluation of the resulting models provided similar results. For this purpose, we selected the models with the optimized hyperparameters' combination (i.e., the output of the optimizeModel and 
            <rs type="software">gridSearch</rs> functions run with 1200 different hyperparameters' combinations). We used the reduceVar function to test if some variables with low contribution could be removed without affecting the validation AUC. We Note: The models were trained using the Maxent method.
        </p>
        <p>The number of tested hyperparameters' combinations is given by "h". A description of the exact hyperparameters' combinations is provided in Appendix A, Table A5. "FC" represents the feature class combination, "reg" the regularization multiplier and "iter" the number of iterations for the best performing model. a FC: (l) linear, (q) quadratic, (p) product, and (h) and hinge.</p>
        <p>TA B L E 3 Performance of the 
            <rs type="software">gridSearch</rs> compared to the optimizeModel function for model tuning regarding execution time (expressed as HH:MM:SS) and evaluation metric (on the training dataset "Train AUC," the validation dataset "Val AUC" and the difference between both "Diff AUC," given as arithmetic mean of the fourfold cross-validation) on the case example data of the bearded vulture considered the Maxent percent contribution to rank the environmental variables, a threshold of 2% for variable removal and used the Jackknife approach. We could remove nine and seven environmental variables, respectively, without reducing the mean validation AUC (Table A6 and Figure A2).
        </p>
        <p>Finally, we trained a model using the full training dataset without cross-validation, the selected environmental variables and the best hyperparameter configuration found by the two functions. We estimated the performance of these tuned models on the held apart testing dataset, obtaining very similar results (Table 4).</p>
        <p>Most of the algorithms supported by the package have predefined default values for the hyperparameters, while ANN requires the size of the hidden layer to be specified (Table 1). Default values are not necessarily the best choice for any given dataset and modeling problem, and a tuning procedure can improve model performance considerably. For example, the default hyperparameters' values of the Maxent algorithm were derived based on an empirical tuning experiment conducted on 226 species (Phillips &amp; Dudík, 2008), however, several authors found that these values were not always optimal for their specific datasets (Anderson &amp; Gonzalez, 2011;Merow et al., 2013;Radosavljevic &amp; Anderson, 2014;Warren &amp; Seifert, 2011;Warren et al., 2014). While dedicated 
            <rs type="software">R</rs> packages are available for fine-tuning Maxent's hyperparameters, like 
            <rs type="software">ENMeval</rs> (Muscarella et al., 2014), 
            <rs type="software">wallace</rs> (Kass et al., 2018), and 
            <rs type="software">kuenm</rs> (Cobos et al., 2019), this process can be very time consuming (Table 2 and3) and limiting, especially when performed for multiple species. With 
            <rs type="software">SDMtune</rs>, we introduce a genetic algorithm that drastically reduces the computation time of hyperperameter tuning while achieving an optimal or near-optimal model configuration.
        </p>
        <p>While the gridSearch function can be preferred for tuning a single or a few hyperparameters, it quickly comes to its limits when testing In the performance assessment of the genetic algorithm and in the example of application presented here (Table 2 and3), default values worked when testing as much as 1200 predefined hyperparameters' combinations. In case of a similar or higher amount of hyperparameters' combinations, these values might require small adjustments to introduce more variability, for instance by increasing the population size and the probability of mutation.</p>
        <p>With the implementation of the genetic algorithm, we introduced a new way of hyperparameters optimization in the field of SDMs. This way could be extended further by testing different modifications. For example, in our implementation only one model is created during the "crossover" event, but two "sibling" models could be produced instead. Furthermore, other optimization algorithms, like the Bayesian optimization, could be implemented. With our op-timizeModel function, we provide a first implementation of a new algorithm that can be extended in future releases of the package.</p>
        <p>Not only the tuning of hyperparameters, but also the selection of environmental variables for SDMs has gained attention in recent years (Jueterbock, Smolina, Coyer, &amp; Hoarau, 2016;Warren et al., 2014;Zeng et al., 2016). Despite the fact that highly correlated environmental variables are not a problem when the aim of the study is prediction in the same extent of the observed data, reducing collinearity is recommended in order to reduce model complexity and increase the interpretability of the predictors (Dormann et al., 2013;Merow et al., 2013). In addition, although the degree of accepted model complexity varies according to the modeling scope(s) (Halvorsen, 2012;Halvorsen, Mazzoni, Bryn, &amp; Bakkestuen, 2015), it has been pointed out that models might perform best when trained with a reduced number of predictors (Brun et al., 2020;Halvorsen et al., 2015). Even though the selection should be driven by the knowledge of the modeled species, this might be difficult when the user must decide among several a priori ecologically relevant predictors for the species, or if the ecology of the species is poorly known. Cobos et al. (2019),</p>
        <p>with their package 
            <rs type="software">kuenm</rs>, provide a framework that enables tuning several models starting with different sets of environmental
        </p>
        <p>variables. Yet, this process still requires predefining the predictor sets. Warren et al. (2014) described a method where environmental variables are removed in a stepwise approach that accounts for regularization tuning, variable importance, and improvements in the AICc metric. A similar approach has been implemented in the package 
            <rs type="software">MaxentVariableSelection</rs> (Jueterbock, 2015), used by Jueterbock et al. (2016) to model the effect of climate change on the Arctic seaweed (Fucus distichus). In both examples, all predictors with a contribution-score lower than a given threshold and predictors highly correlated with the most important variable
        </p>
        <p>were removed simultaneously at each step. Given that removing a variable affects the contribution-score of the remaining predictors and therefore their resulting rank, our functions for data-driven variable selection remove only one variable at a time. For the same reason, removing highly correlated variables and variables with low contribution is performed by two distinct functions and not combined into the same process, as described in the previous examples. Furthermore, instead of relying merely on a variable's rank of importance for deciding which one to retain, our functions base the selection on a leave-one-out Jackknife test, while controlling the desired performance metric. Note that the varSel function aims at maintaining the value of the selected metric for the training dataset (i.e., removes the variables that decreases least the evaluation metric) while the reduceVar function aims to at least maintain the value of the selected metric for the validation dataset (i.e., removes a variable if the evaluation metric does not drop). The reasons are, first, that highly correlated predictors should be removed before performing any tuning, and second, that optimizing the selected metric for the training dataset allows capturing the information contained in the data, which is especially important if ecological selection criteria are lacking. The over-or underfitting can then be controlled later by fine-tuning the hyperparameters.</p>
        <p>On the other hand, removing variables with low predictive contribution aims to reduce model complexity and increase model generalization, which is why the validation dataset is used.</p>
        <p>There are other 
            <rs type="software">R</rs> packages which include functions for variable selection. 
            <rs type="software">Caret</rs>, for instance, implements several methods based, among others, on simulated annealing, recursive elimination, or a genetic algorithm. Whereas these methods aim at identifying the best subset of the available variables, our implementations address different problems: 
            <rs type="software">varSel</rs> removes variables to reduce collinearity, and 
            <rs type="software">reduceVar</rs> removes variables that contribute least to the model to increase parsimony. The functions for data-driven variable selection can be particularly useful when the fitted model is extrapolated in space or time. In such cases, the currently prevailing correlations among the environmental variables may differ from those observed in the new time periods or geographical areas (Braunisch et al., 2013), causing unexpected predictions (Warren et al., 2014). This risk is reduced with a reduced number of predictors. Moreover, reducing the number of predictors may limit overfitting, and thus result in a model that generalizes better and thus yields more accurate predictions for data not used during training. The selection of a threshold to reduce the number of variables with the function reduceVar is quite arbitrary. If the aim is to remove as many variables as possible while preserving model performance, the threshold could be set to 100 and the Jackknife method must be selected. On the contrary, if the user, based on his expertise, judges a certain variable as ecologically important for the species and wants to retain it in the model, he could define a threshold that is lower than the importance of this variable.
        </p>
        <p>Nevertheless, the functions presented in this article should not be applied blindly. Therefore, SDMtune provides interactive real-time charts to visualize every step of the algorithms with the idea that the user further evaluates the validity of the final output.</p>
        <p>These charts are particularly useful for two reasons. First, because they are updated in real time, they confirm that the running function is working properly and is not frozen at some unknown step. This is especially important for functions that take long to be executed. Second, because they are interactive, different types of information can be provided without overloading a single graph, since extra information is embedded in a tooltip that appears when the user hovers over a specific element of the chart. Interactive real-time charts are well known and used in other fields that represent the state-of-the-art of machine learning, and available in few 
            <rs type="software">R</rs> packages such as 
            <rs type="software">keras</rs> (Allaire &amp; Chollet, 2020) which allows the user to build complex deep learning models.
        </p>
        <p>The new 
            <rs type="software">R</rs> package 
            <rs type="software">SDMtune</rs> enables data-driven variable selection and hyperparameters tuning within a unified and user-friendly framework. The core functions provide interactive real-time charts that represent the effect of each step of the implemented algorithms in relation to the model performance and allow a deeper understanding of the automated processes. The new functions we present in this paper (i.e., genetic algorithm for hyperparameter tuning and automated variable selection) are implemented in a framework that also integrates functions already available in other packages. This unification, combining all required functions in a single package, offers the advantage for the user to learn a unique framework instead of jumping from one package to the other, each time having to adapt data structures. Currently, 
            <rs type="software">SDMtune</rs> supports three evaluation metrics (i.e., AUC, TSS, and AICc) and four modeling methods (i.e., ANN, BRT, RF, and ME) and more can be easily added in future releases.
        </p>
        <p>Despite providing comprehensive descriptions and visual illustration of the functions, we still stress that users should be familiar with their data and the selected algorithm used to train their model.</p>
        <p>Particular attention should be paid to preparing the data before modeling. SDMtune also offers functions to prepare the data, but it is upon the user's knowledge and expertise to decide upon the most appropriate way to partition and filter the dataset, accounting for sample size and possible sampling biases, or which metric is best to evaluate the model in relation to the modeling objectives.</p>
        <p>In this respect Araújo et al. (2019) defined best-practice standards for SDMs stressing the importance of evaluating models with a temporally or spatially independent dataset (Araújo et al., 2019: Supplement S2.4B). For this reason, 
            <rs type="software">SDMtune</rs> supports functions well developed in other packages (
            <rs type="software">blockCV</rs> and 
            <rs type="software">ENMeval</rs>) to produce such data partitions. These best-practices have recently gained importance and have been integrated in the ODMAP standard protocol (Zurell et al., 2020) that provides a workflow for reproducible and good quality analyses.
        </p>
        <p>The package documentation provides a more complete description of all the available functions, and the articles hosted on the package webpage (https://consb iol-unibe rn.github.io/SDMtu ne/)</p>
        <p>describe meaningful examples of application in various fields of ecological research. These examples are also included in the package and accessible through the vignettes.</p>
        <p>The package 
            <rs type="software">SDMtune</rs> is available in the CRAN repository at 
            <rs type="url">https:// CRAN.R-proje ct.org/packa ge</rs>=
            <rs type="url">SDMtune</rs> and can be installed in 
            <rs type="software">R</rs> with the 
            <rs type="software">command</rs> install.packages("
            <rs type="software">SDMtune</rs>"). The package is under development and the source code is hosted in 
            <rs type="software">GitHub</rs> (
            <rs type="url">https:// github.com/ConsB iol-unibe rn/SDMtune</rs>). We encourage future users to provide feedback and report bugs by opening an issue on the GitHub platform.
        </p>
        <p>We Note: The model parsimony optimization was performed based on the output of the optimizeModel and 
            <rs type="software">gridSearch</rs> functions respectively, executed to tune 1200 possible combinations of hyperparameters.
        </p>
        <p>Hyperparameter values used during the hyperparameter tuning experiment with h: number of tested hyperparameter combinations, FC: feature class combinations with linear (l), quadratic (q), product (p) and hinge (h) feature classes, reg: regularization multiplier and iter: number of iterations h FC reg iter 75 c("lq", "lh", "lqp", "lqh", "lqph") seq(0.2, 3, 0.2) 500 150 c("lq", "lp", "lh", "lqp", "lqh", "lqph") seq(0.2, 5, 0.2) 500 300 c("lq", "lp", "lh", "lqp", "lqh", "lqph") seq(0.1, 5, 0.1) 500 600 c("lq", "lp", "lh", "lqp", "lqh", "lqph") seq(0.1, 5, 0.1) c (500, 700) 1200 c("lq", "lp", "lh", "lqp", "lqh", "lqph") seq(0.1, 5, 0.1) seq (300,900,200) Note: The values are provided using the R code to generate them. In the optimizeModel function, in order to have consistent results, we set the seed argument to 186,546 (a randomly generated number).</p>
        <p>TA B L E 2</p>
        <p>TA B L E A 2</p>
        <p>c Geo Maps:https://shop.swiss topo.admin.ch/de/produ cts/maps/geolo gy/GK500. d Distribution of ibex colonies:https://www.bafu.admin.ch/bafu/de/home/theme n/biodi versi taet/zusta nd/karten.html. e Centre suisse de cartographie de la faune (CSCF):http://www.cscf.ch/cscf/de/home.html. f Federal Administration for Statistic Switzerland (BsF):https://www.bfs.admin.ch/bfs/de/home/stati stiken.html. g Federal Institute for Forest, Snow and Landscape Research WSL; available upon request: www.wsl.ch. h Topographic position index according to Wilson (1984). i Swiss Wind Atlas (Bundesamt für Energie BFE, 2016).</p>
        <p>Samy Harshallanos; Alfons und Mathilde Suter-Caduff Stiftung; Beat und Dieter Jutzler Stiftung; WWF Switzerland; Ernst Göhner Stiftung; Swiss Federal Office for the Environment; Swiss Federal Office for Energy; Stiftung Temperatio; Stiftung Dreiklang für ökologische Forschung und Bildung; UniBern Forschungsstiftung; Steffen Gysel Stiftung für Natur und Vogelschutz; Parrotia Stiftung; Sophie und Karl Binding Stiftung; Margarethe und Rudolf Gsell-Stiftung</p>
        <p>formal analysis (equal); funding acquisition (supporting); methodology (equal); software (supporting); supervision (lead); writing-original draft (lead).</p>
        <p>The Bearded vulture locations and the environmental variables used in the case example have not been archived because part of the data cannot be made publicly available due to data property rights and conservation vulnerability of the species. However, the analysis steps illustrated in the case example could also be reproduced following the articles published on the SDMtune website (https://consb iol-unibe rn.github.io/SDMtu ne/) and using the virtualSp dataset provided with the package. The code necessary to reproduce the performance assessment of the genetic algorithm is provided in Appendix A.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T08:24+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Airborne laser scanning (ALS) is a remote sensing technology known for its applicability in natural resources management. By quantifying the three-dimensional structure of vegetation and underlying terrain using laser technology, ALS has been used extensively for enhancing geospatial knowledge in the fields of forestry and ecology. Structural descriptions of vegetation provide a means of estimating a range of ecologically pertinent attributes, such as height, volume, and above-ground biomass. The efficient processing of large, often technically complex datasets requires dedicated algorithms and software. The continued promise of ALS as a tool for improving ecological understanding is often dependent on user-created tools, methods, and approaches. Due to the proliferation of ALS among academic, governmental, and private-sector communities, paired with requirements to address a growing demand for open and accessible data, the ALS community is recognising the importance of free and open-source software (FOSS) and the importance of user-defined workflows. Herein, we describe the philosophy behind the development of the 
            <rs type="software">lidR</rs> package. Implemented in the 
            <rs type="software">R</rs> environment with a C/C++ backend, 
            <rs type="software">lidR</rs> is free, open-source and cross-platform software created to enable simple and creative processing workflows for forestry and ecology communities using ALS data. We review current algorithms used by the research community, and in doing so raise awareness of current successes and challenges associated with parameterisation and common implementation approaches. Through a detailed description of the package, we address the key considerations and the design philosophy that enables users to implement user-defined tools. We also discuss algorithm choices that make the package representative of the 'state-of-the-art' and we highlight some internal limitations through examples of processing time discrepancies. We conclude that the development of applications like 
            <rs type="software">lidR</rs> are of fundamental importance for developing transparent, flexible and open ALS tools to ensure not only reproducible workflows, but also to offer researchers the creative space required for the progress and development of the discipline. the targets is made possible using precise geolocation information from Global Navigation Satellite Systems (GNSS), and orientation data from an Inertial Measurement Unit (IMU). When combined, the positional accuracy of the derived ALS data sets provide sub-meter measurement accuracy of surfaces, providing best-available data describing the threedimensional structure of landscapes. As a result, ALS has been widely used for the generation of bare Earth terrain models (Furze et al., 2017), which have wide applicability in land surveying, hydrological studies and urban planning (e.g. Chen et al., 2009;Yu et al., 2010). In addition, high positional accuracy of returned point clouds and the ability of laser
        </p>
        <p>Airborne laser scanning (ALS), also known as LiDAR (Light Detection and Ranging) technology has revolutionised data acquisition and resource quantification in natural sciences and engineering. ALS is an active remote sensing technology, that uses laser pulses to measure the time, and intensity, of backscatter from three-dimensional targets on the Earth's surface (Wulder et al., 2008). Retrieval of the position of pulses to pass through small openings in forest canopies facilitate the direct measurement of a number of key forest attributes such as tree height or canopy cover. Laser energy returned to the sensor can be recorded as either a series of discrete xyz locations (the most common data storage format), or a fully digitised return waveform (Mallet and Bretar, 2009). To date, the majority of ALS providers globally provide data in discrete return format largely due to established processing streams. Development of methods for processing full waveform data are ongoing and not yet considered to be conventional practice.Airborne laser scanning (ALS), also known as LiDAR (Light Detection and Ranging) technology has revolutionised data acquisition and resource quantification in natural sciences and engineering. ALS is an active remote sensing technology, that uses laser pulses to measure the time, and intensity, of backscatter from three-dimensional targets on the Earth's surface (Wulder et al., 2008). Retrieval of the position of pulses to pass through small openings in forest canopies facilitate the direct measurement of a number of key forest attributes such as tree height or canopy cover. Laser energy returned to the sensor can be recorded as either a series of discrete xyz locations (the most common data storage format), or a fully digitised return waveform (Mallet and Bretar, 2009). To date, the majority of ALS providers globally provide data in discrete return format largely due to established processing streams. Development of methods for processing full waveform data are ongoing and not yet considered to be conventional practice.</p>
        <p>In forestry, two well-established methods have been developed for deriving forest attributes: individual tree segmentation (ITS) and the area-based approach (ABA). ITS allow both individual tree tops to be located, and tree crowns delineated (Hyyppä and Inkinen, 1999;Jakubowski et al., 2013), followed by derivation of individual tree attributes within each delineated crown. This method depends on the accuracy of tree identification and can be prone to errors that result from over-or under-estimation of tree crown dimensions (White et al., 2016). In ABA, attributes are estimated for pre-established grid-cells based on metrics that summarise the distribution of the point cloud within each cell (Naesset and Økland, 2002;White et al., 2013). The grid cell is therefore a fundamental unit of the ABA, offering more spatial detail when compared to a traditional polygon-based inventory. For example, attributes that have been successfully modelled using ALS data over various forest types globally include canopy height, canopy cover, stand basal area, biomass, and volume estimates Maltamo et al. (2014); White et al. (2016). In addition to these two methods to measure characteristics of the forest resource, ALS can be used for a wide variety of usages including mapping water bodies (e.g. Morsy, 2017;Demir et al., 2019), forest roads mapping (e.g. Ferraz et al., 2016;Prendes et al., 2019), fire fuel hazard (Price and Gordon, 2016) or wildlife habitat assessment (e.g. Graf et al., 2009;Martinuzzi et al., 2009).In forestry, two well-established methods have been developed for deriving forest attributes: individual tree segmentation (ITS) and the area-based approach (ABA). ITS allow both individual tree tops to be located, and tree crowns delineated (Hyyppä and Inkinen, 1999;Jakubowski et al., 2013), followed by derivation of individual tree attributes within each delineated crown. This method depends on the accuracy of tree identification and can be prone to errors that result from over-or under-estimation of tree crown dimensions (White et al., 2016). In ABA, attributes are estimated for pre-established grid-cells based on metrics that summarise the distribution of the point cloud within each cell (Naesset and Økland, 2002;White et al., 2013). The grid cell is therefore a fundamental unit of the ABA, offering more spatial detail when compared to a traditional polygon-based inventory. For example, attributes that have been successfully modelled using ALS data over various forest types globally include canopy height, canopy cover, stand basal area, biomass, and volume estimates Maltamo et al. (2014); White et al. (2016). In addition to these two methods to measure characteristics of the forest resource, ALS can be used for a wide variety of usages including mapping water bodies (e.g. Morsy, 2017;Demir et al., 2019), forest roads mapping (e.g. Ferraz et al., 2016;Prendes et al., 2019), fire fuel hazard (Price and Gordon, 2016) or wildlife habitat assessment (e.g. Graf et al., 2009;Martinuzzi et al., 2009).</p>
        <p>The widespread interest and adoption of ALS-based technologies in the forestry arena over the past decade have generated a need for visualisation and processing software and scripts. Forest inventory data are often stored within geodatabases linked to aerial photographic interpretation (API) information represented by polygonal topology. As a result, many forest industry professionals and managers are accustomed to a vector-based analysis framework for data manipulation and analysis. The advent of ALS-based datasets in the early 2000s facilitated the need for software solutions to display and analyse three dimensional point cloud data. Initially, software solutions were developed in-house, by university research groups or data providers, with more professional solutions following as the user market grew. Built from the photogrammetry disciplines, Terrascan (Soininen, 2016) was one of the first surveying-based software platforms to be able to input and process ALS-derived 3D point clouds. The program specialised in the classification of point clouds into ground and non-ground returns, allowing generation of terrain models, a fundamental feature of the software. One of the first forestry-specific ALS data analysis platforms was FU
            <rs type="software">-SION</rs> (McGaughey, 2015), originally developed by the 
            <rs type="creator">University of Washington and</rs> the US 
            <rs type="creator">Forest Service (</rs>USFS) and released in the early 2000s. The software, pioneering at the time, was one of the first packages specifically designed for the forestry research community, and was one of the first platforms where 
            <rs type="software">ITS</rs> was available. Its capabilities include extraction of 3D point clouds over forest inventory plots, point cloud visualisation, and calculation of plot and landscape-level metrics. 
            <rs type="software">FUSION</rs> also allows integrated large-scale batch processing of ALS datasets, allowing for simple computation of wall-to-wall metrics, which at the time was cumbersome and could not be easily completed in commercial GIS or image processing environments without extensive pre-processing.
        </p>
        <p>As the user-base of ALS forestry applications grew, so did the requirements for software solutions. A number of commercial companies started to develop ALS-specific software. Today, many GIS and image processing tools have graphical user interfaces and the ability to process ALS data with add-ons available for 
            <rs type="software">ESRI</rs> software, 
            <rs type="software">Whitebox</rs>, and image processing stand-alone software such as 
            <rs type="software">ENVI</rs>.
        </p>
        <p>A number of open source tools exist that allow users to capitalise on the complexity of ALS datasets. 
            <rs type="software">CloudCompare</rs> is an example of a commonly used open source software suite that allows ALS data to be analysed, manipulated, and merged. Among commercial software packages, 
            <rs type="software">LAStools</rs> is also commonly used for various large-scale ALS processing tasks. 
            <rs type="software">LAStools</rs> has been specifically designed for processing ALS data, from plots to large landscapes, and is free to use on small datasets, while commercial solutions unlock advanced capabilities for in-depth processing and broad area implementation. A number of modules available within 
            <rs type="software">LAStools</rs> are open source, including the compressed LAZ format that reduces storage size of original uncompressed files (Isenburg, 2013). Table 1 shows a selected set of active free and open source software currently capable of processing 
            <rs type="software">LiDAR</rs> point clouds and that include useful tools for forestry and ecology applications.
        </p>
        <p>The current software landscape for ALS data processing, in particular for forestry applications, contains a mix of solutions. In some cases, tools are open-source and cross-platform (e.g. PDAL). In other cases, software are free to use, but the source code itself is not publicly available and use is restricted to the 
            <rs type="creator">Microsoft</rs>
            <rs type="software">Windows</rs> platform (e.g A set of open source software tools for processing laser scanning data (i.e., 
            <rs type="software">LiDAR</rs>), including data captured from airborne and terrestrial platforms. Bunting et al. (2011Bunting et al. ( , 2013) ) FUSION, 
            <rs type="software">LAStools</rs>). Some solutions propose free and open-source components on multiple platforms, but also contain closed source functionalities (e.g. 
            <rs type="software">LAStools</rs>). There are also a number of closedsource, single-platform commercial solutions requiring a paid subscription or licence.
        </p>
        <p>Common to all closed source solutions is the use of specific algorithms or workflows for ALS that are essentially black boxes to the users. For example, it is not possible to know what is used internally to segment ground points in software like 
            <rs type="software">TerraScan</rs>. While it can be guessed that it is based on the Progressive TIN Densification (PTD) (Axelsson, 2000), this is not documented nor verifiable. Similarly, the 
            <rs type="software">LAStools</rs> documentation states that "a variation of the PTD" is used without providing further details of the variant. If a closed-source classification routine misclassifies points, it is impossible to glean from the documentation why this happened, and consequently difficult to tune the corresponding settings or improve the method. This example of ground classification also applies to other routines and algorithms, including tree segmentation, point cloud registration, point cloud classification, etc.
        </p>
        <p>Recently, however, the remote sensing community has shown a growing interest in the open-source and open-data philosophy. The availability of free and open-source software (FOSS) is of fundamental importance because it gives direct insight into the processing methods. In addition to its educational value, such insight allows users to obtain a better understanding of the consequences of their parametrisation. This is particularly important in remote sensing, a field in which complex algorithms are often strongly affected by implementation details. FOSS also serves as a catalyst for innovation as it avoids the need to duplicate programming efforts, and thus facilitates community-led development. Large FOSS projects usually assemble communities of users and programmers that can report, fix or enhance the software in a highly interactive and dynamic way. Finally, FOSS are generally free-to-pay and cross-platform, both of which are very important, especially in the academic context.Recently, however, the remote sensing community has shown a growing interest in the open-source and open-data philosophy. The availability of free and open-source software (FOSS) is of fundamental importance because it gives direct insight into the processing methods. In addition to its educational value, such insight allows users to obtain a better understanding of the consequences of their parametrisation. This is particularly important in remote sensing, a field in which complex algorithms are often strongly affected by implementation details. FOSS also serves as a catalyst for innovation as it avoids the need to duplicate programming efforts, and thus facilitates community-led development. Large FOSS projects usually assemble communities of users and programmers that can report, fix or enhance the software in a highly interactive and dynamic way. Finally, FOSS are generally free-to-pay and cross-platform, both of which are very important, especially in the academic context.</p>
        <p>One example of the rapidly growing adoption of the FOSS philosophy is the increasing use of the 
            <rs type="software">R</rs> environment (
            <rs type="creator">R Core Team</rs>, 2019) for research in natural sciences (Carrasco et al., 2019;Crespo-Peremarch et al., 2018;Mulverhill et al., 2018;Tompalski et al., 2019b). 
            <rs type="software">R</rs> is a FOSS and cross platform environment for statistical computing and data visualisation supported by the 
            <rs type="software">R</rs>
            <rs type="creator">Foundation for Statistical Computing</rs>. A key feature of 
            <rs type="software">R</rs> is the ability for users to create package extensions that link to the base R architecture, and to its data storage and manipulation model. Based on this wide uptake of 
            <rs type="software">R</rs>, along with its FOSS philosophy, we describe an open source and cross-platform 
            <rs type="software">R</rs> library titled 
            <rs type="software">lidR</rs> (Roussel and Auty, 2020), created not only to perform many of the tasks commonly required to analyse ALS data in forestry, but also to provide a platform sufficiently flexible for users to design original processes that may not necessarily exist in any other software.
        </p>
        <p>The lidR package has been rapidly adopted and is now widely used, especially in academic research. It is widely cited internationally (see section 6) and there are now several training courses designed to teach the package that have been created independently of the lidR development team. Consequently, this paper was written to clarify the goals and the design of the package for a broad audience in environmental sciences, and to inform the community of our development choices. While the package is comprehensively documented, we have never explicitly set out our motivations underpinning its development because this kind of information does not belong in the user manual. lidR is part of a large suite of available tools that all bear their own strengths and weaknesses. Learning new tools can be laborious, so we believe it worthwhile to clarify what lidR is designed for, to help potential users and developers decide if it will meet their analysis needs upstream of the learning curve. This paper also serves as a general review of the various steps available to the practitioner for processing ALS-based point clouds, with an emphasis on the forestry and ecology contexts. However, our intent was not to provide detailed descriptions of the algorithms hosted in lidR that can be used to perform these steps because these are already documented in detail in peer-reviewed papers, which are all referenced in the text.The lidR package has been rapidly adopted and is now widely used, especially in academic research. It is widely cited internationally (see section 6) and there are now several training courses designed to teach the package that have been created independently of the lidR development team. Consequently, this paper was written to clarify the goals and the design of the package for a broad audience in environmental sciences, and to inform the community of our development choices. While the package is comprehensively documented, we have never explicitly set out our motivations underpinning its development because this kind of information does not belong in the user manual. lidR is part of a large suite of available tools that all bear their own strengths and weaknesses. Learning new tools can be laborious, so we believe it worthwhile to clarify what lidR is designed for, to help potential users and developers decide if it will meet their analysis needs upstream of the learning curve. This paper also serves as a general review of the various steps available to the practitioner for processing ALS-based point clouds, with an emphasis on the forestry and ecology contexts. However, our intent was not to provide detailed descriptions of the algorithms hosted in lidR that can be used to perform these steps because these are already documented in detail in peer-reviewed papers, which are all referenced in the text.</p>
        <p>In this manuscript we focus on four components of the lidR approach. In the first part (section 2), we focus on the architecture of the package. In the second part (section 3), we focus on key processing algorithms for data acquired from discrete return ALS systems, given their prevalence in the ALS market today, to demonstrate the efforts we have made in the 
            <rs type="software">lidR</rs> package to provide a wide range of tools from the peer-reviewed literature. We follow a likely conventional ALS workflow involving ground classification and terrain interpolation, height normalisation, construction of digital canopy models, and extraction of ALS metrics to allow ABA model development. In the third part (section 4) we highlight the versatility of the 
            <rs type="software">lidR</rs> package in its fundamental formulation, which allows for a highly flexible programming environment to implement less common, or more innovative processing approaches. We provide some specific examples on how this flexibility can be used and leveraged. We conclude (section 5) with general comments on the processing speed and future functionalities of the package.
        </p>
        <p>As 
            <rs type="software">lidR</rs> is constantly evolving, it is important to state that this manuscript reflects the state of the package in its 
            <rs type="version">3.0</rs> version. While some information might become outdated, such as the supported formats (section 2.2), the benchmarks (section 5), or the currently implemented algorithms, the overall intentions and design choices will remain valid.
        </p>
        <p>
            <rs type="software">lidR</rs> is a point-cloud oriented 
            <rs type="software">R</rs> package designed to be integrated into the 
            <rs type="software">R</rs> spatial analysis ecosystem by supporting inputs/outputs in formats defined by the packages 
            <rs type="software">sp</rs> (Pebesma and Bivand, 2005;Bivand et al., 2013), 
            <rs type="software">sf</rs> (Pebesma, 2018) and 
            <rs type="software">raster</rs> (Hijmans, 2019). It is designed with the FOSS philosophy in mind: the source code is open and freely modifiable, and the development is both driven by feature requests from users and open to third-party modifications/additions, thus allowing the package to constantly evolve.
        </p>
        <p>The 
            <rs type="software">lidR</rs> package was created to provide a variety of customisable processing strategies with the goal of enabling users to manipulate and analyse their data easily. In addition to a set of internally-defined functions, 
            <rs type="software">lidR</rs> offers the possibility to implement user-defined functions, so that processing can be specialised and tailored to meet individual management and research needs. The package was developed to enable users to try, test and explore methods in a straightforward manner. In other words, it is not only designed as a toolbox, but also as a suite of tools that users can use to build new tools.
        </p>
        <p>To achieve this objective, tools available within 
            <rs type="software">lidR</rs> can be classified into three categories: (1) generic processes used in forestry, such as ABA processing (section 3.5) or digital terrain modelling (section 3.2), (2) specific algorithms implemented from the peer-reviewed literature, such as snag detection (section 3.7) or tree segmentation methods (section 3.6), and (3) versatile tools providing a way to design new processing methods, such as a new classification routine, a new intensity normalisation routine, or new predictive models (section 4).
        </p>
        <p>The architecture behind 
            <rs type="software">lidR</rs> relies on two alternative ways to process data. The first (classical) way involves reading a file and loading the point cloud in memory to enable subsequent processing through the application of 
            <rs type="software">R</rs> functions. This allows the application of built-in functions from the package itself, such as digital terrain/surface model generation, but also any user-defined functions. This first option is designed to develop and test either existing or user-defined methods on simple test cases. Once this is done, the second option is designed to apply a working routine on a broader area involving too many files to be loaded at once into memory. It consist of creating a link to a directory where a collection of files is located. Using what we call the "
            <rs type="software">LAScatalog</rs> processing engine" (section 3), any internal or user-defined function can also be applied over a broad geographic area. The engine allows users to load successive regions of interest (ROI) buffered on-thefly. This option is akin to batch processing, and the internal engine embeds all the complexity of on-the-fly buffering, parallelism, mapping, merging and error handling to help users focus on the development of their methods rather than being hindered by computing issues. With this architecture, the 
            <rs type="software">lidR</rs> package offers a straightforward approach to designing innovative processing workflows and scaling them up over broad landscapes.
        </p>
        <p>Discrete return ALS sensors record three coordinates per point as well as several types of supplementary data for each point. Reading, writing and storing these ALS data is a critical preliminary step before any subsequent analysis. ALS data is most commonly distributed in the LAS open format, which is specifically designed to store LiDAR data that is standardised, and officially and publicly documented and maintained by the American Society for Photogrammetry &amp; Remote Sensing (ASPRS, 2018). LAS format enables point cloud data to be stored using an optimised amount of memory but without being compressed. The requirement to store and share ALS data provided impetus for improved data compression (Pradhan et al., 2005;Mongus and Žalik, 2011). Since there is currently no official standard to compress point cloud data files, several different schemes were developed over the last decade to compress LAS files, such as '
            <rs type="software">LizardTech</rs>
            <rs type="software">LiDAR compressor</rs>' (
            <rs type="creator">LizardTech</rs>), '
            <rs type="software">LAScompression</rs>' (Gemma lab) or '
            <rs type="software">zlas</rs>' (ESRI), many of which are proprietary, resulting in difficulties sharing the data between users or platforms. However, since the development of the 
            <rs type="software">LASzip</rs> library (Isenburg, 2013), the LAZ format has become the de facto standard, because it is free and open-source and can be implemented and supported by any software.
        </p>
        <p>As a consequence of the open nature of the LAS and LAZ formats, their widespread use in the community and their adequacy with processing requirements, the 
            <rs type="software">lidR</rs> package is designed to process LAS and LAZ files both as input and output, taking advantage of the 
            <rs type="software">LASlib</rs> and 
            <rs type="software">LASzip</rs> C++ libraries via the 
            <rs type="software">rlas</rs> package (Roussel and De Boissieu, 2019). Due to the underlying input / output drivers, 
            <rs type="software">lidR</rs> also supports LAX, or spatial indexing, files (Isenburg, 2012) to increase access speeds when undertaking spatial queries (see section 5). Alternative open formats do exist, such as PLY, HDF5 or E57, but their use is either illsuited for large ALS coverages or they have not been widely adopted, in the forestry community at least, and are thus not yet supported by 
            <rs type="software">lidR</rs>.
        </p>
        <p>Fig. 1 shows a selected set of outputs and displays that can be generated by the 
            <rs type="software">lidR</rs> package. Some were produced using built-in, ready-to-use functions and can be easily reproduced from the usermanual examples while others were derived from user-defined functions designed on top of the existing versatile tools provided in the package.
        </p>
        <p>ALS data processing relies on recurring steps that are systematically applied to a dataset. These steps include ground classification, digital terrain generation, digital surface model generation, height normalistion of the point cloud and, in the case of forestry applications, an ABA and/or ITS analysis. While, the 
            <rs type="software">lidR</rs> package was designed primarily to explore processing beyond these steps (section 4), it embeds the required tools to apply these routines. All tools dedicated to such common processing steps are derived from the peer-reviewed literature. As far as possible, 
            <rs type="software">lidR</rs> aims to provide a wide range of processing options implemented from original papers or published methods, with a view to promoting reproducible science.
        </p>
        <p>Strictly speaking, 
            <rs type="software">lidR</rs> contains source code to process point clouds, which are decoupled from any environmental context. However, it is important to recognise that the available methods are not necessarily applicable in all environmental contexts. For example, some ground classification algorithms (section 3.1) may or may not be suitable for all terrain types. Similarly, for individual tree segmentation (section 3.6), we rely on the users to study each available method, ideally from the original peer-reviewed publication, and determine if the context for which the methods were developed applies to their own situation. Users are then provided with the possibility to adjust the parameters of the chosen algorithms to optimise their performance for a specific context. Providing recommendations on the environmental contexts in which algorithms may perform best is beyond the scope of the package, as it would require multiple global studies that could not realistically be conducted by a single research team. Instead, our ambition in developing the package is precisely to facilitate the work of a community of research teams who will conduct such types of studies. As discussed in section 6, there is evidence in the peer-reviewed literature that the 
            <rs type="software">lidR</rs> package is already being used for such purposes. Here, we first describe a common processing workflow applicable to any forested environment.
        </p>
        <p>The classification of ground returns from an ALS point cloud is not only the first step towards generating a ground surface (Evans et al., 2009;Zhao et al., 2016), but is also one of the most critical steps of the workflow (Montealegre et al., 2015a;Zhao et al., 2016). Historically, the classification of ground returns was fundamentally important because ALS was initially used for land topography purposes, before being recognised as a potentially valuable tool for measuring the characteristics of the vegetation (Nelson, 2013). Most published algorithms for ground return classification utilise morphological and spatial filters (Zhang et al., 2003;Kampa and Slatton, 2004) or slope analysis (Vosselman, 2000) to assess the likelihood of a subset of returns belonging to the ground surface. The most commonly used approach is probably the Progressive TIN Densification (PTD) (Axelsson, 2000), which is based on triangular irregular networks (TIN), but many others have been proposed (e.g. Kraus and Pfeifer, 1998;Vosselman, 2000;Kampa and Slatton, 2004;Zhang and Whitman, 2005;Evans and Hudak, 2007;Pirotti et al., 2013;Montealegre et al., 2015b;Zhang et al., 2016). The two following routines are currently implemented within lidR: (a) progressive morphological filter (PMF) and (b) cloth simulation filter (CSF).The classification of ground returns from an ALS point cloud is not only the first step towards generating a ground surface (Evans et al., 2009;Zhao et al., 2016), but is also one of the most critical steps of the workflow (Montealegre et al., 2015a;Zhao et al., 2016). Historically, the classification of ground returns was fundamentally important because ALS was initially used for land topography purposes, before being recognised as a potentially valuable tool for measuring the characteristics of the vegetation (Nelson, 2013). Most published algorithms for ground return classification utilise morphological and spatial filters (Zhang et al., 2003;Kampa and Slatton, 2004) or slope analysis (Vosselman, 2000) to assess the likelihood of a subset of returns belonging to the ground surface. The most commonly used approach is probably the Progressive TIN Densification (PTD) (Axelsson, 2000), which is based on triangular irregular networks (TIN), but many others have been proposed (e.g. Kraus and Pfeifer, 1998;Vosselman, 2000;Kampa and Slatton, 2004;Zhang and Whitman, 2005;Evans and Hudak, 2007;Pirotti et al., 2013;Montealegre et al., 2015b;Zhang et al., 2016). The two following routines are currently implemented within lidR: (a) progressive morphological filter (PMF) and (b) cloth simulation filter (CSF).</p>
        <p>The progressive morphological filter (PMF) described by Zhang et al. (2003) is based on the generation of a raster surface from the point cloud. This initial raster surface undergoes a series of morphological opening operations until stability is reached. The PMF has been implemented in a number of software packages including the 
            <rs type="software">Point Cloud Library (PCL</rs>) Rusu and Cousins (2011) and 
            <rs type="software">Point Data Abstraction Library (PDAL)</rs> PDAL Contributors (2018), two well known C++ open-source libraries for point cloud manipulation. It is also recommended in the 
            <rs type="software">SPDlib</rs> software Bunting et al. (2011Bunting et al. ( , 2013) ) and proposed in the 
            <rs type="software">Laser Information System (LIS)</rs> software (
            <rs type="creator">Laserdata</rs>, 2017). The implementation of the PMF in 
            <rs type="software">lidR</rs> differs slightly from the original description because 
            <rs type="software">lidR</rs> is a point cloud processing software. Similarly to the implementation of PCL and PDAL, the morphological operations in 
            <rs type="software">lidR</rs> are conducted on the point cloud instead of a raster. In addition, because 
            <rs type="software">lidR</rs> is designed for high versatility, the package does not constrain the input parameters with the relationship defined in the original paper, so users are free to explore other possibilities.
        </p>
        <p>The Cloth Simulation Filtering (CSF) ground return selection method consists of a simulated cloth with a given mass that is dropped on the inverted point cloud (Zhang et al., 2016). Ground points are classified by analysing the interactions between the nodes of the cloth and the inverted surface. The CSF provided in the 
            <rs type="software">lidR</rs> package was built by wrapping the original C++ source code provided by the authors through the 
            <rs type="software">RCSF</rs> package (Roussel and Qi, 2018) and, thanks to the open-source nature of the original method, is thus an exact version of the original paper.
        </p>
        <p>Once the classification of ground returns is complete, a digital terrain model (DTM) is derived, most commonly represented by an interpolated ground surface at user-defined spatial resolution. Over the past few decades, a wide variety of methods have been developed to generate DTMs with several algorithms proposed for various terrain situations (Chen et al., 2017). DTMs also allow users to normalise the point cloud to manipulate relative elevations instead of absolute elevations (Fig. 2). The derivation of a DTM involves spatial interpolation between ground returns and is a critical step as its errors will impact directly on the computed point heights, and thus on tree height or derived statistic estimation (Hyyppä et al., 2008). Three implementations of interpolation routines to derive the DTM are currently included in 
            <rs type="software">lidR</rs>: (a) triangular irregular network with linear interpolation using a Delaunay triangulation, (b) inverse-distance weighting, and (c) kriging. These three methods are well known spatial interpolation methods with decades of documentation (e.g. Mitas and Mitasova, 1999). While these methods do not bring much novelty, their availability again demonstrates the importance we put on providing several state-of-the-art options to users. Because 
            <rs type="software">lidR</rs> is a constantly evolving package other methods such as bivariate interpolation (Akima, 1978) or Multilevel B-Spline Approximation (Lee et al., 1997) could be added in future releases, or as third-party extensions (the package supports additional plug-ins).
        </p>
        <p>A common third step in ALS data processing is the subtraction of the terrain surface from the remaining ALS returns (Fig. 2). Point cloud normalisation removes the influence of terrain on above-ground measurements, thus simplifying and facilitating analyses over an area of interest. The most common approach to normalise non-ground returns is to subtract the derived raster DTM from all returns. This method has been widely used (e.g. Wang et al., 2008;van Ewijk et al., 2011;Li et al., 2012;Jakubowski et al., 2013;Ruiz et al., 2014;Racine et al., 2014) and is simple and easy to implement. For each point in the dataset, the algorithm selects the value of the corresponding DTM pixel, then subtracts this value from the raw elevation value of each point. The approach, while simple, can lead to inaccuracies in normalised heights due to the discrete nature of the DTM and the fact that the DTM was created and interpolated using regularly spaced points, which do not match the actual location of the ground points in the dataset.A common third step in ALS data processing is the subtraction of the terrain surface from the remaining ALS returns (Fig. 2). Point cloud normalisation removes the influence of terrain on above-ground measurements, thus simplifying and facilitating analyses over an area of interest. The most common approach to normalise non-ground returns is to subtract the derived raster DTM from all returns. This method has been widely used (e.g. Wang et al., 2008;van Ewijk et al., 2011;Li et al., 2012;Jakubowski et al., 2013;Ruiz et al., 2014;Racine et al., 2014) and is simple and easy to implement. For each point in the dataset, the algorithm selects the value of the corresponding DTM pixel, then subtracts this value from the raw elevation value of each point. The approach, while simple, can lead to inaccuracies in normalised heights due to the discrete nature of the DTM and the fact that the DTM was created and interpolated using regularly spaced points, which do not match the actual location of the ground points in the dataset.</p>
        <p>A second normalisation method utilises all returns, with each ground point interpolated to its exact position beneath the non-ground return (García et al., 2010;Khosravipour et al., 2014). This approach therefore removes any inaccuracies attributed to the abstract representation of the terrain itself. Using this method, every ground point used as reference is exactly normalised at 0, which is the expected definition of a ground point that is independent of the quality of the ground segmentation.A second normalisation method utilises all returns, with each ground point interpolated to its exact position beneath the non-ground return (García et al., 2010;Khosravipour et al., 2014). This approach therefore removes any inaccuracies attributed to the abstract representation of the terrain itself. Using this method, every ground point used as reference is exactly normalised at 0, which is the expected definition of a ground point that is independent of the quality of the ground segmentation.</p>
        <p>In lidR, spatial interpolation can be applied for any location of interest to generate either a DTM, or to normalise the point-cloud using an interpolation between each point. While normalising the point-cloud bears several advantages for subsequent analyses, there are also some drawbacks. The normalisation process implies a distortion of the point cloud and, therefore, of the sampled above-ground objects, such as trees and shrubs. Because this can be exacerbated in areas of high slope (Fig. 3), some authors have chosen to work with raw point-cloud to preserve the geometry of tree tops (Vega et al., 2014;Khosravipour et al., 2015;Alexander et al., 2018). In lidR, normalisation is easily reversible by switching absolute and relative height coordinates allowing versatile back and forth representations from raw to normalised point clouds if desired.In lidR, spatial interpolation can be applied for any location of interest to generate either a DTM, or to normalise the point-cloud using an interpolation between each point. While normalising the point-cloud bears several advantages for subsequent analyses, there are also some drawbacks. The normalisation process implies a distortion of the point cloud and, therefore, of the sampled above-ground objects, such as trees and shrubs. Because this can be exacerbated in areas of high slope (Fig. 3), some authors have chosen to work with raw point-cloud to preserve the geometry of tree tops (Vega et al., 2014;Khosravipour et al., 2015;Alexander et al., 2018). In lidR, normalisation is easily reversible by switching absolute and relative height coordinates allowing versatile back and forth representations from raw to normalised point clouds if desired.</p>
        <p>The Canopy Height Model (CHM) is a digital surface fitted to the highest non-ground returns over vegetated areas (Popescu, 2007;Hilker et al., 2010;Ruiz et al., 2014). It can be interpreted as the aboveground equivalent of the Digital Terrain Model (DTM). It differs from the Digital Surface Model (DSM), which is the non-normalised version of the same surface (Zhao et al., 2009;Ruiz et al., 2014). In this paper the term Digital Canopy Model (DCM), following Clark et al. (2004), is used to capture both surfaces. The two main algorithms used to create DCMs can be classified into two families: (a) the point-to-raster algorithms and (b) the triangulation-based algorithms. 
            <rs type="software">lidR</rs> provides both.
        </p>
        <p>Point-to-raster algorithms are conceptually the simplest and consist of establishing a grid at a user-defined resolution and attributing the elevation of the highest point to each pixel. Algorithmic implementations are computationally simple and fast, which could explain why this method has been cited extensively in the literature (e.g. Hyyppä and Inkinen, 1999;Brandtberg et al., 2003;Popescu, 2007;Liang et al., 2007;Véga and Durrieu, 2011;Jing et al., 2012;Yao et al., 2012;Hunter et al., 2013;Huang and Lian, 2015;Niemi and Vauhkonen, 2016;Dalponte and Coomes, 2016;Véga et al., 2016;Roussel et al., 2017;Alexander et al., 2018). This is the default algorithm implemented in 
            <rs type="software">FUSION/LDV</rs>, 
            <rs type="software">LAStools</rs> and 
            <rs type="software">ArcGIS</rs>.
        </p>
        <p>One drawback of the point-to-raster method is that some pixels can be empty if the grid resolution is too fine for the available point density. Some pixels may then fall within a location that does not contain any points (cf. Fig. 4(a)), and as a result the value is not defined. A simple solution to this issue is post-processing to fill any gaps using an interpolation method (cf. Fig. 4(b)) such as linear interpolation (Dalponte and Coomes, 2016) or inverse distance weighting (Véga and Durrieu, 2011;Ruiz et al., 2014;Véga et al., 2016;Niemi and Vauhkonen, 2016). Another option, initially implemented in 
            <rs type="software">LAStools</rs> but seldom applied is to replace each point by a small circle of a known diameter to simulate the fact that laser beams actually have a footprint (Baltsavias, 1999) i.e. Contains empty pixels because of the absence of points in some pixels (the highest point cannot be defined everywhere); (b) Empty pixels are filled by interpolation, but some pits remain; (c) The resolution was increased without empty pixels, but with many pits due to pulses that deeply penetrated the canopy before generating a first return; (d) Pit-free with high resolution.
        </p>
        <p>they are diffuse circular rays. This option has the effect of artificially densifying the point cloud and smoothing the CHM in a way that has a physical meaning and that cannot be reproduced in post processing. All these options are available in lidR to improve the quality of the DCM.they are diffuse circular rays. This option has the effect of artificially densifying the point cloud and smoothing the CHM in a way that has a physical meaning and that cannot be reproduced in post processing. All these options are available in lidR to improve the quality of the DCM.</p>
        <p>Triangulation-based algorithms most commonly use a Delauney triangulation to interpolate first returns (Fig. 4(c)). Use of this method has been reported by Gaveau and Hill (2003); Barnes et al. (2017). Despite being more complex than point-to-raster algorithms, an advantage of the triangulation approach is that it does not output empty pixels, regardless of the resolution of the output raster (i.e. the entire area is interpolated). However, like the point-to-raster method, it can lead to gaps and other noise from abnormally low pixels compared to neighbouring areas. The so-called 'pits' are formed by first returns that penetrated deep into the canopy (Ben-Arie et al., 2009). To solve such issues, Khosravipour et al. (2014) proposed a natively 'pit-free' algorithm, as well as 'a spike-free' algorithm Khosravipour et al. (2016). The 'pit-free' method consists of a series of Delaunay triangulations made sequentially using points with values higher than a set of specified thresholds. All the above mentioned methods and their adjustments (except the 'spike-free' method) are available in lidR (Fig. 4(d)).Triangulation-based algorithms most commonly use a Delauney triangulation to interpolate first returns (Fig. 4(c)). Use of this method has been reported by Gaveau and Hill (2003); Barnes et al. (2017). Despite being more complex than point-to-raster algorithms, an advantage of the triangulation approach is that it does not output empty pixels, regardless of the resolution of the output raster (i.e. the entire area is interpolated). However, like the point-to-raster method, it can lead to gaps and other noise from abnormally low pixels compared to neighbouring areas. The so-called 'pits' are formed by first returns that penetrated deep into the canopy (Ben-Arie et al., 2009). To solve such issues, Khosravipour et al. (2014) proposed a natively 'pit-free' algorithm, as well as 'a spike-free' algorithm Khosravipour et al. (2016). The 'pit-free' method consists of a series of Delaunay triangulations made sequentially using points with values higher than a set of specified thresholds. All the above mentioned methods and their adjustments (except the 'spike-free' method) are available in lidR (Fig. 4(d)).</p>
        <p>Once ground returns have been classified and a DTM developed, the so-called 'area-based approach' is commonly used to link the 3D structure of the point-cloud to forest attributes. Conceptually simple, the ABA involves the computation of metrics that summarise the point cloud structure in a given area of interest, typically a 400 -900 m 2 square or circle, congruent with that of a conventional forest plot (White et al., 2017). In the ABA, the grid cell represents the fundamental unit of measure. Metrics are then used in predictive statistical models to derive key ground-based inventory variables. Predictions from such models can then be mapped over an area of interest.Once ground returns have been classified and a DTM developed, the so-called 'area-based approach' is commonly used to link the 3D structure of the point-cloud to forest attributes. Conceptually simple, the ABA involves the computation of metrics that summarise the point cloud structure in a given area of interest, typically a 400 -900 m 2 square or circle, congruent with that of a conventional forest plot (White et al., 2017). In the ABA, the grid cell represents the fundamental unit of measure. Metrics are then used in predictive statistical models to derive key ground-based inventory variables. Predictions from such models can then be mapped over an area of interest.</p>
        <p>The design of the package allows users to compute a diverse range of metrics including commonly applied ones derived from the vertical elevation of the points, as well as user-specified metrics tailored to particular needs (see section 4). The package does not embed any statistical models from the peer-reviewed literature because they are too specific to individual studies. Instead, 
            <rs type="software">lidR</rs> enables users to compute any metric so that any existing model from the literature can be reproduced. To ensure this is the case and to facilitate their use, some lesser-used metrics from the literature, such as a rumple index (Blanchette et al., 2015), leaf area density (Bouvier et al., 2015) or vertical complexity index (van Ewijk et al., 2011) have been embedded into the package.
        </p>
        <p>An alternative method to the area based approach consists of calculating summaries of the point cloud at the scale of individual trees (Chen et al., 2006;Koch et al., 2006). An accurate segmentation of individual trees to extract a database of tree-level position and attributes such as height, diameter, volume and biomass is a much desired outcome of research on the use of ALS for forestry applications (Hyyppä et al., 2001;Popescu, 2007;Zhang et al., 2009;Kwak et al., 2010;Yao et al., 2012;Gleason and Im, 2012). Individual tree detection algorithms can be generally divided into two types i.e. those based on a digital canopy models and those utilising the point-cloud directly. The body of literature on individual tree segmentation is considerable, and has been the focus of a number of comprehensive reviews and comparisons (Ke and Quackenbush, 2011;Wang et al., 2016;Yancho et al., 2019). Unfortunately, most of the peer-reviewed papers describe methods without any usable source code provided for the benefit of the users and thus cannot be used, compared or validated.An alternative method to the area based approach consists of calculating summaries of the point cloud at the scale of individual trees (Chen et al., 2006;Koch et al., 2006). An accurate segmentation of individual trees to extract a database of tree-level position and attributes such as height, diameter, volume and biomass is a much desired outcome of research on the use of ALS for forestry applications (Hyyppä et al., 2001;Popescu, 2007;Zhang et al., 2009;Kwak et al., 2010;Yao et al., 2012;Gleason and Im, 2012). Individual tree detection algorithms can be generally divided into two types i.e. those based on a digital canopy models and those utilising the point-cloud directly. The body of literature on individual tree segmentation is considerable, and has been the focus of a number of comprehensive reviews and comparisons (Ke and Quackenbush, 2011;Wang et al., 2016;Yancho et al., 2019). Unfortunately, most of the peer-reviewed papers describe methods without any usable source code provided for the benefit of the users and thus cannot be used, compared or validated.</p>
        <p>Following widely used ITS methods, 
            <rs type="software">lidR</rs> provides raster-based watershed methods (relying on the 
            <rs type="software">EBimage package</rs> (Pau et al., 2010)). But to provide easy access to other options from the literature, 
            <rs type="software">lidR</rs> also has implementations of Dalponte's (Dalponte and Coomes, 2016) and Silva's CHM-based algorithms (Silva et al., 2016), as well as Li's point cloud based algorithm (Li et al., 2012). As a complement to the main 
            <rs type="software">lidR</rs> project we are also developing a more experimental package named 
            <rs type="software">lidRplugins</rs> (Roussel, 2019), which implements additional peer-reviewed methods such as 
            <rs type="software">LayerStacking</rs> (Ayrey et al., 2017), Hamraz's algorithm (Hamraz et al., 2016) and the PTree algorithm (Vega et al., 2014) using the plug-in capability of 
            <rs type="software">lidR</rs>. Before being implemented in 
            <rs type="software">lidR</rs>, several of these algorithms were not available beyond their paper-based formulation, so their performance could not be tested by the community.
        </p>
        <p>Beyond the common tools mentioned above, 
            <rs type="software">lidR</rs> also implements a series of additional algorithms with the intent to (a) assemble state-ofthe-art tools, (b) help users conduct reproducible science, and (c) provide a way to take advantage of, test and compare peer-reviewed methods that would otherwise not be available. For example, the developers and community of 
            <rs type="software">lidR</rs> have implemented a snag segmentation tool (Wing et al., 2015), a planar region detection method (Limberger and Oliveira, 2015), a local maximum filter (LMF) (Popescu et al., 2002), and an intensity normalisation tool (Gatziolis, 2013). The implementation of the LMF algorithm, which is used to locate individual trees, can serve as an example of how 
            <rs type="software">lidR</rs> tools provide flexible options to users, even when they are applying a 'standard' workflow. As per the original design from Popescu et al. (2002), the LMF algorithm can be run with a variable window size, and is not limited to some specific, hard-coded options but gives freedom to users for more tailored options. The versatility of the processing options is further developed in the next section. Fig. 5 summarises potential workflows that could be achieved with 
            <rs type="software">lidR</rs>.
        </p>
        <p>In the previous section we have shown how the 
            <rs type="software">lidR</rs> package has the capability to apply conventional ALS workflows using up-to-date implementations of a number of ALS methods presented in the peerreviewed literature. By providing transparent and functional versions of published routines, they immediately become easy to use, thus extending their value beyond their publication-based formulation, which is commonly the only publicly available format.
        </p>
        <p>The true power of 
            <rs type="software">lidR</rs> however lies in its design as a "toolmaker" to provide users with the capability to create their own applications. In fact, 
            <rs type="software">lidR</rs> provides a programming environment intended to be used by R users who wish to test and explore new processing workflows. While it is not possible to show a complete list of functionalities, some examples are provided below to demonstrate the flexibility brought by the package.
        </p>
        <p>As expressed in previous sections, the derivation of metrics from ALS point clouds is inherent to processing ALS data for any forestry or ecology applications. Conventionally, most software solutions allow users to compute predefined and hard coded statistical summary metrics over an even-sized regular grid system. Unlike common software, 
            <rs type="software">lidR</rs> allows the computation of any user-defined metric for a wide variety of regularisations, and/or within user-defined objects. When metrics are computed at the level of individual points, they can serve for classification purposes i.e. to assign them to classes, such as building, vegetation, water, power lines, etc. When computed at the pixel level, metrics feed directly into conventional ABA analyses. When computed at the individual-tree level, the metrics can be used to estimate crown characteristics, and then to classify species. At the plot or stand scales, they can be useful for developing landscape-level statistical predictive models.
        </p>
        <p>The versatile tools from 
            <rs type="software">lidR</rs> enable the extraction of any programmable metrics in two-dimensional pixels and hexagonal cells, three-dimensional voxels, individual trees and individual points. Fig. 6 provides examples of outputs that can be derived from these versatile functions. It is designed to provide the internal tools for fast and efficient mapping while users focus on defining metrics potentially relevant to their applications. For example, a user could define a function that computes a metric of planarity using an eigenvalue decomposition of the 3D coordinates. Such a metric does not exist in any other software, but it may be very useful to some applications (e.g. Figs. 6d,6g,6h). 
            <rs type="software">lidR</rs> then enables mapping of user-defined metrics at all the levels mentioned above i.e. on the whole point cloud (with the function cloud_metrics()), on each pixel (grid_metrics()), on each hexagonal cell (hexbin_metrics()), on each voxel (voxel_metrics()), on each tree, assuming that the tree segmentation has been performed upstream (tree_metrics()) and on each point using its neighbourhood (point_metrics()).
        </p>
        <p>While the output of a given function is always in the same format, its interpretation and usage may have multiple applications depending on the metrics used. For example, in Fig. 6g the function point_metrics() was used to design a roof segmentation algorithm, in Fig. 6h it was used to design a power line classification, and in Fig. 1h it was used to make a water body segmentation algorithm. Not shown in this article, we also successfully used this same point_metrics() function to attribute false colours to a multispectral point cloud and to prototype a noise filtering method.While the output of a given function is always in the same format, its interpretation and usage may have multiple applications depending on the metrics used. For example, in Fig. 6g the function point_metrics() was used to design a roof segmentation algorithm, in Fig. 6h it was used to design a power line classification, and in Fig. 1h it was used to make a water body segmentation algorithm. Not shown in this article, we also successfully used this same point_metrics() function to attribute false colours to a multispectral point cloud and to prototype a noise filtering method.</p>
        <p>
            <rs type="software">lidR</rs> is designed to define, implement, test, explore and utilise new representations of ALS data, which can ultimately lead to the development of new methods, approaches and functions, including new predictive models, or to develop new applications.
        </p>
        <p>ALS data are divided into many smaller files (known as tiles) that together make a contiguous dataset covering an area of interest. The above-mentioned tools work on point-clouds loaded to memory; however, real applications require upscaling of routines to facilitate processing of large datasets that do not fit into memory. A powerful engine has been developed to achieve this in lidR. Once users have designed a feature that works for a small point cloud, lidR offers the capability of applying the function over the entire coverage area using the "
            <rs type="software">LAScatalog</rs> processing engine" (See section 2 and Fig. 7) that offers several features including:
        </p>
        <p>• The capability to iteratively process 'chunks' of any size. The chunks do not need to respect the original tiling pattern of the data acquisition.• The capability to iteratively process 'chunks' of any size. The chunks do not need to respect the original tiling pattern of the data acquisition.</p>
        <p>• The capability to buffer each chunk on-the-fly to ensure a strict wall- to-wall output without any edge artifacts. This is particularly important for terrain computations and tree segmentation, for example (Fig. 7).• The capability to buffer each chunk on-the-fly to ensure a strict wall- to-wall output without any edge artifacts. This is particularly important for terrain computations and tree segmentation, for example (Fig. 7).</p>
        <p>• The capability to compute each chunk sequentially, or in parallel, allows lidR to take advantage of multi-core or multi-machine architectures. • Automatically merging the outputs computed iteratively into a single valid object.• The capability to compute each chunk sequentially, or in parallel, allows lidR to take advantage of multi-core or multi-machine architectures. • Automatically merging the outputs computed iteratively into a single valid object.</p>
        <p>• Record logs and return of partial outputs in case of a crash in the user-defined routine.• Record logs and return of partial outputs in case of a crash in the user-defined routine.</p>
        <p>• Real-time progress estimation monitoring (Fig. 7) that displays the processed, processing and pending areas.• Real-time progress estimation monitoring (Fig. 7) that displays the processed, processing and pending areas.</p>
        <p>• An error-handling manager (Fig. 7) that displays if a chunk pro- duced a warning or an error.• An error-handling manager (Fig. 7) that displays if a chunk pro- duced a warning or an error.</p>
        <p>In brief, the 
            <rs type="software">LAScatalog</rs> processing engine provides all the tools to apply and extend any user-defined routine to an entire acquisition, taking care of all the internal complexity of on-the-fly buffering, parallelism, error handling, and progress estimation, etc. Users have full access to the engine with the catalog_apply() function that is also heavily used internally in almost every function. Combining the versatility of functions, which goes beyond the short summary provided in section 4.2, to the processing engine, a lot of processes that cannot be explored in traditional software can easily be designed by research teams using 
            <rs type="software">R</rs>. A further illustration of this point can be seen in Fig. 1d where an algorithm to retrieve the position of the sensor was first designed and then applied to a broader area using the engine.
        </p>
        <p>In summary, the versatile functions combined with the 
            <rs type="software">LAScatalog</rs> processing engine offer an almost unlimited number of ways in which ALS data can be processed and analysed. This adds to several other tools provided in lidR to either decimate, smooth, filter or crop point clouds, which all contribute to furthering these possibilities the benefit of users.
        </p>
        <p>This paper, like the overall development of lidR, focuses primarily on ALS-based methods. However, there are many other sampling systems used in forestry and ecology, such as terrestrial laser scanning (TLS), digital aerial photogrammetry (DAP) or LiDAR sensors embarked on unmanned aerial vehicles (UAV), which also generate point clouds. One major issue with processing point clouds from these sources is their Fig. 6. Demonstration of the wide variety of products that can be derived using the versatile functions of the 
            <rs type="software">lidR</rs> package. All metrics presented here can be created by a user and mapped on a point cloud. high point density, which requires a much more careful usage of memory. Considering how R is designed internally, there is very little flexibility in memory management, which makes point clouds from these alternative sources more difficult to manage. An example of the ALS-focused design of lidR is the use of an internal spatial index that is optimized for points evenly-spread on x -y axes with proportionally little variation on the z axis; this means that it will often process TLS point clouds sub-optimally. However, it does not mean that lidR cannot be used to process other sources of point clouds. Some examples of successes in these areas include the 
            <rs type="software">TreeLS</rs> package (de Conto, 2019), which is designed for TLS tree segmentation and is built on top of the lidR architecture. The 
            <rs type="software">viewshed3d</rs> package (Lecigne, 2019;Lecigne et al., 2020) also uses lidR and 
            <rs type="software">TreeLS</rs> to process TLS data and address ecological questions. By paying great attention to memory usage using the tools offered by the package, we have also succeeded in integrating lidR into a DAP point cloud workflow. Fig. 8 shows the result of an individual tree detection and measurement performed in a poplar plantation. Other groups have successfully used lidR to process UAV point clouds, such as VanValkenburgh et al. (2020); Navarro et al. (2020), among others. Despite not being designed for these uses, 
            <rs type="software">lidR</rs> offers a versatile point cloud processing framework that enables usage beyond what it was initially designed for, either by using the existing suite of tools or by extending them.
        </p>
        <p>Expansive ALS acquisitions can result in large amounts of data that need complex processing. As a result, ALS processing software needs to be as efficient as possible when reading, writing, and processing 3D point clouds. As discussed previously, the primary goal of 
            <rs type="software">lidR</rs> is to create a straightforward and versatile toolbox within the 
            <rs type="software">R</rs> ecosystem. This choice comes at a cost of memory usage and runtime, when compared to what could be achieved with specialised software. However, a significant part of the 
            <rs type="software">lidR</rs> package 
            <rs type="software">code</rs> that drives the most demanding computations is written in C++, and is natively parallelised at the C++ level whenever possible. Memory allocations are reduced by recycling the 
            <rs type="software">R</rs> allocated memory whenever possible and the reliance on third-party packages is focused on efficient tools, such as the data.table package (Dowle and Srinivasan, 2019).
        </p>
        <p>We have dedicated significant efforts into improving the execution time and memory usage of the 
            <rs type="software">lidR</rs> package. To illustrate some aspects of the package performance, we provide here some benchmarking tests that can serve, at least to some extent, as runtime comparisons with existing software. We benchmarked a selection of tools to demonstrate the computing efficiency of 
            <rs type="software">lidR</rs>. Tests were performed both on an Intel Core i7-5600U CPU @ 2.60 GHz with 12 GB of RAM running Fig. 7. An annotated screenshot of the live view of the 
            <rs type="software">lidR</rs>
            <rs type="software">LAScatalog</rs> processing engine applying a user-defined function on a 300 km 2 collection of LAS files. At the end of the process, the results are stacked to make a strict wall-towall output. GNU/
            <rs type="software">Linux</rs> and a Xeon CPU E5-2620 v4 @ 2.10GHz with 64GB of RAM running 
            <rs type="software">Windows 10</rs>. The C++ 
            <rs type="software">code</rs> of 
            <rs type="software">lidR</rs> as well as other dependency packages were compiled with g++ with level 2 optimisation (-O2), which is the default in 
            <rs type="software">R</rs> packages for 
            <rs type="creator">Microsoft</rs>
            <rs type="software">Windows</rs> and most 
            <rs type="software">GNU</rs>/
            <rs type="software">Linux</rs> distributions. We did not take advantage of the multicore or multi-machine capabilities of the package for the results presented in this paper. To ensure the reproducibility of the benchmarks, we have provided supplementary materials with an extensive set of tests that can be run on other computers to perform time comparisons. These tests include a dataset of 25 1 km 2 tiles at a mean density of 3 points/m 2 , which was used to perform the following benchmarks and some extra multi-core parallelisation examples.
        </p>
        <p>For comparisons with existing 
            <rs type="software">R</rs> packages, we examined how fast the 
            <rs type="software">lidR</rs> package performed a simple rasterisation task compared with the rasterize() function available in the 
            <rs type="software">raster</rs> package (Hijmans, 2019) using a set of 3 million randomly distributed points. We also compared how fast the 
            <rs type="software">lidR</rs> package performed a triangulation task compared to the delaunayn() function available in the geometry (Sterratt et al., 2019) package using the same set of 3 million points. Lastly, using the same data we compared how fast the 
            <rs type="software">lidR</rs> package performed a k-nearest neighbour (knn) search task compared with the 
            <rs type="software">FNN</rs>, 
            <rs type="software">RANN</rs> and 
            <rs type="software">nabor</rs> packages (Beygelzimer et al., 2019;Arya et al., 2019;Elseberg et al., 2012). The choice of these tasks comes from the fact that, (1) they correspond to recurring computational tasks that occur internally in several analyses, (2) they are computationally demanding, and (3) they have comparable equivalents in existing 
            <rs type="software">R</rs> packages. Results are presented in Fig. 9 and show that 
            <rs type="software">lidR</rs> is 2 to 10 times faster than other tested 
            <rs type="software">R</rs> packages. It is the specialisation that has made 
            <rs type="software">lidR</rs> markedly faster than other more generic equivalent tools available in 
            <rs type="software">R</rs>.
        </p>
        <p>However, the versatility of the package comes at a cost and a drawback is often a longer computation times and a greater memory usage. In 
            <rs type="software">lidR</rs>, versatile functions are offered to prototype new tools that can subsequently be implemented in pure C++, if required. For example, in the current version of the package a user can compute a simple DCM using the versatile grid_metrics() function, but 
            <rs type="software">lidR</rs> has a specialized grid_canopy() function that is faster by an order of magnitude (Fig. 10.a). In 
            <rs type="software">lidR</rs>, these 'specialized functions' are functions that could be replaced with other versatile functions of the package, but that were considered to be of sufficiently high interest to be specialized for much faster processing, less memory usage and, whenever possible, native parallelisation at the C++ level. Fig. 10.a shows that the specialized rasterisation tool was 50 times faster than the versatile one, yet the versatile tool remained ∼10 times faster than its equivalent from the raster package, as mentioned above.
        </p>
        <p>Depending on the functions used, the reading of input files is often the primary processing time bottleneck. In comparison to the actual computation, reading LAS or LAZ files is usually slow. To demonstrate this, we measured the run times required to compute common elevation metrics in a classical ABA analysis using grid_metrics() (Fig. 10.b). This summary plot is divided to illustrate two sub-steps: (1) time spent reading the data, and (2) time spent computing metrics. The amount of runtime dedicated to reading the point cloud from files is particularly high with LAZ files which need to be uncompressed on-the-fly. The calculations performed after the data has been loaded are comparatively fast. It is worth recalling that 
            <rs type="software">lidR</rs> is able to utilise LAX files to index point clouds and dramatically speed-up on-the-fly buffering. In this example, the computation time is constant in each trial but the overall computation can be made two to four times faster if file formats are carefully chosen.
        </p>
        <p>Lastly, and to provide context, we compared the processing time required to extract ground inventories, compute an average intensity image and compute and rasterise a Delaunay triangulation with FUSION and 
            <rs type="software">LAStools</rs> (Fig. 11) using the same test bed of 25 tiles mentioned above. This selection of comparisons was driven by the pragmatic need to ensure comparable tasks i.e. operations that are performed with the same methods and return the same outputs using all three software alternatives. In contrast to 
            <rs type="software">lidR</rs>, 
            <rs type="software">LAStools</rs> is less versatile but is designed to process very large amounts of data extremely efficiently (both in terms of speed and memory usage). Given the different focus of 
            <rs type="software">LAStools</rs>, we expected it to perform much faster. Fig. 11 shows the results of these comparisons and it can be seen that 
            <rs type="software">lidR</rs> is either close to or much faster than FUSION, but always slower than 
            <rs type="software">LAStools</rs>; an outcome that was expected.
        </p>
        <p>The goal of this set of benchmarking tests is not to identify the fastest among a set competing software tools. 
            <rs type="software">LAStools</rs> will always be faster than 
            <rs type="software">lidR</rs> by design and one may find tasks that FUSION can perform faster than lidR, and vice versa. Moreover, most tasks can simply not be compared in a direct way, such as ground segmentation or point cloud decimation, as no two software systems use the exact same methods. In addition, the tests presented here are not actually comparable. For example, we don't know if FUSION and 
            <rs type="software">LAStools</rs> perform the computationally-costly tests of data integrity systematically performed in 
            <rs type="software">lidR</rs> to inform users of potential errors in the data, which account for a non-negligible proportion of the computation time. Alternatively, these tests only aim to demonstrate that, despite being R-based, 
            <rs type="software">lidR</rs> performs comparably in many ways to existing software thanks to its C++ backend. Hence, we argue that 
            <rs type="software">lidR</rs> is suitable for both research purposes and operational processing of small (sub-hectare) to medium-sized (thousands of square kilometres) forest management units at least for common tasks where the methods are supported by decades of literature. However, it is important to note that 
            <rs type="software">lidR</rs> also includes some experimental tools from the literature, such as various tree or ground segmentation algorithms that are, where possible, based on the original source code provided by authors, such as the CSF algorithm (see section 3.1). Consequently, in such cases, adhering strictly to the authors' implementation means that maximum efficiency can not be guaranteed. For example, the algorithm for tree segmentation developed and published by Li et al. (2012) has a quadratic complexity meaning that the computation time is multiplied by four when the number of points is doubled. It can therefore not really be performed on point clouds larger than a few hectares.
        </p>
        <p>As of August 1 st 2020 the 
            <rs type="software">lidR</rs> package has been downloaded on average between 2500-3000 times per month. There have been eleven major updates to the package since its release in early 2017 (Fig. 12). Version 
            <rs type="version">2.0.0</rs>, released in early 2019, made significant changes to the package including full integration with the 
            <rs type="software">R</rs> GIS ecosystem and enhanced large-area processing through the 
            <rs type="software">LAScatalog</rs> processing engine. This was the starting point of a broader adoption of 
            <rs type="software">lidR</rs> in the academic community. The package has currently been referenced by more than 100 scientific publications mentioning the use of the 
            <rs type="software">lidR</rs> package for various purposes. Most reported uses relate to regular processing tasks such as DTM, CHM, ABA metrics or ground classification (e.g. Swanson and Weishampel, 2019;Almeida et al., 2019;Mohan et al., 2019;Stovall et al., 2019;Navarro et al., 2020;Cooper et al., 2020), but sometimes beyond the scope of forestry and ecology applications such as in the study of VanValkenburgh et al. (2020) where authors classified ground points in an archaeology context to capture architectural complexity of lost cities in Peru. Several reported uses also relate to gap fraction profile estimation (e.g. Senn et al., 2020). The package is otherwise regularly used for simple file processing, such as ground plot extraction (e.g. Vanbrabant et al., 2020;Mohan et al., 2019). Comparisons of algorithms from the literature included in the package are also starting to become available (Hastings et al., 2020). In addition, 
            <rs type="software">lidR</rs> was used as the supporting architecture for the development of new packages such as 
            <rs type="software">TreeLS</rs> (de Conto, 2019; de Conto et al., 2017) and 
            <rs type="software">viewshed3d</rs> (Lecigne, 2019;Lecigne et al., 2020), two packages dedicated to TLS processing. Overall, reported uses show that the package has been used in a large range of ecological contexts, such as tropical rain forest (Almeida et al., 2019), subtropical forest (Sothe et al., 2019), boreal forest (Tompalski et al., 2019a), savanna (Zimbres et al., 2020) and many others.
        </p>
        <p>The latest stable release of 
            <rs type="software">lidR</rs> can be downloaded from CRAN and is shipped a 150-page comprehensive user-manual that contains hundreds of reproducible examples simply by copy-pasting a few lines of code and can be installed with the command install.packages("
            <rs type="software">lidR</rs>"). The user can reproduce most of the examples shown in this paper using only this user-manual. In addition to the user-manual (which is the only official source of comprehensive and consistent documentation), some extra and often more user-friendly sources of information, usually in the form of tutorials can be found on Internet. The lidR book, which can be found at https://jeanromain.github.io/lidRbook/ is a guide that contains tutorials for both new and advanced users. Users can also find help from the community at https://gis.stackexchange.com/ using the 
            <rs type="software">lidr</rs> tag that currently hosts more than 100 questions.
        </p>
        <p>The package being in constant evolution, new features are already in development, such as support for full waveform data, processing speed improvements, additional methods from the peer-reviewed literature, new methods for mapping water bodies, forest roads segmentation, power line and transmission tower segmentation, and versatile tools for mesh processing. We also aim to provide better support for processing TLS data by implementing a 3D spatial index that is suitable for this type of point cloud.The package being in constant evolution, new features are already in development, such as support for full waveform data, processing speed improvements, additional methods from the peer-reviewed literature, new methods for mapping water bodies, forest roads segmentation, power line and transmission tower segmentation, and versatile tools for mesh processing. We also aim to provide better support for processing TLS data by implementing a 3D spatial index that is suitable for this type of point cloud.</p>
        <p>The use of ALS data for forestry and ecological applications, including the derivation of highly detailed and accurate terrain models, canopy height model development, and forest inventory estimation at both the plot-and individual tree-scale, is well established, with many researchers and managers regarding ALS as a mature and implementable technology that can be applied in an operational context. Considering the rapid integration of the technology globally, ALS provides a key success story of the evolution of a new technology from research and development to production.The use of ALS data for forestry and ecological applications, including the derivation of highly detailed and accurate terrain models, canopy height model development, and forest inventory estimation at both the plot-and individual tree-scale, is well established, with many researchers and managers regarding ALS as a mature and implementable technology that can be applied in an operational context. Considering the rapid integration of the technology globally, ALS provides a key success story of the evolution of a new technology from research and development to production.</p>
        <p>A simple scan of the available software platforms to process ALS data, however, demonstrates that while the technology is mature, the use of validated, repeatable, transparent and readily available tools is not.A simple scan of the available software platforms to process ALS data, however, demonstrates that while the technology is mature, the use of validated, repeatable, transparent and readily available tools is not.</p>
        <p>In this context, the 
            <rs type="software">lidR</rs> package provides a significant set of algorithms implemented from the peer-reviewed research literature. We showed that 
            <rs type="software">lidR</rs> always provides at least two options for any given task to enable users to rely on potential alternatives when one method does not suit a given scenario. Also, 
            <rs type="software">lidR</rs> aims to provide a space where new algorithms can be tested. By making the methods from the literature available within the package, we intend to enable the community to figure out their strengths and weaknesses across various contexts, which may (or not) eventually lead to wider adoption. In other words, 
            <rs type="software">lidR</rs> is designed as a laboratory software.
        </p>
        <p>This design choice makes 
            <rs type="software">lidR</rs> different to other existing software but its open-source and cross-platform nature, its ease of use and its versatility already make it broadly accepted by the research community, as evidenced by the numerous citations of the package in the peerreviewed literature, as well as some newly developed tools based on the lidR architecture not only focused on ALS point clouds but also TLS, UAV and DAP point clouds.
        </p>
        <p>None.None.</p>
        <p>12. Monthly and total downloads of the 
            <rs type="software">lidR</rs> package since its release. Data obtained from the cranlogs package (Csárdi, 2019) in 
            <rs type="software">R</rs>. The red line indicates monthly downloads, while the blue dashed indicates cumulative total downloads.
        </p>
        <p>This software development was partly funded by AWARE (the Assessment of Wood from Remote Sensing NSERC CRDPJ462973-14, grantee Prof. Nicholas UBC), in collaboration with the Canadian Wood Fibre Centre (CWFC), and FPInnovations. Current support for the development of the package is provided by Quebec's Ministry of Forests, Wildlife and Parks, Quebec, Canada.This software development was partly funded by AWARE (the Assessment of Wood from Remote Sensing NSERC CRDPJ462973-14, grantee Prof. Nicholas UBC), in collaboration with the Canadian Wood Fibre Centre (CWFC), and FPInnovations. Current support for the development of the package is provided by Quebec's Ministry of Forests, Wildlife and Parks, Quebec, Canada.</p>
        <p>Supplementary data to this article can be found online at https:// doi.org/10.1016/j.rse.2020.112061.Supplementary data to this article can be found online at https:// doi.org/10.1016/j.rse.2020.112061.</p>
    </text>
</tei>

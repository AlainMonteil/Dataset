<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T12:47+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>A common challenge in developmental research is the amount of incomplete and missing data that occurs from respondents failing to complete tasks or questionnaires, as well as from disengaging from the study (i.e., attrition). This missingness can lead to biases in parameter estimates and, hence, in the interpretation of findings. These biases can be addressed through statistical techniques that adjust for missing data, such as multiple imputation. Although multiple imputation is highly effective, it has not been widely adopted by developmental scientists given barriers such as lack of training or misconceptions about imputationA common challenge in developmental research is the amount of incomplete and missing data that occurs from respondents failing to complete tasks or questionnaires, as well as from disengaging from the study (i.e., attrition). This missingness can lead to biases in parameter estimates and, hence, in the interpretation of findings. These biases can be addressed through statistical techniques that adjust for missing data, such as multiple imputation. Although multiple imputation is highly effective, it has not been widely adopted by developmental scientists given barriers such as lack of training or misconceptions about imputation</p>
        <p>The foundation for this paper was created during a 'hackathon' session occurring on 23 June 2021, at the annual virtual meeting of the Society for Improving Psychological Science. We invited anyone interested in the topic to attend, welcoming both experts and those with little experience addressing missing data in their research, specifically welcoming participation from those who were not sure how to address the missing data they experienced.The foundation for this paper was created during a 'hackathon' session occurring on 23 June 2021, at the annual virtual meeting of the Society for Improving Psychological Science. We invited anyone interested in the topic to attend, welcoming both experts and those with little experience addressing missing data in their research, specifically welcoming participation from those who were not sure how to address the missing data they experienced.</p>
        <p>Decisional guidelines for analyzing the type and extent of missing data were then crowdsourced and curated during this hackathon, resulting in a missing data and multiple imputation decision tree (Woods et al.Decisional guidelines for analyzing the type and extent of missing data were then crowdsourced and curated during this hackathon, resulting in a missing data and multiple imputation decision tree (Woods et al.</p>
        <p>methods. Utilizing default methods within statistical software programs like listwise deletion is common but may introduce additional bias. This manuscript is intended to provide practical guidelines for developmental researchers to follow when examining their data for missingness, making decisions about how to handle that missingness and reporting the extent of missing data biases and specific multiple imputation procedures in publications.methods. Utilizing default methods within statistical software programs like listwise deletion is common but may introduce additional bias. This manuscript is intended to provide practical guidelines for developmental researchers to follow when examining their data for missingness, making decisions about how to handle that missingness and reporting the extent of missing data biases and specific multiple imputation procedures in publications.</p>
        <p>Adequately addressing missing data is a common challenge in the developmental sciences. Multiple imputation is a feasible, credible and powerful approach to handling missing data that helps reduce bias in several scenarios (Enders, 2017). Multiple imputation attempts to minimize the impact of attrition or non-response bias on the analysis by using available information about individuals to adjust the parameter estimates. Using multiple imputation thus approximates what results would look like with complete observations while allowing for representation of uncertainty in the results and maximizing the data set's statistical power (see Box 1 for an overview) (Cheema, 2014;Dong &amp; Peng, 2013).Adequately addressing missing data is a common challenge in the developmental sciences. Multiple imputation is a feasible, credible and powerful approach to handling missing data that helps reduce bias in several scenarios (Enders, 2017). Multiple imputation attempts to minimize the impact of attrition or non-response bias on the analysis by using available information about individuals to adjust the parameter estimates. Using multiple imputation thus approximates what results would look like with complete observations while allowing for representation of uncertainty in the results and maximizing the data set's statistical power (see Box 1 for an overview) (Cheema, 2014;Dong &amp; Peng, 2013).</p>
        <p>Acronym DefinitionAcronym Definition</p>
        <p>Variables that researchers include in the imputation model (but not the analytic model) because they are either correlates of missingness or correlates of an incomplete variable. This helps to account for the missingness of variables directly related to the research question(s) (Collins et al., 2001;Enders, 2010).Variables that researchers include in the imputation model (but not the analytic model) because they are either correlates of missingness or correlates of an incomplete variable. This helps to account for the missingness of variables directly related to the research question(s) (Collins et al., 2001;Enders, 2010).</p>
        <p>Discarding the first N samples, with N being chosen to be large enough that the chain has reached its stationary regime by this time. The default in the 
            <rs type="software">mice</rs> package is 5000 (van Buuren, 2018).
        </p>
        <p>A procedure that removes participants with any missing information from the analysis. Also known as 'listwise deletion' (van Buuren, 2018).A procedure that removes participants with any missing information from the analysis. Also known as 'listwise deletion' (van Buuren, 2018).</p>
        <p>Occurs for a test statistic when the multiple imputations of that test statistic overlap (e.g., they do not diverge or run in parallel) around a consistent value (e.g., they do not tend to increase or decrease; see van Buuren, 2018 for examples). Researchers can diagnose convergence for a test statistic as occurring when the variance between different imputations is no larger than the variance within each individual imputation (van Buuren, 2018).Occurs for a test statistic when the multiple imputations of that test statistic overlap (e.g., they do not diverge or run in parallel) around a consistent value (e.g., they do not tend to increase or decrease; see van Buuren, 2018 for examples). Researchers can diagnose convergence for a test statistic as occurring when the variance between different imputations is no larger than the variance within each individual imputation (van Buuren, 2018).</p>
        <p>Early Childhood Longitudinal Study, Kindergarten Cohort of 2010-2011 (Tourangeau et al., 2015).Early Childhood Longitudinal Study, Kindergarten Cohort of 2010-2011 (Tourangeau et al., 2015).</p>
        <p>Fully conditional specification FCS Another term for 'multiple imputation by chained equations' (MICE; van Buuren, 2018).Fully conditional specification FCS Another term for 'multiple imputation by chained equations' (MICE; van Buuren, 2018).</p>
        <p>Full information maximum likelihood FIML An approach to handling missing data that computes a casewise likelihood function using only those variables that are observed for each case (Enders, 2010). FIML is embedded into the estimation process and can be described as 'implicit imputation', as the technique creates temporary imputations during the estimation process (Widaman, 2006).Full information maximum likelihood FIML An approach to handling missing data that computes a casewise likelihood function using only those variables that are observed for each case (Enders, 2010). FIML is embedded into the estimation process and can be described as 'implicit imputation', as the technique creates temporary imputations during the estimation process (Widaman, 2006).</p>
        <p>Intraclass/intracluster correlation coefficient ICC A statistic that describes the degree of dependence between the observations taken on a specific unit/patient/participant within the same group/cluster. The values range between 0 (i.e., weak within-cluster correlation) and 1 (strong within-cluster correlation) (Katzmarzyk et al., 2022).Intraclass/intracluster correlation coefficient ICC A statistic that describes the degree of dependence between the observations taken on a specific unit/patient/participant within the same group/cluster. The values range between 0 (i.e., weak within-cluster correlation) and 1 (strong within-cluster correlation) (Katzmarzyk et al., 2022).</p>
        <p>Listwise deletion A procedure that removes participants with any missing information from the analysis. Also known as 'complete case analysis ' (van Buuren, 2018).Listwise deletion A procedure that removes participants with any missing information from the analysis. Also known as 'complete case analysis ' (van Buuren, 2018).</p>
        <p>The number of imputed data sets generated in a multiple imputation procedure.The number of imputed data sets generated in a multiple imputation procedure.</p>
        <p>Missing at random MAR Situations when missing data are generated in a systematic manner that can be fully accounted for using information contained within a data set (Bhaskaran &amp; Smeeth, 2014).Missing at random MAR Situations when missing data are generated in a systematic manner that can be fully accounted for using information contained within a data set (Bhaskaran &amp; Smeeth, 2014).</p>
        <p>The number of iterations beyond the burn in iterations used for each imputation in MICE. The plots of the iterations inform if the imputation achieved convergence (Oberman et al., 2021).The number of iterations beyond the burn in iterations used for each imputation in MICE. The plots of the iterations inform if the imputation achieved convergence (Oberman et al., 2021).</p>
        <p>In MBI, one first specifies their intended analytic model. The MBI procedure then creates m multiply imputed data sets that are tailored to this model.In MBI, one first specifies their intended analytic model. The MBI procedure then creates m multiply imputed data sets that are tailored to this model.</p>
        <p>One can analyze the imputed data sets using the specified model or a model that is nested within the specified model (Keller &amp; Enders, 2021).One can analyze the imputed data sets using the specified model or a model that is nested within the specified model (Keller &amp; Enders, 2021).</p>
        <p>The likelihood of any given data point being missing is the same across all data points and unrelated to any other measured or unmeasured variables (Bhaskaran &amp; Smeeth, 2014).The likelihood of any given data point being missing is the same across all data points and unrelated to any other measured or unmeasured variables (Bhaskaran &amp; Smeeth, 2014).</p>
        <p>Multiple Imputation MI Existing data is used to generate multiple (m) data sets of plausible values for missing data that each incorporate random components to reflect the uncertainty of these values. Each data set is then analyzed individually according to a common statistical model, and parameter estimates are pooled into one set of estimates, variances, and confidence intervals (van Buuren, 2018). See Box 1.Multiple Imputation MI Existing data is used to generate multiple (m) data sets of plausible values for missing data that each incorporate random components to reflect the uncertainty of these values. Each data set is then analyzed individually according to a common statistical model, and parameter estimates are pooled into one set of estimates, variances, and confidence intervals (van Buuren, 2018). See Box 1.</p>
        <p>A multi-step process to create each imputed data set. The steps and an example are laid out in Azur et al. (2011); see also van Buuren (2018).A multi-step process to create each imputed data set. The steps and an example are laid out in Azur et al. (2011); see also van Buuren (2018).</p>
        <p>Missing not at random MNAR Situations when missing data occur in a way that we cannot fully account for through measured data (Bhaskaran &amp; Smeeth, 2014).Missing not at random MNAR Situations when missing data occur in a way that we cannot fully account for through measured data (Bhaskaran &amp; Smeeth, 2014).</p>
        <p>Planned missing design A data collection design in which the researcher randomly assigns certain participants to be missing observation occasions or measurements to minimize research costs and participant burden. Because the missing values are MCAR given the random assignment, they can be imputed without bias or auxiliary variables during analysis (Graham et al., 2006;Rhemtulla &amp; Hancock, 2016;Rhemtulla &amp; Little, 2012;Wu &amp; Jia, 2021) Pairwise deletion To only use a participant's information when they offer complete data for a given analysis. This approach is less restrictive than listwise deletion (van Buuren, 2018).Planned missing design A data collection design in which the researcher randomly assigns certain participants to be missing observation occasions or measurements to minimize research costs and participant burden. Because the missing values are MCAR given the random assignment, they can be imputed without bias or auxiliary variables during analysis (Graham et al., 2006;Rhemtulla &amp; Hancock, 2016;Rhemtulla &amp; Little, 2012;Wu &amp; Jia, 2021) Pairwise deletion To only use a participant's information when they offer complete data for a given analysis. This approach is less restrictive than listwise deletion (van Buuren, 2018).</p>
        <p>(Continues)(Continues)</p>
        <p>Pooling See Multiple Imputation (van Buuren, 2018).Pooling See Multiple Imputation (van Buuren, 2018).</p>
        <p>Predictive Mean Matching PMM PMM uses regression models (linear, logistic, or multinomial, depending on the variable) to find the user-specified number of nearest observed cases that most closely resemble the predicted values of the respondents with missing data rather than imputing random values from the conditional distribution. This results in imputed values that are actually observed in the data set and that are more robust to violations of normality than other approaches (i.e., regress, logit and mlogit) (van Ginkel et al., 2020).Predictive Mean Matching PMM PMM uses regression models (linear, logistic, or multinomial, depending on the variable) to find the user-specified number of nearest observed cases that most closely resemble the predicted values of the respondents with missing data rather than imputing random values from the conditional distribution. This results in imputed values that are actually observed in the data set and that are more robust to violations of normality than other approaches (i.e., regress, logit and mlogit) (van Ginkel et al., 2020).</p>
        <p>Seed value An integer that offsets the random number generator in model estimation.Seed value An integer that offsets the random number generator in model estimation.</p>
        <p>Setting a seed value in generating multiple imputations will make the multiple imputation analysis reproducible, assuming the data and other parameters (e.g., iterations, m, auxiliary variables) are the same.Setting a seed value in generating multiple imputations will make the multiple imputation analysis reproducible, assuming the data and other parameters (e.g., iterations, m, auxiliary variables) are the same.</p>
        <p>Yet, despite its benefits, developmental scientists have been slow to adopt multiple imputation. Many scientists perceive barriers to both understanding and implementing multiple imputation including uncertainties about when it is appropriate to use multiple imputation, and concerns that multiple imputation is 'making data up' (Nguyen et al., 2021;Rombach et al., 2018;White et al., 2010). In addition, researchers often find that lower-quality methods for handling missing data are both easy to use and still readily accepted by many developmental scientists. Developmental scientists might be more willing to overcome these barriers if they had good examples of multiple imputation that they could apply to their own work. Unfortunately, few practical examples have been offered using the complex data and analyses commonly encountered in developmental research, such as a multilevel data structure and analysis using multilevel or growth curve models.Yet, despite its benefits, developmental scientists have been slow to adopt multiple imputation. Many scientists perceive barriers to both understanding and implementing multiple imputation including uncertainties about when it is appropriate to use multiple imputation, and concerns that multiple imputation is 'making data up' (Nguyen et al., 2021;Rombach et al., 2018;White et al., 2010). In addition, researchers often find that lower-quality methods for handling missing data are both easy to use and still readily accepted by many developmental scientists. Developmental scientists might be more willing to overcome these barriers if they had good examples of multiple imputation that they could apply to their own work. Unfortunately, few practical examples have been offered using the complex data and analyses commonly encountered in developmental research, such as a multilevel data structure and analysis using multilevel or growth curve models.</p>
        <p>Our aim is to provide a set of decision points to address this gap. We are basing these decision points on prior work detailing best practices for addressing missing data through multiple imputation. Similar to work on best practices in preregistration (van den Akker et al., 2021) and open science (Adelson et al., 2019), we hope this paper demystifies the process of understanding and applying multiple imputation. We provide a practical guide for authors, reviewers and editors, and include recommendations for the information that should be included in peer-reviewed manuscripts and their supplements.Our aim is to provide a set of decision points to address this gap. We are basing these decision points on prior work detailing best practices for addressing missing data through multiple imputation. Similar to work on best practices in preregistration (van den Akker et al., 2021) and open science (Adelson et al., 2019), we hope this paper demystifies the process of understanding and applying multiple imputation. We provide a practical guide for authors, reviewers and editors, and include recommendations for the information that should be included in peer-reviewed manuscripts and their supplements.</p>
        <p>We begin with a brief overview of why developmental scientists should adjust for missingness in quantitative analyses, including discussions of common barriers to adopting best practices for handling missing data, misconceptions of employing multiple imputation, and the implications of failing to adjust for missing data in developmental science. Next, we review the mechanisms that lead to missingness and the multiple imputation model. We conclude with a worked example of missing data analysis and multiple imputation using complex data and analyses to match the kind of work done by developmental scientists. This example uses publicly available data from the Early Childhood Longitudinal Study, Kindergarten Cohort of 2010-2011(ECLS-K: 2011;;Tourangeau et al., 2015). Though this worked example will be particularly helpful for developmental scientists, we hope to persuade all quantitative researchers to consider the implications of missing data and more appropriately adjust for missing data in their research.We begin with a brief overview of why developmental scientists should adjust for missingness in quantitative analyses, including discussions of common barriers to adopting best practices for handling missing data, misconceptions of employing multiple imputation, and the implications of failing to adjust for missing data in developmental science. Next, we review the mechanisms that lead to missingness and the multiple imputation model. We conclude with a worked example of missing data analysis and multiple imputation using complex data and analyses to match the kind of work done by developmental scientists. This example uses publicly available data from the Early Childhood Longitudinal Study, Kindergarten Cohort of 2010-2011(ECLS-K: 2011;;Tourangeau et al., 2015). Though this worked example will be particularly helpful for developmental scientists, we hope to persuade all quantitative researchers to consider the implications of missing data and more appropriately adjust for missing data in their research.</p>
        <p>Missing data have been described as the norm rather than an exception in quantitative research (Dong &amp; Peng, 2013).Missing data have been described as the norm rather than an exception in quantitative research (Dong &amp; Peng, 2013).</p>
        <p>Missingness can occur when a participant disengages with a task before completing enough items or trials for a reliable answer (e.g., skips parts of a questionnaire or stops responding during a task measure), misses measurement occasions (e.g., is not present during a specific assessment session) or withdraws from the study completely (i.e., attrition). These scenarios almost always occur in developmental research, especially in longitudinal studies.Missingness can occur when a participant disengages with a task before completing enough items or trials for a reliable answer (e.g., skips parts of a questionnaire or stops responding during a task measure), misses measurement occasions (e.g., is not present during a specific assessment session) or withdraws from the study completely (i.e., attrition). These scenarios almost always occur in developmental research, especially in longitudinal studies.</p>
        <p>Missing data can negatively affect our ability to draw valid conclusions because it both reduces statistical power and introduces bias to parameter estimates. Yet rather than adjust for this bias, many developmental researchers opt to simply remove participants with missing information from the data set (i.e., listwise deletion, also referred to as complete case analyses) or to only use their information when they offer complete data for a given analysisMissing data can negatively affect our ability to draw valid conclusions because it both reduces statistical power and introduces bias to parameter estimates. Yet rather than adjust for this bias, many developmental researchers opt to simply remove participants with missing information from the data set (i.e., listwise deletion, also referred to as complete case analyses) or to only use their information when they offer complete data for a given analysis</p>
        <p>Following data collection, several strategies may be used to handle missing data. The correct choice depends on the context of the analysis (see Supporting Information: The results of analyses on each m data set will differ, as random components will have led to different values being generated within each data set. Finally, parameter estimates are pooled into a single set of estimates, variances and confidence intervals (Baraldi &amp; Enders, 2010;Enders, 2016;Schafer &amp; Graham, 2002;van Buuren, 2018).Following data collection, several strategies may be used to handle missing data. The correct choice depends on the context of the analysis (see Supporting Information: The results of analyses on each m data set will differ, as random components will have led to different values being generated within each data set. Finally, parameter estimates are pooled into a single set of estimates, variances and confidence intervals (Baraldi &amp; Enders, 2010;Enders, 2016;Schafer &amp; Graham, 2002;van Buuren, 2018).</p>
        <p>The main steps in multiple imputation:The main steps in multiple imputation:</p>
        <p>Note: although single imputation (e.g., mean imputation) methods exist, we do not recommend their use under most circumstances due to resulting bias and reduced generalizability of results.Note: although single imputation (e.g., mean imputation) methods exist, we do not recommend their use under most circumstances due to resulting bias and reduced generalizability of results.</p>
        <p>(i.e., pairwise deletion). These 'easy' deletion methods are often the default setting in common software programs.(i.e., pairwise deletion). These 'easy' deletion methods are often the default setting in common software programs.</p>
        <p>However, these options often increase bias and create inefficient estimation of parameters, confidence intervals, and significance tests (Baraldi &amp; Enders, 2010). When conclusions are drawn from biased statistics, the work that comes after is likely to be biased or fail to replicate previous findings (Lee et al., 2021), and the line of research can ultimately lead to intervention or policy recommendations that are grounded in biased results. Underpowered studies are also often cited as a significant contributing factor to the 'Replication Crisis' in psychology (Button et al., 2013;Nosek et al., 2015). These negative consequences ultimately reduce the validity and reliability of inferences to the population.However, these options often increase bias and create inefficient estimation of parameters, confidence intervals, and significance tests (Baraldi &amp; Enders, 2010). When conclusions are drawn from biased statistics, the work that comes after is likely to be biased or fail to replicate previous findings (Lee et al., 2021), and the line of research can ultimately lead to intervention or policy recommendations that are grounded in biased results. Underpowered studies are also often cited as a significant contributing factor to the 'Replication Crisis' in psychology (Button et al., 2013;Nosek et al., 2015). These negative consequences ultimately reduce the validity and reliability of inferences to the population.</p>
        <p>Below, we discuss why and how barriers have slowed the widespread adoption of missing data practices. We also outline how failing to adjust for missing data has ethical implications that are especially relevant for developmental researchers invested in open science, diversity, equity, inclusion and/or accessibility initiatives.Below, we discuss why and how barriers have slowed the widespread adoption of missing data practices. We also outline how failing to adjust for missing data has ethical implications that are especially relevant for developmental researchers invested in open science, diversity, equity, inclusion and/or accessibility initiatives.</p>
        <p>3 | WHY IS APPROPRIATELY ADDRESSING MISSING DATA IMPORTANT?3 | WHY IS APPROPRIATELY ADDRESSING MISSING DATA IMPORTANT?</p>
        <p>Systemic and individual barriers slow the adoption of evidence-based practices, whether in psychology (Nosek et al., 2015), economics (Delios et al., 2022;Tierney et al., 2020Tierney et al., , 2021)), medicine (Grol &amp; Wensing, 2004) or other disciplines (see Proctor et al., 2009 for an overview). These barriers are no different for quantitative methods (King et al., 2019). Many 'best practices' in statistics are slowly (if ever) adopted. This lag may be accounted for by both individual factors (i.e., lack of access to statistical training or technology) and systematic barriers (i.e., field-wide norms about what data analysis methods are considered acceptable). The practice of transparently and appropriately addressing missing data has achieved widespread methodological support (Appelbaum et al., 2018;Manly &amp; Wells, 2015;Nicholson et al., 2017;Sterne et al., 2009;Sterner, 2011;Vandenbroucke et al., 2007). Yet, repeatedly, reviews have found that progress has been slow in increasing its implementation (Bodner, 2006;Burton &amp; Altman, 2004;Karahalios et al., 2012;Lang &amp; Little, 2018).Systemic and individual barriers slow the adoption of evidence-based practices, whether in psychology (Nosek et al., 2015), economics (Delios et al., 2022;Tierney et al., 2020Tierney et al., , 2021)), medicine (Grol &amp; Wensing, 2004) or other disciplines (see Proctor et al., 2009 for an overview). These barriers are no different for quantitative methods (King et al., 2019). Many 'best practices' in statistics are slowly (if ever) adopted. This lag may be accounted for by both individual factors (i.e., lack of access to statistical training or technology) and systematic barriers (i.e., field-wide norms about what data analysis methods are considered acceptable). The practice of transparently and appropriately addressing missing data has achieved widespread methodological support (Appelbaum et al., 2018;Manly &amp; Wells, 2015;Nicholson et al., 2017;Sterne et al., 2009;Sterner, 2011;Vandenbroucke et al., 2007). Yet, repeatedly, reviews have found that progress has been slow in increasing its implementation (Bodner, 2006;Burton &amp; Altman, 2004;Karahalios et al., 2012;Lang &amp; Little, 2018).</p>
        <p>Beyond the systemic barriers that are common across research fields, developmental scientists have other reasons for being slow to adopt modern missing data practices like multiple imputation. It is difficult to adopt any practice when guidelines and practical demonstrations of that practice have not been tailored to the research of developmental scientists. This gap around addressing missing data likely disproportionately affects early career researchers, especially those from backgrounds that are traditionally underrepresented in science. A well-established or more privileged researcher is more likely to (1) have the resources needed to enroll in a course on handling missing data;Beyond the systemic barriers that are common across research fields, developmental scientists have other reasons for being slow to adopt modern missing data practices like multiple imputation. It is difficult to adopt any practice when guidelines and practical demonstrations of that practice have not been tailored to the research of developmental scientists. This gap around addressing missing data likely disproportionately affects early career researchers, especially those from backgrounds that are traditionally underrepresented in science. A well-established or more privileged researcher is more likely to (1) have the resources needed to enroll in a course on handling missing data;</p>
        <p>(2) hire a statistician to do the work; and/or (3) seek formal or informal mentorship about designing studies to minimize missingness including planned-missing designs, as well as about appropriately addressing missing data once it occurs. Early career researchers and/or those from underrepresented backgrounds are less likely to have access to these resources and would probably benefit the most from a well-tailored guide to handling missing data.(2) hire a statistician to do the work; and/or (3) seek formal or informal mentorship about designing studies to minimize missingness including planned-missing designs, as well as about appropriately addressing missing data once it occurs. Early career researchers and/or those from underrepresented backgrounds are less likely to have access to these resources and would probably benefit the most from a well-tailored guide to handling missing data.</p>
        <p>Developmental psychologists may also be hindered in adopting multiple imputation by several individual-level barriers. These may include (1) a lack of familiarity with or confidence using statistical software;Developmental psychologists may also be hindered in adopting multiple imputation by several individual-level barriers. These may include (1) a lack of familiarity with or confidence using statistical software;</p>
        <p>(2) pressure from colleagues and advisors to submit and publish manuscripts as quickly as possible by using software defaults that match established norms for handling missing data (i.e., listwise deletion methods); (3) worries over whether the decision-making process required by multiple imputation is 'correct'; and (4) added complexity in the data analysis process. On top of these barriers, a number of common misconceptions about multiple imputation further limit its adoption (e.g., multiple imputation is 'making up' data, should not be used for dependent variables, and/or is only appropriate when data are missing at random, which is defined in further detail below; see Table 1 for a full list of common misconceptions). Many of these misconceptions are based on the general idea that using multiple imputation to manage missing data is ethically questionable. We argue the opposite-the ethical risks from failing to properly adjust for missing data far outweigh those raised by multiple imputation.(2) pressure from colleagues and advisors to submit and publish manuscripts as quickly as possible by using software defaults that match established norms for handling missing data (i.e., listwise deletion methods); (3) worries over whether the decision-making process required by multiple imputation is 'correct'; and (4) added complexity in the data analysis process. On top of these barriers, a number of common misconceptions about multiple imputation further limit its adoption (e.g., multiple imputation is 'making up' data, should not be used for dependent variables, and/or is only appropriate when data are missing at random, which is defined in further detail below; see Table 1 for a full list of common misconceptions). Many of these misconceptions are based on the general idea that using multiple imputation to manage missing data is ethically questionable. We argue the opposite-the ethical risks from failing to properly adjust for missing data far outweigh those raised by multiple imputation.</p>
        <p>Properly adjusting for missing data is vital for investigations of diversity, equity, inclusion and accessibility. These investigations aim to counteract the historical and continued oppression of minoritized groups in scientific research (Zuberi, 2001;Zuberi &amp; Bonilla-Silva, 2008) and are crucial to creating a more open science. There are similar implications for clinical trials, interventions and meta-analyses (see review by Rioux &amp; Little, 2021). For example, missing participants might experience more favourable outcomes in the treatment group and poorer outcomes in the control group (or vice versa), which would bias conclusions toward (or away) from the true efficacy of the intervention. It is important to consider and adjust for missing data because this can invalidate the conclusions we draw and, in turn, waste resources and lead to poor policies (Mavridis et al., 2014;Rioux &amp; Little, 2021).Properly adjusting for missing data is vital for investigations of diversity, equity, inclusion and accessibility. These investigations aim to counteract the historical and continued oppression of minoritized groups in scientific research (Zuberi, 2001;Zuberi &amp; Bonilla-Silva, 2008) and are crucial to creating a more open science. There are similar implications for clinical trials, interventions and meta-analyses (see review by Rioux &amp; Little, 2021). For example, missing participants might experience more favourable outcomes in the treatment group and poorer outcomes in the control group (or vice versa), which would bias conclusions toward (or away) from the true efficacy of the intervention. It is important to consider and adjust for missing data because this can invalidate the conclusions we draw and, in turn, waste resources and lead to poor policies (Mavridis et al., 2014;Rioux &amp; Little, 2021).</p>
        <p>Adjusting for missing data through appropriate and replicable methods is also an important step in promoting open science initiatives. Many developmental scientists advocating for open scholarship work to improve openness, integrity, social justice, diversity, equity, inclusivity and accessibility in all areas of their scholarly activities. By extension, they hope to improve both their academic field and the societies they live in (Ledgerwood et al., 2022;Pownall et al., 2021). Streamlining procedures to address missing data and increasing the transparency of those procedures through consensus on reporting standards will advance these goals (Randall et al., 2021). Several outlets, includingAdjusting for missing data through appropriate and replicable methods is also an important step in promoting open science initiatives. Many developmental scientists advocating for open scholarship work to improve openness, integrity, social justice, diversity, equity, inclusivity and accessibility in all areas of their scholarly activities. By extension, they hope to improve both their academic field and the societies they live in (Ledgerwood et al., 2022;Pownall et al., 2021). Streamlining procedures to address missing data and increasing the transparency of those procedures through consensus on reporting standards will advance these goals (Randall et al., 2021). Several outlets, including</p>
        <p>Infant and Child Development, have called for researchers to prioritize similar 'rigorous, transparent, credible and robust' methods in the work they submit for publication (Syed, 2021).Infant and Child Development, have called for researchers to prioritize similar 'rigorous, transparent, credible and robust' methods in the work they submit for publication (Syed, 2021).</p>
        <p>To maximize the contribution of our participants' data, we must plan for handling missing data during the early phases of research designfor example, by designing data collection procedures to minimize missing data. Practically, this means researchers need to collect information on additional (auxiliary) variables that may be related to missing data. This is because structural barriers to participation in research can lead to participants from minoritized groups disproportionately dropping out of longitudinal studies or not completing measures (Randall et al., 2021).To maximize the contribution of our participants' data, we must plan for handling missing data during the early phases of research designfor example, by designing data collection procedures to minimize missing data. Practically, this means researchers need to collect information on additional (auxiliary) variables that may be related to missing data. This is because structural barriers to participation in research can lead to participants from minoritized groups disproportionately dropping out of longitudinal studies or not completing measures (Randall et al., 2021).</p>
        <p>Thus, data from minoritized students may be most likely to be excluded from longitudinal studies investigating academic achievement using pairwise or listwise deletion methods. This selection effect can bias model estimates and confidence intervals, obscuring the inequities in student outcomes and possibly leading to unsubstantiated claims about achieving equity (Rhodes, 2015). Collecting demographic data that is often associated with attrition (e.g., income, education level and occupation) during recruitment or early in the study can help researchers better understand missingness in their data set, even if a participant is lost to follow-up or fails to complete the full trial.Thus, data from minoritized students may be most likely to be excluded from longitudinal studies investigating academic achievement using pairwise or listwise deletion methods. This selection effect can bias model estimates and confidence intervals, obscuring the inequities in student outcomes and possibly leading to unsubstantiated claims about achieving equity (Rhodes, 2015). Collecting demographic data that is often associated with attrition (e.g., income, education level and occupation) during recruitment or early in the study can help researchers better understand missingness in their data set, even if a participant is lost to follow-up or fails to complete the full trial.</p>
        <p>We must also identify ways to address missingness when it occurs. Our participants donate valuable time to us when they participate in our studies. When participants provide valid, albeit partial, data, we should maximize their contributions whenever possible by leveraging the incomplete data. When scientists drop records because of partial missing data by using deletion methods, they nullify the donation of time from their participants.We must also identify ways to address missingness when it occurs. Our participants donate valuable time to us when they participate in our studies. When participants provide valid, albeit partial, data, we should maximize their contributions whenever possible by leveraging the incomplete data. When scientists drop records because of partial missing data by using deletion methods, they nullify the donation of time from their participants.</p>
        <p>These ethical considerations are not an exhaustive list. Additional considerations may need to be weighed when choosing strategies to adjust for missing data (e.g., cultural considerations, protection of data or participants, etc.). In a subsequent section, we address specific ethical considerations in the process of conducting multiple imputation analysis.These ethical considerations are not an exhaustive list. Additional considerations may need to be weighed when choosing strategies to adjust for missing data (e.g., cultural considerations, protection of data or participants, etc.). In a subsequent section, we address specific ethical considerations in the process of conducting multiple imputation analysis.</p>
        <p>Deletion methods for handling missing data are the default option in most software analysis platforms. We argue that there are increasingly limited situations in which deletion methods may be used. Deletion methods exacerbate bias in parameter estimates when some participants are more likely to have missing data than others (e.g., Curran, Bacchi, et al., 1998;Curran, Molenberghs, et al., 1998;Fairclough et al., 1998;Widaman, 2006). The two most common deletion methods are pairwise deletion and listwise deletion. Pairwise deletion is a common practice that excludes missing data on an analysis-by-analysis basis; only complete cases for relevant variables are included (Myers, 2011).Deletion methods for handling missing data are the default option in most software analysis platforms. We argue that there are increasingly limited situations in which deletion methods may be used. Deletion methods exacerbate bias in parameter estimates when some participants are more likely to have missing data than others (e.g., Curran, Bacchi, et al., 1998;Curran, Molenberghs, et al., 1998;Fairclough et al., 1998;Widaman, 2006). The two most common deletion methods are pairwise deletion and listwise deletion. Pairwise deletion is a common practice that excludes missing data on an analysis-by-analysis basis; only complete cases for relevant variables are included (Myers, 2011).</p>
        <p>Entirely excluding participants who have any missing data on at least one of the variables included in the analysis is known as listwise deletion (Myers, 2011). This approach further exacerbates bias as it ignores all information from participants who have any missing data (Altmann &amp; Bland, 2007;Howell, 2007;Kang, 2013). Deletion methods are simple to implement and time efficient, particularly when the loss of statistical power is inconsequential (Kang, 2013;Schafer, 1999). But these deletion methods may be misaligned with the researcher's intentions to make their work as inclusive as possible.Entirely excluding participants who have any missing data on at least one of the variables included in the analysis is known as listwise deletion (Myers, 2011). This approach further exacerbates bias as it ignores all information from participants who have any missing data (Altmann &amp; Bland, 2007;Howell, 2007;Kang, 2013). Deletion methods are simple to implement and time efficient, particularly when the loss of statistical power is inconsequential (Kang, 2013;Schafer, 1999). But these deletion methods may be misaligned with the researcher's intentions to make their work as inclusive as possible.</p>
        <p>Deletion methods are appropriate only in certain limited circumstances because they generally assume that the data are Missing Completely at Random (MCAR; discussed in more detail, below). With MCAR data, and only with MCAR data, deletion methods will not bias inferences. This is because the complete records in an MCAR data set are a random sample drawn from the larger sample of participants. This larger sample includes records with missing data and is, in turn, drawn from the population (Kang, 2013). When researchers conduct analyses using this 'random sample' of complete records, the analyses will not lead to biased parameter estimates, although tests of statistical significance will have decreased power due to the loss of observations. In practice, MCAR data are very rare. This is why we do not recommend deletion methods 1 -because of the resulting loss of statistical power, constraints on the generalizability of the results, and the likelihood that the MCAR assumption is not met. The conclusions researchers draw when they use deletion methods are generalizable only to a population similar to participants with complete data (e.g., those participants who fully complete surveys). The use of deletion methods with data Missing at Random or Missing Not at Random (MAR and MNAR; explained in detail, below) will always introduce selection bias into inferences. This bias undermines the validity of researchers' conclusions by greatly decreasing the probability that researchers will statistically detect true inequalities across groups (Hernán et al., 2004).Deletion methods are appropriate only in certain limited circumstances because they generally assume that the data are Missing Completely at Random (MCAR; discussed in more detail, below). With MCAR data, and only with MCAR data, deletion methods will not bias inferences. This is because the complete records in an MCAR data set are a random sample drawn from the larger sample of participants. This larger sample includes records with missing data and is, in turn, drawn from the population (Kang, 2013). When researchers conduct analyses using this 'random sample' of complete records, the analyses will not lead to biased parameter estimates, although tests of statistical significance will have decreased power due to the loss of observations. In practice, MCAR data are very rare. This is why we do not recommend deletion methods 1 -because of the resulting loss of statistical power, constraints on the generalizability of the results, and the likelihood that the MCAR assumption is not met. The conclusions researchers draw when they use deletion methods are generalizable only to a population similar to participants with complete data (e.g., those participants who fully complete surveys). The use of deletion methods with data Missing at Random or Missing Not at Random (MAR and MNAR; explained in detail, below) will always introduce selection bias into inferences. This bias undermines the validity of researchers' conclusions by greatly decreasing the probability that researchers will statistically detect true inequalities across groups (Hernán et al., 2004).</p>
        <p>Here, we offer an example of how listwise deletion may bias estimates, impede replicability, and disproportionately impact minoritized individuals from Nissen et al. ( 2018) and Van Dusen and Nissen (2020). Developmental psychologists often administer assessments before and after an intervention, for example, to measure growth in students' knowledge (Singer &amp; Smith, 2013)Here, we offer an example of how listwise deletion may bias estimates, impede replicability, and disproportionately impact minoritized individuals from Nissen et al. ( 2018) and Van Dusen and Nissen (2020). Developmental psychologists often administer assessments before and after an intervention, for example, to measure growth in students' knowledge (Singer &amp; Smith, 2013)</p>
        <p>Multiple imputation should only be used when the missingness is MAR MAR is the least restrictive assumption for multiple imputation. Therefore, multiple imputation is also appropriate (and better than listwise deletion due to increased statistical power) under the more restrictive MCAR assumption. Even under MNAR, multiple imputation (used with sufficient auxiliary variables) can offer advantages over other approaches (e.g., deletion-based methods).Multiple imputation should only be used when the missingness is MAR MAR is the least restrictive assumption for multiple imputation. Therefore, multiple imputation is also appropriate (and better than listwise deletion due to increased statistical power) under the more restrictive MCAR assumption. Even under MNAR, multiple imputation (used with sufficient auxiliary variables) can offer advantages over other approaches (e.g., deletion-based methods).</p>
        <p>Multiple imputation should only be used when too few cases are left after listwise deletionMultiple imputation should only be used when too few cases are left after listwise deletion</p>
        <p>Multiple imputation has advantages even when the amount of missing data is low (i.e., because multiple imputation will eliminate bias under MAR and can partially eliminate bias under MNAR).Multiple imputation has advantages even when the amount of missing data is low (i.e., because multiple imputation will eliminate bias under MAR and can partially eliminate bias under MNAR).</p>
        <p>If results from statistical analyses obtained from multiple imputation differ from those of listwise deletion, the results of multiple imputations must be wrongIf results from statistical analyses obtained from multiple imputation differ from those of listwise deletion, the results of multiple imputations must be wrong</p>
        <p>Results of multiple imputation have been shown to be more accurate and reduce bias in parameter estimates compared to deletion techniques when the multiple imputation model is correctly specified.Results of multiple imputation have been shown to be more accurate and reduce bias in parameter estimates compared to deletion techniques when the multiple imputation model is correctly specified.</p>
        <p>Certain variables must not be imputed (outcomes/predictors)Certain variables must not be imputed (outcomes/predictors)</p>
        <p>With the exception of special instances, most variables can be multiply imputed with benefits. Caution in using multiple imputations is, however, warranted for missing social identity data for ethical concerns (Randall et al., 2021).With the exception of special instances, most variables can be multiply imputed with benefits. Caution in using multiple imputations is, however, warranted for missing social identity data for ethical concerns (Randall et al., 2021).</p>
        <p>Following the computation of multiply imputed data, point estimates from the analysis of each data set are pooled to provide one overall estimate. Generally, this is done using Rubin's (1987) rules. However, sometimes a pooling method is not available for certain commands in your software package of choice. In these instances, we recommend switching to another package. If this is not possible, transparently reporting an ad hoc solution is key.Following the computation of multiply imputed data, point estimates from the analysis of each data set are pooled to provide one overall estimate. Generally, this is done using Rubin's (1987) rules. However, sometimes a pooling method is not available for certain commands in your software package of choice. In these instances, we recommend switching to another package. If this is not possible, transparently reporting an ad hoc solution is key.</p>
        <p>Multiple imputation is making data up Algorithms for imputing missing data use the available data to optimize the accuracy of missing values that are replaced. Sufficient multiple imputations allow researchers to estimate the most likely values for the variable and case while incorporating uncertainty.Multiple imputation is making data up Algorithms for imputing missing data use the available data to optimize the accuracy of missing values that are replaced. Sufficient multiple imputations allow researchers to estimate the most likely values for the variable and case while incorporating uncertainty.</p>
        <p>Doing anything other than listwise or pairwise deletion is hard enough that it is not worth doing With some training, researchers can develop skills to implement best practices for handling missingness such as multiple imputations, which can be completed in a reasonable amount of time and will ultimately provide knowledge producers and consumers with a more accurate understanding of the relations that are being examined. Researchers may also utilize the skills of a methodological consultant to help incorporate best practices for missing data analysis in their design and analysis.Doing anything other than listwise or pairwise deletion is hard enough that it is not worth doing With some training, researchers can develop skills to implement best practices for handling missingness such as multiple imputations, which can be completed in a reasonable amount of time and will ultimately provide knowledge producers and consumers with a more accurate understanding of the relations that are being examined. Researchers may also utilize the skills of a methodological consultant to help incorporate best practices for missing data analysis in their design and analysis.</p>
        <p>The computational demands of multiple imputation are too intensive and/or will take too long to complete Thanks to advances in computing power, only very complex analyses or 'big data' such as neuroimaging and genomics data sets are likely to have computational constraints. For most studies, multiple imputation can be performed in a reasonable amount of time with modern hardware. Multi-core processors are common, and modern software can create multiple imputed data sets concurrently. Moreover, refusing to adjust for missing data given time constraints is not a valid reason to avoid multiple imputation. Good science is not always fast science.The computational demands of multiple imputation are too intensive and/or will take too long to complete Thanks to advances in computing power, only very complex analyses or 'big data' such as neuroimaging and genomics data sets are likely to have computational constraints. For most studies, multiple imputation can be performed in a reasonable amount of time with modern hardware. Multi-core processors are common, and modern software can create multiple imputed data sets concurrently. Moreover, refusing to adjust for missing data given time constraints is not a valid reason to avoid multiple imputation. Good science is not always fast science.</p>
        <p>Note: Adapted from van Ginkel et al. (2020Ginkel et al. ( ). et al., 2010;;Nissen et al., 2018;Nissen &amp; Shemwell, 2016). This means students with lower grades are more likely to have missing data. If the researcher uses listwise deletion, students in their sample with lower grades are most likely to be removed from analyses. Because minoritized students experience structural barriers to success that increase their likelihood of having higher rates of failing grades (Benford &amp; Gess-Newsome, 2006;Van Dusen &amp; Nissen, 2020), using listwise deletion may shrink sample sizes for minoritized students. This will artificially inflate group mean grades, making inequalities in outcomes between majority and minoritized groups appear smaller and ultimately biasing estimates and interpretations based on post-intervention assessment scores (Dynan &amp; Rouse, 1997;Hutchins et al., 1999;Kanim &amp; Cid, 2020; National Academy of Sciences, 2011) 2 .Note: Adapted from van Ginkel et al. (2020Ginkel et al. ( ). et al., 2010;;Nissen et al., 2018;Nissen &amp; Shemwell, 2016). This means students with lower grades are more likely to have missing data. If the researcher uses listwise deletion, students in their sample with lower grades are most likely to be removed from analyses. Because minoritized students experience structural barriers to success that increase their likelihood of having higher rates of failing grades (Benford &amp; Gess-Newsome, 2006;Van Dusen &amp; Nissen, 2020), using listwise deletion may shrink sample sizes for minoritized students. This will artificially inflate group mean grades, making inequalities in outcomes between majority and minoritized groups appear smaller and ultimately biasing estimates and interpretations based on post-intervention assessment scores (Dynan &amp; Rouse, 1997;Hutchins et al., 1999;Kanim &amp; Cid, 2020; National Academy of Sciences, 2011) 2 .</p>
        <p>Figure 1 illustrates this by showing how analyzing complete data can skew findings about inequities across student groups using simulated data based on performance and failure rates from university physics courses. In these data, the true mean score for non-Hispanic White students (65%) is similar to the collected data (68%), while it is meaningfully lower for minoritized students (53%) than the collected data (63%). This bias in data collection reduced the effect sizes between groups from d = 0.75 to d = 0.43 and misrepresented the impacts of systemic barriers to minoritized student success. In contrast, using multiple imputation will retain students across the grade distribution, and more accurately estimate the true group means for students from all groups.Figure 1 illustrates this by showing how analyzing complete data can skew findings about inequities across student groups using simulated data based on performance and failure rates from university physics courses. In these data, the true mean score for non-Hispanic White students (65%) is similar to the collected data (68%), while it is meaningfully lower for minoritized students (53%) than the collected data (63%). This bias in data collection reduced the effect sizes between groups from d = 0.75 to d = 0.43 and misrepresented the impacts of systemic barriers to minoritized student success. In contrast, using multiple imputation will retain students across the grade distribution, and more accurately estimate the true group means for students from all groups.</p>
        <p>4 | HOW DOES ONE ADJUST FOR MISSING DATA?4 | HOW DOES ONE ADJUST FOR MISSING DATA?</p>
        <p>Researchers first should try to understand why data may be missing before making any adjustments or conducting analyses (see Box 1). Data can be missing for many different reasons, including item non-response, attrition during longitudinal studies (Jeliči c et al., 2009), participants' inability to complete tasks, or not passing quality controls.Researchers first should try to understand why data may be missing before making any adjustments or conducting analyses (see Box 1). Data can be missing for many different reasons, including item non-response, attrition during longitudinal studies (Jeliči c et al., 2009), participants' inability to complete tasks, or not passing quality controls.</p>
        <p>When researchers discuss missing data, they usually make a distinction between three main reasons why data may be missing, referred to as missing data mechanisms. These mechanisms are missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR; Heitjan &amp; Basu, 1996;Little &amp; Rubin, 2002). MCAR, MAR and MNAR each lead to distinct assumptions about the generalizability and validity of the inferences drawn from a data set. Although these distinctions are useful for researchers thinking about why data might be missing from a given data set, these are theoretical distinctions. In practice, with a few important exceptions (e.g., planned missing designs), knowing or uncovering the true mechanism causing missing data is not possible. Absolutely distinguishing between these mechanisms would require observing values that are unobserved in the data set.When researchers discuss missing data, they usually make a distinction between three main reasons why data may be missing, referred to as missing data mechanisms. These mechanisms are missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR; Heitjan &amp; Basu, 1996;Little &amp; Rubin, 2002). MCAR, MAR and MNAR each lead to distinct assumptions about the generalizability and validity of the inferences drawn from a data set. Although these distinctions are useful for researchers thinking about why data might be missing from a given data set, these are theoretical distinctions. In practice, with a few important exceptions (e.g., planned missing designs), knowing or uncovering the true mechanism causing missing data is not possible. Absolutely distinguishing between these mechanisms would require observing values that are unobserved in the data set.</p>
        <p>MCAR refers to situations when missing data are the result of a truly random process. Formally, MCAR means that the likelihood of any given data point being missing for a participant is unrelated to the rest of the participant's data.MCAR refers to situations when missing data are the result of a truly random process. Formally, MCAR means that the likelihood of any given data point being missing for a participant is unrelated to the rest of the participant's data.</p>
        <p>The most unambiguous cases of MCAR come from missingness generated at random by design. Researchers can implement planned missing designs when collecting data (Graham et al., 2006;Rhemtulla &amp; Hancock, 2016;Rhemtulla &amp; Little, 2012;Wu &amp; Jia, 2021). For example, each participant may only be given a random subset of the assessments to complete (e.g., the design of the National Assessment of Educational Progress). Another example is when a random subset of participants are given resource-intensive measures (i.e., direct observations of classroom behaviour) in addition to similar but less intensive measures that may be more biased (i.e., teacher-reported classroom behaviour). In a third variation of planned missing designs, different participants are given random subsets of scale items to collect data on more variables overall while minimizing participant burden.The most unambiguous cases of MCAR come from missingness generated at random by design. Researchers can implement planned missing designs when collecting data (Graham et al., 2006;Rhemtulla &amp; Hancock, 2016;Rhemtulla &amp; Little, 2012;Wu &amp; Jia, 2021). For example, each participant may only be given a random subset of the assessments to complete (e.g., the design of the National Assessment of Educational Progress). Another example is when a random subset of participants are given resource-intensive measures (i.e., direct observations of classroom behaviour) in addition to similar but less intensive measures that may be more biased (i.e., teacher-reported classroom behaviour). In a third variation of planned missing designs, different participants are given random subsets of scale items to collect data on more variables overall while minimizing participant burden.</p>
        <p>MCAR is considered safe to 'ignore' because most missing data approaches (including listwise deletion) provide unbiased parameter estimates under MCAR. However, the precision of parameter estimates is still reduced (Pedersen et al., 2017) (see Supporting Information: Table A1 for a list of missing data approaches). In theory, researchers can test if data are not MCAR by examining distributional differences between cases with fully observed data and cases with missing data (Raykov, 2011). However, the absence of evidence that data are MAR or MNAR does not constitute evidence that data are MCAR. In practice, determining that data are MCAR is impossible unless the researcher used a planned missing design and there are no other sources of missing data.MCAR is considered safe to 'ignore' because most missing data approaches (including listwise deletion) provide unbiased parameter estimates under MCAR. However, the precision of parameter estimates is still reduced (Pedersen et al., 2017) (see Supporting Information: Table A1 for a list of missing data approaches). In theory, researchers can test if data are not MCAR by examining distributional differences between cases with fully observed data and cases with missing data (Raykov, 2011). However, the absence of evidence that data are MAR or MNAR does not constitute evidence that data are MCAR. In practice, determining that data are MCAR is impossible unless the researcher used a planned missing design and there are no other sources of missing data.</p>
        <p>MAR refers to situations when missing data are generated in a systematic manner that can be fully accounted for using information contained within a data set. Following our previous example on listwise deletion, students who do not complete post-tests are more likely to have lower scores on pre-tests of educational knowledge (Kost et al., 2009;Kost-Smith et al., 2010;Nissen et al., 2018;Nissen &amp; Shemwell, 2016). Since lower pre-test scores predict students' missingness on post-tests, the missing post-test data are MAR. Researchers can use modern missing data methods (e.g., multiple imputation, full information maximum likelihood estimation or FIML) to incorporate variables that account for MAR missingness in their data. These methods allow researchers to estimate parameters with less bias.MAR refers to situations when missing data are generated in a systematic manner that can be fully accounted for using information contained within a data set. Following our previous example on listwise deletion, students who do not complete post-tests are more likely to have lower scores on pre-tests of educational knowledge (Kost et al., 2009;Kost-Smith et al., 2010;Nissen et al., 2018;Nissen &amp; Shemwell, 2016). Since lower pre-test scores predict students' missingness on post-tests, the missing post-test data are MAR. Researchers can use modern missing data methods (e.g., multiple imputation, full information maximum likelihood estimation or FIML) to incorporate variables that account for MAR missingness in their data. These methods allow researchers to estimate parameters with less bias.</p>
        <p>Variables that explain the mechanism behind missing data are called auxiliary variables if they are included in the missing data model, but not included in the analytic model (Collins et al., 2001). In the example about test scores, above, pre-test scores should be included in the imputation model to help adjust for the missing post-test scores.Variables that explain the mechanism behind missing data are called auxiliary variables if they are included in the missing data model, but not included in the analytic model (Collins et al., 2001). In the example about test scores, above, pre-test scores should be included in the imputation model to help adjust for the missing post-test scores.</p>
        <p>However, if pre-test scores were included in the imputation model but not included in the final analytic model, they would be considered an auxiliary variable. As another example, suppose a researcher did not want to control for socioeconomic status (SES) in their analytic model but SES predicted patterns of missingness in other variables. If the researcher included SES in the missing data model, SES would be an auxiliary variable. Notably, non-auxiliary variables used in the analysis could also account for MAR. For example, achievement may be both a predictor in analysis and could predict patterns of missing data. Including this variable in the imputation model could also help account for MAR.However, if pre-test scores were included in the imputation model but not included in the final analytic model, they would be considered an auxiliary variable. As another example, suppose a researcher did not want to control for socioeconomic status (SES) in their analytic model but SES predicted patterns of missingness in other variables. If the researcher included SES in the missing data model, SES would be an auxiliary variable. Notably, non-auxiliary variables used in the analysis could also account for MAR. For example, achievement may be both a predictor in analysis and could predict patterns of missing data. Including this variable in the imputation model could also help account for MAR.</p>
        <p>Researchers would ideally design their studies to collect auxiliary variables to help account for missingness and aid in building missing data models. Including many auxiliary variables in a model can increase the plausibility that missing data are MAR (Collins et al., 2001). However, including lots of auxiliary variables may not be feasible in large secondary data sets, in part because increasing the number of variables in a model can lead to computational problems like non-convergence due to multicollinearity (van Buuren &amp; Groothuis-Oudshoorn, 2011). When building the missing data model, van Buuren and Groothuis-Oudshoorn (2011) and van Buuren and Oudshoorn (2000) recommend including all variables the researcher plans to use in the analytic model as well as all auxiliary variables for which the distributions between the response and nonresponse groups differ by a certain reasonable magnitude (e.g., based on an expected minimum correlation with the target variables or that explain a predetermined amount of variance). Some software programs include functions that help select these variables automatically, such as the quickpred function in the 
            <rs type="software">mice</rs> package in 
            <rs type="software">R</rs> (further discussed in the worked example section, below).
        </p>
        <p>MNAR refers to missingness that cannot be fully accounted for with other variables in the data set. With MNAR, we can only guess what the missing data mechanism may be. This is because MNAR can occur if missingness depends on either the unobserved data or on the missing values themselves (Fielding et al., 2008). This is true regardless of whether missingness also depends on observed data. For example, parents of children experiencing more behavioural concerns might be less likely to return a questionnaire on the impact of certain parenting practices on their child's behaviour. In this example, the missing value for behavioural problems depends on the missing parent questionnaires. Ignoring this missingness would lead to significant biases in estimating the relation between parenting practices and behaviour due to selection effects.MNAR refers to missingness that cannot be fully accounted for with other variables in the data set. With MNAR, we can only guess what the missing data mechanism may be. This is because MNAR can occur if missingness depends on either the unobserved data or on the missing values themselves (Fielding et al., 2008). This is true regardless of whether missingness also depends on observed data. For example, parents of children experiencing more behavioural concerns might be less likely to return a questionnaire on the impact of certain parenting practices on their child's behaviour. In this example, the missing value for behavioural problems depends on the missing parent questionnaires. Ignoring this missingness would lead to significant biases in estimating the relation between parenting practices and behaviour due to selection effects.</p>
        <p>Unfortunately, there is no simple or straightforward way to combat MNAR missingness. Methods for handling MNAR data attempt to model the reason, or mechanism, for missingness. These methods include selection and pattern mixture models (Heckman, 1979;Little, 1993). Modern efforts have focused on applying selection or pattern mixture models towards longitudinal data (e.g., Enders, 2010Enders, , 2011)). The quality of the correction depends on the quality of the model for the missing data mechanism. In principle, if the mechanism is modelled correctly, bias due to MNAR would be negated; however, in practice and by definition, knowing the exact nature of the missingness mechanism is impossible. Consequently, some researchers avoid using most missing data tools like multiple imputation under MNAR due to concerns about inadequately addressing bias in their models.Unfortunately, there is no simple or straightforward way to combat MNAR missingness. Methods for handling MNAR data attempt to model the reason, or mechanism, for missingness. These methods include selection and pattern mixture models (Heckman, 1979;Little, 1993). Modern efforts have focused on applying selection or pattern mixture models towards longitudinal data (e.g., Enders, 2010Enders, , 2011)). The quality of the correction depends on the quality of the model for the missing data mechanism. In principle, if the mechanism is modelled correctly, bias due to MNAR would be negated; however, in practice and by definition, knowing the exact nature of the missingness mechanism is impossible. Consequently, some researchers avoid using most missing data tools like multiple imputation under MNAR due to concerns about inadequately addressing bias in their models.</p>
        <p>On the other hand, the use of modern missing data adjustments is likely a better solution than simply ignoring missingness (i.e., defaulting to methods like listwise deletion), even under MNAR. For instance, van Ginkel et al.On the other hand, the use of modern missing data adjustments is likely a better solution than simply ignoring missingness (i.e., defaulting to methods like listwise deletion), even under MNAR. For instance, van Ginkel et al.</p>
        <p>(2020) argued that using a sufficient number of auxiliary variables for multiple imputation can still produce less biased estimates than listwise deletion under MNAR. In addition, using multiple imputation with auxiliary variables can restore statistical power lost due to missingness (Collins et al., 2001;Graham, 2009). Other recommendations suggest conducting sensitivity analyses. For example, a researcher would fit multiple types of missing data models (e.g., selection or pattern mixture models) to the same data set to check the impact of different MNAR assumptions on parameter estimates (Demirtas &amp; Schafer, 2003). Overall, we believe the use of multiple imputation under MNAR is justified and provides important advantages over more common deletion techniques. That said, because bias cannot be fully eliminated, keeping this limitation in mind when reporting findings is important. Ideally, researchers will prevent MNAR from the outset by designing robust studies.(2020) argued that using a sufficient number of auxiliary variables for multiple imputation can still produce less biased estimates than listwise deletion under MNAR. In addition, using multiple imputation with auxiliary variables can restore statistical power lost due to missingness (Collins et al., 2001;Graham, 2009). Other recommendations suggest conducting sensitivity analyses. For example, a researcher would fit multiple types of missing data models (e.g., selection or pattern mixture models) to the same data set to check the impact of different MNAR assumptions on parameter estimates (Demirtas &amp; Schafer, 2003). Overall, we believe the use of multiple imputation under MNAR is justified and provides important advantages over more common deletion techniques. That said, because bias cannot be fully eliminated, keeping this limitation in mind when reporting findings is important. Ideally, researchers will prevent MNAR from the outset by designing robust studies.</p>
        <p>The amount of time that multiple imputation takes will vary according to the complexity of the multiple imputation model and size of your data set as well as by the software used. For example, our worked example below uses a highly complex data set with thousands of participants. The complexity of the multiple imputation models we created to handle missingness is reflected in the many potential auxiliary variables we could have used and the nested structure of the models (measurements within students, within schools). van Buuren (2018) recommended including no more than 15-25 auxiliary variables (van Buuren, 2018), but there are minimal downsides to including a large number of auxiliary variables (Enders, 2010). However, including more auxiliary variables does increase model complexity. This increased complexity could lead to substantial increases in computation time and risk of non-convergence. Researchers need to balance the information added from additional auxiliary variables with computation time and potential non-convergence when choosing the final set of variables. Multiple imputation models for less complex data sets may not contain nearly as many auxiliary variables; yet, as discussed earlier, auxiliary variables that could lead to a reasonable assumption of MAR should be considered during the design of the study. Categorical variables are also associated with added computational time. Including information associated with attrition or non-response, though requiring more time, can reduce the likelihood of encountering MNAR missingness, especially in prospective longitudinal designs. Some software, such as 
            <rs type="software">mice</rs> in 
            <rs type="software">R</rs> (van Buuren &amp; Groothuis-Oudshoorn, 2011), include parallel processing functions that greatly reduce the amount of time needed for the imputations to run.
        </p>
        <p>Information that must be reported in academic articles:Information that must be reported in academic articles:</p>
        <p>□ Did the authors include a 'Missing Data' section? Failure to discuss missing data could be grounds for rejection or major revision. Within this section, the authors should also include: Example methods paragraphs can be examined in Enders (2010) and Manly and Wells (2015).□ Did the authors include a 'Missing Data' section? Failure to discuss missing data could be grounds for rejection or major revision. Within this section, the authors should also include: Example methods paragraphs can be examined in Enders (2010) and Manly and Wells (2015).</p>
        <p>Preregistration is where a researcher publishes their planned study procedure as an immutable document in a time-stamped database (e.g., Open Science Framework, As Predicted; Baum et al., 2022;Parsons et al., 2022). Typically, preregistration includes specifying research questions/hypotheses, the research design, and data analysis plan before conducting analyses (e.g. Mertens &amp; Krypotos, 2019;Nosek et al., 2018;Pownall et al., 2021Pownall et al., , 2022;;Tierney et al., 2020Tierney et al., , 2021;;Topor et al., 2020). This process helps to avoid too many 'researcher degrees of freedom' leading to potentially spurious findings (Azevedo et al., 2019(Azevedo et al., , 2022;;Wicherts et al., 2016). Therefore, preregistering missing data decisions is ideal. We provide more information, including links to templates, in Woods et al., (2021) at https://doi.org/10.31234/ osf.io/mdw5r. A registered/exploratory report is more ideal to implement than pre-registration, as the rationale, methods and analysis can be reported a priori and reviewed by peer reviewers. Once in-principle acceptance is received, authors can begin data collection and analysis; however, they cannot change the rationale or methodology. Thus, in a registered report, the focus is less on the findings and more on the research question, methodology and analysis (see review by Chambers &amp; Tzavella, 2022). This process helps reduce publication bias, allowing the literature to be less distorted (Findley et al., 2016). If time is a factor that affects project completion, pre-registration is an adequate approach to reduce researcher degrees of freedom.Preregistration is where a researcher publishes their planned study procedure as an immutable document in a time-stamped database (e.g., Open Science Framework, As Predicted; Baum et al., 2022;Parsons et al., 2022). Typically, preregistration includes specifying research questions/hypotheses, the research design, and data analysis plan before conducting analyses (e.g. Mertens &amp; Krypotos, 2019;Nosek et al., 2018;Pownall et al., 2021Pownall et al., , 2022;;Tierney et al., 2020Tierney et al., , 2021;;Topor et al., 2020). This process helps to avoid too many 'researcher degrees of freedom' leading to potentially spurious findings (Azevedo et al., 2019(Azevedo et al., , 2022;;Wicherts et al., 2016). Therefore, preregistering missing data decisions is ideal. We provide more information, including links to templates, in Woods et al., (2021) at https://doi.org/10.31234/ osf.io/mdw5r. A registered/exploratory report is more ideal to implement than pre-registration, as the rationale, methods and analysis can be reported a priori and reviewed by peer reviewers. Once in-principle acceptance is received, authors can begin data collection and analysis; however, they cannot change the rationale or methodology. Thus, in a registered report, the focus is less on the findings and more on the research question, methodology and analysis (see review by Chambers &amp; Tzavella, 2022). This process helps reduce publication bias, allowing the literature to be less distorted (Findley et al., 2016). If time is a factor that affects project completion, pre-registration is an adequate approach to reduce researcher degrees of freedom.</p>
        <p>In practice, researchers making decisions about missing data analysis and multiple imputation may need to revisit choices made earlier in the process. It is very common to uncover information down-stream that leads to rethinking an up-stream choice. The goal is not to approach multiple imputation perfectly, or even linearly, but instead to think carefully about decisions and report these decisions with transparency. Although circling back to an earlier step or redefining assumptions during the process is normal, these revisions and the rationale behind them should be clearly documented (see Box 2).In practice, researchers making decisions about missing data analysis and multiple imputation may need to revisit choices made earlier in the process. It is very common to uncover information down-stream that leads to rethinking an up-stream choice. The goal is not to approach multiple imputation perfectly, or even linearly, but instead to think carefully about decisions and report these decisions with transparency. Although circling back to an earlier step or redefining assumptions during the process is normal, these revisions and the rationale behind them should be clearly documented (see Box 2).</p>
        <p>While multiple imputation will nearly always provide less biased findings than listwise deletion, imputing some variables raises ethical considerations (Brown et al., 2021). There is a long history of minoritized individuals having their social identifiers assigned in ways that do not align with their identities (Ford, 2001;Puthillam et al., 2022;Shih &amp; Sanchez, 2009). This context has made some researchers wary of any practices that ascribe social identifiers to research participants beyond what they have self-selected (Brown et al., 2021). However, some diversity, equity, inclusion, and accessibility research would not be possible without multiple imputation (Rhodes, 2015). While this practice of assigning social identifiers can be problematic in many settings, the nature of multiple imputation limits its potential harms in two important ways. First, multiple imputation does not simply 'assign' a social identifier to an individual with missing data. It creates a probability distribution of multiple social identifiers based on the rest of the information known about an individual. Second, multiply imputed data do not create findings about any specific individual who has had their data imputed; instead, conclusions are drawn about an aggregated population from the models containing individual data. Researchers have found that imputing social identifiers can limit bias from missing data and meaningfully improve the accuracy of model predictions (Rhodes, 2015).While multiple imputation will nearly always provide less biased findings than listwise deletion, imputing some variables raises ethical considerations (Brown et al., 2021). There is a long history of minoritized individuals having their social identifiers assigned in ways that do not align with their identities (Ford, 2001;Puthillam et al., 2022;Shih &amp; Sanchez, 2009). This context has made some researchers wary of any practices that ascribe social identifiers to research participants beyond what they have self-selected (Brown et al., 2021). However, some diversity, equity, inclusion, and accessibility research would not be possible without multiple imputation (Rhodes, 2015). While this practice of assigning social identifiers can be problematic in many settings, the nature of multiple imputation limits its potential harms in two important ways. First, multiple imputation does not simply 'assign' a social identifier to an individual with missing data. It creates a probability distribution of multiple social identifiers based on the rest of the information known about an individual. Second, multiply imputed data do not create findings about any specific individual who has had their data imputed; instead, conclusions are drawn about an aggregated population from the models containing individual data. Researchers have found that imputing social identifiers can limit bias from missing data and meaningfully improve the accuracy of model predictions (Rhodes, 2015).</p>
        <p>While multiple imputation of social identifiers can be an important step to preparing data, it is worth considering the specifics of a data set before imputation. For example, if a question about gender identity is asked as a binary (only offering man or woman), a blank answer might be a participant's way of communicating that they do not identify as either gender. In this case, imputing gender as man or woman would be misinterpreting the participant's response. Brown et al. (2021) have provided a set of recommendations on when and how researchers should impute social identifier data for investigations of racial equity. In sum, there is no singular answer to whether missing social identifier data should be imputed. Researchers should weigh the ethical considerations for and against imputing social identifier data within the context of their research and the communities that their research impacts.While multiple imputation of social identifiers can be an important step to preparing data, it is worth considering the specifics of a data set before imputation. For example, if a question about gender identity is asked as a binary (only offering man or woman), a blank answer might be a participant's way of communicating that they do not identify as either gender. In this case, imputing gender as man or woman would be misinterpreting the participant's response. Brown et al. (2021) have provided a set of recommendations on when and how researchers should impute social identifier data for investigations of racial equity. In sum, there is no singular answer to whether missing social identifier data should be imputed. Researchers should weigh the ethical considerations for and against imputing social identifier data within the context of their research and the communities that their research impacts.</p>
        <p>Multiple imputation models should be based on and match their corresponding analytic model to satisfy the congeniality assumption (Meng, 1994; see also a discussion in van Buuren, 2018). The data set we use in our worked example, below, contains nested data (occasions within students, students within schools). Thus, a question arises about how to best accommodate clustered data in multiple imputation models.Multiple imputation models should be based on and match their corresponding analytic model to satisfy the congeniality assumption (Meng, 1994; see also a discussion in van Buuren, 2018). The data set we use in our worked example, below, contains nested data (occasions within students, students within schools). Thus, a question arises about how to best accommodate clustered data in multiple imputation models.</p>
        <p>Researchers planning to analyze both single-and multilevel models could develop separate imputation models for each planned analytic model, or impute the most complex model and use these imputed data sets to analyze similar but less complex models (Graham, 2012). For example, when the researcher plans to analyze both single-and multilevel models, they could impute into a multilevel model and use these imputed data for their single-level analyses.Researchers planning to analyze both single-and multilevel models could develop separate imputation models for each planned analytic model, or impute the most complex model and use these imputed data sets to analyze similar but less complex models (Graham, 2012). For example, when the researcher plans to analyze both single-and multilevel models, they could impute into a multilevel model and use these imputed data for their single-level analyses.</p>
        <p>However, researchers generally should not use single-level multiple imputation when they intend to analyze multilevel models. Lüdtke et al. (2017) showed that when a single-level multiple imputation model was used and data were then analyzed via a multilevel model, both the resulting within-and between-group coefficients and their standard errors were biased, especially when the intraclass correlation coefficients (ICCs) were larger. The only scenario in which single-level multiple imputation produced results similar to multilevel multiple imputation occurred when the missing data rate was low and ICCs were small (Lüdtke et al., 2017). In contrast, when a multilevel multiple imputation model was used, no substantial bias in between-or within-group coefficients or standard errors was observed.However, researchers generally should not use single-level multiple imputation when they intend to analyze multilevel models. Lüdtke et al. (2017) showed that when a single-level multiple imputation model was used and data were then analyzed via a multilevel model, both the resulting within-and between-group coefficients and their standard errors were biased, especially when the intraclass correlation coefficients (ICCs) were larger. The only scenario in which single-level multiple imputation produced results similar to multilevel multiple imputation occurred when the missing data rate was low and ICCs were small (Lüdtke et al., 2017). In contrast, when a multilevel multiple imputation model was used, no substantial bias in between-or within-group coefficients or standard errors was observed.</p>
        <p>Multilevel multiple imputation is not yet available in some statistical software programs that are commonly used in developmental psychology. Software capabilities are being rapidly developed or expanded, and it is reasonable to expect that these best options may soon be available across all platforms. In the meantime, researchers may not be willing or able to learn a new software language to conduct multiple imputation, and hence, may need to divert to less optimal solutions that would still be an improvement over listwise deletion.Multilevel multiple imputation is not yet available in some statistical software programs that are commonly used in developmental psychology. Software capabilities are being rapidly developed or expanded, and it is reasonable to expect that these best options may soon be available across all platforms. In the meantime, researchers may not be willing or able to learn a new software language to conduct multiple imputation, and hence, may need to divert to less optimal solutions that would still be an improvement over listwise deletion.</p>
        <p>There are several alternative approaches one could use to ensure their multiple imputation model is as congenial as possible to their planned multilevel analytic models. Two of these approaches are to include cluster variables as dummy indicators in the multiple imputation model, or to multiply impute data separately within each individual cluster (Graham, 2012). However, these options do not work equally well, nor are they as efficient as multilevel multiple imputation. In both approaches, imputing higher-level variables (e.g., school characteristics) is not straightforward.There are several alternative approaches one could use to ensure their multiple imputation model is as congenial as possible to their planned multilevel analytic models. Two of these approaches are to include cluster variables as dummy indicators in the multiple imputation model, or to multiply impute data separately within each individual cluster (Graham, 2012). However, these options do not work equally well, nor are they as efficient as multilevel multiple imputation. In both approaches, imputing higher-level variables (e.g., school characteristics) is not straightforward.</p>
        <p>The dummy-indicator approach leads to overestimated ICCs and underestimated between-group coefficients and standard errors, especially for smaller cluster sizes, although within-group coefficients and standard errors may not be substantially biased (Lüdtke et al., 2017). The Impute-Within-Clusters strategy preserves means, variances, and covariances within each cluster, but nonetheless still leads to the problem of overestimated ICCs. This approach also needs large cluster sizes, which are not always available (Graham, 2012). For example, our worked example data set contains a large number of schools (N = 893) and fairly few students per school (average n per school = 8.4), which produced convergence problems with the Impute-Within-Clusters approach.The dummy-indicator approach leads to overestimated ICCs and underestimated between-group coefficients and standard errors, especially for smaller cluster sizes, although within-group coefficients and standard errors may not be substantially biased (Lüdtke et al., 2017). The Impute-Within-Clusters strategy preserves means, variances, and covariances within each cluster, but nonetheless still leads to the problem of overestimated ICCs. This approach also needs large cluster sizes, which are not always available (Graham, 2012). For example, our worked example data set contains a large number of schools (N = 893) and fairly few students per school (average n per school = 8.4), which produced convergence problems with the Impute-Within-Clusters approach.</p>
        <p>Another alternative ad hoc solution is to conduct both a single-level multiple imputation model at level 1 that includes dummy indicators for the clusters as predictors (e.g., school ID variables) as well as a separate level 2 multiple imputation model using aggregated level 1 data. These two data sets can then be merged into one multilevel data set for analysis (Grund et al., 2018;van Buuren, 2011). We took this approach to demonstrating multiple imputation in our worked example for one software program that does not yet allow multiple imputation (
            <rs type="software">Stata</rs>). We caution that there is little evidence evaluating the effectiveness of this approach. However, our results are similar to those obtained from other software that implemented a multilevel imputation model (see Tables 3456). We also believe that adjusting for missing data in this ad hoc fashion represents an improvement over both listwise deletion methods and single-level imputation models, because it is more congenial with our planned multilevel analyses. But simulation studies are needed to further evaluate this approach.
        </p>
        <p>Developmental researchers may often include derived variables in their analytic models. Examples of derived variables are multi-item scales, in which individual items are averaged or added together, or interaction effects, which may include cross-level interactions. Multiple ways to handle derived variables have been proposed. For derived categorical variables (e.g., interaction effects with categorical predictors), imputation may be conducted separately for each category (van Buuren, 2018). For multi-item scales, scale-level imputation could be conducted, in which only the derived variable (the composite) is used in the imputation and the individual items are not included. However, this method disregards information from participants who answered some but not all items, leading to loss of power (Gottschall et al., 2012).Developmental researchers may often include derived variables in their analytic models. Examples of derived variables are multi-item scales, in which individual items are averaged or added together, or interaction effects, which may include cross-level interactions. Multiple ways to handle derived variables have been proposed. For derived categorical variables (e.g., interaction effects with categorical predictors), imputation may be conducted separately for each category (van Buuren, 2018). For multi-item scales, scale-level imputation could be conducted, in which only the derived variable (the composite) is used in the imputation and the individual items are not included. However, this method disregards information from participants who answered some but not all items, leading to loss of power (Gottschall et al., 2012).</p>
        <p>An alternative option is to impute, then transform (von Hippel, 2009). In this approach, variables used to create the derived variables are imputed individually, and the derived variables are computed after the data are imputed.An alternative option is to impute, then transform (von Hippel, 2009). In this approach, variables used to create the derived variables are imputed individually, and the derived variables are computed after the data are imputed.</p>
        <p>T A B L E 2 Main variables and complete list of all potential auxiliary variables evaluated for use in our multiple imputation worked example.T A B L E 2 Main variables and complete list of all potential auxiliary variables evaluated for use in our multiple imputation worked example.</p>
        <p>This option is problematic because it may bias parameter estimates involving the derived variable toward zero (van Buuren, 2018). Another option is to treat derived variables as 'just another variable' (JAV; White et al., 2011), also referred to as transform, then impute (von Hippel, 2009). With this option, both the variables used to create the derived variables and the derived variables themselves are imputed. The problem here is that the imputed derived score might differ from the score computed from the imputed variables (van Buuren, 2018). As an alternative, passive imputation (van Buuren &amp; Oudshoorn, 2000) occurs when computation of the derived variable is conducted as part of multiple imputation 'on-the-fly' (see section 6.4 in van Buuren, 2018). This method aims to address the problems with impute, then transform and JAV. methods exist, methodological work to determine which method is best in which situation is ongoing. Researchers should therefore be clear about which method of handling derived variables they used in their imputation procedures.This option is problematic because it may bias parameter estimates involving the derived variable toward zero (van Buuren, 2018). Another option is to treat derived variables as 'just another variable' (JAV; White et al., 2011), also referred to as transform, then impute (von Hippel, 2009). With this option, both the variables used to create the derived variables and the derived variables themselves are imputed. The problem here is that the imputed derived score might differ from the score computed from the imputed variables (van Buuren, 2018). As an alternative, passive imputation (van Buuren &amp; Oudshoorn, 2000) occurs when computation of the derived variable is conducted as part of multiple imputation 'on-the-fly' (see section 6.4 in van Buuren, 2018). This method aims to address the problems with impute, then transform and JAV. methods exist, methodological work to determine which method is best in which situation is ongoing. Researchers should therefore be clear about which method of handling derived variables they used in their imputation procedures.</p>
        <p>The remainder of this manuscript presents a worked example in which we highlight how to adopt multiple imputation techniques using a nested series of research questions that build in complexity and are often encountered in quantitative developmental research. Although we recommend that researchers perform any appropriate adjustments for missing data rather than defaulting to deletion methods, addressing full information maximum likelihood estimation (FIML) in addition to multiple imputation is beyond the scope of this paper. Multiple imputation generally produces similar results as FIML (Lee &amp; Shi, 2021). We have described common differences between FIML and multiple imputation elsewhere (see Woods et al., 2021). Tourangeau et al., 2015) to demonstrate how multiple imputation procedures can be implemented, including how performing multiple imputation may differ depending on software, research questions, and planned substantive analyses. The ECLS-K: 2011 is a nationally representative study of 18,174 US students who began kindergarten during the 2010-2011 school year and were followed longitudinally through the spring of expected fifth grade in 2016.The remainder of this manuscript presents a worked example in which we highlight how to adopt multiple imputation techniques using a nested series of research questions that build in complexity and are often encountered in quantitative developmental research. Although we recommend that researchers perform any appropriate adjustments for missing data rather than defaulting to deletion methods, addressing full information maximum likelihood estimation (FIML) in addition to multiple imputation is beyond the scope of this paper. Multiple imputation generally produces similar results as FIML (Lee &amp; Shi, 2021). We have described common differences between FIML and multiple imputation elsewhere (see Woods et al., 2021). Tourangeau et al., 2015) to demonstrate how multiple imputation procedures can be implemented, including how performing multiple imputation may differ depending on software, research questions, and planned substantive analyses. The ECLS-K: 2011 is a nationally representative study of 18,174 US students who began kindergarten during the 2010-2011 school year and were followed longitudinally through the spring of expected fifth grade in 2016.</p>
        <p>Data were collected on a variety of factors thought to influence children's development across elementary school.Data were collected on a variety of factors thought to influence children's development across elementary school.</p>
        <p>Specifically, the researchers collected data about home, neighbourhood, cognitive, behavioural, academic, and school factors. The ECLS-K: 2011 data are publicly available and maintained by the National Center for Education Statistics.Specifically, the researchers collected data about home, neighbourhood, cognitive, behavioural, academic, and school factors. The ECLS-K: 2011 data are publicly available and maintained by the National Center for Education Statistics.</p>
        <p>Our cleaned data sets and all 
            <rs type="software">code</rs> for 
            <rs type="software">Stata</rs>, 
            <rs type="software">R</rs> and 
            <rs type="software">Blimp</rs> are available at 
            <rs type="url">https://osf.io/j3f8m</rs>. Our worked example is based on Ahmed et al. (2022) 4 . This worked example follows the steps outlined in our multiple imputation decision tree (Woods et al., 2021), available at https://doi.org/10.31234/osf.io/mdw5r. Readers may find the decision tree useful as a step-by-step procedure for handling missing data. This procedure covers decision points researchers will encounter based on considerations for their data and the missingness mechanisms in their data.
        </p>
        <p>We mapped the choices we made during this worked example including within each software package onto each step of the decision tree, which is available in Supporting Information: Table A5.We mapped the choices we made during this worked example including within each software package onto each step of the decision tree, which is available in Supporting Information: Table A5.</p>
        <p>We addressed four research questions (RQs) about child-and school-level predictors and longitudinal development of working memory, the complex cognitive ability to maintain and manipulate information in immediately accessible memory systems (Cowan, 2008). Our questions were designed to emulate common developmental research questions. To demonstrate several common considerations and approaches to multiple imputation, we included different types of variables (e.g., binary, ordinal, nominal, continuous and scales) at different levels of analysis (e.g., child-level and school-level). We also looked at interaction effects between variables.We addressed four research questions (RQs) about child-and school-level predictors and longitudinal development of working memory, the complex cognitive ability to maintain and manipulate information in immediately accessible memory systems (Cowan, 2008). Our questions were designed to emulate common developmental research questions. To demonstrate several common considerations and approaches to multiple imputation, we included different types of variables (e.g., binary, ordinal, nominal, continuous and scales) at different levels of analysis (e.g., child-level and school-level). We also looked at interaction effects between variables.</p>
        <p>(RQ1) What predicts kindergarten working memory? Expanding on Ahmed et al. ( 2022), we used a linear regression model to evaluate whether any of several variables predict working memory in the spring of kindergarten. These variables were: math achievement, age at assessment, disability status, cognitive stimulation, sex, race or ethnicity, household income, parent education, and parent employment. Variables are discussed in detail below.(RQ1) What predicts kindergarten working memory? Expanding on Ahmed et al. ( 2022), we used a linear regression model to evaluate whether any of several variables predict working memory in the spring of kindergarten. These variables were: math achievement, age at assessment, disability status, cognitive stimulation, sex, race or ethnicity, household income, parent education, and parent employment. Variables are discussed in detail below.</p>
        <p>(RQ2) Does disability status moderate the relation between working memory and math achievement in the spring of kindergarten? We included this research question to demonstrate how to include an interaction term in multiple imputation models. To answer RQ2, we expanded the RQ1 model by including an interaction term evaluating whether disability status moderated the relation between math achievement and working memory.(RQ2) Does disability status moderate the relation between working memory and math achievement in the spring of kindergarten? We included this research question to demonstrate how to include an interaction term in multiple imputation models. To answer RQ2, we expanded the RQ1 model by including an interaction term evaluating whether disability status moderated the relation between math achievement and working memory.</p>
        <p>(RQ3) Do students who attend more economically advantaged schools have higher working memory in the spring of kindergarten? To answer this question, we evaluated a two-level random-intercept model in the spring of kindergarten. In this model, students are nested within schools. We also added an additional school-level predictor, the proportion of students receiving free-or reduced-price school lunch, that serves as a proxy for schoolwide economic advantage.(RQ3) Do students who attend more economically advantaged schools have higher working memory in the spring of kindergarten? To answer this question, we evaluated a two-level random-intercept model in the spring of kindergarten. In this model, students are nested within schools. We also added an additional school-level predictor, the proportion of students receiving free-or reduced-price school lunch, that serves as a proxy for schoolwide economic advantage.</p>
        <p>(RQ4) What kindergarten factors predict growth in working memory from kindergarten to fifth grade? For our final research question, we evaluated a three-level growth curve model. Level 1 modelled the influence of time. Level 2 modelled the longitudinal influence of kindergarten child-level characteristics. Level 3 modelled the longitudinal influence of the proportion of students receiving free-or reduced-price school lunch in the school a given child attended for kindergarten.(RQ4) What kindergarten factors predict growth in working memory from kindergarten to fifth grade? For our final research question, we evaluated a three-level growth curve model. Level 1 modelled the influence of time. Level 2 modelled the longitudinal influence of kindergarten child-level characteristics. Level 3 modelled the longitudinal influence of the proportion of students receiving free-or reduced-price school lunch in the school a given child attended for kindergarten.</p>
        <p>Researchers evaluating a question like RQ4 using nested, longitudinal data may need to decide whether they want to allow group membership to be dynamic over time. For our worked example, we chose to restrict the analytic sample to only students who did not change schools 5 between kindergarten and fifth grade. We also chose to remove students who were homeschooled or who did not have a proper school ID (i.e., the data collectors could not locate a child, or a child had moved into a non-sampled county during data collection). With these listwise deletions, our model accounts for time-invariant school-level features in a sample of 7509 students (43% of the original sample).Researchers evaluating a question like RQ4 using nested, longitudinal data may need to decide whether they want to allow group membership to be dynamic over time. For our worked example, we chose to restrict the analytic sample to only students who did not change schools 5 between kindergarten and fifth grade. We also chose to remove students who were homeschooled or who did not have a proper school ID (i.e., the data collectors could not locate a child, or a child had moved into a non-sampled county during data collection). With these listwise deletions, our model accounts for time-invariant school-level features in a sample of 7509 students (43% of the original sample).</p>
        <p>Researchers faced with a similar scenario should choose whichever model best fits the research question, the complexity of the data set, and the ability for their data to meet the congeniality assumption.Researchers faced with a similar scenario should choose whichever model best fits the research question, the complexity of the data set, and the ability for their data to meet the congeniality assumption.</p>
        <p>As is appropriate to do in circumstances when listwise deletion cannot be avoided, we evaluated how these excluded students differed from our included participants. The students we removed from our analytic sample appeared to have more socioeconomic markers of disadvantage, lower executive function, and lower achievement than students we retained in analyses (Table 3). This means that our results likely only generalize to populations of relatively more advantaged students who remain in the same school from kindergarten to fifth grade. Descriptive sample information for key variables before and after multiple imputation is available in Table 3. The same information for auxiliary variables is available in Supporting Information: Table A2.As is appropriate to do in circumstances when listwise deletion cannot be avoided, we evaluated how these excluded students differed from our included participants. The students we removed from our analytic sample appeared to have more socioeconomic markers of disadvantage, lower executive function, and lower achievement than students we retained in analyses (Table 3). This means that our results likely only generalize to populations of relatively more advantaged students who remain in the same school from kindergarten to fifth grade. Descriptive sample information for key variables before and after multiple imputation is available in Table 3. The same information for auxiliary variables is available in Supporting Information: Table A2.</p>
        <p>Researchers collecting data for the ECLS-K: 2011 measured working memory at each wave using the Numbers Reversed subtest of the Woodcock-Johnson Tests of Achievement (WJ-NR; Mather et al., 2001). At each wave, students were asked to orally repeat increasing number sequences in reverse order, beginning with two-number sequences up to a maximum of eight numbers. Performance was converted into W-scores, a standardized scale of equal intervals that is normed to a mean of 500 and a standard deviation of 100 among children aged 10 years 0 months. Because W-scores are sensitive to longitudinal change, they can be considered a growth scale and used across multiple age ranges. Younger children will typically display scores below the mean. The working memory variables included in the analytic model were measured by ECLS-K: 2011 data collectors in the spring term of each academic year.Researchers collecting data for the ECLS-K: 2011 measured working memory at each wave using the Numbers Reversed subtest of the Woodcock-Johnson Tests of Achievement (WJ-NR; Mather et al., 2001). At each wave, students were asked to orally repeat increasing number sequences in reverse order, beginning with two-number sequences up to a maximum of eight numbers. Performance was converted into W-scores, a standardized scale of equal intervals that is normed to a mean of 500 and a standard deviation of 100 among children aged 10 years 0 months. Because W-scores are sensitive to longitudinal change, they can be considered a growth scale and used across multiple age ranges. Younger children will typically display scores below the mean. The working memory variables included in the analytic model were measured by ECLS-K: 2011 data collectors in the spring term of each academic year.</p>
        <p>We included the same key predictors of working memory Ahmed et al. (2022) included in their longitudinal analysis of working memory development: math achievement, male sex, racial or ethnic identity, age at assessment (in months), disability status, household income, parent education level and parent employment status. These are the predictors for RQ1 and RQ4. For RQ2, we include the same predictors in addition to a key interaction term for disability by math. For RQ3, we expand on RQ1 by adding the schoolwide proportion of students receiving free and reduced-price lunch as a key predictor in a multilevel framework. The models we developed for RQ1-3 model working memory cross-sectionally at kindergarten. Our RQ4 model includes working memory across all timepoints in a longitudinal framework.We included the same key predictors of working memory Ahmed et al. (2022) included in their longitudinal analysis of working memory development: math achievement, male sex, racial or ethnic identity, age at assessment (in months), disability status, household income, parent education level and parent employment status. These are the predictors for RQ1 and RQ4. For RQ2, we include the same predictors in addition to a key interaction term for disability by math. For RQ3, we expand on RQ1 by adding the schoolwide proportion of students receiving free and reduced-price lunch as a key predictor in a multilevel framework. The models we developed for RQ1-3 model working memory cross-sectionally at kindergarten. Our RQ4 model includes working memory across all timepoints in a longitudinal framework.</p>
        <p>Math achievement was directly measured in the spring of kindergarten using Item Response Theory (IRT) procedures in a two-stage assessment. This assessment was designed to capture conceptual knowledge, procedural knowledge, and problem-solving skills (α = 0.94). We also included age at assessment in months. Our decision was based on ECLS-K: 2011 recommendations for using direct assessment data (Tourangeau et al., 2015). Racial or ethnic identity was a variable constructed by ECLS-K: 2011 staff. We recoded the variable so that 1 = White, 2 = Black, 3 = Hispanic, 4 = Asian and 5 = Others. Disability status was included as a binary variable in which parents reported at the spring of kindergarten whether their child was professionally diagnosed with or had received therapy for an emotional, psychological, learning, communicative or developmental difference. 6 Household income was measured in the spring of kindergarten. We treated it as a continuous variable because it contained 18 nearly equal-interval categories ranging from 1 = $5,000 or less to 18 = $200,001 or more. Parent education level was constructed by ECLS-K: 2011 staff from both fall and spring parent surveys. We treated these data as ordinal and recoded the original values to 1 = High school diploma or less, 2 = Some college, 3 = College degree or higher. Employment was measured in the fall of kindergarten and treated as a nominal variable coded as 1 = Employed full-time, 2 = Employed part-time, 3 = Not employed or looking for work.Math achievement was directly measured in the spring of kindergarten using Item Response Theory (IRT) procedures in a two-stage assessment. This assessment was designed to capture conceptual knowledge, procedural knowledge, and problem-solving skills (α = 0.94). We also included age at assessment in months. Our decision was based on ECLS-K: 2011 recommendations for using direct assessment data (Tourangeau et al., 2015). Racial or ethnic identity was a variable constructed by ECLS-K: 2011 staff. We recoded the variable so that 1 = White, 2 = Black, 3 = Hispanic, 4 = Asian and 5 = Others. Disability status was included as a binary variable in which parents reported at the spring of kindergarten whether their child was professionally diagnosed with or had received therapy for an emotional, psychological, learning, communicative or developmental difference. 6 Household income was measured in the spring of kindergarten. We treated it as a continuous variable because it contained 18 nearly equal-interval categories ranging from 1 = $5,000 or less to 18 = $200,001 or more. Parent education level was constructed by ECLS-K: 2011 staff from both fall and spring parent surveys. We treated these data as ordinal and recoded the original values to 1 = High school diploma or less, 2 = Some college, 3 = College degree or higher. Employment was measured in the fall of kindergarten and treated as a nominal variable coded as 1 = Employed full-time, 2 = Employed part-time, 3 = Not employed or looking for work.</p>
        <p>In addition to using the same variables as Ahmed et al. (2022), we created a cognitive stimulation scale to demonstrate how multi-item scale variables can be imputed. We averaged together nine items measured at the fall of kindergarten assessing how often any member of the family cognitively engaged with the child. These engagement items included: telling stories; singing songs; helping with arts and crafts; involving the child in household chores; playing games or doing puzzles; talking about nature or science projects; building something or playing with construction toys; playing a sport or exercising together; or practising reading, writing or working with numbers (where 1 = not at all and 4 = every day).In addition to using the same variables as Ahmed et al. (2022), we created a cognitive stimulation scale to demonstrate how multi-item scale variables can be imputed. We averaged together nine items measured at the fall of kindergarten assessing how often any member of the family cognitively engaged with the child. These engagement items included: telling stories; singing songs; helping with arts and crafts; involving the child in household chores; playing games or doing puzzles; talking about nature or science projects; building something or playing with construction toys; playing a sport or exercising together; or practising reading, writing or working with numbers (where 1 = not at all and 4 = every day).</p>
        <p>Finally, we included a proxy marker of student socioeconomic disadvantage in the focal child's school to demonstrate the influence of a school-level predictor in a multilevel model. At each wave, the school administrator reported what percentage of students attending the school received free-or reduced-price lunch (recoded so that 1 = 0%-25%, 2 = 26%-50%, 3 = 51%-75% and 4 = 76%-100%).Finally, we included a proxy marker of student socioeconomic disadvantage in the focal child's school to demonstrate the influence of a school-level predictor in a multilevel model. At each wave, the school administrator reported what percentage of students attending the school received free-or reduced-price lunch (recoded so that 1 = 0%-25%, 2 = 26%-50%, 3 = 51%-75% and 4 = 76%-100%).</p>
        <p>Data were normally distributed on our variables of interest. 7 We did not transform any variables before imputation. Researchers who do encounter non-normality will need to investigate the impact this may have on their analyses and take necessary steps. Non-normality is discussed in section 3.3 of van Buuren (2018) with several references. Readers may also take guidance from Lüdtke et al. (2020), Lun and Khattree (2022) and Lee and Carlin (2010).Data were normally distributed on our variables of interest. 7 We did not transform any variables before imputation. Researchers who do encounter non-normality will need to investigate the impact this may have on their analyses and take necessary steps. Non-normality is discussed in section 3.3 of van Buuren (2018) with several references. Readers may also take guidance from Lüdtke et al. (2020), Lun and Khattree (2022) and Lee and Carlin (2010).</p>
        <p>Before conducting any evaluations with the data, we thought about why data may be missing on these key variables (as outlined by Woods et al., 2021; see Supporting Information: Table A5). We hypothesized that data could be MNAR if the child's working memory was too low to complete the direct assessment. In these cases, the child would probably also be missing math achievement and other direct assessment data. To adjust for this possible bias, we chose a set of auxiliary variables that could approximate low working memory (i.e., variables that could account for or be related to missing observations). These variables included parent and teacher observations of child's behaviour and other aspects of executive functioning. We also included working memory and math achievement from the fall of kindergarten as auxiliary variables, along with a host of other child, home and school-level variables that could influence both patterns of missingness as well as working memory and math. The final number of auxiliary variables differed by software to maximize convergence (see Supporting Information: Table A5).Before conducting any evaluations with the data, we thought about why data may be missing on these key variables (as outlined by Woods et al., 2021; see Supporting Information: Table A5). We hypothesized that data could be MNAR if the child's working memory was too low to complete the direct assessment. In these cases, the child would probably also be missing math achievement and other direct assessment data. To adjust for this possible bias, we chose a set of auxiliary variables that could approximate low working memory (i.e., variables that could account for or be related to missing observations). These variables included parent and teacher observations of child's behaviour and other aspects of executive functioning. We also included working memory and math achievement from the fall of kindergarten as auxiliary variables, along with a host of other child, home and school-level variables that could influence both patterns of missingness as well as working memory and math. The final number of auxiliary variables differed by software to maximize convergence (see Supporting Information: Table A5).</p>
        <p>In addition to our 11 main analytic variables, we tested 18 auxiliary variables to evaluate the MAR missingness mechanism. Eleven of these auxiliary variables contained repeated observations (see Table 2 for a complete list of variables available across timepoints). Repeated observations of auxiliary variables occurred at the fall of kindergarten for both working memory and math as well as from first to fifth grade for working memory (note these are considered main analytic variables for RQ4), math, income, disability and lunch. There were four school-level auxiliary variables each with six repeated observations that we hypothesized may predict missingness, particularly on lunch; these variables measured neighbourhood disadvantage, Title I funding, proportion of non-white students, and whether the school was public or private. Five kindergarten variables captured parent-rated behaviour, language status, and parenting stress, which could influence longitudinal study attrition. Six repeated observations of parent marital status were reported from kindergarten through fifth grade, and parents rated students' working memory capabilities at third and fourth grade, which could help predict missingness on the direct assessment of working memory. Finally, there were seven repeated observations each of directly assessed executive function and teacher ratings of behaviour.In addition to our 11 main analytic variables, we tested 18 auxiliary variables to evaluate the MAR missingness mechanism. Eleven of these auxiliary variables contained repeated observations (see Table 2 for a complete list of variables available across timepoints). Repeated observations of auxiliary variables occurred at the fall of kindergarten for both working memory and math as well as from first to fifth grade for working memory (note these are considered main analytic variables for RQ4), math, income, disability and lunch. There were four school-level auxiliary variables each with six repeated observations that we hypothesized may predict missingness, particularly on lunch; these variables measured neighbourhood disadvantage, Title I funding, proportion of non-white students, and whether the school was public or private. Five kindergarten variables captured parent-rated behaviour, language status, and parenting stress, which could influence longitudinal study attrition. Six repeated observations of parent marital status were reported from kindergarten through fifth grade, and parents rated students' working memory capabilities at third and fourth grade, which could help predict missingness on the direct assessment of working memory. Finally, there were seven repeated observations each of directly assessed executive function and teacher ratings of behaviour.</p>
        <p>We created several auxiliary variable composites from this information before imputation. We had many potential auxiliary variables and hoped to minimize convergence issues. We hypothesized that parent-and teacherreported behaviour and executive function would influence missingness on the direct assessments, including for working memory. However, there were five teacher-reported auxiliary variables at the spring of each wave and three parent-reported variables at the fall and spring of kindergarten. To manage this suite of potential auxiliary variables when using data in the wide format, we conducted additional data exploration of the relations between these variables (e.g., correlation matrices, evaluation of missing patterns among these variables) and distilled them into two auxiliary variables: one parent-reported composite averaging scores from the fall and spring of kindergarten, and one teacher-reported composite at the spring of kindergarten. Each of these composites was created using all available data (e.g., through pairwise deletion) and had high reliability (α &gt; 0.80). In testing our auxiliary variables, we discovered that repeated observations of public, non-white and neighbourhood disadvantage variables were collinear with their spring kindergarten values (i.e., strongly enough correlated to cause problems in estimation; r &gt; 0.80; Berry &amp; Feldman, 1985), so we dropped these repeated observations to minimize convergence issues.We created several auxiliary variable composites from this information before imputation. We had many potential auxiliary variables and hoped to minimize convergence issues. We hypothesized that parent-and teacherreported behaviour and executive function would influence missingness on the direct assessments, including for working memory. However, there were five teacher-reported auxiliary variables at the spring of each wave and three parent-reported variables at the fall and spring of kindergarten. To manage this suite of potential auxiliary variables when using data in the wide format, we conducted additional data exploration of the relations between these variables (e.g., correlation matrices, evaluation of missing patterns among these variables) and distilled them into two auxiliary variables: one parent-reported composite averaging scores from the fall and spring of kindergarten, and one teacher-reported composite at the spring of kindergarten. Each of these composites was created using all available data (e.g., through pairwise deletion) and had high reliability (α &gt; 0.80). In testing our auxiliary variables, we discovered that repeated observations of public, non-white and neighbourhood disadvantage variables were collinear with their spring kindergarten values (i.e., strongly enough correlated to cause problems in estimation; r &gt; 0.80; Berry &amp; Feldman, 1985), so we dropped these repeated observations to minimize convergence issues.</p>
        <p>To evaluate which of these potential auxiliary variables should be included in our imputation models to adjust for MAR data, we created dummy variables for our key predictors and outcome where 1 = missing, 0 = non-missing.To evaluate which of these potential auxiliary variables should be included in our imputation models to adjust for MAR data, we created dummy variables for our key predictors and outcome where 1 = missing, 0 = non-missing.</p>
        <p>We then conducted t tests between these dummy variables and the auxiliary variables as well as examined correlations between these missing dummies and key variables (Supporting Information: Table A3). All tests were conducted in 
            <rs type="software">Stata</rs>. The results file is available at https://osf.io/j3f8m. We retained those auxiliary variables that showed significant mean differences between missing and nonmissing values on key variables at p &lt; 0.05 and with correlations between missing dummies and key variables r &gt; 0.10 (i.e., at least a small effect size; Funder &amp; Ozer, 2019). All of our hypothesized auxiliary variables were significantly and meaningfully related to missingness on at least some of our key predictors and outcomes, including longitudinally. For example, there was more study attrition among students attending school in more disadvantaged neighbourhoods relative to students attending school in less disadvantaged neighbourhoods (i.e., students attending kindergarten schools receiving Title I funding were no more or less likely to have missing working memory scores from kindergarten to third grade, but they were more likely to be missing these scores in fourth and fifth grade). Further, there were significant differences between missing and non-missing values on direct cognitive assessments (i.e., math achievement and working memory, as well as lagged predictors of achievement and executive function measured in the fall of kindergarten) and on parent and teacher ratings of problem behaviours and executive functioning including self-regulation. Consistent with our expectation, this indicates that students with lower executive functioning were less likely to complete the direct assessments, resulting in missing data on these items. Failing to include these auxiliary variables could bias our analyses consistent with MNAR. Incorporating these variables into our imputation model results in a reasonable assumption of MAR.
        </p>
        <p>For the purposes of demonstrating multiple imputation procedures, we chose to multiply impute race/ethnicity and sex. We were only missing 0.23% of cases for race/ethnicity and 0.13% of cases for sex, so this decision was unlikely to substantively impact our results. Other researchers using the ECLS-K: 2011 who have different research questions or different predictors may choose not to impute these variables. Either way, this decision should always be transparent and well-justified or, alternatively, in the absence of a good justification, both ways could be conducted and compared.For the purposes of demonstrating multiple imputation procedures, we chose to multiply impute race/ethnicity and sex. We were only missing 0.23% of cases for race/ethnicity and 0.13% of cases for sex, so this decision was unlikely to substantively impact our results. Other researchers using the ECLS-K: 2011 who have different research questions or different predictors may choose not to impute these variables. Either way, this decision should always be transparent and well-justified or, alternatively, in the absence of a good justification, both ways could be conducted and compared.</p>
        <p>Many researchers using large secondary data sets like the ECLS-K: 2011 are interested in making claims about whether their results are nationally representative. This can be accomplished through weighting. For congeniality, if weights are to be used in the analytic model, they should also be used in the imputation model. However, the addition of weights demonstrates an important congeniality issue for our present example. If a researcher were only estimating our RQ1-2, the multiple imputation models would be identically estimated to RQ3 but for the weight variable. This is because the ECLS-K user's guide (Tourangeau et al., 2015) instructs researchers to use a child-level weight (e.g., W1C0) with singlelevel kindergarten data and the school-level weight (W2SCH0) with nested or multilevel kindergarten data. Because RQ1-2 does not ask about the influence of schools, we need only account for the nested structure of the data by clustering standard errors by school ID in these analyses. We would use a child-level weight if we were to weight these analyses, per the ECLS-K: 2011 user's guide. It would be inappropriate to use the school-level weight in lieu of a child-level weight. Yet, in contrast, we would need to use the school-level weight for RQ3 since we specifically analyze a multilevel model. Without weights, we can estimate one imputation model for RQ1-3 since our RQs and analytic models are all nested. However, our imputation and analytic models would not be congenial if we used one weight in the imputation and a different weight in the analysis. Therefore, researchers who need to weight their data to make nationally representative claims may find that their imputation models differ from those who do not need to weight their data. Moreover, there is only one school-level weight in the ECLS-K: 2011 because students (not schools) were followed longitudinally. This means that after the kindergarten wave, there is no way to weight the data to obtain a nationally representative sample of US first-to fifth-grade schools. For RQ4, we could opt to use a child-level attrition weight (i.e., one that accounts for both selection into the sample and longitudinal non-response bias). Davis-Kean et al. (2015) recommend against the use of attrition weights because they can diminish sample size and power.Many researchers using large secondary data sets like the ECLS-K: 2011 are interested in making claims about whether their results are nationally representative. This can be accomplished through weighting. For congeniality, if weights are to be used in the analytic model, they should also be used in the imputation model. However, the addition of weights demonstrates an important congeniality issue for our present example. If a researcher were only estimating our RQ1-2, the multiple imputation models would be identically estimated to RQ3 but for the weight variable. This is because the ECLS-K user's guide (Tourangeau et al., 2015) instructs researchers to use a child-level weight (e.g., W1C0) with singlelevel kindergarten data and the school-level weight (W2SCH0) with nested or multilevel kindergarten data. Because RQ1-2 does not ask about the influence of schools, we need only account for the nested structure of the data by clustering standard errors by school ID in these analyses. We would use a child-level weight if we were to weight these analyses, per the ECLS-K: 2011 user's guide. It would be inappropriate to use the school-level weight in lieu of a child-level weight. Yet, in contrast, we would need to use the school-level weight for RQ3 since we specifically analyze a multilevel model. Without weights, we can estimate one imputation model for RQ1-3 since our RQs and analytic models are all nested. However, our imputation and analytic models would not be congenial if we used one weight in the imputation and a different weight in the analysis. Therefore, researchers who need to weight their data to make nationally representative claims may find that their imputation models differ from those who do not need to weight their data. Moreover, there is only one school-level weight in the ECLS-K: 2011 because students (not schools) were followed longitudinally. This means that after the kindergarten wave, there is no way to weight the data to obtain a nationally representative sample of US first-to fifth-grade schools. For RQ4, we could opt to use a child-level attrition weight (i.e., one that accounts for both selection into the sample and longitudinal non-response bias). Davis-Kean et al. (2015) recommend against the use of attrition weights because they can diminish sample size and power.</p>
        <p>Instead, researchers can account for the same factors that influence attrition and retention rates in their multiple imputation models as auxiliary variables and continue to use the base-year selection weights. Thus, for a weighted RQ4, a researcher might include additional variables that can explain this longitudinal attrition alongside the schoollevel base-year weight for initial selection into the study (W2SCH0).Instead, researchers can account for the same factors that influence attrition and retention rates in their multiple imputation models as auxiliary variables and continue to use the base-year selection weights. Thus, for a weighted RQ4, a researcher might include additional variables that can explain this longitudinal attrition alongside the schoollevel base-year weight for initial selection into the study (W2SCH0).</p>
        <p>We conducted multiple imputation by chained equations (MICE), also known as fully conditional specification (FCS), in 
            <rs type="software">Stata</rs> and 
            <rs type="software">R</rs> (using the 
            <rs type="software">mice</rs> package). We also conducted multiple imputation via fully Bayesian model-based imputation (MBI) in 
            <rs type="software">Blimp</rs>. Whereas recommendations have been made with respect to ideal or best practices in multiple imputation, software capabilities differ. Thus, each software produced similar imputation results with some convergence given sparse cell sizes for some auxiliary variables; convergence was achieved by removing repeated observations of variables capturing school Title I funding status from first to fifth grade, school neighbourhood disadvantage, parent-reported working memory at third and fourth grade, and all repeated observations of single parent status. To specify the model, we developed a predictor matrix using the quickpred function in mice. The predictor matrix is a matrix that defines the equations that will be used to impute each variable. The quickpred function analyzes the correlations between variables to define which variables should be included in the imputation model for each other variable. We were unable to achieve convergence with a three-level model in long format (i.e., perfectly congenial to RQ4). To create two-level imputation models in wide format, we defined the school ID as the nesting variable in the predictor matrix. We then identified the imputation algorithms as pmm for our level 1 variables and 2lonly.norm for our level 2 variables. We ran m = 30 imputations, maxit = 30 iterations, and used the default burn-in of 5000. Visual inspection of the model plots indicates that the models reached sufficient convergence.
        </p>
        <p>Blimp (Keller &amp; Enders, 2021) has two algorithms: FCS and MBI (Enders et al., 2020). With FCS, the process of specifying the multiple imputation model is similar to mice in R, but cannot accommodate nonlinear terms (e.g., interaction effects, random slopes, polynomial terms, etc.). In contrast, MBI allows one to specify complex models up to three levels with non-linear terms. For congeniality to our RQs, we elected to use MBI.Blimp (Keller &amp; Enders, 2021) has two algorithms: FCS and MBI (Enders et al., 2020). With FCS, the process of specifying the multiple imputation model is similar to mice in R, but cannot accommodate nonlinear terms (e.g., interaction effects, random slopes, polynomial terms, etc.). In contrast, MBI allows one to specify complex models up to three levels with non-linear terms. For congeniality to our RQs, we elected to use MBI.</p>
        <p>We specified one random intercept imputation model in long format in Blimp, Version 2 for RQ1-3. We specified the school ID as a cluster variable and working memory as an outcome. We included our main predictors (coded as nominal or ordinal where appropriate), 11 auxiliary variables (9 at the student level and 2 at the school level, all measured during the fall and/or spring of kindergarten), and an interaction effect between disability and mathematics achievement for congeniality with RQ2. We imputed m = 30 data sets. We set a very large number of burn-in iterations (50,000) to solve convergence problems, which led to an acceptable potential scale reduction (psr) factor of 1.074 (Gelman &amp; Rubin, 1992;Keller &amp; Enders, 2021).We specified one random intercept imputation model in long format in Blimp, Version 2 for RQ1-3. We specified the school ID as a cluster variable and working memory as an outcome. We included our main predictors (coded as nominal or ordinal where appropriate), 11 auxiliary variables (9 at the student level and 2 at the school level, all measured during the fall and/or spring of kindergarten), and an interaction effect between disability and mathematics achievement for congeniality with RQ2. We imputed m = 30 data sets. We set a very large number of burn-in iterations (50,000) to solve convergence problems, which led to an acceptable potential scale reduction (psr) factor of 1.074 (Gelman &amp; Rubin, 1992;Keller &amp; Enders, 2021).</p>
        <p>We modified the syntax for RQ1-3 to produce a multiple imputation model congenial with RQ4 by adding student ID as a cluster variable, adding a time variable to model linear growth, and specifying a random slope for time along with interaction effects of analytic predictors of growth and time. As in the model for RQ1-3, we imputed m = 30 data sets and set the number of burn-in iterations to 50. This model was extremely computationally intensive but eventually reached convergence. The psr factor was 1.061, which we deemed acceptable.We modified the syntax for RQ1-3 to produce a multiple imputation model congenial with RQ4 by adding student ID as a cluster variable, adding a time variable to model linear growth, and specifying a random slope for time along with interaction effects of analytic predictors of growth and time. As in the model for RQ1-3, we imputed m = 30 data sets and set the number of burn-in iterations to 50. This model was extremely computationally intensive but eventually reached convergence. The psr factor was 1.061, which we deemed acceptable.</p>
        <p>Descriptive statistics for auxiliary variables and derived variables for the sample after multiple imputation in each software program is compared to the complete case sample in Table 3. Based on these descriptive statistics, we find that complete case analysis would restrict the sample to include more white students, students with higher working memory scores, and students from households with higher parental education and income. The imputation model for mice was the only model to not include the kindergarten measure of school neighborhood disadvantage given convergence problems. Potentially highlighting the importance of auxiliary variables, results produced by mice display some differences in disability, employment, and education values (Table 3). This might have led to slight variations in analysis results (Tables 456).Descriptive statistics for auxiliary variables and derived variables for the sample after multiple imputation in each software program is compared to the complete case sample in Table 3. Based on these descriptive statistics, we find that complete case analysis would restrict the sample to include more white students, students with higher working memory scores, and students from households with higher parental education and income. The imputation model for mice was the only model to not include the kindergarten measure of school neighborhood disadvantage given convergence problems. Potentially highlighting the importance of auxiliary variables, results produced by mice display some differences in disability, employment, and education values (Table 3). This might have led to slight variations in analysis results (Tables 456).</p>
        <p>The average per cent of missing observations across analytic variables was 16.8%, ranging from a low of 0.1% for race and a high of 28.0% on employment status. We would have retained only 59% of our sample under listwise deletion methods for RQ1-2. Twenty-five per cent of cases were missing parent survey items (i.e., presumably from attrition non-response in failing to return the entire survey rather than item non-response on individual questions; 14% of these cases were missing responses from the fall of kindergarten and 9% were missing both fall and spring waves). Nine per cent of cases were only missing kindergarten income and disability status, and an additional 2% were missing only kindergarten disability status. Finally, 1% of cases were missing all items from the spring of kindergarten (direct assessment and parent survey responses). The remaining 6% of cases had no discernable pattern in non-response (e.g., could have been due to selectively or inadvertently skipping a question, coding errors, etc.). For RQ3, we would have retained 49% of our sample under listwise deletion methods. Seventeen per cent of cases were missing parent survey items (10% from the fall of kindergarten, and 7% from the spring of kindergarten). Sixteen per cent of cases were missing school administrator data (10% missing only the administrator survey, and an additional 6% missing fall and/or spring parent surveys). Ten per cent of cases were missing disability status either alone (1%) or in combination with income (7%) and administrator data (2%). The remaining 8% of cases had no discernable pattern in non-response. For RQ4, which added repeated observations of our dependent variable working memory, we would have retained only 46% of our measurement occasions under listwise deletion methods. Twenty-two per cent of these observations were missing kindergarten parent survey data (9% were missing information from the fall of kindergarten, 7% were missing information from the spring of kindergarten, and 6% were missing all parent survey information). Eighteen per cent of these observations were missing school administrator data (10% of cases missing only school administrator data, and an additional 8% missing school administrator and fall and/or spring parent survey data). One per cent of these observations were missing only disability status. The remaining 14% had no discernible missing pattern.The average per cent of missing observations across analytic variables was 16.8%, ranging from a low of 0.1% for race and a high of 28.0% on employment status. We would have retained only 59% of our sample under listwise deletion methods for RQ1-2. Twenty-five per cent of cases were missing parent survey items (i.e., presumably from attrition non-response in failing to return the entire survey rather than item non-response on individual questions; 14% of these cases were missing responses from the fall of kindergarten and 9% were missing both fall and spring waves). Nine per cent of cases were only missing kindergarten income and disability status, and an additional 2% were missing only kindergarten disability status. Finally, 1% of cases were missing all items from the spring of kindergarten (direct assessment and parent survey responses). The remaining 6% of cases had no discernable pattern in non-response (e.g., could have been due to selectively or inadvertently skipping a question, coding errors, etc.). For RQ3, we would have retained 49% of our sample under listwise deletion methods. Seventeen per cent of cases were missing parent survey items (10% from the fall of kindergarten, and 7% from the spring of kindergarten). Sixteen per cent of cases were missing school administrator data (10% missing only the administrator survey, and an additional 6% missing fall and/or spring parent surveys). Ten per cent of cases were missing disability status either alone (1%) or in combination with income (7%) and administrator data (2%). The remaining 8% of cases had no discernable pattern in non-response. For RQ4, which added repeated observations of our dependent variable working memory, we would have retained only 46% of our measurement occasions under listwise deletion methods. Twenty-two per cent of these observations were missing kindergarten parent survey data (9% were missing information from the fall of kindergarten, 7% were missing information from the spring of kindergarten, and 6% were missing all parent survey information). Eighteen per cent of these observations were missing school administrator data (10% of cases missing only school administrator data, and an additional 8% missing school administrator and fall and/or spring parent survey data). One per cent of these observations were missing only disability status. The remaining 14% had no discernible missing pattern.</p>
        <p>For each research question, there were few dissimilarities across results from different software packages but marked differences in the pattern of results compared to complete case analysis. For RQ1-3, estimated gaps in working memory for racially minoritized students were overestimated relative to white students using complete case analysis. The magnitude of these differences sometimes affected statistical significance. In the models for RQ1 and RQ2 (Table 4), family income was a significant predictor of working memory after (but not before) multiple imputation, and the effects of parent education were overestimated in complete case analysis. In RQ3 (Table 5), the effect of the school-level predictor of economic disadvantage (per cent of students receiving free or reduced-price lunch) significantly predicted kindergarten working memory scores after multiple imputation but not in complete case analysis. Interestingly, in the model for RQ3, complete case analysis would lead researchers to conclude that age of assessment was a significant predictor of working memory. After accounting for missing data using multiple imputation, we observe marked differences in effects for the sociodemographic variables of parent employment, parent education, child sex and cognitive stimulation. Examining predictors of trajectories of working memory in RQ4 (Table 6) via a growth curve modelling approach again reveals minimal differences in multiple imputation results across software programs, but larger differences in results between multiple imputation and listwise deletion. As shown in Table 6, the pattern is not entirely consistent, but listwise deletion appeared to underestimate the effect of parent unemployment, cognitive stimulation and free lunch, and overestimate the effects of age, part-time employment and parent education.For each research question, there were few dissimilarities across results from different software packages but marked differences in the pattern of results compared to complete case analysis. For RQ1-3, estimated gaps in working memory for racially minoritized students were overestimated relative to white students using complete case analysis. The magnitude of these differences sometimes affected statistical significance. In the models for RQ1 and RQ2 (Table 4), family income was a significant predictor of working memory after (but not before) multiple imputation, and the effects of parent education were overestimated in complete case analysis. In RQ3 (Table 5), the effect of the school-level predictor of economic disadvantage (per cent of students receiving free or reduced-price lunch) significantly predicted kindergarten working memory scores after multiple imputation but not in complete case analysis. Interestingly, in the model for RQ3, complete case analysis would lead researchers to conclude that age of assessment was a significant predictor of working memory. After accounting for missing data using multiple imputation, we observe marked differences in effects for the sociodemographic variables of parent employment, parent education, child sex and cognitive stimulation. Examining predictors of trajectories of working memory in RQ4 (Table 6) via a growth curve modelling approach again reveals minimal differences in multiple imputation results across software programs, but larger differences in results between multiple imputation and listwise deletion. As shown in Table 6, the pattern is not entirely consistent, but listwise deletion appeared to underestimate the effect of parent unemployment, cognitive stimulation and free lunch, and overestimate the effects of age, part-time employment and parent education.</p>
        <p>For each research question, listwise deletion might lead researchers to overestimate the working memory gap between students from different sociodemographic and socioeconomic backgrounds. For example, researchers using complete case analysis might overestimate the working memory gap between white and minoritized students, particularly Black or Hispanic students, despite the fact that less than 1% of these observations were missing. Similarly, complete case analysis would lead researchers to overestimate the gap by parent education level as well as underestimate the effect of parent unemployment as well as schoolwide socioeconomic disadvantage. Thus, even when using population data like the ECLS-K: 2011, failure to adjust for missing data can introduce bias into results and analysis, particularly on important sociodemographic predictors. Although we found results to be mostly similar across imputation models in different software, other studies with different analytic models and data may have produced discrepant results. Future methodological research on fairly complex analytic models is needed to evaluate the effects of different imputation models implemented in different software packages on the bias in parameter estimates and standard errors. Recommendations from simulation studies will be helpful for applied researchers when choosing between different imputation models.For each research question, listwise deletion might lead researchers to overestimate the working memory gap between students from different sociodemographic and socioeconomic backgrounds. For example, researchers using complete case analysis might overestimate the working memory gap between white and minoritized students, particularly Black or Hispanic students, despite the fact that less than 1% of these observations were missing. Similarly, complete case analysis would lead researchers to overestimate the gap by parent education level as well as underestimate the effect of parent unemployment as well as schoolwide socioeconomic disadvantage. Thus, even when using population data like the ECLS-K: 2011, failure to adjust for missing data can introduce bias into results and analysis, particularly on important sociodemographic predictors. Although we found results to be mostly similar across imputation models in different software, other studies with different analytic models and data may have produced discrepant results. Future methodological research on fairly complex analytic models is needed to evaluate the effects of different imputation models implemented in different software packages on the bias in parameter estimates and standard errors. Recommendations from simulation studies will be helpful for applied researchers when choosing between different imputation models.</p>
        <p>Missing data are ubiquitous in developmental research. The choice of how to address missing data is as crucial to the validity of results as the choice of analysis. The goal of this paper was to elucidate the importance of addressing missing data, to outline recommended multiple imputation reporting standards (e.g., Box 2), and to provide worked software examples across multiple approaches to handling missing data. Our recommendations are applicable to all social scientists but are critical for developmental scientists who often use complex analytic models.Missing data are ubiquitous in developmental research. The choice of how to address missing data is as crucial to the validity of results as the choice of analysis. The goal of this paper was to elucidate the importance of addressing missing data, to outline recommended multiple imputation reporting standards (e.g., Box 2), and to provide worked software examples across multiple approaches to handling missing data. Our recommendations are applicable to all social scientists but are critical for developmental scientists who often use complex analytic models.</p>
        <p>Even when researchers do not explicitly adjust for missing data, there is often still an adjustment for missingness made in analyses that can impact results (e.g., software programs usually default to deletion methods when nothing else is specified). We argue that this process should be conscious and well-informed given the ethical, practical and moral implications of ignoring missing data. Because the choice of how to handle missing data can have important effects on the accuracy and precision of one's inferences, researchers should not only carefully consider why they are implementing a chosen method, but also how such decisions will affect their final study outcomes. We recommend that decisions be clearly communicated, driven by theory including a thorough conceptual understanding of one's data, and delineated at the level of the proposed analysis rather than specified for a data set as a whole.Even when researchers do not explicitly adjust for missing data, there is often still an adjustment for missingness made in analyses that can impact results (e.g., software programs usually default to deletion methods when nothing else is specified). We argue that this process should be conscious and well-informed given the ethical, practical and moral implications of ignoring missing data. Because the choice of how to handle missing data can have important effects on the accuracy and precision of one's inferences, researchers should not only carefully consider why they are implementing a chosen method, but also how such decisions will affect their final study outcomes. We recommend that decisions be clearly communicated, driven by theory including a thorough conceptual understanding of one's data, and delineated at the level of the proposed analysis rather than specified for a data set as a whole.</p>
        <p>Because there are many potential decisions, researchers should conduct sensitivity analyses such as applying different decision-making rules to test the robustness of results (e.g., threshold of significance or meaningful effect sizes for the inclusion of auxiliary variables) or examining samples and model results before and after multiple imputation.Because there are many potential decisions, researchers should conduct sensitivity analyses such as applying different decision-making rules to test the robustness of results (e.g., threshold of significance or meaningful effect sizes for the inclusion of auxiliary variables) or examining samples and model results before and after multiple imputation.</p>
        <p>We also recommend that researchers incorporate open science practices into multiple imputation so that others may replicate their work (e.g., pre-registering imputation decisions or conducting registered reports, openly sharing data and code). Overall, despite the many potential decisions that can be made in the process of multiply imputing data, choosing to not consider robust ways of addressing missingness is a decision that is likely to have more serious consequences than using one type of approach (e.g., multiple imputation, FIML) or algorithm (e.g., MICE) over another.We also recommend that researchers incorporate open science practices into multiple imputation so that others may replicate their work (e.g., pre-registering imputation decisions or conducting registered reports, openly sharing data and code). Overall, despite the many potential decisions that can be made in the process of multiply imputing data, choosing to not consider robust ways of addressing missingness is a decision that is likely to have more serious consequences than using one type of approach (e.g., multiple imputation, FIML) or algorithm (e.g., MICE) over another.</p>
        <p>In sum, addressing missing data appropriately takes additional time, effort and thought, and involves additional analysis steps than what is done automatically in most programs. Such barriers may prevent adoption of multiple imputation for many researchers, but any well-designed analysis or study design includes consideration of missing data. We hope our guidance will inspire researchers to question their default practices, describe and justify their approach to missing data when reporting results, and implement multiple imputation in future analyses. Appropriately addressing missing data is key to transparent analyses and to engaging in the most robust, most unbiased science possible. AFFILIATIONS 1 Center for Learning and Development, Education, SRI International, Arlington, Virginia, USA random subsample of students in the fall of first grade (wave 3) and the fall of second grade (wave 5). In a departure from Ahmed's analyses and for simplicity in demonstration, we do not use data from the planned missing waves 3 and 5, but instead use data from waves 1 and 2 (fall and spring of kindergarten), 4, 6, 7, 8 and 9 (spring of first, second, third, fifth and fifth grade, respectively). Readers interested in an example of imputation with planned missing data should consult Ahmed et al. (2022). 5 In this worked example, allowing group membership to vary over time would have produced a more complex model for RQ4 (e.g., a cross-classified random effects model where group membership within schools may change over time instead of a linear growth model; see Cafri et al., 2015). Retaining these participants who changed schools by running a more complex model is a commendable goal for a substantive study but is beyond the scope of our worked methodological example.In sum, addressing missing data appropriately takes additional time, effort and thought, and involves additional analysis steps than what is done automatically in most programs. Such barriers may prevent adoption of multiple imputation for many researchers, but any well-designed analysis or study design includes consideration of missing data. We hope our guidance will inspire researchers to question their default practices, describe and justify their approach to missing data when reporting results, and implement multiple imputation in future analyses. Appropriately addressing missing data is key to transparent analyses and to engaging in the most robust, most unbiased science possible. AFFILIATIONS 1 Center for Learning and Development, Education, SRI International, Arlington, Virginia, USA random subsample of students in the fall of first grade (wave 3) and the fall of second grade (wave 5). In a departure from Ahmed's analyses and for simplicity in demonstration, we do not use data from the planned missing waves 3 and 5, but instead use data from waves 1 and 2 (fall and spring of kindergarten), 4, 6, 7, 8 and 9 (spring of first, second, third, fifth and fifth grade, respectively). Readers interested in an example of imputation with planned missing data should consult Ahmed et al. (2022). 5 In this worked example, allowing group membership to vary over time would have produced a more complex model for RQ4 (e.g., a cross-classified random effects model where group membership within schools may change over time instead of a linear growth model; see Cafri et al., 2015). Retaining these participants who changed schools by running a more complex model is a commendable goal for a substantive study but is beyond the scope of our worked methodological example.</p>
        <p>Other, newer options accommodate substantive models, such as the smcfcs methodOther, newer options accommodate substantive models, such as the smcfcs method</p>
        <p>(Bartlett et al., 2015)(Bartlett et al., 2015)</p>
        <p>and fully Bayesian model-based imputationand fully Bayesian model-based imputation</p>
        <p>(Enders et al., 2020)(Enders et al., 2020)</p>
        <p>. Although a variety of T A B L E 3 T A B L E 4 Results for RQ1-2 under multiple imputation in 
            <rs type="software">R</rs>, 
            <rs type="software">Stata</rs> and 
            <rs type="software">Blimp</rs> relative to listwise deletion. + p &lt; 0.10. *p &lt; 0.05.**p &lt; 0.01.***p &lt; 0.001.
        </p>
        <p>Cham et al. (2017)Cham et al. (2017)</p>
        <p>,,</p>
        <p>Dong and Peng (2013)Dong and Peng (2013)</p>
        <p>, and, and</p>
        <p>Lee and Shi (2021)Lee and Shi (2021)</p>
        <p>provide additional information about and examples of FIML.provide additional information about and examples of FIML.</p>
        <p>T A B L E 5T A B L E 5</p>
        <p>We used data from the Early Childhood Longitudinal Study, Kindergarten Cohort of 2010-2011 (ECLS-K: 2011;We used data from the Early Childhood Longitudinal Study, Kindergarten Cohort of 2010-2011 (ECLS-K: 2011;</p>
        <p>ofof</p>
        <p>WOODS ET AL. 15227219, 0, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/icd.2407 by Test, Wiley Online Library on [24/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons LicenseWOODS ET AL. 15227219, 0, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/icd.2407 by Test, Wiley Online Library on [24/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</p>
        <p>of 37 WOODS ET AL. 15227219, 0, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/icd.2407 by Test, Wiley Online Library on [24/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons Licenseof 37 WOODS ET AL. 15227219, 0, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/icd.2407 by Test, Wiley Online Library on [24/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</p>
        <p>15227219, 0, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/icd.2407 by Test, Wiley Online Library on [24/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License15227219, 0, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/icd.2407 by Test, Wiley Online Library on [24/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License</p>
        <p>WM direct X X d X d X d X d X d WM parent X X Math X X X X X X Income X X X X X Disability DCCS X Tch behavior X e X X X X X XPar Behavior:Approaches X b Control X b Impulsive X b Bilingual X Single Par. X X X X X X Burnout X Public Sch. X X e X e X e X e X e % Non-White X X e X e X e X e X e Title I funds X X X X X X % Lunch X X X X X Disadvantage X X e X e X eX e X e a Disability*Math interaction term could be computed from these two items before or after imputation; see Section 4.2.5.b One variable constructed with information averaged across the fall and spring of kindergarten.c Scale created from nine items. Either the final scale could be created and imputed, or the nine items could be used separately in the imputation model; see Section 4.2.5. d Variables were auxiliary for RQ1-3 imputation models and main for RQ4 imputation models. e Variables were discovered to be collinear with the spring K value and were dropped from all imputation models.Abbreviations: Cog. Stim., cognitive stimulation; DCCS, Dimensional Change Card Sort, an executive function task; Par, parent; Sch, school; Tch, teacher; WM, working memory.WM direct X X d X d X d X d X d WM parent X X Math X X X X X X Income X X X X X Disability DCCS X Tch behavior X e X X X X X XPar Behavior:Approaches X b Control X b Impulsive X b Bilingual X Single Par. X X X X X X Burnout X Public Sch. X X e X e X e X e X e % Non-White X X e X e X e X e X e Title I funds X X X X X X % Lunch X X X X X Disadvantage X X e X e X eX e X e a Disability*Math interaction term could be computed from these two items before or after imputation; see Section 4.2.5.b One variable constructed with information averaged across the fall and spring of kindergarten.c Scale created from nine items. Either the final scale could be created and imputed, or the nine items could be used separately in the imputation model; see Section 4.2.5. d Variables were auxiliary for RQ1-3 imputation models and main for RQ4 imputation models. e Variables were discovered to be collinear with the spring K value and were dropped from all imputation models.Abbreviations: Cog. Stim., cognitive stimulation; DCCS, Dimensional Change Card Sort, an executive function task; Par, parent; Sch, school; Tch, teacher; WM, working memory.</p>
        <p>The term disorder was used in the parent questionnaire. To enhance inclusivity, we follow the neurodiverse movement and the social model of disability by using the word difference instead of disorder(Elsherif et al., 2022).The term disorder was used in the parent questionnaire. To enhance inclusivity, we follow the neurodiverse movement and the social model of disability by using the word difference instead of disorder(Elsherif et al., 2022).</p>
        <p>Distributional assumptions are usually placed only on the residuals of the dependent variable Y in typical models. In imputation models, the independent variables (X) take turns serving as the 'outcome' to be imputed, so distributional assumptions also apply to the residuals of any X variable that is imputed.Distributional assumptions are usually placed only on the residuals of the dependent variable Y in typical models. In imputation models, the independent variables (X) take turns serving as the 'outcome' to be imputed, so distributional assumptions also apply to the residuals of any X variable that is imputed.</p>
        <p>, 2021, available at https://doi.org/10.31234/osf.io/mdw5r) and a companion infographic (Woods &amp; Schmidt, 2021, available at https://miro.com/app/board/o9_J18JGJQk=/). We also created multiple imputation coding templates for several prominent software languages (
            <rs type="software">Stata</rs>, 
            <rs type="software">Mplus</rs>, 
            <rs type="software">R</rs>, 
            <rs type="software">SPSS</rs>, 
            <rs type="software">SAS</rs> and 
            <rs type="software">Blimp</rs>). All hackathon materials and coding templates are available at https://osf.io/j3f8m/.
        </p>
        <p>Data are publicly available and are maintained by the National Center for Education Statistics. Our cleaned data sets and all 
            <rs type="software">code</rs> for 
            <rs type="software">Stata</rs>, 
            <rs type="software">R</rs> and 
            <rs type="software">Blimp</rs> are available at our osf.io page (https://osf.io/j3f8m/).
        </p>
        <p>exceptions. The exact number of both main analytic variables and auxiliary variables included in the imputation model varied slightly by software program. We decided each given computation and convergence concerns as well as how the software handled derived variables (i.e., whether we created the disability by math interaction term for RQ2 before, during, or after multiple imputation, as well as whether we chose to create the cognitive stimulation scale from its 9 items before or after multiple imputation).exceptions. The exact number of both main analytic variables and auxiliary variables included in the imputation model varied slightly by software program. We decided each given computation and convergence concerns as well as how the software handled derived variables (i.e., whether we created the disability by math interaction term for RQ2 before, during, or after multiple imputation, as well as whether we chose to create the cognitive stimulation scale from its 9 items before or after multiple imputation).</p>
        <p>In 
            <rs type="software">Blimp</rs>, we developed two multiple imputation models: one two-level model for RQ1-3 and one three-level model for RQ4. We conducted multilevel multiple imputation in R. We implemented the dummy-indicator approach with the previously described ad hoc solution for imputing school-level variables in 
            <rs type="software">Stata</rs>. For RQ4, the longitudinal aspect of the data was handled by working with data in the wide format in both 
            <rs type="software">R</rs> and 
            <rs type="software">Stata</rs>, whereas it was handled by working with data in the long format in 
            <rs type="software">Blimp</rs>. More detailed discussion of specific models and current considerations for each software program are available in the Appendix. Syntax files, results from convergence checks, and all imputed data sets can be found at https://osf.io/j3f8m/. For additional step-by-step information regarding best practices for setting up imputation models and checking for appropriate convergence and results, please see our decision tree at https://doi.org/10.31234/osf.io/mdw5r (Woods et al., 2021). We also overlap this decision tree with the specific decisions made in each software package in Supporting Information: Table A4.
        </p>
        <p>To closely approximate multilevel congeniality (Grund et al., 2018;van Buuren, 2011), we created two data sets in 
            <rs type="software">Stata</rs> v.
            <rs type="version">15.1</rs>: one at the individual level (level 1), and one in which the individual level variables were aggregated to a within-school (level 2) mean. We then ran separate imputation models for each level, including the ID variables childid (child identification number) and s_id (kindergarten school identification number) as predictors in the level 1 imputation model following 
            <rs type="software">Stata</rs>'s recommendations for clustered data. The level 2 imputation model was a single-level model incorporating the same main and auxiliary variables and model specifications (m, burn-ins, etc.) as those included at level 1. Following imputation, we recombined these two imputed data sets into one using the mi merge command. The level 1 imputation model was congenial with the analysis models for RQ1-2, and the level 2 model combined with the level 1 model was congenial with the analysis model for RQ3-4.
        </p>
        <p>We included all auxiliary variables noted in Table 2 except for repeated observations of age at assessment and fall teacher-reported behaviour given convergence problems. Data for all variables were imputed using predictive mean matching (PMM) with 10 nearest neighbours. The passive imputation approach (mi passive: generate) was used to create the cognitive stimulation scale and the disability X math interaction term following multiple imputation.We included all auxiliary variables noted in Table 2 except for repeated observations of age at assessment and fall teacher-reported behaviour given convergence problems. Data for all variables were imputed using predictive mean matching (PMM) with 10 nearest neighbours. The passive imputation approach (mi passive: generate) was used to create the cognitive stimulation scale and the disability X math interaction term following multiple imputation.</p>
        <p>Consistent with recommendations by White et al. (2011) to set m &gt; 100 times the highest fraction of missing information (FMI), we imputed m = 40 data sets. Convergence appeared adequate based on visual inspection of convergence plots (for an illustration of sufficient vs. non-sufficient convergence, see Nassiri et al., 2020;van Buuren &amp; Groothuis-Oudshoorn, 2011) and plots of imputed versus observed values (e.g., box plots, scatterplots and density distributions) were similar between imputed and observed values. The distributions of imputed values can slightly differ from observed values given the reduction in bias due to missing data, but these differences should not be unreasonable. Anomalies evident in a few imputations but not others would indicate problems with the imputation model; White et al. (2010).Consistent with recommendations by White et al. (2011) to set m &gt; 100 times the highest fraction of missing information (FMI), we imputed m = 40 data sets. Convergence appeared adequate based on visual inspection of convergence plots (for an illustration of sufficient vs. non-sufficient convergence, see Nassiri et al., 2020;van Buuren &amp; Groothuis-Oudshoorn, 2011) and plots of imputed versus observed values (e.g., box plots, scatterplots and density distributions) were similar between imputed and observed values. The distributions of imputed values can slightly differ from observed values given the reduction in bias due to missing data, but these differences should not be unreasonable. Anomalies evident in a few imputations but not others would indicate problems with the imputation model; White et al. (2010).</p>
        <p>The 
            <rs type="software">mice</rs> package v
            <rs type="version">3.13.0</rs> (van Buuren &amp; Groothuis-Oudshoorn, 2011)
        </p>
    </text>
</tei>

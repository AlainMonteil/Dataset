<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T12:45+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Where a licence is displayed above, please note the terms and conditions of the licence govern your use of this document.Where a licence is displayed above, please note the terms and conditions of the licence govern your use of this document.</p>
        <p>When citing, please reference the published version. Take down policy While the University of Birmingham exercises care and attention in making items available there are rare occasions when an item has been uploaded in error or has been deemed to be commercially or otherwise sensitive.When citing, please reference the published version. Take down policy While the University of Birmingham exercises care and attention in making items available there are rare occasions when an item has been uploaded in error or has been deemed to be commercially or otherwise sensitive.</p>
        <p>Clinical prediction models aim to improve healthcare by providing timely information for shared decision-making between clinician and their patients, risk stratification, changes in behaviour, and to counsel patients and their relatives [1]. A prediction model can be defined as the (weighted) combination of several predictors to estimate the likelihood or probability of the presence or absence of a certain disease (diagnostic model), or the occurrence of an outcome over a time period (prognostic model) [2]. Traditionally, prediction models were developed using regression techniques, such as logistic or time-to-event regression. However, in the past decade, the attention and use of machine learning approaches to developing clinical prediction models has rapidly grown.Clinical prediction models aim to improve healthcare by providing timely information for shared decision-making between clinician and their patients, risk stratification, changes in behaviour, and to counsel patients and their relatives [1]. A prediction model can be defined as the (weighted) combination of several predictors to estimate the likelihood or probability of the presence or absence of a certain disease (diagnostic model), or the occurrence of an outcome over a time period (prognostic model) [2]. Traditionally, prediction models were developed using regression techniques, such as logistic or time-to-event regression. However, in the past decade, the attention and use of machine learning approaches to developing clinical prediction models has rapidly grown.</p>
        <p>Machine learning can be broadly defined as the use of computer systems that fit mathematical models that assume nonlinear associations and complex interactions. Machine learning has a wide range of potential applications in different pathways of healthcare. For example, machine learning is applied in stratified medicine, triage tools, image-driven diagnosis, online consultations, medication management, and to mine electronic medical records [3]. Most of these applications make use of supervised machine learning whereby a model is fitted to learn the conditional distribution of the outcome given a set of predictors with little assumption on data distributions, nonlinear associations, and interactions. This model can be later applied in other but related individuals to predict their (yet unknown) outcome. Support vector machines (SVMs), random forests (RFs), and neural networks (NNs) are some examples of these techniques [4].Machine learning can be broadly defined as the use of computer systems that fit mathematical models that assume nonlinear associations and complex interactions. Machine learning has a wide range of potential applications in different pathways of healthcare. For example, machine learning is applied in stratified medicine, triage tools, image-driven diagnosis, online consultations, medication management, and to mine electronic medical records [3]. Most of these applications make use of supervised machine learning whereby a model is fitted to learn the conditional distribution of the outcome given a set of predictors with little assumption on data distributions, nonlinear associations, and interactions. This model can be later applied in other but related individuals to predict their (yet unknown) outcome. Support vector machines (SVMs), random forests (RFs), and neural networks (NNs) are some examples of these techniques [4].</p>
        <p>The number of studies on prediction models published in the biomedical literature increases every year [5,6]. With more healthcare data being collected and increasing computational power, we expect studies on clinical prediction models based on (supervised) machine learning techniques to become even more popular. Although numerous models are being developed and validated for various outcomes, patients' populations, and healthcare settings, only a minority of these published models are successfully implemented in clinical practice [7,8].The number of studies on prediction models published in the biomedical literature increases every year [5,6]. With more healthcare data being collected and increasing computational power, we expect studies on clinical prediction models based on (supervised) machine learning techniques to become even more popular. Although numerous models are being developed and validated for various outcomes, patients' populations, and healthcare settings, only a minority of these published models are successfully implemented in clinical practice [7,8].</p>
        <p>The use of appropriate study designs and prediction model strategies to develop or validate a prediction model could improve their transportability into clinical settings [9]. However, currently there is a dearth of information about which study designs, what modelling strategies, and which performance measures do studies on clinical prediction models report when choosing machine learning as modelling approach [10e12]. Therefore, our aim was to systematically review and summarize the characteristics on study design, modelling steps, and performance measures reported in studies of prediction models using supervised machine learning.The use of appropriate study designs and prediction model strategies to develop or validate a prediction model could improve their transportability into clinical settings [9]. However, currently there is a dearth of information about which study designs, what modelling strategies, and which performance measures do studies on clinical prediction models report when choosing machine learning as modelling approach [10e12]. Therefore, our aim was to systematically review and summarize the characteristics on study design, modelling steps, and performance measures reported in studies of prediction models using supervised machine learning.</p>
        <p>We followed the PRISMA 2020 statement to report this systematic review [13].We followed the PRISMA 2020 statement to report this systematic review [13].</p>
        <p>We searched via PubMed (search date 19 December 2019) for articles published between 1 January 2018 and 31 December 2019 (Supplemental File 1). We focused on primary studies that described the development or validation of at least one multivariable diagnostic or prognostic prediction model(s) using any supervised machine learning technique. A multivariable prediction model was defined as a model aiming to predict a health outcome by using two or more predictors (features). We considered a study to be an instance of supervised machine learning when reporting a nonregression approach to model development. If a study reported machine learning models alongside regressionbased models, this was included. We excluded studies reporting only regression-based approaches such as unpenalized regression (for example, ordinary least squares or maximum likelihood logistic regression), or penalized regression (for example, lasso, ridge, elastic net, or Firth's regression), regardless of whether they referred to them as machine learning. Any study design, data source, study population, predictor type or patient-related health outcome was considered.We searched via PubMed (search date 19 December 2019) for articles published between 1 January 2018 and 31 December 2019 (Supplemental File 1). We focused on primary studies that described the development or validation of at least one multivariable diagnostic or prognostic prediction model(s) using any supervised machine learning technique. A multivariable prediction model was defined as a model aiming to predict a health outcome by using two or more predictors (features). We considered a study to be an instance of supervised machine learning when reporting a nonregression approach to model development. If a study reported machine learning models alongside regressionbased models, this was included. We excluded studies reporting only regression-based approaches such as unpenalized regression (for example, ordinary least squares or maximum likelihood logistic regression), or penalized regression (for example, lasso, ridge, elastic net, or Firth's regression), regardless of whether they referred to them as machine learning. Any study design, data source, study population, predictor type or patient-related health outcome was considered.</p>
        <p>We excluded studies investigating a single predictor, test, or biomarker. Similarly, studies using machine learning or AI to enhance the reading of images or signals, rather than predicting health outcomes in individuals, or studies that used only genetic traits or molecular (''omics'') markers as predictors, were excluded. Furthermore, we also excluded reviews, meta-analyses, conference abstracts, and What is new?We excluded studies investigating a single predictor, test, or biomarker. Similarly, studies using machine learning or AI to enhance the reading of images or signals, rather than predicting health outcomes in individuals, or studies that used only genetic traits or molecular (''omics'') markers as predictors, were excluded. Furthermore, we also excluded reviews, meta-analyses, conference abstracts, and What is new?</p>
        <p>Design and methodological conduct of studies on clinical prediction models based on machine learning vary substantially.Design and methodological conduct of studies on clinical prediction models based on machine learning vary substantially.</p>
        <p>Studies on clinical prediction models based on machine learning suffered from poor methodology and reporting similar to studies using regression approaches.Studies on clinical prediction models based on machine learning suffered from poor methodology and reporting similar to studies using regression approaches.</p>
        <p>Methodologies for model development and validation should be more carefully designed and reported to avoid research waste.Methodologies for model development and validation should be more carefully designed and reported to avoid research waste.</p>
        <p>More attention is needed to missing data, internal validation procedures, and calibration.More attention is needed to missing data, internal validation procedures, and calibration.</p>
        <p>Methodological guidance for studies on prediction models based on machine learning techniques is urgently needed.Methodological guidance for studies on prediction models based on machine learning techniques is urgently needed.</p>
        <p>articles for which no full text was available via our institution. The selection was restricted to humans and Englishlanguage studies. Further details about eligibility criteria can be found in our protocol [14].articles for which no full text was available via our institution. The selection was restricted to humans and Englishlanguage studies. Further details about eligibility criteria can be found in our protocol [14].</p>
        <p>Titles and abstracts were screened to identify potentially eligible studies by two independent reviewers from a group of seven (CLAN, TT, SWJN, PD, JM, RB, JAAD). After selection of potentially eligible studies, full-text articles were retrieved and two independent researchers reviewed them for eligibility; one researcher (CLAN) screened all articles and six researchers (TT, SWJN, PD, JM, RB, JAAD) collectively screened the same articles for agreement. In case of any disagreement during screening and selection, a third reviewer was asked to read the article in question and resolve.Titles and abstracts were screened to identify potentially eligible studies by two independent reviewers from a group of seven (CLAN, TT, SWJN, PD, JM, RB, JAAD). After selection of potentially eligible studies, full-text articles were retrieved and two independent researchers reviewed them for eligibility; one researcher (CLAN) screened all articles and six researchers (TT, SWJN, PD, JM, RB, JAAD) collectively screened the same articles for agreement. In case of any disagreement during screening and selection, a third reviewer was asked to read the article in question and resolve.</p>
        <p>We selected several items from existing methodological guidelines for reporting and critical appraisal of prediction model studies to build our data extraction form (CHARMS, TRIPOD, PROBAST) [15e18]. Per study, we extracted the following items: characteristics of study design (for example, cohort, case-control, randomized trial) and data source (for example, routinely collected data, registries, administrative databases), study population, outcome, setting, prediction horizon, country, patient characteristics, sample size (before and after exclusion of participants), number of events, number of candidate and final predictors, handling of missing data, hyperparameter optimization, dataset splitting (for example, train-validation-test), method for internal validation (for example., bootstrapping, crossvalidation), number of models developed and/or validated, and availability of code, data, and model. We defined country as the location of the first author's affiliation. Per model, we extracted information regarding the following items: type of algorithm used, selection of predictors, reporting of variable importance, penalization techniques, reporting of hyperparameters, and metrics of performance (for example, discrimination and calibration).We selected several items from existing methodological guidelines for reporting and critical appraisal of prediction model studies to build our data extraction form (CHARMS, TRIPOD, PROBAST) [15e18]. Per study, we extracted the following items: characteristics of study design (for example, cohort, case-control, randomized trial) and data source (for example, routinely collected data, registries, administrative databases), study population, outcome, setting, prediction horizon, country, patient characteristics, sample size (before and after exclusion of participants), number of events, number of candidate and final predictors, handling of missing data, hyperparameter optimization, dataset splitting (for example, train-validation-test), method for internal validation (for example., bootstrapping, crossvalidation), number of models developed and/or validated, and availability of code, data, and model. We defined country as the location of the first author's affiliation. Per model, we extracted information regarding the following items: type of algorithm used, selection of predictors, reporting of variable importance, penalization techniques, reporting of hyperparameters, and metrics of performance (for example, discrimination and calibration).</p>
        <p>Items were recorded by two independent reviewers. One reviewer (CLAN) recorded all items, while the other reviewers collectively assessed all articles (CLAN, TT, SWJN, PD, JM, RB, JAAD). Articles were assigned to reviewers in a random manner. To accomplish consistent data extraction, the standardized data extraction form was piloted by all reviewers on five articles. Discrepancies in data extraction were discussed and solved between the pair of reviewers. The full list of extracted items is available in our published protocol [14].Items were recorded by two independent reviewers. One reviewer (CLAN) recorded all items, while the other reviewers collectively assessed all articles (CLAN, TT, SWJN, PD, JM, RB, JAAD). Articles were assigned to reviewers in a random manner. To accomplish consistent data extraction, the standardized data extraction form was piloted by all reviewers on five articles. Discrepancies in data extraction were discussed and solved between the pair of reviewers. The full list of extracted items is available in our published protocol [14].</p>
        <p>We extracted information on a maximum number of 10 models per article. We selected the first 10 models reported in the methods section of articles and extracted items accordingly in the results section. For articles describing external validation or updating, we carried out a separate data extraction with similar items. If studies referred to the supplemental file for detailed descriptions, the items were checked in those files. Reviewers could also score an item as not applicable, not reported, or unclear.We extracted information on a maximum number of 10 models per article. We selected the first 10 models reported in the methods section of articles and extracted items accordingly in the results section. For articles describing external validation or updating, we carried out a separate data extraction with similar items. If studies referred to the supplemental file for detailed descriptions, the items were checked in those files. Reviewers could also score an item as not applicable, not reported, or unclear.</p>
        <p>Results were summarized as percentages (with confidence intervals calculated using the Wilson score interval and the Wilson score continuity-corrected interval, when appropriated), medians, and interquartile range (IQR), alongside a narrative synthesis. The reported number of events was combined with the reported number of candidate predictors to calculate the number of events per variable (EPV). Data on a model's predictive performance were summarized for the apparent performance, corrected performance, and externally validated performance. We defined ''apparent performance'' when studies reported model performance assessed in the same dataset or sample in which the model was developed and in case no resampling methods were used; ''corrected performance'' when studies reported model performance assessed in test dataset and/or using resampling methods; and ''externally validated performance'' when studies reported model performance assessed in another sample than the one use for model development. As we wanted to identify the methodological conduct of studies on prediction models developed using machine learning, we did not evaluate the nuances of each modelling approach or its performance, instead we kept our evaluations at study level. We did not perform a quantitative synthesis of the model' performance (that is, meta-analysis), as this was beyond the scope of our review. Analysis and synthesis of data was presented overall. Analyses were performed using 
            <rs type="software">R</rs> (version 
            <rs type="version">4.1.0</rs>, 
            <rs type="creator">R Core Team</rs>, Vienna, Austria).
        </p>
        <p>Among the 24,814 articles retrieved, we drew a random sample of 2,482 articles. After title and abstract screening, 312 references potentially met the eligibility criteria. After full-text screening, 152 articles were included in this review: 94 (61.8% [95% confidence interval (CI) 53.9e69.2]) prognostic and 58 (38.2% [95% CI 30.8e46.1]) diagnostic prediction model studies (Fig. 1). Detailed description of the included articles is provided in Supplemental File 2.Among the 24,814 articles retrieved, we drew a random sample of 2,482 articles. After title and abstract screening, 312 references potentially met the eligibility criteria. After full-text screening, 152 articles were included in this review: 94 (61.8% [95% confidence interval (CI) 53.9e69.2]) prognostic and 58 (38.2% [95% CI 30.8e46.1]) diagnostic prediction model studies (Fig. 1). Detailed description of the included articles is provided in Supplemental File 2.</p>
        <p>In 152 articles, 132 (86.8% [95% CI 80.5e91.3]) studies developed prediction models and evaluated their performance using an internal validation technique, 19 ( Abbreviations: CART, classification and regression tree; LASSO, least absolute shrinkage and selection operator; XGBoost, extreme gradient boosting; CI, confidence interval.In 152 articles, 132 (86.8% [95% CI 80.5e91.3]) studies developed prediction models and evaluated their performance using an internal validation technique, 19 ( Abbreviations: CART, classification and regression tree; LASSO, least absolute shrinkage and selection operator; XGBoost, extreme gradient boosting; CI, confidence interval.</p>
        <p>a Discriminant analysis, generalized additive models (GAM), partial least squares were extracted as OLS regression.a Discriminant analysis, generalized additive models (GAM), partial least squares were extracted as OLS regression.</p>
        <p>b This includes conditional inference tree (n 5 3), optimal tree (n 5 1).b This includes conditional inference tree (n 5 3), optimal tree (n 5 1).</p>
        <p>c This includes Random Survival Forest (n 5 2). d This includes lightGBM (n 5 1), adaBoost (n 5 8), catBoost (n 5 1), logitboost (n 5 1), RUSBoost (n 5 1), and stochastic (n 5 1).c This includes Random Survival Forest (n 5 2). d This includes lightGBM (n 5 1), adaBoost (n 5 8), catBoost (n 5 1), logitboost (n 5 1), RUSBoost (n 5 1), and stochastic (n 5 1).</p>
        <p>e Multilayer perceptron, denseNet, convolutional, recurrent, and Bayesian neural networks were extracted as neural networks.e Multilayer perceptron, denseNet, convolutional, recurrent, and Bayesian neural networks were extracted as neural networks.</p>
        <p>f This includes bayesian network (n 5 3), rule-based classifier (n 5 1), highly predictive signatures (n 5 1), Kalman filtering (n 5 1), fuzzy soft set (n 5 1), adaptive neuro-fuzzy inference system (n 5 1), stochastic gradient descent (n 5 1), fully corrective binning (n 5 1).f This includes bayesian network (n 5 3), rule-based classifier (n 5 1), highly predictive signatures (n 5 1), Kalman filtering (n 5 1), fuzzy soft set (n 5 1), adaptive neuro-fuzzy inference system (n 5 1), stochastic gradient descent (n 5 1), fully corrective binning (n 5 1).</p>
        <p>with a clinical affiliation (n 5 85/152, 56% [95% CI 48e63.6]). Other characteristics are shown in Table 1.with a clinical affiliation (n 5 85/152, 56% [95% CI 48e63.6]). Other characteristics are shown in Table 1.</p>
        <p>Overall, 1,429 prediction models were developed (Median: 9.4 models per study, IQR: 2e8, Range: 1e156). As we set a limit on data extraction to 10 models per article, we evaluated 522 models. a Counts are absolute numbers, with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies reported more than one measure. We report then the raw percentages. NA, not applicable.Overall, 1,429 prediction models were developed (Median: 9.4 models per study, IQR: 2e8, Range: 1e156). As we set a limit on data extraction to 10 models per article, we evaluated 522 models. a Counts are absolute numbers, with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies reported more than one measure. We report then the raw percentages. NA, not applicable.</p>
        <p>b We collected the longest follow-up and longest prediction horizon, both in months. c Data sources also included surveys (n 5 2), cross-sectional studies (n 5 2). d Unclear whether split sample was performed random or nonrandom.b We collected the longest follow-up and longest prediction horizon, both in months. c Data sources also included surveys (n 5 2), cross-sectional studies (n 5 2). d Unclear whether split sample was performed random or nonrandom.</p>
        <p>Participants included in the reviewed studies were mostly recruited from secondary (n 5 32/152, 21.1% [95% CI 15.3e28.2]) and tertiary care (n 5 78/152, 51.3% [95% CI 43.4e59.1]) settings (Table 1). Approximately half of the studies involved data from one center (n 5 73/152, 48% [95% CI 40.2e55.9]) (Table 3).Participants included in the reviewed studies were mostly recruited from secondary (n 5 32/152, 21.1% [95% CI 15.3e28.2]) and tertiary care (n 5 78/152, 51.3% [95% CI 43.4e59.1]) settings (Table 1). Approximately half of the studies involved data from one center (n 5 73/152, 48% [95% CI 40.2e55.9]) (Table 3).</p>
        <p>The prediction models were most frequently developed using cohort data, either prospective (n 5 50/152, 32.9% [95% CI 25.9e40.7]) or retrospective (n 5 48/152, 31.6% [95% CI 24.7e39.3]). Electronic medical records were used in 30/152 studies (19.7% [95% CI 14.2e26.8]). Data collection was conducted on average for 41.9 months (IQR 3 to 60 months) when used to develop models, while for externally validation this was 44.4 months (IQR 1.75 to 42 months). In 101 out of 152 studies (66.4% [95% CI 58.6e73.5]), the time horizon for the predictions was mostly unspecified. However, when reported (n 5 51/152, 33.6% [95% CI 26.5e41.4]), the time horizon of prediction ranged from 24 hours to 8 years (Table 3).The prediction models were most frequently developed using cohort data, either prospective (n 5 50/152, 32.9% [95% CI 25.9e40.7]) or retrospective (n 5 48/152, 31.6% [95% CI 24.7e39.3]). Electronic medical records were used in 30/152 studies (19.7% [95% CI 14.2e26.8]). Data collection was conducted on average for 41.9 months (IQR 3 to 60 months) when used to develop models, while for externally validation this was 44.4 months (IQR 1.75 to 42 months). In 101 out of 152 studies (66.4% [95% CI 58.6e73.5]), the time horizon for the predictions was mostly unspecified. However, when reported (n 5 51/152, 33.6% [95% CI 26.5e41.4]), the time horizon of prediction ranged from 24 hours to 8 years (Table 3).</p>
        <p>Most models were developed to predict a binary outcome (n 5 131/152, 86.2% [95% CI 79.8e90.8]). The most frequent predicted outcome was complications after a certain treatment (n 5 66/152, 43.4% [95% CI 35.8e51.4]). Mortality was also a common endpoint (n 5 21/152, 13.8% [95% CI 9.2e20.2]) (Table 1). 4).Most models were developed to predict a binary outcome (n 5 131/152, 86.2% [95% CI 79.8e90.8]). The most frequent predicted outcome was complications after a certain treatment (n 5 66/152, 43.4% [95% CI 35.8e51.4]). Mortality was also a common endpoint (n 5 21/152, 13.8% [95% CI 9.2e20.2]) (Table 1). 4).</p>
        <p>Studies had a median sample size of 587 participants (IQR 172e6,328). The number of events across the studies had a median of 106 (IQR 50e364). Based on studies with available information (n 5 28/152, 18.4% [95% CI 13.1e25.3]), a median of 12.5 events per candidate predictors were used for model development (IQR 5.7e27.7) (Table 5). Most studies did not report a sample size calculation or justification for sample size (n 5 125/152, 82.2% [95% CI 75.4e87.5]). When sample size justification was provided, the most frequent rationale given was based on the size of existing/available data used (n 5 16/ 27, 59.3% [95% CI 40.7e75.5]) (Table 3).Studies had a median sample size of 587 participants (IQR 172e6,328). The number of events across the studies had a median of 106 (IQR 50e364). Based on studies with available information (n 5 28/152, 18.4% [95% CI 13.1e25.3]), a median of 12.5 events per candidate predictors were used for model development (IQR 5.7e27.7) (Table 5). Most studies did not report a sample size calculation or justification for sample size (n 5 125/152, 82.2% [95% CI 75.4e87.5]). When sample size justification was provided, the most frequent rationale given was based on the size of existing/available data used (n 5 16/ 27, 59.3% [95% CI 40.7e75.5]) (Table 3).</p>
        <p>Missing values were an explicit exclusion criterion of participants in 56 studies (n 5 56/152, 36.8% [95% CI 6.Missing values were an explicit exclusion criterion of participants in 56 studies (n 5 56/152, 36.8% [95% CI 6.</p>
        <p>In our sample, 27/152 (17.8% [95% CI 12.5e24.6]) studies applied at least one method to purportedly address class imbalance, that isewhen one class of the outcome outnumbers the other class (Table 7). The most applied technique was Synthetic Minority Over-sampling Technique (SMOTE), a method that combines oversampling the minority class with undersampling the majority class [19,20].In our sample, 27/152 (17.8% [95% CI 12.5e24.6]) studies applied at least one method to purportedly address class imbalance, that isewhen one class of the outcome outnumbers the other class (Table 7). The most applied technique was Synthetic Minority Over-sampling Technique (SMOTE), a method that combines oversampling the minority class with undersampling the majority class [19,20].</p>
        <p>Tree-based methods were applied in 166/522 (Tree-based methods were applied in 166/522 (</p>
        <p>The strategy to build models was unclear in 168 out of 522 models (32.2% [95% CI 28.2e36.4]). Most models reported a data-driven approach for model building (n 5 192/ 522, 36.8% [95% CI 32.7e41.1]). One study reported the use of recursive feature elimination for model building (n 5 3/522, 0.6% [95% CI 0.1e1.8]). Selection of candidate predictors based on univariable predictoreoutcome associations was used in 27/522 (5.2% [95% CI 3.5e7.5]) of the models. Further details on modelling strategies are presented in Table 8. Of the three studies that reported time-toevent outcomes none reported how they dealt with censoring.The strategy to build models was unclear in 168 out of 522 models (32.2% [95% CI 28.2e36.4]). Most models reported a data-driven approach for model building (n 5 192/ 522, 36.8% [95% CI 32.7e41.1]). One study reported the use of recursive feature elimination for model building (n 5 3/522, 0.6% [95% CI 0.1e1.8]). Selection of candidate predictors based on univariable predictoreoutcome associations was used in 27/522 (5.2% [95% CI 3.5e7.5]) of the models. Further details on modelling strategies are presented in Table 8. Of the three studies that reported time-toevent outcomes none reported how they dealt with censoring.</p>
        <p>Variable importance scores show insight into how much each variable contributed to the prediction model [21]. For 316/522 (60.5% [95% CI 56.2e64.7]) models, authors did not provide these scores, while in 115/522 (22% [95% CI 18.6e25.9]) models these scores were reported without specifying the methods applied to obtain such calculations (Table 8). When reported, the mean decrease in node impurity was the most popular method (n 5 31/522, 5.9% [95% CI 4.1e8.4]). Hyperparameters (including default settings) were reported in 160/552 ( The most common method reported was cross-validation (n 5 15/152) [9.9% [95% CI 6.1e15.6]. Nine studies (n 5 9/152, 5.9% [95% CI 3.1-10.9]) split their dataset into a validation set for hyperparameter tuning (Table 7).Variable importance scores show insight into how much each variable contributed to the prediction model [21]. For 316/522 (60.5% [95% CI 56.2e64.7]) models, authors did not provide these scores, while in 115/522 (22% [95% CI 18.6e25.9]) models these scores were reported without specifying the methods applied to obtain such calculations (Table 8). When reported, the mean decrease in node impurity was the most popular method (n 5 31/522, 5.9% [95% CI 4.1e8.4]). Hyperparameters (including default settings) were reported in 160/552 ( The most common method reported was cross-validation (n 5 15/152) [9.9% [95% CI 6.1e15.6]. Nine studies (n 5 9/152, 5.9% [95% CI 3.1-10.9]) split their dataset into a validation set for hyperparameter tuning (Table 7).</p>
        <p>Most models used measures of the area under the Receiver Operating Characteristic curve (AUC/ROC or the concordance (c)-statistic) (n 5 358/522, 68.6% [95% CI 64.4e72.5]) to describe the discriminative ability of the model (Table 9). A variety of methods were used to describe the agreement between predictions and observations (that is, calibration), the most frequent being a calibration plot (n 5 23/522, 4.4% [95% CI 2.9e6.6]), calibration slope (n 5 17/522, 3.3% [95% CI 2e5.3]), and calibration intercept (n 5 16/522, 3.1% [95% CI 1.8e5]). However, for the large majority no calibration metrics were reported (n 5 494/522, 94.6% [95% CI 92.2e96.3]). Decision curve analysis was reported for two models (n 5 2/522, 0.4% [95% CI 0.1e1.5]) [22].Most models used measures of the area under the Receiver Operating Characteristic curve (AUC/ROC or the concordance (c)-statistic) (n 5 358/522, 68.6% [95% CI 64.4e72.5]) to describe the discriminative ability of the model (Table 9). A variety of methods were used to describe the agreement between predictions and observations (that is, calibration), the most frequent being a calibration plot (n 5 23/522, 4.4% [95% CI 2.9e6.6]), calibration slope (n 5 17/522, 3.3% [95% CI 2e5.3]), and calibration intercept (n 5 16/522, 3.1% [95% CI 1.8e5]). However, for the large majority no calibration metrics were reported (n 5 494/522, 94.6% [95% CI 92.2e96.3]). Decision curve analysis was reported for two models (n 5 2/522, 0.4% [95% CI 0.1e1.5]) [22].</p>
        <p>We also found overall metrics such as classification accuracy (n 5 324/522, 62.1% [95% CI 57.8e66.2]) and F1score (n 5 79/522, 15.1% [95% CI 12.2e18.6]).We also found overall metrics such as classification accuracy (n 5 324/522, 62.1% [95% CI 57.8e66.2]) and F1score (n 5 79/522, 15.1% [95% CI 12.2e18.6]).</p>
        <p>In 53/152 (34.9% [95% CI 22.8e42.7]) studies, discrimination was reported without precision estimates (that is, confidence intervals or standard errors). Likewise, 7/152 (4.6% [95% CI 2.2e9.2]) studies reported model calibration without precision estimates.In 53/152 (34.9% [95% CI 22.8e42.7]) studies, discrimination was reported without precision estimates (that is, confidence intervals or standard errors). Likewise, 7/152 (4.6% [95% CI 2.2e9.2]) studies reported model calibration without precision estimates.</p>
        <p>Most models achieved discriminative ability better than chance (that is, AUC 0.5) with a median apparent AUC of 0.82 (IQR 0.75e0.90; range 0.45 to 1.00), while internally validated AUC was also 0.82 (IQR: 0.74e0.89; range 0.46 to 0.99). For external validation, the median AUC was 0.73 (IQR: 0.70e0.78, range: 0.51-0.88). For calibration and overall performance metrics, see Table 10.Most models achieved discriminative ability better than chance (that is, AUC 0.5) with a median apparent AUC of 0.82 (IQR 0.75e0.90; range 0.45 to 1.00), while internally validated AUC was also 0.82 (IQR: 0.74e0.89; range 0.46 to 0.99). For external validation, the median AUC was 0.73 (IQR: 0.70e0.78, range: 0.51-0.88). For calibration and overall performance metrics, see Table 10.</p>
        <p>In total, 86/152 studies (56.6% [95% CI 48.6e64.2]) internally validated their models, most often splitting the dataset into a training and test set. The train-test sets were often split randomly (n 5 49/86, 57% [95% CI 46.4e66.9]) and in a few studies a temporal (nonrandom) split was applied (n 5 9/86, 10.5% [95% CI 5.6e18.7]). The proportion of the data used for test sets ranged from 10% to 50% of the total dataset. Seventy studies also performed cross-validation (46.1% [95% CI 38.3e54]) with ten studies reporting nested cross-validation (6.6% [95% CI 3.6e11.7]). Out of five studies performing bootstrapping (n 5 5/152, 3.3% [95% CI 1.4e7.5]), one reported 250 iterations, three reported 1,000 iterations and one did not report the number of iterations. For further details see Table 3.In total, 86/152 studies (56.6% [95% CI 48.6e64.2]) internally validated their models, most often splitting the dataset into a training and test set. The train-test sets were often split randomly (n 5 49/86, 57% [95% CI 46.4e66.9]) and in a few studies a temporal (nonrandom) split was applied (n 5 9/86, 10.5% [95% CI 5.6e18.7]). The proportion of the data used for test sets ranged from 10% to 50% of the total dataset. Seventy studies also performed cross-validation (46.1% [95% CI 38.3e54]) with ten studies reporting nested cross-validation (6.6% [95% CI 3.6e11.7]). Out of five studies performing bootstrapping (n 5 5/152, 3.3% [95% CI 1.4e7.5]), one reported 250 iterations, three reported 1,000 iterations and one did not report the number of iterations. For further details see Table 3.</p>
        <p>Few studies (n 5 19/152, 12.5% [95% CI 8.2e18.7]) performed an external validation. Eleven studies (n 5 11/ 19, 57.9% [95% CI 36.3e76.9]) used data from independent cohorts and eight (n 5 8/19, 42.1% [95% CI 23.1e63.7]) used subcohorts within the main cohort to validate their developed models. From the independent cohorts, three studies (n 5 3/19, 15.8% [95% CI 5.5e37.6]) used data from a different country. Five studies (n 5 5/19, 26.3% [95% CI 11.8e48.8]) described an external on clinical prediction models using machine learning. The methodology varied substantially between studies, including modelling algorithms, sample size, and performance measures reported. Unfortunately, longstanding deficiencies in reporting and methodological conduct previously seen in studies with a regression-based approach, were also extensively found in our sample of studies on machine learning models [9,23].Few studies (n 5 19/152, 12.5% [95% CI 8.2e18.7]) performed an external validation. Eleven studies (n 5 11/ 19, 57.9% [95% CI 36.3e76.9]) used data from independent cohorts and eight (n 5 8/19, 42.1% [95% CI 23.1e63.7]) used subcohorts within the main cohort to validate their developed models. From the independent cohorts, three studies (n 5 3/19, 15.8% [95% CI 5.5e37.6]) used data from a different country. Five studies (n 5 5/19, 26.3% [95% CI 11.8e48.8]) described an external on clinical prediction models using machine learning. The methodology varied substantially between studies, including modelling algorithms, sample size, and performance measures reported. Unfortunately, longstanding deficiencies in reporting and methodological conduct previously seen in studies with a regression-based approach, were also extensively found in our sample of studies on machine learning models [9,23].</p>
        <p>The spectrum of supervised machine learning techniques is quite broad [24,25]. In this study, the most popular modelling algorithms were tree-based methods (RF in particular) and SVM. RF is an ensemble of random trees trained on bootstrapped subsets of the dataset [26]. On the other hand, SVM first map each data point into a feature space to then identify the hyperplane that separates the data items into two classes while maximizing the marginal distance for both classes and minimizing the classification errors [27]. Several studies also applied regression-based methods (LR in particular) as benchmark to compare against the predictive performance of machine learningbased models.The spectrum of supervised machine learning techniques is quite broad [24,25]. In this study, the most popular modelling algorithms were tree-based methods (RF in particular) and SVM. RF is an ensemble of random trees trained on bootstrapped subsets of the dataset [26]. On the other hand, SVM first map each data point into a feature space to then identify the hyperplane that separates the data items into two classes while maximizing the marginal distance for both classes and minimizing the classification errors [27]. Several studies also applied regression-based methods (LR in particular) as benchmark to compare against the predictive performance of machine learningbased models.</p>
        <p>Various other well-known methodological issues in prediction model research need to be further discussed. Our reported estimate on EPV is likely to be overestimated given than we were unable to calculate it based on number of parameters, and instead we used only the number of candidate predictors. A simulation study concluded that modern modelling techniques such as SVM and RF might even require 10 times more events [28]. Hence, the sample size in most studies on prediction models using ML remains relatively low. Furthermore, splitting datasets persists as a method for internal validation (that is, testing), reducing even more the actual sample size for model development and increasing the risk of overfitting [29,30]. Whilst AUC was a frequently reported metric to assess predictive performance, calibration or prediction error was often overlooked [31]. Moreover, a quarter of studies in our sample corrected for class imbalance without reporting recalibration, although recent research has shown that correcting for class imbalance may lead to poor calibration and thus, prediction errors [32]. Finally, therapeutic interventions were rarely considered as predictors in the prognostic models, although these can affect the accuracy and transportability of models [33].Various other well-known methodological issues in prediction model research need to be further discussed. Our reported estimate on EPV is likely to be overestimated given than we were unable to calculate it based on number of parameters, and instead we used only the number of candidate predictors. A simulation study concluded that modern modelling techniques such as SVM and RF might even require 10 times more events [28]. Hence, the sample size in most studies on prediction models using ML remains relatively low. Furthermore, splitting datasets persists as a method for internal validation (that is, testing), reducing even more the actual sample size for model development and increasing the risk of overfitting [29,30]. Whilst AUC was a frequently reported metric to assess predictive performance, calibration or prediction error was often overlooked [31]. Moreover, a quarter of studies in our sample corrected for class imbalance without reporting recalibration, although recent research has shown that correcting for class imbalance may lead to poor calibration and thus, prediction errors [32]. Finally, therapeutic interventions were rarely considered as predictors in the prognostic models, although these can affect the accuracy and transportability of models [33].</p>
        <p>Variable importance scores, tuning of hyperparameters, and data preparation (that is, data preprocessing) are items a Counts are absolute numbers with column percentages in parentheses. The percentages sometimes do not add up to 100% because some studies did not report performance measure for all models prespecified.Variable importance scores, tuning of hyperparameters, and data preparation (that is, data preprocessing) are items a Counts are absolute numbers with column percentages in parentheses. The percentages sometimes do not add up to 100% because some studies did not report performance measure for all models prespecified.</p>
        <p>b We considered corrected performance only when authors stated results as such. Otherwise, performance measures were considered apparent performance by default.b We considered corrected performance only when authors stated results as such. Otherwise, performance measures were considered apparent performance by default.</p>
        <p>closely related to machine learning prediction models. We found that most studies reporting variable importance scores did not specify the calculation method. Data preparation steps (that is, data quality assessment, cleaning, transformation, reduction) were often not described in enough transparent detail. Complete-case analysis remains a popular method to handle missing values in machine learning based models. Detailed description and evaluation on how missing values were handled in our included studies has been provided elsewhere [34]. Last, only one-third of models reported their hyperparameters settings, which is needed for reproducibility purposes.closely related to machine learning prediction models. We found that most studies reporting variable importance scores did not specify the calculation method. Data preparation steps (that is, data quality assessment, cleaning, transformation, reduction) were often not described in enough transparent detail. Complete-case analysis remains a popular method to handle missing values in machine learning based models. Detailed description and evaluation on how missing values were handled in our included studies has been provided elsewhere [34]. Last, only one-third of models reported their hyperparameters settings, which is needed for reproducibility purposes.</p>
        <p>Although regression methods were not our focus (as we did not define them to be machine learning methods), other reviews including both approaches show similar issues with methodological conduct and reporting [12,35e37]. Missing data, sample size, calibration, and model availability remain largely neglected aspects [7,12,37e40]. A review looking at the trends of prediction models using electronic health records (EHR) observed an increase in the use of ensemble models from 6% to 19% [41]. Another detailed review on prediction models for hospital readmission shows that the use of algorithms such as SVM, RF, and NN increased from none to 38% over the last 5 years [10]. Methods to correct for class imbalance in datasets concerning EHR increased from 7% to 13% [41].Although regression methods were not our focus (as we did not define them to be machine learning methods), other reviews including both approaches show similar issues with methodological conduct and reporting [12,35e37]. Missing data, sample size, calibration, and model availability remain largely neglected aspects [7,12,37e40]. A review looking at the trends of prediction models using electronic health records (EHR) observed an increase in the use of ensemble models from 6% to 19% [41]. Another detailed review on prediction models for hospital readmission shows that the use of algorithms such as SVM, RF, and NN increased from none to 38% over the last 5 years [10]. Methods to correct for class imbalance in datasets concerning EHR increased from 7% to 13% [41].</p>
        <p>In this comprehensive review, we summarized the study design, data sources, modelling strategies, and reported predictive performance in a large and diverse sample of studies on clinical prediction model studies. We focused on all types of studies on clinical prediction models rather than on a specific type of outcome, population, clinical specialty, or methodological aspect. We appraised studies published almost 3 years ago and thus, it is possible that further improvements might have raised. However, improvements in methodology and reporting are usually small and slow even when longer periods are considered [42]. Hence, we believe that the results presented in this comprehensive review still largely apply to the current situation of studies on machine learning-based prediction models. Given the limited sample, our findings can be considered a representative rather than exhaustive description of studies on machine learning models.In this comprehensive review, we summarized the study design, data sources, modelling strategies, and reported predictive performance in a large and diverse sample of studies on clinical prediction model studies. We focused on all types of studies on clinical prediction models rather than on a specific type of outcome, population, clinical specialty, or methodological aspect. We appraised studies published almost 3 years ago and thus, it is possible that further improvements might have raised. However, improvements in methodology and reporting are usually small and slow even when longer periods are considered [42]. Hence, we believe that the results presented in this comprehensive review still largely apply to the current situation of studies on machine learning-based prediction models. Given the limited sample, our findings can be considered a representative rather than exhaustive description of studies on machine learning models.</p>
        <p>Our data extraction was restricted to what was reported in articles. Unfortunately, few articles reported the minimum information required by reporting guidelines, thereby hampering data extraction [23]. Furthermore, terminology differed between papers. For example, the term ''validation'' was often used to describe tuning, as well as testing (that is, internal validation). An issue already observed by a previous review of studies on deep learning models [43]. This shows the need to harmonize the terminology for critical appraisal of machine learning models [44]. Our data extraction form was based mainly on the items and signaling questions from TRIPOD and PROBAST. Although both tools were primarily developed for studies on regression-based prediction models, most items and signaling questions were largely applicable for studies on ML-based models as well.Our data extraction was restricted to what was reported in articles. Unfortunately, few articles reported the minimum information required by reporting guidelines, thereby hampering data extraction [23]. Furthermore, terminology differed between papers. For example, the term ''validation'' was often used to describe tuning, as well as testing (that is, internal validation). An issue already observed by a previous review of studies on deep learning models [43]. This shows the need to harmonize the terminology for critical appraisal of machine learning models [44]. Our data extraction form was based mainly on the items and signaling questions from TRIPOD and PROBAST. Although both tools were primarily developed for studies on regression-based prediction models, most items and signaling questions were largely applicable for studies on ML-based models as well.</p>
        <p>In our sample, it is questionable whether studies ultimately aimed to improve clinical care [45]. Aim, clinical workflow, outcome format, prediction horizon, and clinically relevant performance metrics received very little attention. The importance of applying optimal methodology and transparent reporting in studies on prediction models has been intensively and extensively stressed by guidelines and meta-epidemiological studies [46e48]. Researchers can benefit from TRIPOD and PROBAST, as these provide guidance on best practices for prediction model study design, conduct and reporting regardless of their modelling technique [16,17,46,47]. However, special attention is required on extending the recommendations to include areas such as data preparation, tunability, fairness, and data leakage. In this review, we have provided evidence on the use and reporting of methods to correct for class imbalance, data preparation, data splitting, and hyperparameter optimization. PROBAST-AI and TRIPOD-AI, both extensions to artificial intelligence (AI) or machine learning based prediction models are underway [44,49]. As machine learning continues to emerge as a relevant player in healthcare, we recommend researchers and editors to reinforce a minimum standard on methodological conduct and reporting to ensure further transportability [16,17,46,47].In our sample, it is questionable whether studies ultimately aimed to improve clinical care [45]. Aim, clinical workflow, outcome format, prediction horizon, and clinically relevant performance metrics received very little attention. The importance of applying optimal methodology and transparent reporting in studies on prediction models has been intensively and extensively stressed by guidelines and meta-epidemiological studies [46e48]. Researchers can benefit from TRIPOD and PROBAST, as these provide guidance on best practices for prediction model study design, conduct and reporting regardless of their modelling technique [16,17,46,47]. However, special attention is required on extending the recommendations to include areas such as data preparation, tunability, fairness, and data leakage. In this review, we have provided evidence on the use and reporting of methods to correct for class imbalance, data preparation, data splitting, and hyperparameter optimization. PROBAST-AI and TRIPOD-AI, both extensions to artificial intelligence (AI) or machine learning based prediction models are underway [44,49]. As machine learning continues to emerge as a relevant player in healthcare, we recommend researchers and editors to reinforce a minimum standard on methodological conduct and reporting to ensure further transportability [16,17,46,47].</p>
        <p>We identified that studies covering the general population (for example, for personalized screening), primary care settings, and time-to-event outcomes are underrepresented in current research. Similarly, only a relatively small proportion of the studies evaluated (validated) their prediction model on a different dataset (that is, external validation) [50]. In addition, the poor availability of the developed models hampers further independent validation, an important step before their implementation in clinical practice. Sharing the code and ultimately the clinical prediction model is a fundamental step to create trustworthiness on AI and machine learning for clinical application [51].We identified that studies covering the general population (for example, for personalized screening), primary care settings, and time-to-event outcomes are underrepresented in current research. Similarly, only a relatively small proportion of the studies evaluated (validated) their prediction model on a different dataset (that is, external validation) [50]. In addition, the poor availability of the developed models hampers further independent validation, an important step before their implementation in clinical practice. Sharing the code and ultimately the clinical prediction model is a fundamental step to create trustworthiness on AI and machine learning for clinical application [51].</p>
        <p>Our study provides a comprehensive overview of the applied study designs, data sources, modelling steps, and performance measures used. Special focus is required in areas such as handling of missing values, methods for internal validation, and reporting of calibration to improve the methodological conduct of studies on prediction models developed using machine learning techniques.Our study provides a comprehensive overview of the applied study designs, data sources, modelling steps, and performance measures used. Special focus is required in areas such as handling of missing values, methods for internal validation, and reporting of calibration to improve the methodological conduct of studies on prediction models developed using machine learning techniques.</p>
        <p>-Non-humans (n=17) -No prediction model studies (n=65) -Only conventional statistical techniques (n=36) -Unsupervised Machine Learning (n=27) -Publication type (n=12) -Language limitation (n=2) -No full-text available (n=1) 10 random samples of 249 records Diagnosis (n= 58, 38.2%) Fig. 1. Flowchart of included studies. (n 5 20/152, 13.5% [95% CI 8.7e19.5), and neurology (n 5 20/152, 13.5% [95% CI 8.7e19.5]). Most articles originated from North America (n 5 59/152, 38.8% [95% CI 31.4e46.7]), followed by Asia (n 5 46/152, 30.3% [95% CI 23.5e38]) and Europe (n 5 37/152, 24.3% [95% CI 18.2e31.7]). Half of the studies had a first author-Non-humans (n=17) -No prediction model studies (n=65) -Only conventional statistical techniques (n=36) -Unsupervised Machine Learning (n=27) -Publication type (n=12) -Language limitation (n=2) -No full-text available (n=1) 10 random samples of 249 records Diagnosis (n= 58, 38.2%) Fig. 1. Flowchart of included studies. (n 5 20/152, 13.5% [95% CI 8.7e19.5), and neurology (n 5 20/152, 13.5% [95% CI 8.7e19.5]). Most articles originated from North America (n 5 59/152, 38.8% [95% CI 31.4e46.7]), followed by Asia (n 5 46/152, 30.3% [95% CI 23.5e38]) and Europe (n 5 37/152, 24.3% [95% CI 18.2e31.7]). Half of the studies had a first author</p>
        <p>b As data preparation.b As data preparation.</p>
        <p>29.6e44.7]29.6e44.7]</p>
        <p>). To handle missing values, complete-case analysis was the most common method (n 5 30/152, 19.7% [95% CI). To handle missing values, complete-case analysis was the most common method (n 5 30/152, 19.7% [95% CI</p>
        <p>14.2e26.8]14.2e26.8]</p>
        <p>). Other methods were median imputation (n 5 10/152, 6.6% [95% CI 3.6e11.7), multiple). Other methods were median imputation (n 5 10/152, 6.6% [95% CI 3.6e11.7), multiple</p>
        <p>a External validation was performed in 19 studies. b Combines all internal validation methods, for example, split sample, cross-validation, bootstrapping. c For model development.a External validation was performed in 19 studies. b Combines all internal validation methods, for example, split sample, cross-validation, bootstrapping. c For model development.</p>
        <p>C.L. Andaur Navarro et al. / Journal of Clinical Epidemiology 154 (2023) 8e22C.L. Andaur Navarro et al. / Journal of Clinical Epidemiology 154 (2023) 8e22</p>
        <p>(79.6) [72.5e85.2] a Counts are absolute numbers with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies reported more than one option. b This includes length of stay, medication dose, patient's disposition, order type, lesion extension, laboratory results, cancer stage, treatment option, attendance, equipment usage, operative time. c Guidelines for developing and reporting machine learning models in biomedical research (n 5 2), STARD (n 5 2), BRISQ (n 5 1). d This includes simplified scoring rule, chart, nomogram, online calculator, or worked examples.(79.6) [72.5e85.2] a Counts are absolute numbers with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies reported more than one option. b This includes length of stay, medication dose, patient's disposition, order type, lesion extension, laboratory results, cancer stage, treatment option, attendance, equipment usage, operative time. c Guidelines for developing and reporting machine learning models in biomedical research (n 5 2), STARD (n 5 2), BRISQ (n 5 1). d This includes simplified scoring rule, chart, nomogram, online calculator, or worked examples.</p>
        <p>The authors would like to thank and acknowledge the support of Ren e Spijker, information specialist. The peerreviewers are thanked for critically reading the manuscript and suggesting substantial improvements.The authors would like to thank and acknowledge the support of Ren e Spijker, information specialist. The peerreviewers are thanked for critically reading the manuscript and suggesting substantial improvements.</p>
        <p>GSC is funded by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC) and by Cancer Research UK program grant (C49297/A27294). PD is funded by the NIHR Oxford BRC. RB is affiliated to the National Institute for Health and Care Research (NIHR) Applied Research Collaboration (ARC) West Midlands. The views expressed are those of the authors and not necessarily those of the NHS, NIHR, or Department of Health and Social Care. None of the funding sources had a role in the design, conduct, analyses, or reporting of the study or in the decision to submit the manuscript for publication.GSC is funded by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC) and by Cancer Research UK program grant (C49297/A27294). PD is funded by the NIHR Oxford BRC. RB is affiliated to the National Institute for Health and Care Research (NIHR) Applied Research Collaboration (ARC) West Midlands. The views expressed are those of the authors and not necessarily those of the NHS, NIHR, or Department of Health and Social Care. None of the funding sources had a role in the design, conduct, analyses, or reporting of the study or in the decision to submit the manuscript for publication.</p>
        <p>Registration and protocol: This review was registered in PROSPERO (CRD42019161764). The study protocol can be accessed in https://doi. org/10.1136/bmjopen-2020-038832.Registration and protocol: This review was registered in PROSPERO (CRD42019161764). The study protocol can be accessed in https://doi. org/10.1136/bmjopen-2020-038832.</p>
        <p>Articles that support our findings are publicly available. Template data collection forms, detailed data extraction on all included studies, and analytical code are available upon reasonable request.Articles that support our findings are publicly available. Template data collection forms, detailed data extraction on all included studies, and analytical code are available upon reasonable request.</p>
        <p>Counts are absolute numbers, with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies can report more than one measure.Counts are absolute numbers, with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies can report more than one measure.</p>
        <p>validation based on temporal differences on the inclusion of participants. Seven studies (36.8% [95% CI 19.1e59]) reported differences and similarities in definitions between the development and validation data.validation based on temporal differences on the inclusion of participants. Seven studies (36.8% [95% CI 19.1e59]) reported differences and similarities in definitions between the development and validation data.</p>
        <p>Some studies shared their prediction model either as a web-calculator or worked example (n 5 31/152, 20.4% [95% CI 14.8e27.5]). Furthermore, in a minority of studies datasets and code were accessible through repositories, which were shared as supplemental material (n 5 18/152, 11.8% [95% CI 7.6e17.9]; n 5 13/152, 8.6% [95% CI 5.1e14.1]). Details in Table 1.Some studies shared their prediction model either as a web-calculator or worked example (n 5 31/152, 20.4% [95% CI 14.8e27.5]). Furthermore, in a minority of studies datasets and code were accessible through repositories, which were shared as supplemental material (n 5 18/152, 11.8% [95% CI 7.6e17.9]; n 5 13/152, 8.6% [95% CI 5.1e14.1]). Details in Table 1.</p>
        <p>In this study, we evaluated the study design, data sources, modelling steps, and performance measures in studies Abbreviations: DEV, developed model; VAL, validation; AUC-ROC, area under the receiver operation characteristic curve; NRI, net reclassification index; IDI, integrated discrimination improvement; AUPR, area under the precision-recall curve; CI, confidence interval.In this study, we evaluated the study design, data sources, modelling steps, and performance measures in studies Abbreviations: DEV, developed model; VAL, validation; AUC-ROC, area under the receiver operation characteristic curve; NRI, net reclassification index; IDI, integrated discrimination improvement; AUPR, area under the precision-recall curve; CI, confidence interval.</p>
        <p>a Counts are absolute numbers, with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies can report more than one performance measure.a Counts are absolute numbers, with column percentages in parentheses. The percentages sometimes do not add up to 100% because studies can report more than one performance measure.</p>
        <p>b This includes models reporting positive predictive value as precision. c This includes models reporting balance accuracy.b This includes models reporting positive predictive value as precision. c This includes models reporting balance accuracy.</p>
        <p>Supplementary data to this article can be found online at https://doi.org/10.1016/j.jclinepi.2022.11.015.Supplementary data to this article can be found online at https://doi.org/10.1016/j.jclinepi.2022.11.015.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T12:45+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Self-Adaptive Learnable Transforms Tongle Wu, Bin Gao, Senior Member, IEEE, Jicong Fan, Jize Xue, and W.L. Woo, Senior Member, IEEE Abstract-The tensor nuclear norm (TNN), defined as the sum of nuclear norms of frontal slices of the tensor in a frequency domain, has been found useful in solving low rank tensor recovery problems. Existing TNN-based methods use either fixed or dataindependent transformations, which may not be the optimal choices for the given tensors. As the consequence, these methods cannot exploit the potential low rank structure of tensor data adaptively. In this paper, we propose a framework called selfadaptive learnable transform (SALT) to learn a transformation matrix from the given tensor. Specifically, SALT aims to learn a lossless transformation that induces a lower average-rank tensor, where the Schatten-p quasi-norm is used as the rank proxy. Then because SALT is less sensitive to the orientation, we generalize SALT to other dimensions of tensor (SALTS), namely learning three self-adaptive transformation matrices simultaneously from given tensor. SALTS is able to adaptively exploit the potential low rank structures in all directions.We provide a unified optimization framework based on alternating direction multiplier method for SALTS model and theoretically prove the weak convergence property of proposed algorithm. Experimental results in hyperspectral image (HSI), color video, Magnetic Resonance imaging (MRI), and COIL-20 datasets show that SALTS is much more accurate in tensor completion than existing methods. Demo code can be found at https://faculty.uestc.edu.cn/gaobin/zh_ CN/lwcg/153392/list/index.htm.Self-Adaptive Learnable Transforms Tongle Wu, Bin Gao, Senior Member, IEEE, Jicong Fan, Jize Xue, and W.L. Woo, Senior Member, IEEE Abstract-The tensor nuclear norm (TNN), defined as the sum of nuclear norms of frontal slices of the tensor in a frequency domain, has been found useful in solving low rank tensor recovery problems. Existing TNN-based methods use either fixed or dataindependent transformations, which may not be the optimal choices for the given tensors. As the consequence, these methods cannot exploit the potential low rank structure of tensor data adaptively. In this paper, we propose a framework called selfadaptive learnable transform (SALT) to learn a transformation matrix from the given tensor. Specifically, SALT aims to learn a lossless transformation that induces a lower average-rank tensor, where the Schatten-p quasi-norm is used as the rank proxy. Then because SALT is less sensitive to the orientation, we generalize SALT to other dimensions of tensor (SALTS), namely learning three self-adaptive transformation matrices simultaneously from given tensor. SALTS is able to adaptively exploit the potential low rank structures in all directions.We provide a unified optimization framework based on alternating direction multiplier method for SALTS model and theoretically prove the weak convergence property of proposed algorithm. Experimental results in hyperspectral image (HSI), color video, Magnetic Resonance imaging (MRI), and COIL-20 datasets show that SALTS is much more accurate in tensor completion than existing methods. Demo code can be found at https://faculty.uestc.edu.cn/gaobin/zh_ CN/lwcg/153392/list/index.htm.</p>
        <p>Index Terms-self-adaptive, learnable transform, low rank, tensor completionIndex Terms-self-adaptive, learnable transform, low rank, tensor completion</p>
        <p>A S a higher-order extension of matrix, N -th tensor (N ≥ 3) is a useful mathematical form to represent a multiway array along N modes and has attracted considerable attention involving tasks of collection, processing, and analysis of highdimensional data. It is essential to investigate robust and efficient algorithms to deal with tensors. The tensors often have low rank structures in many areas such as computer vision and neuroscience, which reveal that the data samples are generally embedded in low-dimensional manifolds [1]. A number of tensor methods utilizing the low rank structures have been successfully applied to practical problems [2], [3] such as color image and video processing [4]- [9], hyperspectral data recovery [10]- [13], collaborative filtering [14], recommender system design, [15] clustering, etc [16], [17].A S a higher-order extension of matrix, N -th tensor (N ≥ 3) is a useful mathematical form to represent a multiway array along N modes and has attracted considerable attention involving tasks of collection, processing, and analysis of highdimensional data. It is essential to investigate robust and efficient algorithms to deal with tensors. The tensors often have low rank structures in many areas such as computer vision and neuroscience, which reveal that the data samples are generally embedded in low-dimensional manifolds [1]. A number of tensor methods utilizing the low rank structures have been successfully applied to practical problems [2], [3] such as color image and video processing [4]- [9], hyperspectral data recovery [10]- [13], collaborative filtering [14], recommender system design, [15] clustering, etc [16], [17].</p>
        <p>There are various definitions of tensor rank originating from different tensor decomposition frameworks [18], which means that the rank of tensor is different from the rank of matrix. It is well known that the CP rank [19] and Tucker rank [20] based on the CANDECOMP/PARAFAC(CP) [21] decomposition and Tucker decomposition [22] respectively. These two kinds of tensor decompositions have been widely investigated and have achieved competitive performance in low rank tensor recovery. However, computing the CP rank is NP-hard [23], [24] and computing the Tucker rank requires unfolding tensors along each mode into matrices directly, which may destroy the intrinsic high-order interactive information.There are various definitions of tensor rank originating from different tensor decomposition frameworks [18], which means that the rank of tensor is different from the rank of matrix. It is well known that the CP rank [19] and Tucker rank [20] based on the CANDECOMP/PARAFAC(CP) [21] decomposition and Tucker decomposition [22] respectively. These two kinds of tensor decompositions have been widely investigated and have achieved competitive performance in low rank tensor recovery. However, computing the CP rank is NP-hard [23], [24] and computing the Tucker rank requires unfolding tensors along each mode into matrices directly, which may destroy the intrinsic high-order interactive information.</p>
        <p>An intuitive extension of matrix singular value decomposition (SVD) for tensor has been proposed for third-order tensor , which is known as tensor singular value decomposition (t-SVD) [25]. t-SVD was initially derived from the novel definition of tensor-tensor(t-t) product [26], which could operate integral third-order tensor rather than reshape the tensor into matrices [27]. Subsequently Kernfeld [28] discovered that tt product based on circular convolution operator could be transformed into matrix-matrix product by Discrete Fourier Transform (DFT), which generates a transformed tensor in the frequency domain using DFT along the third mode. Based on the DFT transformed t-SVD, Zhang et al. [29], [30] proposed novel methods for multilinear data completion and denoising based on minimizing the tensor nuclear norm (TNN). Lu et al. [31] defined the tensor average rank and proved that a tensor always has low average rank if it has low tubal rank, which was defined in [28]. Lu et al. also revealed that is the convex envelope of the tensor average rank within the unit ball of the tensor spectral norm and proved the theoretical guarantee for the exact recovery in tensor robust principal component analysis (TRPCA). Following the DFT transformed t-SVD, a few variations of TNN such as weight TNN [32], partial sum of TNN [33], Schatten-p norm TNN [34], p-shrinkage TNN [35], tensor spectral k-support norm [36], have been proposed and tensor factorization methods based on t-t product derived from DFT have also been investigated in [17], [37]- [39].An intuitive extension of matrix singular value decomposition (SVD) for tensor has been proposed for third-order tensor , which is known as tensor singular value decomposition (t-SVD) [25]. t-SVD was initially derived from the novel definition of tensor-tensor(t-t) product [26], which could operate integral third-order tensor rather than reshape the tensor into matrices [27]. Subsequently Kernfeld [28] discovered that tt product based on circular convolution operator could be transformed into matrix-matrix product by Discrete Fourier Transform (DFT), which generates a transformed tensor in the frequency domain using DFT along the third mode. Based on the DFT transformed t-SVD, Zhang et al. [29], [30] proposed novel methods for multilinear data completion and denoising based on minimizing the tensor nuclear norm (TNN). Lu et al. [31] defined the tensor average rank and proved that a tensor always has low average rank if it has low tubal rank, which was defined in [28]. Lu et al. also revealed that is the convex envelope of the tensor average rank within the unit ball of the tensor spectral norm and proved the theoretical guarantee for the exact recovery in tensor robust principal component analysis (TRPCA). Following the DFT transformed t-SVD, a few variations of TNN such as weight TNN [32], partial sum of TNN [33], Schatten-p norm TNN [34], p-shrinkage TNN [35], tensor spectral k-support norm [36], have been proposed and tensor factorization methods based on t-t product derived from DFT have also been investigated in [17], [37]- [39].</p>
        <p>In fact, three-dimensional data such as color images and videos always possess a notable 'spatial-shifting' correlation pattern and make similar spatial variation of data along temporal orientation smooth, which makes the DFT obtain low rankness in the frequency domain. [39], [40]. In this case, TNN based on fixed DFT transformation may be sensitive to data that violates the smooth pattern along the third mode. Fig. 1 shows the PSNR of different videos varying with the measurement of smoothness along frame direction. It can be observed that the less degree of smoothness the video has, the lower PSNR value tends to obtain. This phenomenon explains that the effect of completion for TNN is dependent on the smoothness of original data.In fact, three-dimensional data such as color images and videos always possess a notable 'spatial-shifting' correlation pattern and make similar spatial variation of data along temporal orientation smooth, which makes the DFT obtain low rankness in the frequency domain. [39], [40]. In this case, TNN based on fixed DFT transformation may be sensitive to data that violates the smooth pattern along the third mode. Fig. 1 shows the PSNR of different videos varying with the measurement of smoothness along frame direction. It can be observed that the less degree of smoothness the video has, the lower PSNR value tends to obtain. This phenomenon explains that the effect of completion for TNN is dependent on the smoothness of original data.</p>
        <p>More specifically, for the data that is not smooth enough, the frequency domain tensor obtained by DFT does not satisfy fair low rank property, so the effect of completion based on low rank tensor methods will be degraded. The simple case is illustrated in Lemma 1, which shows a kind of tensors with low rank under identity transformation while with full rank under DFT transformation. In order to solve this problem, it is expected to replace the fixed DFT with other transforms that are not fixed but could be adaptively learned from the given data. Besides the fixed DFT transformations, another limitation of TNN is that the transformation is performed only along the third mode. Thus the orientation-sensitive transformation fails to capture the informative intra-mode and inter-mode correlations in other two modes. Obviously, the limitation becomes more significant for higher-order (e.g. N ≥ 4) tensors. In this paper, our contributions are summarized as follows, which aims to overcome these limitationsMore specifically, for the data that is not smooth enough, the frequency domain tensor obtained by DFT does not satisfy fair low rank property, so the effect of completion based on low rank tensor methods will be degraded. The simple case is illustrated in Lemma 1, which shows a kind of tensors with low rank under identity transformation while with full rank under DFT transformation. In order to solve this problem, it is expected to replace the fixed DFT with other transforms that are not fixed but could be adaptively learned from the given data. Besides the fixed DFT transformations, another limitation of TNN is that the transformation is performed only along the third mode. Thus the orientation-sensitive transformation fails to capture the informative intra-mode and inter-mode correlations in other two modes. Obviously, the limitation becomes more significant for higher-order (e.g. N ≥ 4) tensors. In this paper, our contributions are summarized as follows, which aims to overcome these limitations</p>
        <p>• We propose to learn a adaptive transformation matrix from the given tensor data via minimizing the sum of the Schatten-p quasi-norms of the frontal slices of the transformed tensor. The learned transformation can induce a minimum-rank tensor in the transformation domain and hence has powerful potential in low rank tensor recovery. • Due to the adaptive merit of learnable transformation, we generalize self-adaptive learnable transform (SALT) to the other modes of a given tensor to fully exploit the low rank, namely SALTS. SALTS overcomes the disadvantage of orientation sensitivity in TNN. More importantly, SALTS shows superior performance in tensor completion. • We develop an efficient alternating direction multiplier method (ADMM) to solve the optimization of proposed model for tensor completion, and provide a weak convergence guarantee for the proposed algorithm, which is also validated by the numerical results. Results in synthetic data and several real datasets have further shown that the self-adaptive learnable transform based TNN achieves the state-of-the-art performance. The rest of this paper is structured as follows. Section II introduces the notations, preliminaries , and related works. Section III presents the proposed method SALTS. Section IV consists of the experiments. Finally, we conclude the work in Section V.• We propose to learn a adaptive transformation matrix from the given tensor data via minimizing the sum of the Schatten-p quasi-norms of the frontal slices of the transformed tensor. The learned transformation can induce a minimum-rank tensor in the transformation domain and hence has powerful potential in low rank tensor recovery. • Due to the adaptive merit of learnable transformation, we generalize self-adaptive learnable transform (SALT) to the other modes of a given tensor to fully exploit the low rank, namely SALTS. SALTS overcomes the disadvantage of orientation sensitivity in TNN. More importantly, SALTS shows superior performance in tensor completion. • We develop an efficient alternating direction multiplier method (ADMM) to solve the optimization of proposed model for tensor completion, and provide a weak convergence guarantee for the proposed algorithm, which is also validated by the numerical results. Results in synthetic data and several real datasets have further shown that the self-adaptive learnable transform based TNN achieves the state-of-the-art performance. The rest of this paper is structured as follows. Section II introduces the notations, preliminaries , and related works. Section III presents the proposed method SALTS. Section IV consists of the experiments. Finally, we conclude the work in Section V.</p>
        <p>In this section, we show the notations, definitions, and preliminaries which are necessary to present the proposed methods in this paper.In this section, we show the notations, definitions, and preliminaries which are necessary to present the proposed methods in this paper.</p>
        <p>We use a, a, A, A to denote scalar, vector, matrix , and tensor, respectively. The fields of real numbers and complex numbers are denoted as R and C, respectively. Third-order tensors have column, row, and tube fibers, which are denoted Fig. 1: The relationship between the effect (PSNR) of completion and smoothness along the dimension of video's frame with sampling rate 0.3. The test videos are listed in the order: 1: Akiyo; 2: Bridge_close; 3: Carphone ; 4: Coastguard; 5: Suzie; 6: Miss-America in YUV data. The smoothness is defined as:We use a, a, A, A to denote scalar, vector, matrix , and tensor, respectively. The fields of real numbers and complex numbers are denoted as R and C, respectively. Third-order tensors have column, row, and tube fibers, which are denoted Fig. 1: The relationship between the effect (PSNR) of completion and smoothness along the dimension of video's frame with sampling rate 0.3. The test videos are listed in the order: 1: Akiyo; 2: Bridge_close; 3: Carphone ; 4: Coastguard; 5: Suzie; 6: Miss-America in YUV data. The smoothness is defined as:</p>
        <p>by A :ij , A i:j , A ij: , respectively. The i-th horizontal, lateral, and frontal slice of a third-order tensor, are presented as A i:: , A :i: , A ::i , respectively. Specially, the i-th frontal slice can also be denoted as A (n) .by A :ij , A i:j , A ij: , respectively. The i-th horizontal, lateral, and frontal slice of a third-order tensor, are presented as A i:: , A :i: , A ::i , respectively. Specially, the i-th frontal slice can also be denoted as A (n) .</p>
        <p>The 1 -norm and ∞ norm of a tensor A n1×n2×n3 is defined asThe 1 -norm and ∞ norm of a tensor A n1×n2×n3 is defined as</p>
        <p>The inner product of two tensors is defined as A, B = ijk A ijk B ijk .The Frobenius norm of tensor is defined asThe inner product of two tensors is defined as A, B = ijk A ijk B ijk .The Frobenius norm of tensor is defined as</p>
        <p>to denote the nuclear norm of matrix A, where σ i (A) is the i-th singular values of A in descent order. The Schatten-p (quasi) norm of matrix is defined asto denote the nuclear norm of matrix A, where σ i (A) is the i-th singular values of A in descent order. The Schatten-p (quasi) norm of matrix is defined as</p>
        <p>, where p &gt; 0., where p &gt; 0.</p>
        <p>a = diag(A), which means a(i) = A ii . A = diag(a), denotes that A ii = a(i) and other elements of A is 0. The symbol ⊗ stands for the Kroneck product. The moden product of a tensora = diag(A), which means a(i) = A ii . A = diag(a), denotes that A ii = a(i) and other elements of A is 0. The symbol ⊗ stands for the Kroneck product. The moden product of a tensor</p>
        <p>Then mode-n product could be represented asThen mode-n product could be represented as</p>
        <p>Definition 1. (Trace norm (SNN)) [18] The trace norm of tensor is sum of nuclear norms (SNN), defined as:Definition 1. (Trace norm (SNN)) [18] The trace norm of tensor is sum of nuclear norms (SNN), defined as:</p>
        <p>denotes performing the DFT on all the tubes of A ∈ R n1×n2×n3 , whose mode-n expression is A = A × 3 F n3 . The inverse fft on A can turn into original tensor, i.e. A=ifft(A, [ ], 3). Definition 2. (Tensor Average Rank) [31] For A ∈ R n1×n2×n3 , the tensor average rank, denoted as Rank a (A) is defined as:denotes performing the DFT on all the tubes of A ∈ R n1×n2×n3 , whose mode-n expression is A = A × 3 F n3 . The inverse fft on A can turn into original tensor, i.e. A=ifft(A, [ ], 3). Definition 2. (Tensor Average Rank) [31] For A ∈ R n1×n2×n3 , the tensor average rank, denoted as Rank a (A) is defined as:</p>
        <p>Definition 3. (Tensor Nuclear Norm) [29] Tensor nuclear norm (TNN) of A ∈ R n1×n2×n3 is sum of nuclear norm of frontal slice matrices of DFT transformed tensor A defined as:Definition 3. (Tensor Nuclear Norm) [29] Tensor nuclear norm (TNN) of A ∈ R n1×n2×n3 is sum of nuclear norm of frontal slice matrices of DFT transformed tensor A defined as:</p>
        <p>Definition 4. (Transform Induced Tensor Average Rank) [41] Let L be any invertible linear transform and satisfies L T L = LL T = lI n3 , tensor average rank of A ∈ R n1×n2×n3 induced by L is sum of rank of frontal slices of transformed tensor A L = A × 3 L defined as:Definition 4. (Transform Induced Tensor Average Rank) [41] Let L be any invertible linear transform and satisfies L T L = LL T = lI n3 , tensor average rank of A ∈ R n1×n2×n3 induced by L is sum of rank of frontal slices of transformed tensor A L = A × 3 L defined as:</p>
        <p>Definition 5. (Transform Induced Tensor Nuclear Norm) [41] Let L be any invertible linear transform and satisfies L T L = LL T = lI n3 , tensor nuclear norm (TNN) of A ∈ R n1×n2×n3 induced by L is sum of nuclear norm of frontal sliced matrices of transformed tensor A L = A × 3 L defined as:Definition 5. (Transform Induced Tensor Nuclear Norm) [41] Let L be any invertible linear transform and satisfies L T L = LL T = lI n3 , tensor nuclear norm (TNN) of A ∈ R n1×n2×n3 induced by L is sum of nuclear norm of frontal sliced matrices of transformed tensor A L = A × 3 L defined as:</p>
        <p>Although there are many low rank methods for tensor completion, the intuition of proposed SALTS is derived from t-SVD decomposition. Thus, the introduction of related works mainly concentrates on t-SVD based low rank tensor completion.Although there are many low rank methods for tensor completion, the intuition of proposed SALTS is derived from t-SVD decomposition. Thus, the introduction of related works mainly concentrates on t-SVD based low rank tensor completion.</p>
        <p>Zhang et al. [29], [30] proposed novel methods for multilinear data completion based on minimizing the tensor nuclear norm. Liu et.al [39] proposed fast Tubal-AltMin for the low-tubal-rank tensor completion problem and given the theoretical performance guarantee. Wang et al. [42] defined the generalized tensor Dantzig selector to recover a low-tubal-rank tensor from noisy linear measurements. Hou et al. [43] studied low-tubal-rank tensor recovery from binary measurements. Jiang [44] provided convex method for low-tubal-rank tensor completion with provable theoretical guarantee. Sun et al. [45] defined a novel generalized tensor tubal rank to tensor completion. Zhang et al. [46] given the minimal number of linear observations to reconstruct the low tubal rank tensor. Wang et al. [47] considered the structural difference between the observed data and missing data under t-SVD decomposition.Zhang et al. [29], [30] proposed novel methods for multilinear data completion based on minimizing the tensor nuclear norm. Liu et.al [39] proposed fast Tubal-AltMin for the low-tubal-rank tensor completion problem and given the theoretical performance guarantee. Wang et al. [42] defined the generalized tensor Dantzig selector to recover a low-tubal-rank tensor from noisy linear measurements. Hou et al. [43] studied low-tubal-rank tensor recovery from binary measurements. Jiang [44] provided convex method for low-tubal-rank tensor completion with provable theoretical guarantee. Sun et al. [45] defined a novel generalized tensor tubal rank to tensor completion. Zhang et al. [46] given the minimal number of linear observations to reconstruct the low tubal rank tensor. Wang et al. [47] considered the structural difference between the observed data and missing data under t-SVD decomposition.</p>
        <p>In fact, Kernfeld et al. [28] advocated that the t-t product can be modified to be equipped with any invertible transform which can replace the DFT transform with any invertible transform along the third mode. Following this research line, researchers in [48], [49] adopted Discrete Cosine Transform (DCT) which achieved superior performance than DFT in tensor completion. They proposed that any TNN induced from any invertible transform can be applied in TRPCA [50] and low rank tensor completion with exact recovery [41], but they did not give the specific transform. Authors in [51] proposed that unitary transform like Haar Wavelet transform could obtain sufficient low rank tensor in transformation domain. Recently, Jiang et al. [52] proposed framelet F-TNN that used the framelet representation of each tube to construct framelet transformed tensor. They advocated that the representation of each tube is represented sparsely due to redundancy of framelet basis. Then Jiang et al. [53] proposed low rank coefficient based on learnable dictionary derived from t-SVD framework. Although Kong et al. [54] have defined new tensor Q-rank based on novel data-dependent transformation for tensor completion, their data-dependent transformation can be learnt by principal component analysis, which is different with the proposed SALTS. In addition, they have not extended Q-rank to multi-modes and was only utilized in one dimension.In fact, Kernfeld et al. [28] advocated that the t-t product can be modified to be equipped with any invertible transform which can replace the DFT transform with any invertible transform along the third mode. Following this research line, researchers in [48], [49] adopted Discrete Cosine Transform (DCT) which achieved superior performance than DFT in tensor completion. They proposed that any TNN induced from any invertible transform can be applied in TRPCA [50] and low rank tensor completion with exact recovery [41], but they did not give the specific transform. Authors in [51] proposed that unitary transform like Haar Wavelet transform could obtain sufficient low rank tensor in transformation domain. Recently, Jiang et al. [52] proposed framelet F-TNN that used the framelet representation of each tube to construct framelet transformed tensor. They advocated that the representation of each tube is represented sparsely due to redundancy of framelet basis. Then Jiang et al. [53] proposed low rank coefficient based on learnable dictionary derived from t-SVD framework. Although Kong et al. [54] have defined new tensor Q-rank based on novel data-dependent transformation for tensor completion, their data-dependent transformation can be learnt by principal component analysis, which is different with the proposed SALTS. In addition, they have not extended Q-rank to multi-modes and was only utilized in one dimension.</p>
        <p>In this section, we provide the following Lemma 1 to show an example of the limitation when minimizing TNN of vanilla t-SVD with fixed Discrete Fourier Transform.In this section, we provide the following Lemma 1 to show an example of the limitation when minimizing TNN of vanilla t-SVD with fixed Discrete Fourier Transform.</p>
        <p>Lemma 1. For the set S = {Rank(X (:, :, k)) = 1, ∀k ∈ [n]|X ∈ C n×n×n }, there always exists a tensor X ∈ S, which satisfiesLemma 1. For the set S = {Rank(X (:, :, k)) = 1, ∀k ∈ [n]|X ∈ C n×n×n }, there always exists a tensor X ∈ S, which satisfies</p>
        <p>which means there always exists tensors ∈ C n×n×n , whose rank of each frontal slice is 1, while rank of each frontal slice of DFT transformed tensor is full n.which means there always exists tensors ∈ C n×n×n , whose rank of each frontal slice is 1, while rank of each frontal slice of DFT transformed tensor is full n.</p>
        <p>Proof. Our proof is constructive. Denote A as the random full rank matrix ∈ R n×n . Then we construct a target tensor from frequency domain, X is constructed as follows:Proof. Our proof is constructive. Denote A as the random full rank matrix ∈ R n×n . Then we construct a target tensor from frequency domain, X is constructed as follows:</p>
        <p>It can be observed that vectorization of each frontal slice of X is corresponding to each row of A, which equals cyclic queues of A. So we could get Rank aIt can be observed that vectorization of each frontal slice of X is corresponding to each row of A, which equals cyclic queues of A. So we could get Rank a</p>
        <p>n. Then we should verify that the tensor X in the frequency domain constructed above as X is in the set S by the inverse Fourier transform.n. Then we should verify that the tensor X in the frequency domain constructed above as X is in the set S by the inverse Fourier transform.</p>
        <p>Defining a new tensor T ∈ R 1×n×n whose frontal slice isDefining a new tensor T ∈ R 1×n×n whose frontal slice is</p>
        <p>Then we could get X (3) = bcirc(T ), further the block circulant matrix can be block diagonalize:Then we could get X (3) = bcirc(T ), further the block circulant matrix can be block diagonalize:</p>
        <p>T (:, :, 2) . . .T (:, :, 2) . . .</p>
        <p>we can conclude that there exists X Rank(X (:, :we can conclude that there exists X Rank(X (:, :</p>
        <p>Lemma 1 exposes the shortcoming of fixed Discrete Fourier Transform. There exists kinds of tensors which is full average rank in transformed domain by DFT, which prevents the tensor nuclear norm(TNN) based methods [29]- [31] from these tensor sets.Lemma 1 exposes the shortcoming of fixed Discrete Fourier Transform. There exists kinds of tensors which is full average rank in transformed domain by DFT, which prevents the tensor nuclear norm(TNN) based methods [29]- [31] from these tensor sets.</p>
        <p>The novelty of this paper is to learn self-adaptive transform L and conduct the low rank tensor recovery simultaneously under a unified optimization framework, which is achieved by minimizing Rank L t induced by L. Concretely, we learn L and low rank induced tensor as follows:The novelty of this paper is to learn self-adaptive transform L and conduct the low rank tensor recovery simultaneously under a unified optimization framework, which is achieved by minimizing Rank L t induced by L. Concretely, we learn L and low rank induced tensor as follows:</p>
        <p>where C y and C z are constraint sets. In (13), we aim to learn a transform from the data itself to minimize the SALT induced rank of Y plus a general function on Y and Z. More specifically, Y denotes the structured tensor which needs to be solved by (13) where Z is the observed corrupted tensor data. Our intention is to illustrate that model ( 13) can be employed to obtain the recovered tensor Y when given the corrupted tensor Z . The first term in (13) utilizes the low rank structure prior under adaptive transform L, while the second term guarantees that recovered Y should fit the observed corrupted Z. Problem (13) covers a few important topics such as low rank tensor completion [30] and tensor robust PCA [31]. In this paper, we focus on the application of SALT in tensor completion. Given an incompletewhere C y and C z are constraint sets. In (13), we aim to learn a transform from the data itself to minimize the SALT induced rank of Y plus a general function on Y and Z. More specifically, Y denotes the structured tensor which needs to be solved by (13) where Z is the observed corrupted tensor data. Our intention is to illustrate that model ( 13) can be employed to obtain the recovered tensor Y when given the corrupted tensor Z . The first term in (13) utilizes the low rank structure prior under adaptive transform L, while the second term guarantees that recovered Y should fit the observed corrupted Z. Problem (13) covers a few important topics such as low rank tensor completion [30] and tensor robust PCA [31]. In this paper, we focus on the application of SALT in tensor completion. Given an incomplete</p>
        <p>where π Ω : R n1×n2×n3 → R n1×n2×n3 is a linear operator that keeps the entries in Ω unchanged and sets those outside Ω(i.e., in Ω c ) to zero. According to (5), we could reformulate ( 14) into:where π Ω : R n1×n2×n3 → R n1×n2×n3 is a linear operator that keeps the entries in Ω unchanged and sets those outside Ω(i.e., in Ω c ) to zero. According to (5), we could reformulate ( 14) into:</p>
        <p>It is worth noting that there are main three different points with (5). 1) In ( 5), the L is fixed, predefined and independent with tensor completion task. Our L is needed to learn by solving (15) to minimize the induced tensor average rank. Although [54] also proposed data-dependent, learnable transform L, our learning approach is different. 2) In ( 5), L is square matrix and satisfies L T L = LL T = lI n3 , however, our L is not necessarily a square matrix, the k we set may k &lt; n 3 . We only require L as column orthogonal, which would reduce the computation of multiple SVD. 3) We add another constraint that XIt is worth noting that there are main three different points with (5). 1) In ( 5), the L is fixed, predefined and independent with tensor completion task. Our L is needed to learn by solving (15) to minimize the induced tensor average rank. Although [54] also proposed data-dependent, learnable transform L, our learning approach is different. 2) In ( 5), L is square matrix and satisfies L T L = LL T = lI n3 , however, our L is not necessarily a square matrix, the k we set may k &lt; n 3 . We only require L as column orthogonal, which would reduce the computation of multiple SVD. 3) We add another constraint that X</p>
        <p>, which keeps the power and information of original tensor from being lost after being transformed by L. The last two equality constraints may not hold simultaneously in (15). The following lemma gives the condition that (15) has feasible solution, which guides the setting of k., which keeps the power and information of original tensor from being lost after being transformed by L. The last two equality constraints may not hold simultaneously in (15). The following lemma gives the condition that (15) has feasible solution, which guides the setting of k.</p>
        <p>In addition, L is easy to solve in the optimization, which will be verified later. Because of the presence of the rank function, it is NP-hard to solve (14) or (15). We use the Schatten-p quasi-norm as a surrogate function of rank and consider the following problem instead of (15):In addition, L is easy to solve in the optimization, which will be verified later. Because of the presence of the rank function, it is NP-hard to solve (14) or (15). We use the Schatten-p quasi-norm as a surrogate function of rank and consider the following problem instead of (15):</p>
        <p>This is similar to the definition of t-Schatten-p norm in [34] while their t-Schatten-p norm is based on DFT framework which is different from our SALT paradigm. In addition, the proposed optimization approach to minimizing the SALT is different with theirs. We choose 0 &lt; p ≤ 1 in our model since this kind of non-convex norm can achieve a tighter bound in approximating the tensor multi-rank and obtain higher accuracy of solution [55], [56], which would guarantee the sufficient low rank of the transformed tensor.This is similar to the definition of t-Schatten-p norm in [34] while their t-Schatten-p norm is based on DFT framework which is different from our SALT paradigm. In addition, the proposed optimization approach to minimizing the SALT is different with theirs. We choose 0 &lt; p ≤ 1 in our model since this kind of non-convex norm can achieve a tighter bound in approximating the tensor multi-rank and obtain higher accuracy of solution [55], [56], which would guarantee the sufficient low rank of the transformed tensor.</p>
        <p>The disadvantage of t-SVD is sensitivity of the orientation due to the orientation of the low tubal rank definition. In real multi-way data like images and videos, there are ubiquitous "spatial-shifting" correlations (color channels and temporal frames) making such data spatially smooth along the third mode. This pattern can be exploited via performing DFT along the third mode to get the low-multi-rank transformed tensor [39] but the complex intra-mode and inter-mode correlations in the other two orientations have not been explored [40]. Although work in [57] proposed a method called sum of tensor nuclear norm (STNN) that applied DFT to all modes of a thirdorder tensor, the smooth pattern may not always exist or be captured by DFT in other two modes, which also makes STNN lack of physical explanation on real datasets.The disadvantage of t-SVD is sensitivity of the orientation due to the orientation of the low tubal rank definition. In real multi-way data like images and videos, there are ubiquitous "spatial-shifting" correlations (color channels and temporal frames) making such data spatially smooth along the third mode. This pattern can be exploited via performing DFT along the third mode to get the low-multi-rank transformed tensor [39] but the complex intra-mode and inter-mode correlations in the other two orientations have not been explored [40]. Although work in [57] proposed a method called sum of tensor nuclear norm (STNN) that applied DFT to all modes of a thirdorder tensor, the smooth pattern may not always exist or be captured by DFT in other two modes, which also makes STNN lack of physical explanation on real datasets.</p>
        <p>Since the proposed SALT-3 is merited by adaptivity, the learned transformation is less orientation sensitive than DFT. We generalize the SALT-3 to the other two orientations with two main aims: overcoming the orientation sensitivity inherent with DFT and exploiting the intrinsic low rank property along the multi-orientations. This improves the limited representation ability and flexibility of TNN.Since the proposed SALT-3 is merited by adaptivity, the learned transformation is less orientation sensitive than DFT. We generalize the SALT-3 to the other two orientations with two main aims: overcoming the orientation sensitivity inherent with DFT and exploiting the intrinsic low rank property along the multi-orientations. This improves the limited representation ability and flexibility of TNN.</p>
        <p>The relaxation model for learning the SALTS and achieving tensor completion extends from SALT-3 (16) to SALTS as follows:The relaxation model for learning the SALTS and achieving tensor completion extends from SALT-3 (16) to SALTS as follows:</p>
        <p>wherewhere</p>
        <p>) are the SALTS learned by (17) along all modes. Model (16) only apply transformation to third mode, while (17) generalizes the (16) by applying three transformations to all modes of tensor simultaneously.) are the SALTS learned by (17) along all modes. Model (16) only apply transformation to third mode, while (17) generalizes the (16) by applying three transformations to all modes of tensor simultaneously.</p>
        <p>The integral framework of our proposed SALTS for tensor completion and illustration of it's superiority are presented in Fig. 2. To understand the advantage of SALTS for tensor recovery, we show the distributions of singular values of all the frontal slices in the transformed tensor. As an example, we conduct experiment on the chart and stuffed toy in CAVE data with sample rate 0.1 in tensor completion task. The input tensor size is 256 × 256 × 31 and pixels are normalized into (0,1). The Fig. 2 illustrate that singular values in SALTS are much smaller than that of Fourier transform domain, which reveals that transformed tensor via SALTS would achieve and ensure sufficient and better low rank property.The integral framework of our proposed SALTS for tensor completion and illustration of it's superiority are presented in Fig. 2. To understand the advantage of SALTS for tensor recovery, we show the distributions of singular values of all the frontal slices in the transformed tensor. As an example, we conduct experiment on the chart and stuffed toy in CAVE data with sample rate 0.1 in tensor completion task. The input tensor size is 256 × 256 × 31 and pixels are normalized into (0,1). The Fig. 2 illustrate that singular values in SALTS are much smaller than that of Fourier transform domain, which reveals that transformed tensor via SALTS would achieve and ensure sufficient and better low rank property.</p>
        <p>In the next part, we will derive the optimization algorithm based on alternative direction multiplier method (ADMM) to solve the constrained minimization problems for relaxation of ( 16) and (17). Because the relaxation of ( 16) is a special case with only one SALT, so we only provide the optimization algorithm of (17).In the next part, we will derive the optimization algorithm based on alternative direction multiplier method (ADMM) to solve the constrained minimization problems for relaxation of ( 16) and (17). Because the relaxation of ( 16) is a special case with only one SALT, so we only provide the optimization algorithm of (17).</p>
        <p>Actually, due to the orthogonal constraint on L i , it is a challenge to solve above constrained optimization problem. Usually, solving the L i needs to apply manifold optimization method, which is time consuming. However, according to theorem 1 in [58], the optimization problem of ( 17) is equivalent to the following optimization model, which means that the solutions of the two optimization problems are equivalent.Actually, due to the orthogonal constraint on L i , it is a challenge to solve above constrained optimization problem. Usually, solving the L i needs to apply manifold optimization method, which is time consuming. However, according to theorem 1 in [58], the optimization problem of ( 17) is equivalent to the following optimization model, which means that the solutions of the two optimization problems are equivalent.</p>
        <p>wherewhere</p>
        <p>P i (i = 1, 2, 3) are Lagrange multipliers and µ is an increased positive penalty in iterative steps to achieve the holding of equality constraints.P i (i = 1, 2, 3) are Lagrange multipliers and µ is an increased positive penalty in iterative steps to achieve the holding of equality constraints.</p>
        <p>(1)(1)</p>
        <p>This quadratic objective function has an unique closed-form solution, set the derivative of ( 19) with respect to Y k+1 i and derive from first order optimal condition:This quadratic objective function has an unique closed-form solution, set the derivative of ( 19) with respect to Y k+1 i and derive from first order optimal condition:</p>
        <p>. From the solution of well-known Orthogonal Procrustes [59]:. From the solution of well-known Orthogonal Procrustes [59]:</p>
        <p>It is non-trial to use proximal operator because there is a linear transform L in front of XIt is non-trial to use proximal operator because there is a linear transform L in front of X</p>
        <p>Then, instead of (25), we useThen, instead of (25), we use</p>
        <p>wherewhere</p>
        <p>and i ∈ {1, 2, 3}. Specifically, we haveand i ∈ {1, 2, 3}. Specifically, we have</p>
        <p>This can be formulated as the generalized singular value thresholding [60], [61] of each frontal slices of A k 1 , i.e., where X k+1 1n::This can be formulated as the generalized singular value thresholding [60], [61] of each frontal slices of A k 1 , i.e., where X k+1 1n::</p>
        <p>) is optimal for the following problem:) is optimal for the following problem:</p>
        <p>we adapt generalization of soft-thresholding (GST) in [62] to solve (30) effciently, Σ k+1 1n = diag(GST (diag(S k 1n ), µ k , p, J)), the GST is summarized in Algorithm 1.we adapt generalization of soft-thresholding (GST) in [62] to solve (30) effciently, Σ k+1 1n = diag(GST (diag(S k 1n ), µ k , p, J)), the GST is summarized in Algorithm 1.</p>
        <p>(5) Update the multipliers {P i } 3 i=1(5) Update the multipliers {P i } 3 i=1</p>
        <p>The optimization for ( 18) is summarized in Algorithm 2. Algorithm 1 Generalization of Soft-Thresholding(GST)The optimization for ( 18) is summarized in Algorithm 2. Algorithm 1 Generalization of Soft-Thresholding(GST)</p>
        <p>Input: s, µ, p, J .Input: s, µ, p, J .</p>
        <p>end for 10:end for 10:</p>
        <p>Because the SALTS model in ( 18) is a non-convex and nonsmooth optimization problem with more than two variables. As the convergence analysis of the original ADMM algorithm has not been established for non-convex problems or for convex problems with more than two block variables in general cases, it is difficult to determine the global convergence under the framework of ADMM [68], [69]. However in this paper we will prove the weak convergence of Algorithm 2 in Theorem Algorithm 2 SALTS for tensor completionBecause the SALTS model in ( 18) is a non-convex and nonsmooth optimization problem with more than two variables. As the convergence analysis of the original ADMM algorithm has not been established for non-convex problems or for convex problems with more than two block variables in general cases, it is difficult to determine the global convergence under the framework of ADMM [68], [69]. However in this paper we will prove the weak convergence of Algorithm 2 in Theorem Algorithm 2 SALTS for tensor completion</p>
        <p>while not convergence and k ≤ Maxiter do 3:while not convergence and k ≤ Maxiter do 3:</p>
        <p>Update L k i by (Update L k i by (</p>
        <p>Update X k i by ( 28), (30) and GST in Algorithm 1; Update µ k = min(µmax, ηµ k-1 );Update X k i by ( 28), (30) and GST in Algorithm 1; Update µ k = min(µmax, ηµ k-1 );</p>
        <p>12:12:</p>
        <p>Check the convergence conditions:Check the convergence conditions:</p>
        <p>1 which is essential to guarantee that the iterative sequence can attain stable solution. In addition, the stable convergence of algorithm and efficiency of the proposed method will be validated in the experimental section.1 which is essential to guarantee that the iterative sequence can attain stable solution. In addition, the stable convergence of algorithm and efficiency of the proposed method will be validated in the experimental section.</p>
        <p>Due to space limits, the detailed proof of Theorem 1 is in Supplementary Material.Due to space limits, the detailed proof of Theorem 1 is in Supplementary Material.</p>
        <p>Without loss of generality, for the tensor Y, we assumeWithout loss of generality, for the tensor Y, we assume</p>
        <p>The main computational complexity of Algorithm 2 is the updating for {X i }, which needs to compute the SVD of multiple matrices. For X i , computation of SVD is O(kn 3 ), which is because there are k slices of n × n needed to compute full SVD occupying O(n 3 ). It is worth mentioning that the time complexity of SVD can be reduced significantly when using partial SVD.The main computational complexity of Algorithm 2 is the updating for {X i }, which needs to compute the SVD of multiple matrices. For X i , computation of SVD is O(kn 3 ), which is because there are k slices of n × n needed to compute full SVD occupying O(n 3 ). It is worth mentioning that the time complexity of SVD can be reduced significantly when using partial SVD.</p>
        <p>The last computational complexity is multiplication operation of updating {Y i }, which occupies O(kn 3 ). Thus the overall time complexity of Algorithm 2 in O(6kn 3 + 3nk 2 ). In addition, the space complexity of Algorithm 2 is O(n 3 ), owing to storage requirement of X i , Y i , P i ,Z, D, L i , 1 α .The last computational complexity is multiplication operation of updating {Y i }, which occupies O(kn 3 ). Thus the overall time complexity of Algorithm 2 in O(6kn 3 + 3nk 2 ). In addition, the space complexity of Algorithm 2 is O(n 3 ), owing to storage requirement of X i , Y i , P i ,Z, D, L i , 1 α .</p>
        <p>In order to validate the effectiveness of the proposed SALTS in tensor completion task, we consider the following datasets: Synthetic Data, Hyper-Spectral Image (HSI), Color Video Sequence, Magnetic Resonance Imaging data (MRI), and COIL-20. Due to the limitation of space, the experimental results in verification of robustness , analysis of low rank structure learned, parameters of robustness, convergence verification and more detailed results for proposed SALTS are placed in Supplementary Material. We compare proposed methods with nine recently developed algorithms of low rank tensor completion: tSVD [65], FTNN [52] TCTF [37], OITNN [40], HaLRTC [18], TMac [63], ESP-LRTC [66], KBR-TC [67], TRLRF [64].In order to validate the effectiveness of the proposed SALTS in tensor completion task, we consider the following datasets: Synthetic Data, Hyper-Spectral Image (HSI), Color Video Sequence, Magnetic Resonance Imaging data (MRI), and COIL-20. Due to the limitation of space, the experimental results in verification of robustness , analysis of low rank structure learned, parameters of robustness, convergence verification and more detailed results for proposed SALTS are placed in Supplementary Material. We compare proposed methods with nine recently developed algorithms of low rank tensor completion: tSVD [65], FTNN [52] TCTF [37], OITNN [40], HaLRTC [18], TMac [63], ESP-LRTC [66], KBR-TC [67], TRLRF [64].</p>
        <p>In addition to the intuitive visual display, we apply four quantitative image quality indices used in [67] to evaluate performance of all algorithms numerically, including peak signal-to-noise ratio (PSNR), structure similarity (SSIM), feature similarity (FSIM), erreur relative globale adimensionnelle de synthèse (ERGAS). The larger former three indices, the better recovery performance. On the contrary, smaller ERGAS means better recovery performance. The parameters of all algorithms are well tuned to provide their best performance as possible according to guidance in their reference papers or codes. All experiments are conducted in 
            <rs type="software">MATLAB</rs>
            <rs type="version">2020a</rs> in 
            <rs type="software">Windows</rs> 10 on a computer with Intel(R) Core(TM) i9-9900K CPU@3.60GHz and 128GB RAM. In addition, we record the time costs of all methods.
        </p>
        <p>We generate tensor X ∈ R n1×n2×n3 with lrank r for each frontal slice, which means X (:, :, n) = randn(n 1 , r) * randn(r, n 2 ) by 
            <rs type="software">MATLAB</rs> commands. Then we construct the random orthogonal square transform L 3 ∈ R n3×n3 along the third dimension. The ground-truth tensor is generated by transformation as Y = X × 3 L 3 . We test two synthetic data: synthetic data-1 with n 1 = n 2 = 50, n 3 = 20, r = 5 and synthetic data-2 with n 1 = n 2 = 100, n 3 = 50, r = 10 with different sampling rates (SRs) in {20%, 30%, 40%, 50%, 60%, 70%, 80%}. We use the relative error (RE) to quantitatively evaluate the performance of all methods:
        </p>
        <p>where Y and Ŷ are the original tensor and the recovered tensor, respectively. The RE values are summarized in Table I for synthetic data-1 and synthetic data-2, respectively. We use red color to highlight the best result in each case. Obviously, our method SALTS achieved the minimum RE across all SRs.where Y and Ŷ are the original tensor and the recovered tensor, respectively. The RE values are summarized in Table I for synthetic data-1 and synthetic data-2, respectively. We use red color to highlight the best result in each case. Obviously, our method SALTS achieved the minimum RE across all SRs.</p>
        <p>Even in case of SR ≥ 40%, the proposed SALTS could achieve almost exactly recovery, while all other methods fail. Because all the methods achieve very accurate recovery results when the sampling rate (SR) is high, so we test four different SRs: 3%, 5%, 10% and 20%. The average metric of each tested algorithm for all 32 hyperspectral objects under these four SRs are reported in TABLE II, which shows that our SALTS outperforms all competing methods under all SRs with respect to all metrics. Selected visual results in Fig. 4 and Fig. 3 from fake and real lemons and chart and stuffed toy reveal the superiority of the SALTS, both are in the completion of finergrained textures and coarser-grained structures. The values of four metric PSNR, SSIM, FSIM and ERGAS across all bands in Supplementary Material. It can be seen that the proposed SALTS obtains the best metric in all bands.Even in case of SR ≥ 40%, the proposed SALTS could achieve almost exactly recovery, while all other methods fail. Because all the methods achieve very accurate recovery results when the sampling rate (SR) is high, so we test four different SRs: 3%, 5%, 10% and 20%. The average metric of each tested algorithm for all 32 hyperspectral objects under these four SRs are reported in TABLE II, which shows that our SALTS outperforms all competing methods under all SRs with respect to all metrics. Selected visual results in Fig. 4 and Fig. 3 from fake and real lemons and chart and stuffed toy reveal the superiority of the SALTS, both are in the completion of finergrained textures and coarser-grained structures. The values of four metric PSNR, SSIM, FSIM and ERGAS across all bands in Supplementary Material. It can be seen that the proposed SALTS obtains the best metric in all bands.</p>
        <p>We select four color video sequences Akiyo, Hall, Container, and Grandma from open-source YUV video dataset 2 . For comparing efficiently, we choose the first 100 frames of each video to test all algorithms. As the color video inWe select four color video sequences Akiyo, Hall, Container, and Grandma from open-source YUV video dataset 2 . For comparing efficiently, we choose the first 100 frames of each video to test all algorithms. As the color video in</p>
        <p>In this subsection, we use the MRI data with volume 181 × 217 × 181 from Simulated Brain Database3 , which contains a set of realistic MRI data volumes produced by an MRI simulator used by the neuroimaging community widely. We set SR as 10%, 20%, and 30% for MRI. Table IV and Fig. 6 show the quantitative assessments and recovered results of all methods for the 120-th slice, respectively. It is observed from visual images that not only can the contour of missed MRI be best restored, but also the details of the images are inpainted with more fullness and integrity by our proposed method. In order to fully verify the recovery effect in all frames of MRI, we plot the four metrics across all frames for all competitive methods in Supplementary Material. In addition, our algorithm achieves much superior performance of restoration on all frames in all metrics.In this subsection, we use the MRI data with volume 181 × 217 × 181 from Simulated Brain Database3 , which contains a set of realistic MRI data volumes produced by an MRI simulator used by the neuroimaging community widely. We set SR as 10%, 20%, and 30% for MRI. Table IV and Fig. 6 show the quantitative assessments and recovered results of all methods for the 120-th slice, respectively. It is observed from visual images that not only can the contour of missed MRI be best restored, but also the details of the images are inpainted with more fullness and integrity by our proposed method. In order to fully verify the recovery effect in all frames of MRI, we plot the four metrics across all frames for all competitive methods in Supplementary Material. In addition, our algorithm achieves much superior performance of restoration on all frames in all metrics.</p>
        <p>In this subsection, we adapt COIL-20 4 , which includes 1440 various images of different 20 objects taken from different angles and the size of each image is 128 × 128. We select 3 different objects from 20 objects with all 72 pictures of each from different angles, which means that each selected object's tensor data size is 128 × 128 × 72. We set SR as 10%, 20%, 30% for each object, respectively. For visual comparison, we set the SR as 5% for object 2 with comparison of all methods and the corresponding visual, average metric of three objects for different SRs and metrics across all angles for object 2 are reported in Fig. 7, TABLE V and Supplementary Material, respectively. In comparison, the proposed algorithm obtains the optimal restoration result from all aspects of the experimental verification.In this subsection, we adapt COIL-20 4 , which includes 1440 various images of different 20 objects taken from different angles and the size of each image is 128 × 128. We select 3 different objects from 20 objects with all 72 pictures of each from different angles, which means that each selected object's tensor data size is 128 × 128 × 72. We set SR as 10%, 20%, 30% for each object, respectively. For visual comparison, we set the SR as 5% for object 2 with comparison of all methods and the corresponding visual, average metric of three objects for different SRs and metrics across all angles for object 2 are reported in Fig. 7, TABLE V and Supplementary Material, respectively. In comparison, the proposed algorithm obtains the optimal restoration result from all aspects of the experimental verification.</p>
        <p>In this paper, we have proposed a novel multiple selfadaptive learnable transforms (SALTS) based tensor nuclear norm and developed an efficient ADMM to update multiple learnable transforms and transformed tensors along all modes alternately. In the proposed framework, SALTS are jointly learned with transformed tensors by minimizing its induced Schatten-p quasi norms, which are data-dependent and flexibly adaptable to a wide range of data. Due to the adaptation of SALTS, its insensitivity of orientation guarantees sufficient low rank structure of transformed tensors along multiple modes. The visual and numerical results on four benchmark datasets have shown the superiority of the proposed SALTS. In future work, we will apply our SALTS to tensor robust principal component analysis, which is another fundamental topic in tensor data processing.In this paper, we have proposed a novel multiple selfadaptive learnable transforms (SALTS) based tensor nuclear norm and developed an efficient ADMM to update multiple learnable transforms and transformed tensors along all modes alternately. In the proposed framework, SALTS are jointly learned with transformed tensors by minimizing its induced Schatten-p quasi norms, which are data-dependent and flexibly adaptable to a wide range of data. Due to the adaptation of SALTS, its insensitivity of orientation guarantees sufficient low rank structure of transformed tensors along multiple modes. The visual and numerical results on four benchmark datasets have shown the superiority of the proposed SALTS. In future work, we will apply our SALTS to tensor robust principal component analysis, which is another fundamental topic in tensor data processing.</p>
        <p>http://www1.cs.columnbia.edu/CAVE/databases/multispectral/http://www1.cs.columnbia.edu/CAVE/databases/multispectral/</p>
        <p>http://trace.eas.asu.edu/yuv/http://trace.eas.asu.edu/yuv/</p>
        <p>http://brainweb.bic.mni.mcgill.ca/brainweb/http://brainweb.bic.mni.mcgill.ca/brainweb/</p>
        <p>
            <rs type="url">http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.PHP</rs>
        </p>
        <p>ACKNOWLEDGEMENT The work was supported by National Natural Science Foundation of China (No. 61971093, No. 61960206010, No. 61527803). The work was supported by NSFC Projects of International Cooperation and Exchanges No. 61960206010, supported by the International Science and Technology Innovation Cooperation Project of Sichuan Province: 2021YFH0036, and Fundamental Research Funds for the Central Universities (Grant No. ZYGX2019J067). This work was also partially supported by the research funding T00120210002 of Shenzhen Research Institute of Big Data and the Youth program 62106211 of the National Natural Science Foundation of China.ACKNOWLEDGEMENT The work was supported by National Natural Science Foundation of China (No. 61971093, No. 61960206010, No. 61527803). The work was supported by NSFC Projects of International Cooperation and Exchanges No. 61960206010, supported by the International Science and Technology Innovation Cooperation Project of Sichuan Province: 2021YFH0036, and Fundamental Research Funds for the Central Universities (Grant No. ZYGX2019J067). This work was also partially supported by the research funding T00120210002 of Shenzhen Research Institute of Big Data and the Youth program 62106211 of the National Natural Science Foundation of China.</p>
    </text>
</tei>

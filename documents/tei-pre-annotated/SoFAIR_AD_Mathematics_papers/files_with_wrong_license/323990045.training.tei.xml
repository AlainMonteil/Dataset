<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:47+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>The COVID-19 pandemic is expected to have profound and enduring effects on mental health but, until we have data, we will not know its form, extent, duration, or distribution. An appropriate public health response to mitigate and manage mental health sequelae is likely to require significant diversion of resources. Such decisions must be underpinned by reliable information: policymakers, commissioners and services need to know both the scale of need and who is most vulnerable. A recent position paper in this journal 1 highlights "an immediate priority is collecting high-quality data on the mental health effects of the COVID-19 pandemic across the whole population and vulnerable groups". This should be a clarion-call for governments to fund, and researchers to gather, timely, high-quality population mental health data which represents the true need arising from the pandemic. Instead, our desire for quick information has driven rapid propagation of online surveys using non-probability and convenience samples, some of which claim to be representative.</p>
        <p>Understandably, many are receiving widespread media attention. These early insights might be valuable, but we caution against relying on them to drive policy and resource because they are prone to substantial bias: acting on misleading information could be worse than having no information at all. Survey sampling and design choices must be led by their purpose. If the survey is to generate quick ideas, consult on perspectives, or foster community engagement, rapid, low-cost convenience sampling are appropriate. However, to understand prevalence in a population, how survey respondents are recruited is critically important. Non-probability samples are usually recruited by approaching membership lists; through service providers; existing large convenience panels; or from snowball recruitment using word-of-mouth, often via social media. Such samples attract volunteers who are already well-engaged, interested in the topic, and who do/can access the internet. Bias can affect any survey, but can be particularly problematic for social/mental health surveys where those excluded are often most in need.</p>
        <p>Individuals with existing or severe mental illness are less likely to participate online 2 , while half of over 75's and many with mental illness, (key COVID-19 at-risk groups), are not regular internet users; access to digital devices is also limited amongst the most vulnerable and deprived children. Most surveys will weight their sample to match their target population by certain characteristics; however, these adjustments miss crucial elements of bias and cannot account for groups not included at all, particularly if the response rate is unknowable. A common misconception is that larger samples solve these biases. One Chinese study of mental health responses to the pandemic gathered an impressive 52,730 respondents 3 ; however, 65% were female, indicating a highly skewed sample in a population with significantly fewer women than men 4 . We recommend all surveys detail their sampling strategy and to publish comparative statistics with the population they are sampled from, so informed judgements can be made about representativeness.</p>
        <p>The value of a survey depends on its data utility. Non-probability sampling lacks a sound theoretical basis for statistical inference 5 , which means basic descriptive analyses and explorations of potential associations are appropriate but measures of uncertainty i.e. confidence intervals around estimates of prevalence are generally not valid. Moreover, our ability to compare the population's mental health pre-and post-COVID is compromised if surveys do not use standardised measures which are reliable and stable over time and if prepandemic baseline data from the same population are not available.</p>
        <p>The current crisis has compromised several established data sources: health registries that previously quantified mental illness prevalence 6 are reporting a drop in patient contacts -in the UK during March 2020, significantly fewer people presented to A&amp;E and primary care with mental illness (personal communications). National registries of mental illness and suicide will catch-up, but are a poor tool in the short-term. Many official surveys have suspended data-collection in response to social distancing guidelines or transferred to remote interviews affecting comparability with prior waves 7 and creating new challenges, such as how to gather sensitive data on self-harm/suicidality or intimate partner violence 8 . We believe it is both possible and cost-effective to generate high-quality evidence of mental health need in the current crisis. We recommend using random sampling to reduce risk of bias, allow quantification of non-response, and permit valid statistical analysis. A major investigation into online survey panels 9 concluded that "Researchers should avoid nonprobability online panels when...[the] objective is to accurately estimate population values." Determining prevalence of COVID-19 mental health effects should use rigorous methods that sample from the whole population to reduce erroneous conclusions and potentially damaging actions. This approach may be more expensive, but is essential to gain reliable insights into how we mitigate psychological risks during this and future pandemics.</p>
        <p>Cutting corners to provide quick, cheap answers will result in poorer quality evidence, poorer policy, and wasted resources in the longer term. We can and must do better.</p>
    </text>
</tei>

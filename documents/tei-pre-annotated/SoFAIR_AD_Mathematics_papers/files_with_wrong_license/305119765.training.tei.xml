<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:43+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Penalization of the likelihood by Jeffreys' invariant prior, or a positive power thereof, is shown to produce finite-valued maximum penalized likelihood estimates in a broad class of binomial generalized linear models. The class of models includes logistic regression, where the Jeffreys-prior penalty is known additionally to reduce the asymptotic bias of the maximum likelihood estimator, and models with other commonly used link functions, such as probit and log-log. Shrinkage towards equiprobability across observations, relative to the maximum likelihood estimator, is established theoretically and studied through illustrative examples. Some implications of finiteness and shrinkage for inference are discussed, particularly when inference is based on Wald-type procedures. A widely applicable procedure is developed for computation of maximum penalized likelihood estimates, by using repeated maximum likelihood fits with iteratively adjusted binomial responses and totals. These theoretical results and methods underpin the increasingly widespread use of reduced-bias and similarly penalized binomial regression models in many applied fields.</p>
        <p>Logistic regression is one of the most frequently applied generalized linear models in statistical practice, both for inference about covariate effects on binomial probabilities and for prediction. Consider realizations y 1 , . . . , y n of independent binomial random variables Y 1 , . . . , Y n with success probabilities π 1 , . . . , π n and totals m 1 , . . . , m n , respectively. Suppose that each y i is accompanied by a p-dimensional covariate vector x i and that the model matrix X with rows x 1 , . . . , x n is of full rank. A logistic regression model has</p>
        <p>where β = (β 1 , . . . , β p ) T is the p-dimensional parameter vector and x it is the tth element of x i (i = 1, . . . , n); if an intercept parameter is present in the model, then the first column of X is a vector of ones. The maximum likelihood estimator β of β in (1) maximizes the loglikelihood</p>
        <p>c 2020 Biometrika Trust This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/ licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. Albert &amp; Anderson (1984) categorized the possible settings for the sample points (y 1 , x T 1 ) T , . . . , (y n , x T n ) T into complete separation, quasi-complete separation and overlap. Specifically, if there exists a vector γ ∈ R p such that γ T x i &gt; 0 for all i with y i &gt; 0 and γ T x i &lt; 0 for all i with y i = 0, then there is complete separation in the sample points; if there exists a vector γ ∈ R p such that γ T x i 0 for all i with y i &gt; 0 and γ T x i 0 for all i with y i = 0, then there is quasi-complete separation in the sample points; and if the sample points exhibit neither complete separation nor quasi-complete separation, then they are said to overlap. Albert &amp; Anderson (1984) showed that separation is necessary and sufficient for the maximum likelihood estimate to have at least one infinite-valued component. A parallel result appears in Silvapulle (1981), where it is shown that if G(η) in ( 1) is any strictly increasing distribution function such that -log G(η) and log{1 -G(η)} are convex and if x i1 = 1 for all i ∈ {1, . . . , n}, then the maximum likelihood estimate has all components finite if and only if there is overlap.</p>
        <p>When data separation occurs, standard maximum likelihood estimation procedures, such as iteratively reweighted least squares (Green, 1984), can be numerically unstable due to the occurrence of large parameter values as the procedures attempt to maximize (2). In addition, inferential procedures that directly depend on the estimates and the estimated standard errors, such as Wald tests, can give misleading results. For a recent review of such problems and some solutions, see Mansournia et al. (2018). Firth (1993) showed that if the logistic regression likelihood is penalized by Jeffreys' invariant prior, then the resulting maximum penalized likelihood estimator has bias of smaller asymptotic order than that of the maximum likelihood estimator in general. Specifically, for logistic regressions the reduced-bias estimator β results from maximization of</p>
        <p>is the working weight for the ith observation with ω(η) = exp(η)/{1 + exp(η)} 2 . Heinze &amp; Schemper (2002), in extensive numerical studies, observed that the reduced-bias estimates have finite values even when data separation occurs. Based on an argument about parameter-dependent adjustments to y 1 , . . . , y n and m 1 , . . . , m n stemming from the form of the gradient of (3), Heinze &amp; Schemper (2002) conjectured that finiteness of the reduced-bias estimates holds for every combination of data and logistic regression model. Heinze &amp; Schemper (2002) also observed that the reducedbias estimates are typically smaller in absolute value than the corresponding maximum likelihood estimates when the latter are finite. These observations are in agreement with the asymptotic bias of the maximum likelihood estimator in logistic regressions being approximately collinear with the parameter vector (see, e.g., Cordeiro &amp; McCullagh, 1991).</p>
        <p>Example 1 illustrates the finiteness and shrinkage properties of the maximum penalized likelihood estimator in the context of estimating the strengths of NBA basketball teams using a Bradley-Terry model (Bradley &amp; Terry, 1952).</p>
        <p>Example 1. Suppose that y ij = 1 when team i beats team j and y ij = 0 otherwise. The Bradley-Terry model assumes that the contest outcome y ij is the realization of a Bernoulli random variable with probability π ij = exp(β i -β j )/{1 + exp(β i -β j )} and that the outcomes of the available contests are independent. The Bradley-Terry model is a logistic regression with probabilities as in (1), for the particular X matrix whose rows are indexed by contest identifiers (i, j) and whose general element is x ij,t = δ it -δ jt (t = 1, . . . , p). Here, δ it is the Kronecker delta, taking value 1 when t = i and value 0 otherwise. The parameter β t can be thought of as measuring the ability or strength of team t (t = 1, . . . , p). Only contrasts are estimable, and an identifiable parameterization can be achieved by setting one of the abilities to zero. See, for example, Agresti (2013, § 11.6) for a general discussion of the model. We use the Bradley-Terry model to estimate the abilities of basketball teams from game outcomes in the regular season of the 2014-2015 NBA conference. For illustrative purposes, we use only the 262 games that took place before 3 December 2014, up to which date the Philadelphia 76ers had recorded 17 straight losses and no win. The dataset was obtained from www.basketball-reference.com and is provided in the Supplementary Material. The ability of the San Antonio Spurs, the champion team of the 2013-2014 conference, is set to zero, so that each β i represents the contrast of the ability of team i with that of the San Antonio Spurs. The model is estimated via iteratively reweighted least squares, as implemented in the glm function of 
            <rs type="software">R</rs> (
            <rs type="creator">R Development Core Team</rs>, 2021) with default settings for the optimization. No warnings or errors were returned during the fitting process.
        </p>
        <p>The top part of Fig. 1(a) shows the reported maximum likelihood estimates of the contrasts, along with their corresponding nominally 95% individual Wald-type confidence intervals. The contrast for the Philadelphia 76ers stands out in the output from 
            <rs type="software">glm</rs>, with a value of -19.24 and a corresponding estimated standard error of 844.97. These values are in fact representations of -∞ and ∞, respectively, as confirmed by the detect_separation method of the 
            <rs type="software">R</rs> package 
            <rs type="software">brglm2</rs> (Kosmidis, 2020), which implements separation-detection algorithms from a 2007 University of Oxford Department of Statistics PhD thesis by K. Konis. The data are separated, with the maximum likelihood estimates for all teams being finite except that for the Philadelphia 76ers, which is -∞. A particularly worrying side-effect of data separation here is that if the computer output is used naively, a Wald test for difference in ability between the Philadelphia 76ers and the San Antonio Spurs results in no apparent evidence of a difference, which is counterintuitive given that the former had no wins in 17 games and the latter had 13 wins in 17 games. In contrast, the reduced-bias estimates in the bottom part of Fig 1(a) all have finite values and finite standard errors. Figure 1(b) illustrates the shrinkage of the reduced-bias estimates towards zero, which has also been discussed in a range of different settings, such as in Heinze &amp; Schemper (2002) and Zorn (2005).
        </p>
        <p>The apparent finiteness and shrinkage properties of the reduced-bias estimator, together with the fact that the estimator has the same first-order asymptotic distribution as the maximum likelihood estimator, are key reasons for the increasingly widespread use of Jeffreys-prior penalized logistic regression in applied work. At the time of writing, 
            <rs type="software">Google Scholar</rs> recorded approximately 2700 citations of Firth (1993), more than half of which were from 2015 or later. The list of application areas is diverse, including agriculture and fisheries research, animal and plant ecology, criminology, commerce, economics, psychology, health and medical sciences, politics, and many more. The particularly strong uptake of the method in health and medical sciences and in politics stems largely from the works of Heinze &amp; Schemper (2002) and Zorn (2005), respectively. The reduced-bias estimator is also implemented in dedicated open-source software, such as the 
            <rs type="software">brglm2</rs> (Kosmidis, 2020) and 
            <rs type="software">logistf</rs> (Heinze &amp; Ploner, 2018) 
            <rs type="software">R</rs> packages, and it has now become part of textbook treatments of logistic regression; see, for example, Agresti (2013, § 7.4) or Hosmer et al. (2013, § 10.3).
        </p>
        <p>However, a definitive theoretical account of the empirically evident finiteness and shrinkage properties has yet to appear in the literature. Such a formal account is much needed, particularly in light of recent advances that demonstrate benefits of the reduced-bias estimator in wider contexts than the ones for which it was originally developed. An example of such an advance is the work of Lunardon (2018), which explores the performance of bias reduction in stratified settings and shows that it is particularly effective for inference about a low-dimensional parameter of interest in the presence of high-dimensional nuisance parameters. For the estimation of high-dimensional logistic regression models with p/n → κ ∈ (0, 1), experiments reported in the supplementary information of Sur &amp; Candès (2019), see also the Supplementary Material for the present article, show that bias reduction performs similarly to their newly proposed method and markedly better than maximum likelihood. These new theoretical and empirical results justify and motivate the use of the reduced-bias estimator in even more complex applied settings than that covered by the framework of Firth (1993); in such settings, more involved methods such as modified profile likelihoods (see, e.g., Sartori, 2003) and approximate message-passing algorithms (see, e.g., Sur &amp; Candès, 2019) have also been proposed for recovering inferential accuracy.</p>
        <p>In this article we formally derive the finiteness and shrinkage properties of reduced-bias estimators for logistic regressions under only the condition that the model matrix X has full rank. We also provide geometric insights into how penalized likelihood estimators shrink towards zero and discuss the implications of finiteness and shrinkage for inference, especially with regard to hypothesis tests and confidence regions using Wald-type procedures.</p>
        <p>We show how the results can be extended in a direct way to other commonly used link functions, such as the probit, log-log, complementary log-log and cauchit links, whenever the Jeffreys prior is used as a likelihood penalty. The work presented here thus complements earlier work of Ibrahim &amp; Laud (1991) and especially Chen et al. (2008), which considers the same models from a Bayesian perspective. Here we study the behaviour of the posterior mode and thereby derive results that add to those earlier findings, whose focus was instead on important Bayesian aspects such as propriety and moments of the posterior distribution.</p>
        <p>The results in this paper also extend readily to situations where penalized loglikelihoods of the form</p>
        <p>are used, with a allowed to take values other than 1/2. Such penalized loglikelihoods have proven useful in prediction contexts, where the value of a can be tuned to deliver better estimates of the binomial probabilities, and they are the subject of ongoing research (see, e.g., Elgmati et al., 2015;Puhr et al., 2017). The procedure of repeated maximum likelihood fits with iteratively adjusted binomial responses and totals, derived in § 4, maximizes l † (β; a) for general binomial-response generalized linear models and any a &gt; 0.</p>
        <p>2. Logistic regression 2.1. Finiteness We first derive results on finiteness and shrinkage of the maximum penalized likelihood estimator for logistic regression, which is the most common case in applications and also the case for which maximum penalized likelihood, with the Jeffreys-prior penalty, coincides with asymptotic bias reduction. These results provide a platform for the generalization to link functions other than logit in § 3.</p>
        <p>Let W * (r) be W (β) at β = β(r), for r ∈ R, where β(r) is a path in R p such that β(r) → β 0 as r → ∞, with β 0 having at least one infinite component. Theorem 1 below describes the limiting behaviour of the determinant of the expected information matrix X T W * (r)X as r diverges to infinity, under only the assumption that X has full rank. An important implication of Theorem 1 is Corollary 1, which says that the reduced-bias estimators for logistic regressions are always finite. These new results formalize a sketch argument made in Firth (1993, § 3.3).</p>
        <p>Corollary 1. Suppose that X is of full rank. The vector β that maximizes l(β) has all of its components finite.</p>
        <p>The proofs of Theorem 1 and Corollary 1 are given in the Supplementary Material. Corollary 1 also holds for any fixed a &gt; 0 in (4). As a result, the maximum penalized likelihood estimators from the maximization of l † (β; a) in (4) have finite components for any a &gt; 0.</p>
        <p>Despite its practical utility, the finiteness of the reduced-bias estimator results in some notable, and perhaps undesirable, side-effects for Wald-type inferences based on the reduced-bias estimator that have been largely overlooked in the literature. The finiteness of β implies that the estimated standard errors s t ( β) (t = 1, . . . , p), calculated as the square roots of the diagonal elements of the inverse of X T W ( β)X , are also always finite. Since y 1 , . . . , y n are realizations of binomial random variables, there is only a finite number of values that the estimator β can take for any given x 1 , . . . , x n . Hence, there will always be a parameter vector with large enough components that the usual Wald-type confidence intervals βt ± z 1-α/2 s t ( β), or confidence regions in general, will fail to cover regardless of the nominal level α that is used. This phenomenon has also been observed in the complete enumerations of Kosmidis (2014) for proportional odds models which are extensions of logistic regression to ordinal responses; and it is also the case when the penalized likelihood is profiled for the construction of confidence intervals, as proposed, for example, in Heinze &amp; Schemper (2002), and in Bull et al. (2007) for multinomial regression models.</p>
        <p>The following theorem is key when exploring the shrinkage properties of the reduced-bias estimator that have been illustrated in Example 1.</p>
        <p>Theorem 2. Suppose that X is of full rank. Then the following hold.</p>
        <p>A complete proof of Theorem 2 is given in the Supplementary Material. Part (i) also follows directly from Theorem 1 in Chen et al. (2008).</p>
        <p>Consider estimation by maximization of the penalized loglikelihood l † (β; a) in ( 4) for a = a 1 and a = a 2 with a 1 &gt; a 2 0. Let β (a 1 ) and β (a 2 ) be the maximizers of l † (β; a 1 ) and l † (β; a 2 ), respectively, and π (a 1 ) and π (a 2 ) the corresponding estimated n-vectors of probabilities. Then, by the concavity of log |X T W (π)X |, the vector π (a 1 ) is closer to (1/2, . . . , 1/2) T than is π (a 2 ) , in the sense that π (a 1 ) lies within the hull of that convex contour of log |X T W (π)X | containing π (a 2 ) . With the specific values a 1 = 1/2 and a 2 = 0, the last result refers to maximization of the likelihood penalized by Jeffreys' prior and to maximization of the unpenalized likelihood, respectively. Hence, using reduced-bias estimators for logistic regressions has the effect of shrinking towards the model that implies equiprobability across observations, relative to maximum likelihood. Shrinkage here is with respect to a metric based on the expected information matrix rather than with respect to Euclidean distance. Hence, the reduced-bias estimates are only typically, rather than always, smaller in absolute value than the corresponding maximum likelihood estimates.</p>
        <p>If the determinant of the inverse of the expected information matrix is taken as a generalized measure of the asymptotic variance, then the estimated generalized asymptotic variance at the reduced-bias estimates is always smaller than the corresponding estimated variance at the maximum likelihood estimates. Hence, approximate confidence ellipsoids based on asymptotic normality of the reduced-bias estimator are reduced in volume.</p>
        <p>3.1. Finiteness The results in this section generalize those of § 2.1 and § 2.2 beyond the logit link function, still for estimators from penalized likelihoods of the form (4). For non-logistic link functions, such estimators no longer coincide with the bias-reduced estimator of Firth (1993).</p>
        <p>Theorem 1 and Corollary 1 readily extend to link functions other than the logistic one. Specifically, if G(η) = exp(η)/{1 + exp(η)} in model ( 1) is replaced by an at least twice differentiable and invertible function G : R → (0, 1), then the expected information matrix again has the form X T W (β)X , but with working weights</p>
        <p>and g(η) = dG(η)/dη. If the link function is such that ω(η) → 0 as η diverges to either -∞ or ∞, then the proofs of Theorem 1 and Corollary 1 in the Supplementary Material carry through unaltered to show that lim r→∞ |X T W * (r)X | = 0 and that, when the penalty is a positive power of Jeffreys' invariant prior, the maximum penalized likelihood estimates have finite components. The logit, probit, complementary log-log, log-log and cauchit links are some commonly used link functions for which ω(η) → 0. The functions G(η) and ω(η) for each of these link functions are shown in Table 1.</p>
        <p>If the link function is such that ω(z) is maximized at some value z 0 ∈ (0, 1), then the same arguments as in the proof of Theorem 2(i) can be used Table 1. Common link functions and the corresponding forms for G(η) and ω(η); for all the displayed link functions, ω(η) vanishes as η diverges</p>
        <p>, where x 1 = -1 and x 2 = 1, and with m 1 = 9 and m 2 = 9; the arrows point from the estimated probabilities based on the maximum likelihood estimates to those based on the penalized likelihood estimates, and the grey curves are the contours of log</p>
        <p>to show that |X T W (π)X | is globally maximized at (z 0 , . . . , z 0 ) T . Figure 2(a) illustrates that this condition is satisfied for the logit, probit, log-log and complementary log-log link functions. If</p>
        <p>where b 0 = g -1 (z 0 ). Moreover, it can be seen directly from the proof of Theorem 2 that a sufficient condition for the log-concavity of |X T W (π)X | for non-logit link functions is that ω(z) is concave.</p>
        <p>The maximum penalized likelihood estimates, when X has full rank, can be computed by direct numerical optimization of the penalized loglikelihood l † (β; a) in (4) or by using a quasi-Newton-Raphson iteration as in Kosmidis &amp; Firth (2010). Nevertheless, the particular form of the Jeffreys prior allows the convenient computation of penalized likelihood estimates by leveraging readily available maximum likelihood implementations for binomial-response generalized linear models.</p>
        <p>If G(η) = exp(η)/{1 + exp(η)} in model ( 1) is replaced by any invertible function G : R → (0, 1) that is at least twice differentiable, then differentiation of l † (β; a) with respect to β t (t = 1, . . . , q) gives that the penalized likelihood estimates are the solutions to</p>
        <p>where</p>
        <p>If we temporarily omit the observation index and suppress the dependence of the various quantities on β, the derivatives of l † (β; a) are the derivatives of the binomial loglikelihood l(β) with link function G(η), after adjusting the binomial response y to y + 2ah(q -1/2). Hence, the penalized likelihood estimates can be conveniently computed through repeated maximum likelihood fits, where each repetition consists of two steps: (i) the adjusted responses are computed at the current parameter values; and (ii) the maximum likelihood estimates of β are computed at the current value of the adjusted responses.</p>
        <p>However, depending on the sign and magnitude of 2ah(q -1/2), the adjusted response can be either negative or greater than the binomial total m. In such cases, standard implementations of maximum likelihood either are unstable or report an error. This is because the binomial loglikelihood is not necessarily concave when y &lt; 0 or y &gt; m for at least one observation, if a link function with concave log{G(η)} and log{1 -G(η)} is used. The logit, probit, log-log and complementary log-log link functions are of this kind. See, for example, Pratt (1981, § 5) for results and discussion on concavity of the loglikelihood.</p>
        <p>Such issues with the use of repeated maximum likelihood fits can be avoided by noting that expression (5) results if, in the derivatives of the loglikelihood, y and m are replaced by their adjusted versions</p>
        <p>Here c is some arbitrarily chosen function of β. The following theorem identifies one function c for which 0 ỹ m.</p>
        <p>Theorem 3. Let I (A) be equal to 1 if A holds and 0 otherwise.</p>
        <p>The proof of Theorem 3 is given in the Supplementary Material, which also provides pseudocode and R code for the algorithm JeffreysMPL that implements repeated maximum likelihood fits to maximize l † (β; a) for any supplied a and link function G(η).</p>
        <p>The variance-covariance matrix of the penalized likelihood estimator can be obtained as (R T R) -1 , where R is the upper triangular matrix from the QR decomposition of W (β) 1/2 X at the final iteration of the procedure. That decomposition is a by-product of the algorithm JeffreysMPL.</p>
        <p>If, in addition to X having of full rank, we require that X has a column of ones and that g(η) is a unimodal density function, it can be shown that if the starting value of the parameter vector β in the repeated maximum likelihood fits procedure has finite components, then the values of β computed in step (ii) will also have finite components at all repetitions. This is because, with a column of ones in the full-rank X , the adjusted responses and totals in (6) satisfy 0 &lt; ỹ &lt; m, and hence maximum likelihood estimates with infinite components are not possible. The strict inequalities 0 &lt; ỹ &lt; m hold because, under the aforementioned conditions, w i (β) &gt; 0 and X T W (β)X is positive definite for β with finite components. Then, Theorem 4 in Magnus &amp; Neudecker (1999, Ch. 11) on bounds for the Rayleigh quotient gives the inequality h i (β) w i (β)x T i x i λ(β) &gt; 0 (1, . . . , n), where λ(β) &gt; 0 is the minimum eigenvalue of (X T W (β)X ) -1 .</p>
        <p>The repeated maximum likelihood fits procedure has the correct fixed point even if in step (ii) full maximum likelihood estimation is replaced by a procedure that merely increases the loglikelihood, such as a single step of iteratively reweighted least squares for the adjusted responses and totals. Firth (1992) suggested such a scheme for logistic regressions with a = 1/2. There is currently no conclusive result on whether full maximum likelihood iteration with a reasonable stopping criterion is better or worse than, for example, one step of iteratively reweighted least squares in terms of computational efficiency. A satisfactory starting value for the above procedure is the maximum likelihood estimate of β, after adding a small positive constant and twice that constant to the actual binomial responses and totals, respectively.</p>
        <p>Finally, for a = 1/2, repeated maximum likelihood fits can be used to compute the posterior normalizing constant when implementing the importance sampling algorithm in Chen et al. (2008, § 5) for posterior sampling of the parameters of Bayesian binomial-response generalized linear models with the Jeffreys prior.</p>
        <p>The Supplementary Material illustrates the evolution of adjusted responses and totals through the iterations of JeffreysMPL, for the first six games of the Philadelphia 76ers in Example 1. We also compute the reduced-bias estimates for a logistic regression model with n = 1000 binary responses and p = 200 covariates, as considered in Fig. 2(b) of the supplementary information appendix of Sur &amp; Candès (2019), and show that such computation takes only a couple of seconds on a standard laptop computer.</p>
        <p>Figure 2(a) shows ω(z) and z 0 for the various link functions. The plot for the log-log link is the reflection of the one for the complementary log-log link in z = 0.5. As is apparent, ω(z) is concave for the logit, probit and complementary log-log links, but not for the cauchit link. Figure 2(b) visualizes the shrinkage induced by the penalization by Jeffreys' invariant prior for the logit, probit, complementary log-log and cauchit links. For each link function, we obtain all possible fitted probabilities from a complete enumeration of a saturated model with π i = G(β 1 + β 2 x i ) (i = 1, 2), where x 1 = -1, x 2 = 1, m 1 = 9 and m 2 = 9. The grey curves are the contours of log |X T W (π)X |. An arrow is drawn from each pair of estimated probabilities based on the maximum likelihood estimates to the corresponding pair of estimated probabilities based on the penalized likelihood estimates, to demonstrate the induced shrinkage towards (z 0 , z 0 ) T according to the results in § 3. Despite the fact that ω(z) is not concave for the cauchit link, the fitted probabilities still shrink towards (z 0 , z 0 ) T = (1/2, 1/2) T . The plots in Fig. 2 are invariant with respect to the particular choices of x 1 and x 2 , as long as x 1 | = x 2 . For either maximum likelihood or maximum penalized likelihood, if the estimates of β 1 and β 2 are b 1 and b 2 for x 1 = -1 and x 2 = 1, then the new estimates for any x 1 , x 2 ∈ R with x 1 | = x 2 are b 1b 2 (x 1 + x 2 )/(x 2x 1 ) and 2b 2 /(x 2x 1 ), respectively. Hence, the fitted probabilities will be identical.</p>
        <p>Another illustration of finiteness and shrinkage follows from Example 1. Figure 3 shows the paths of the team ability contrasts as a varies from 0 to 5. The estimates are obtained using for one-parameter logistic regression models is equivalent to maximizing (4) with a = 1/6. Hence, the results in § 2 also establish the finiteness of the estimate from median bias reduction in one-parameter logistic regression, and imply that the induced shrinkage to equiprobability will be less strong than penalization by the Jeffreys prior. Kenne Pagui et al. ( 2017) observed such properties in numerical studies for p &gt; 1. When p &gt; 1, though, median bias reduction is no longer equivalent to maximizing (4) with a = 1/6.</p>
        <p>Downloaded from https://academic.oup.com/biomet/article/108/1/71/5880219 by guest on 08 March 2021</p>
        <p>The authors are affiliated with The Alan Turing Institute and were supported by the U.K. Engineering and Physical Sciences Research Council, EPSRC, under grant EP/N510129/1. Firth was also partly supported by the EPSRC programme 'Intractable Likelihood: New Challenges from Modern Applications' under grant EP/K014463/1.</p>
        <p>Fig. 3. Paths of the estimated ability contrasts from the maximization of (4) for a ∈ (0, 5]; in each plot the dashed horizontal line is at zero and the dotted vertical line is at a = 0.5, identifying the reduced-bias estimates on the paths.</p>
        <p>JeffreysMPL, starting at the maximum likelihood estimates of the ability contrasts after adding 0.01 and 0.02 to the actual responses and totals, respectively. In accordance with the theoretical results in § 2.1, the estimated ability contrasts are finite for every a &gt; 0; and, as expected from the results in § 2.2, shrinkage towards equiprobability becomes stronger as a increases.</p>
        <p>A recent stream of literature investigates the use of the coefficient path defined by maximization of the penalized loglikelihood (4) for the prediction of rare events through logistic regression. Elgmati et al. (2015) studied that path for a ∈ (0, 1/2] and proposed taking a to be around 0.1 in order to handle issues related to infinite estimates, and they obtained predicted probabilities that are less biased than those based on the reduced-bias estimates (a = 0.5). More recently, Puhr et al. (2017) proposed two new methods for the prediction of rare events, and performed extensive simulation studies to compare the performance of their methods and various others, including maximum penalized likelihood with a = 0.1 and a = 0.5.</p>
        <p>The coefficient path can be computed efficiently by using repeated maximum likelihood fits with warm starts. For a grid of values a 1 &lt; • • • &lt; a k with a j &gt; 0 (j = 1, . . . , k), the algorithm JeffreysMPL is first applied with a = a 1 to obtain the maximum penalized likelihood estimates β (a 1 ) ; then JeffreysMPL is applied again with a = a 2 and starting values b = β (a 1 ) , and so on, until β (a k ) has been computed. This process supplies JeffreysMPL with the best available starting values as the algorithm walks through the grid. The finiteness of the components of β (a 1 ) , . . . , β (a k ) and the shrinkage properties described in § 2.2 and § 3 contribute to the stability of the overall process. The properties of the coefficient path for inference and prediction from binomial regression models, and the development of general procedures for selecting a, are interesting open research topics.</p>
        <p>Kenne Pagui et al. ( 2017) developed a method that can reduce the median bias of the components of the maximum likelihood estimator. According to their results, median bias reduction</p>
        <p>Supplementary material available at Biometrika online includes proofs of Theorems 1-3 and Corollary 1, the algorithm JeffreysMPL, additional numerical results, and R code and data to reproduce all of the numerical work and graphs.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T12:43+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>In a frequentist framework, the exact fit of a structural equation model (SEM) is typically evaluated with the chi-square test and at least one index of approximate fit. Current Bayesian SEM (
            <rs type="software">BSEM)</rs> software provides one measure of overall fit: the posterior predictive p value (PPP 2). Because of the noted limitations of PPP 2, common practice for evaluating Bayesian model fit instead focuses on model comparison, using information criteria or Bayes factors. Fit indices developed under maximumlikelihood estimation have not been incorporated into software for 
            <rs type="software">BSEM</rs>. We propose adapting 7 chi-square-based approximate fit indices for 
            <rs type="software">BSEM</rs>, using a Bayesian analog of the chi-square model-fit statistic. Simulation results show that the sampling distributions of the posterior means of these fit indices are similar to their frequentist counterparts across sample sizes, model types, and levels of misspecification when 
            <rs type="software">BSEMs</rs> are estimated with noninformative priors. The proposed fit indices therefore allow overall model-fit evaluation using familiar metrics of the original indices, with an accompanying interval to quantify their uncertainty. Illustrative examples with real data raise some important issues about the proposed fit indices' application to models specified with informative priors, when Bayesian and frequentist estimation methods might not yield similar results.
        </p>
        <p>The availability of Markov chain Monte Carlo (MCMC) estimation in user-friendly structural equation modeling (SEM) programs such as 
            <rs type="software">Mplus</rs> (Muthén &amp; Muthén, 1998-2017), 
            <rs type="software">Amos</rs> (Arbuckle, 2012), and the 
            <rs type="software">R</rs> (
            <rs type="creator">R Core Team</rs>, 2018) package 
            <rs type="software">blavaan</rs> (Merkle &amp; Rosseel, 2018) has contributed to the increasing popularity of Bayesian SEM (BSEM; Muthén &amp; Asparouhov, 2012;Song &amp; Lee, 2012). Model-fit evaluation for BSEM has therefore become a topic of recent debate. In a frequentist framework, the exact fit of an SEM is tested with the chi-square statistic, typically complemented by reporting at least one index of approximate fit (Brown, 2006;Hu &amp; Bentler, 1999;Kline, 2016). These familiar fit measures have not traditionally been provided as standard output in 
            <rs type="software">BSEM</rs> software. Building on the recent proposal of a Bayesian approximate fit measure (Hoofs, van de Schoot, Jansen, &amp; Kant, 2018), we propose several approximate chi-square-based fit indices for BSEM that are calculated from a Bayesian analog of the chi-square statistic. Hoofs et al. (2018) were motivated to define a fit index for BSEM for the same reason that motivated the development of numerous fit indices for SEM: to supplement a test of exact fit with a descriptive measure of approximate fit, which they noted is especially useful when large samples provide great power to detect trivial misspecifications. We were motivated to extend their ideas by defining fit measures that would behave consistently with the familiar fit measures in SEM, so we begin by considering the case of models with noninformative priors, in which case Bayesian and frequentist estimation routines provide asymptotically equivalent results.
        </p>
        <p>Although it is also of interest to employ fit indices in the more general case, in which priors may be weakly or strongly informative, there are still advantages to fitting an SEM in a Bayesian framework that do not involve the incorporation of prior information. First, more complex (even intractable) models can be fit in a Bayesian framework that are not feasible with standard estimation routines based on covariance structure analysis, such as models that are nonlinear either in the latent variables1 (e.g., including polynomial or interaction terms among latent variables) or in the parameters 2 or that account for complex dependencies among observations-examples of such BSEM applications can be found in Congdon (2009), van der Lans, van den Bergh, and Dieleman (2014), and Song and Lee (2006). The blavaan package, in particular, allows users to easily specify a basic SEM in lavaan (Rosseel, 2012) model syntax, then edit the automatically generated syntax from a general Bayesian modeling program to include features unavailable in standard SEM software (e.g., by specifying a beta regression for a proportion outcome). Second, models can be fit to smaller samples without violating an asymptotic assumption. Third, rather than relying on normal-theory confidence intervals (CIs) derived from the delta method for functions of parameters (e.g., indirect effects are products of direct effects), estimated posterior distributions can be used to calculate complex functions of parameters without assuming they must also have normal sampling distributions, yielding more robust credible intervals (the Bayesian analog of CIs; Y. Yuan &amp; MacKinnon, 2009). Chisquare-based fit indices are very complex functions of model parameters, and most have unknown sampling distributions, so defining fit indices for BSEM allows access to intervals 3 that can quantify uncertainty due to sampling error. Fourth, 95% credible intervals for model parameters have a more intuitive probabilistic interpretation than a 95% CI (Morey, Hoekstra, Rouder, Lee, &amp; Wagenmakers, 2016). Fifth, Bayesian inference has multiple advantages over frequentist inference, even in simpler applications, such as the direct inference about the parameters of interest () instead of inferring about a null hypothesis (related to the unintuitive interpretation of CIs); by conditioning on the data, the accuracy of a credible interval is not identified with the long-run behavior of the estimator (Kruschke, 2010); and frequentist inferential procedures tend to result in misinterpreting the p value and overestimating its information about the "significance" of a result (Matthews, 2001). Efron (1986), Matthews (2001), Wagenmakers, Lee, Lodewyckx, and Iverson (2008), and Kruschke (2010) provide details about the comparison of Bayesian and frequentist inference.Although it is also of interest to employ fit indices in the more general case, in which priors may be weakly or strongly informative, there are still advantages to fitting an SEM in a Bayesian framework that do not involve the incorporation of prior information. First, more complex (even intractable) models can be fit in a Bayesian framework that are not feasible with standard estimation routines based on covariance structure analysis, such as models that are nonlinear either in the latent variables1 (e.g., including polynomial or interaction terms among latent variables) or in the parameters 2 or that account for complex dependencies among observations-examples of such BSEM applications can be found in Congdon (2009), van der Lans, van den Bergh, and Dieleman (2014), and Song and Lee (2006). The blavaan package, in particular, allows users to easily specify a basic SEM in lavaan (Rosseel, 2012) model syntax, then edit the automatically generated syntax from a general Bayesian modeling program to include features unavailable in standard SEM software (e.g., by specifying a beta regression for a proportion outcome). Second, models can be fit to smaller samples without violating an asymptotic assumption. Third, rather than relying on normal-theory confidence intervals (CIs) derived from the delta method for functions of parameters (e.g., indirect effects are products of direct effects), estimated posterior distributions can be used to calculate complex functions of parameters without assuming they must also have normal sampling distributions, yielding more robust credible intervals (the Bayesian analog of CIs; Y. Yuan &amp; MacKinnon, 2009). Chisquare-based fit indices are very complex functions of model parameters, and most have unknown sampling distributions, so defining fit indices for BSEM allows access to intervals 3 that can quantify uncertainty due to sampling error. Fourth, 95% credible intervals for model parameters have a more intuitive probabilistic interpretation than a 95% CI (Morey, Hoekstra, Rouder, Lee, &amp; Wagenmakers, 2016). Fifth, Bayesian inference has multiple advantages over frequentist inference, even in simpler applications, such as the direct inference about the parameters of interest () instead of inferring about a null hypothesis (related to the unintuitive interpretation of CIs); by conditioning on the data, the accuracy of a credible interval is not identified with the long-run behavior of the estimator (Kruschke, 2010); and frequentist inferential procedures tend to result in misinterpreting the p value and overestimating its information about the "significance" of a result (Matthews, 2001). Efron (1986), Matthews (2001), Wagenmakers, Lee, Lodewyckx, and Iverson (2008), and Kruschke (2010) provide details about the comparison of Bayesian and frequentist inference.</p>
        <p>Thus, we consider it meaningful to define fit indices for BSEM at least in the limited case of noninformative priors (and not just in the case of very large samples; Hoofs et al., 2018), in which case decades of research on fit indices in SEM might also be relevant at least for this subset of BSEMs. As noted by van de Schoot, Winter, Ryan, Zondervan-Zwijnenburg, and Depaoli (2017) in their review of 25 years of Bayesian psychological science, only 12.6% of published articles reported the ability to specify priors as a motivating factor for using Bayesian methods. Because 31.1% of publications failed to report information about their priors at all (van de Schoot et al., 2017), it is difficult to discern how many BSEM publications have solely used noninformative priors since Scheines, Hoijtink, and Boomsma (1999) proposed their estimation, but 8.4% of publications using Bayesian methods seemed to imply that the authors relied on software packages' default priors (typically noninformative; Arbuckle, 2012;Merkle &amp; Rosseel, 2018;Muthén &amp; Muthén, 1998-2017). So despite the popularity of small-variance priors proposed by Muthén and Asparouhov (2012) to account for trivial model misspecifications, we posit that it is not unreasonable to assume that 
            <rs type="software">BSEM</rs> is also commonly applied with noninformative priors.
        </p>
        <p>We begin by reviewing some frequentist measures of model fit as well as fit measures currently provided by 
            <rs type="software">BSEM</rs> software. We then propose how to adapt familiar SEM fit measures for 
            <rs type="software">BSEM</rs> and compare our proposal with that of Hoofs et al. (2018). We present results from a simulation study designed to compare our proposed 
            <rs type="software">BSEM</rs> fit indices with their frequentist counterparts under maximum likelihood estimation (MLE) in various conditions, after which we enumerate important issues for further investigation-namely, the effects of missing data and informative priors. Using the well-known Holzinger and Swineford (1939) data set, we verify our expectations about these effects in the section Illustrative Examples, which demonstrate the importance of future Monte Carlo research into these issues. Our online OSF materials4 include R syntax to replicate these example analyses, which also show how to obtain the proposed fit indices with the 
            <rs type="software">R</rs> package 
            <rs type="software">blavaan</rs> (Merkle &amp; Rosseel, 2018).
        </p>
        <p>The Chi-Square StatisticThe Chi-Square Statistic</p>
        <p>The chi-square test statistic is calculated from the discrepancy function used to obtain parameter estimates when fitting a hypothesized model to data. Because SEMs were traditionally developed as analyses of covariance structure, most discrepancy functions available in SEM software (Kline, 2016) are based on comparing the sample covariance matrix S with the model-implied covariance matrix Α( ) (or simply ͚ ˆ). The most commonly implemented is the maximum likelihood discrepancy function:The chi-square test statistic is calculated from the discrepancy function used to obtain parameter estimates when fitting a hypothesized model to data. Because SEMs were traditionally developed as analyses of covariance structure, most discrepancy functions available in SEM software (Kline, 2016) are based on comparing the sample covariance matrix S with the model-implied covariance matrix Α( ) (or simply ͚ ˆ). The most commonly implemented is the maximum likelihood discrepancy function:</p>
        <p>where p is the number of variables in the model, x is the vector of sample means, and ˆis the vector of model-implied means. The corresponding statistic is calculated5 as ML 2 ϭ N ϫ F ML .where p is the number of variables in the model, x is the vector of sample means, and ˆis the vector of model-implied means. The corresponding statistic is calculated5 as ML 2 ϭ N ϫ F ML .</p>
        <p>More generally, ML 2 is a likelihood ratio test (LRT) statistic, calculated by plugging the nth observation's (perhaps incomplete) data vector y n into the multivariate normal log-likelihood (ᐉ) function:More generally, ML 2 is a likelihood ratio test (LRT) statistic, calculated by plugging the nth observation's (perhaps incomplete) data vector y n into the multivariate normal log-likelihood (ᐉ) function:</p>
        <p>using model-implied ˆand ͚ ˆderived from estimated model parameters in place of and ⌺ above. Summing yields the loglikelihood of the hypothesized model (M H ),using model-implied ˆand ͚ ˆderived from estimated model parameters in place of and ⌺ above. Summing yields the loglikelihood of the hypothesized model (M H ),</p>
        <p>Similarly, using the observed sample statistics x and S in place of and ⌺ above yields the log-likelihood (ᐉ S ) of the saturated model (M S ). The LRT statistic for M H is also called its deviance from M S :Similarly, using the observed sample statistics x and S in place of and ⌺ above yields the log-likelihood (ᐉ S ) of the saturated model (M S ). The LRT statistic for M H is also called its deviance from M S :</p>
        <p>which is distributed as a chi-square random variable with df ϭ p ‫ء‬ Ϫ q, where p ‫ء‬ is the number of nonredundant sample moments and q is the number of estimated parameters. In analyses of covariance structure only, p * ϭ p͑p ϩ 1͒ 2 , whereas p * ϭ p͑p ϩ 3͒ 2 in mean and covariance structure (MACS) models. MLE assumes the observed variables are multivariate normally distributed in order for the test statistic to be asymptotically distributed as a chi-square random variable. Other discrepancy functions can be applied and multiplied by N to calculate a modelfit statistic. Some discrepancy functions (e.g., unweighted least squares) allow the normality assumption to be relaxed (Browne, 1984). Robust corrections have also been developed to adjust the ML test statistic to be more approximately chi-square distributed when assumptions are violated (Savalei, 2014).which is distributed as a chi-square random variable with df ϭ p ‫ء‬ Ϫ q, where p ‫ء‬ is the number of nonredundant sample moments and q is the number of estimated parameters. In analyses of covariance structure only, p * ϭ p͑p ϩ 1͒ 2 , whereas p * ϭ p͑p ϩ 3͒ 2 in mean and covariance structure (MACS) models. MLE assumes the observed variables are multivariate normally distributed in order for the test statistic to be asymptotically distributed as a chi-square random variable. Other discrepancy functions can be applied and multiplied by N to calculate a modelfit statistic. Some discrepancy functions (e.g., unweighted least squares) allow the normality assumption to be relaxed (Browne, 1984). Robust corrections have also been developed to adjust the ML test statistic to be more approximately chi-square distributed when assumptions are violated (Savalei, 2014).</p>
        <p>Regardless of the discrepancy function (or whether a robust correction was applied), the chi-square statistic tests the null hy-pothesis of exact model fit (H 0 : the hypothesized model perfectly represents the true data-generating process). Because theoretical models are, by necessity, merely approximations of reality (Mac-Callum, 2003), the H 0 of exact fit is often considered a priori to be false. Because researchers typically cannot reasonably expect to retain H 0 in practice, the chi-square test is often of limited general or practical interest (West, Taylor, &amp; Wu, 2012).Regardless of the discrepancy function (or whether a robust correction was applied), the chi-square statistic tests the null hy-pothesis of exact model fit (H 0 : the hypothesized model perfectly represents the true data-generating process). Because theoretical models are, by necessity, merely approximations of reality (Mac-Callum, 2003), the H 0 of exact fit is often considered a priori to be false. Because researchers typically cannot reasonably expect to retain H 0 in practice, the chi-square test is often of limited general or practical interest (West, Taylor, &amp; Wu, 2012).</p>
        <p>Furthermore, the power of the chi-square test to detect small (even negligible) inconsistencies with H 0 increases with N. To assess whether a model's misspecification is of any practical importance (i.e., whether predicted values are close enough to observed values to be useful in practice), several methodologists have proposed indices of approximate fit to complement the chisquare significance test, functionally similar to providing measures of practical significance to complement significance tests in other contexts (e.g., Cohen's d to complement a t test). Most proposed fit indices make use of the chi-square value by adjusting it or comparing it with another model's chi square, for example, to correct for model complexity, number of parameters, or overfitting. So the chi-square statistic has long remained the focus of overall model fit in SEM, even if indirectly.Furthermore, the power of the chi-square test to detect small (even negligible) inconsistencies with H 0 increases with N. To assess whether a model's misspecification is of any practical importance (i.e., whether predicted values are close enough to observed values to be useful in practice), several methodologists have proposed indices of approximate fit to complement the chisquare significance test, functionally similar to providing measures of practical significance to complement significance tests in other contexts (e.g., Cohen's d to complement a t test). Most proposed fit indices make use of the chi-square value by adjusting it or comparing it with another model's chi square, for example, to correct for model complexity, number of parameters, or overfitting. So the chi-square statistic has long remained the focus of overall model fit in SEM, even if indirectly.</p>
        <p>When H 0 is false, the model-fit test statistic follows a noncentral chi-square distribution, with noncentrality parameter . When H 0 is true, ϭ 0 and the expected value of chi square is its degrees of freedom (df), whereas the expected value of a noncentral chi square is the sum of its df and . Thus, the difference between a model's chi-square statistic and df is a sample estimate of the model's noncentrality parameter: ˆ ϭ 2 Ϫ df. Several fit indices are based on a rescaling of ˆ; we present the most popular ones below.When H 0 is false, the model-fit test statistic follows a noncentral chi-square distribution, with noncentrality parameter . When H 0 is true, ϭ 0 and the expected value of chi square is its degrees of freedom (df), whereas the expected value of a noncentral chi square is the sum of its df and . Thus, the difference between a model's chi-square statistic and df is a sample estimate of the model's noncentrality parameter: ˆ ϭ 2 Ϫ df. Several fit indices are based on a rescaling of ˆ; we present the most popular ones below.</p>
        <p>Each df represents a restriction on how estimated model parameters can reproduce the observed sample moments x and S. Thus, greater df implies a more parsimonious model, which might therefore deviate even more from the true data-generating processreferred to as approximation discrepancy (MacCallum, 2003). Steiger and Lind (1980) proposed expressing model misfit as an average across the number of restrictions the model made. To express this average misfit per df in the metric of the discrepancy function (F ML ), ˆ is divided by N as well as df:Each df represents a restriction on how estimated model parameters can reproduce the observed sample moments x and S. Thus, greater df implies a more parsimonious model, which might therefore deviate even more from the true data-generating processreferred to as approximation discrepancy (MacCallum, 2003). Steiger and Lind (1980) proposed expressing model misfit as an average across the number of restrictions the model made. To express this average misfit per df in the metric of the discrepancy function (F ML ), ˆ is divided by N as well as df:</p>
        <p>(5)(5)</p>
        <p>Note that the population root mean square error of approximation (RMSEA; ε) is independent of sample size:Note that the population root mean square error of approximation (RMSEA; ε) is independent of sample size:</p>
        <p>Unlike most fit indices, the RMSEA has a known sampling distribution (Browne &amp; Cudeck, 1992), so a CI can be provided to test null hypotheses about specific population values of RMSEA (MacCallum, Browne, &amp; Cai, 2006). Higher values of RMSEA correspond to worse fit. (Browne &amp; Cudeck, 1992;Hu &amp; Bentler, 1999;MacCallum, Browne, &amp; Sugawara, 1996).Unlike most fit indices, the RMSEA has a known sampling distribution (Browne &amp; Cudeck, 1992), so a CI can be provided to test null hypotheses about specific population values of RMSEA (MacCallum, Browne, &amp; Cai, 2006). Higher values of RMSEA correspond to worse fit. (Browne &amp; Cudeck, 1992;Hu &amp; Bentler, 1999;MacCallum, Browne, &amp; Sugawara, 1996).</p>
        <p>McDonald (1989) also proposed dividing ˆ by N but also exponentiated to express misfit in terms of likelihood rather than log-likelihood. Taking the reciprocal (i.e., a negative exponent) results in an index that measures goodness (rather than badness) of fit, with a theoretical upper bound of 1 indicating excellent model fit,McDonald (1989) also proposed dividing ˆ by N but also exponentiated to express misfit in terms of likelihood rather than log-likelihood. Taking the reciprocal (i.e., a negative exponent) results in an index that measures goodness (rather than badness) of fit, with a theoretical upper bound of 1 indicating excellent model fit,</p>
        <p>Mc ϭ e Ϫ 1 2 ͩ N ͪ . ( 7) Steiger (1989) proposed another goodness-of-fit index as a function of ˆ (again, divided by N to express misfit in the metric of the discrepancy function) and the number of variables p,Mc ϭ e Ϫ 1 2 ͩ N ͪ . ( 7) Steiger (1989) proposed another goodness-of-fit index as a function of ˆ (again, divided by N to express misfit in the metric of the discrepancy function) and the number of variables p,</p>
        <p>Like McDonald's (1989) centrality index (Mc), values of "gamma-hat" (⌫ ˆ) approaching 1 indicate better fit. Maiti and Mukherjee (1990) described this as an unbiased estimator of the population goodness-of-fit index (GFI; Jöreskog &amp; Sörbom, 2006). An adjusted version (⌫ ˆadj ) is less likely to be positively biased in small samples, although Hu and Bentler (1998) and Fan and Sivo (2007) showed that ⌫ ˆis already very independent of sample size,Like McDonald's (1989) centrality index (Mc), values of "gamma-hat" (⌫ ˆ) approaching 1 indicate better fit. Maiti and Mukherjee (1990) described this as an unbiased estimator of the population goodness-of-fit index (GFI; Jöreskog &amp; Sörbom, 2006). An adjusted version (⌫ ˆadj ) is less likely to be positively biased in small samples, although Hu and Bentler (1998) and Fan and Sivo (2007) showed that ⌫ ˆis already very independent of sample size,</p>
        <p>Incremental fit indices are based on the idea that there is a continuum between a worst-fitting (but still theoretically defensible) baseline model (M 0 ; typically an independence model, in which all correlations are fixed to zero) and the best-fitting model (M S ; represented by the saturated model, in which all observed covariances are freely estimated). A hypothesized model (M H ) should lie somewhere between these two extremes, and incremental fit indices indicate where along the continuum M H is located. Values closer to zero indicate M H is closer to the poor-fitting M 0 , and values closer to 1 indicate M H is closer to the perfect-fitting M S . This allows nonnested hypothesized models to be compared, so long as they are both nested within the same M S (true by definition) and the same specified M 0 is nested within all competing models (Bentler &amp; Bonett, 1980;Widaman &amp; Thompson, 2003).Incremental fit indices are based on the idea that there is a continuum between a worst-fitting (but still theoretically defensible) baseline model (M 0 ; typically an independence model, in which all correlations are fixed to zero) and the best-fitting model (M S ; represented by the saturated model, in which all observed covariances are freely estimated). A hypothesized model (M H ) should lie somewhere between these two extremes, and incremental fit indices indicate where along the continuum M H is located. Values closer to zero indicate M H is closer to the poor-fitting M 0 , and values closer to 1 indicate M H is closer to the perfect-fitting M S . This allows nonnested hypothesized models to be compared, so long as they are both nested within the same M S (true by definition) and the same specified M 0 is nested within all competing models (Bentler &amp; Bonett, 1980;Widaman &amp; Thompson, 2003).</p>
        <p>The earliest incremental fit index was proposed by Tucker and Lewis (1973) as a reliability index to assist selecting the number of factors in exploratory factor analysis. Bentler and Bonett (1980) later referred to the Tucker-Lewis index (TLI) as the nonnormed fit index (NNFI) because its values can fall outside the 0 to 1 range (e.g., TLI Ͼ 1 whenThe earliest incremental fit index was proposed by Tucker and Lewis (1973) as a reliability index to assist selecting the number of factors in exploratory factor analysis. Bentler and Bonett (1980) later referred to the Tucker-Lewis index (TLI) as the nonnormed fit index (NNFI) because its values can fall outside the 0 to 1 range (e.g., TLI Ͼ 1 when</p>
        <p>where the subscripts "H" and "0" indicate to which model the chi-square and df belong. Bentler and Bonett also proposed a normed fit index (NFI) that is bound to a 0 to 1 scale,where the subscripts "H" and "0" indicate to which model the chi-square and df belong. Bentler and Bonett also proposed a normed fit index (NFI) that is bound to a 0 to 1 scale,</p>
        <p>Although NFI has the advantage over TLI of being restricted to a 0 to 1 scale, NFI is unfortunately heavily influenced by sample size, whereas TLI is relatively independent of sample size (Fan &amp; Sivo, 2007;Hu &amp; Bentler, 1998). Bentler (1990) later proposed a similar index based on noncentrality-the comparative fit index (CFI)-which is also normed to fall between 0 and 1,Although NFI has the advantage over TLI of being restricted to a 0 to 1 scale, NFI is unfortunately heavily influenced by sample size, whereas TLI is relatively independent of sample size (Fan &amp; Sivo, 2007;Hu &amp; Bentler, 1998). Bentler (1990) later proposed a similar index based on noncentrality-the comparative fit index (CFI)-which is also normed to fall between 0 and 1,</p>
        <p>CFI has the advantage over both TLI and NFI by being both normed and relatively independent of sample size (Fan &amp; Sivo, 2007;Hu &amp; Bentler, 1998).CFI has the advantage over both TLI and NFI by being both normed and relatively independent of sample size (Fan &amp; Sivo, 2007;Hu &amp; Bentler, 1998).</p>
        <p>We find it reasonable to interpret fit indices as merely descriptive measures of approximate fit, and to interpret questionable values as indicating not that the model should be rejected outright but that further investigation of local sources of misspecification (e.g., correlation residuals or modification indices) would be warranted. Fit indices were not proposed to function as test statistics but rather to "provide important adjunct information in evaluating models" (Bentler &amp; Bonett, 1980, p. 604). Thus, similar to measures of effect size in other contexts (e.g., Cohen, 1992, provided guidelines to interpret correlations, standardized mean differences, and proportions of variance explained as small, medium, or large), guidelines were sought for interpreting the magnitude of fit indices. Because the H 0 tested by ML 2 is that there is no discrepancy between the model-implied (mean and) covariance structure and the true population structure, a large "effect size" would indicate that a significant ML 2 test has identified a large discrepancy (i.e., "badness of fit") between a hypothesized model and the true data-generating process. Initially proposed guidelines were largely heuristic, based on experience (see Bentler &amp; Bonett, 1980, p. 600;Browne &amp; Cudeck, 1992, p. 239;MacCallum et al., 1996, p. 134). However, Hu andBentler (1998, 1999) used Monte Carlo simulation results to develop a more objective set of criteria for interpreting fit indices, based on a hypothesis-testing rationale (i.e., This document is copyrighted by the American Psychological Association or one of its allied publishers.We find it reasonable to interpret fit indices as merely descriptive measures of approximate fit, and to interpret questionable values as indicating not that the model should be rejected outright but that further investigation of local sources of misspecification (e.g., correlation residuals or modification indices) would be warranted. Fit indices were not proposed to function as test statistics but rather to "provide important adjunct information in evaluating models" (Bentler &amp; Bonett, 1980, p. 604). Thus, similar to measures of effect size in other contexts (e.g., Cohen, 1992, provided guidelines to interpret correlations, standardized mean differences, and proportions of variance explained as small, medium, or large), guidelines were sought for interpreting the magnitude of fit indices. Because the H 0 tested by ML 2 is that there is no discrepancy between the model-implied (mean and) covariance structure and the true population structure, a large "effect size" would indicate that a significant ML 2 test has identified a large discrepancy (i.e., "badness of fit") between a hypothesized model and the true data-generating process. Initially proposed guidelines were largely heuristic, based on experience (see Bentler &amp; Bonett, 1980, p. 600;Browne &amp; Cudeck, 1992, p. 239;MacCallum et al., 1996, p. 134). However, Hu andBentler (1998, 1999) used Monte Carlo simulation results to develop a more objective set of criteria for interpreting fit indices, based on a hypothesis-testing rationale (i.e., This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>minimizing Type I and II errors), which has been met with much criticism (Beauducel &amp; Wittmann, 2005;Fan &amp; Sivo, 2005;Heene, Hilbert, Draxler, Ziegler, &amp; Bühner, 2011;Heene, Hilbert, Freudenthaler, &amp; Bühner, 2012;Marsh, Hau, &amp; Wen, 2004). In our view, there are at least three major limitations preventing any of the previously proposed cutoffs from being generally useful to "test hypotheses" about approximate fit. First, they do not account for sampling variability. When claiming fit indices are relatively unaffected by sample size (e.g., Hu &amp; Bentler, 1998), that only refers to the mean of their sampling distribution; as with any statistic, their sampling variance shrinks with larger N (Marsh et al., 2004). Second, sampling distributions of fit indices (including their means) vary across characteristics of the data (e.g., missing data: Davey, 2005; categorical data: Sass, Schmitt, &amp; Marsh, 2014) and the model (e.g., number of factors and indicators; Jorgensen, Kite, Chen, &amp; Short, 2018). Third, it is rarely clear what the numerical value of an index means in any absolute sense, so even when using a fit index (typically RMSEA's 90% CI) to actually test a less restrictive H 0 (MacCallum et al., 2006;K.-H. Yuan, Chan, Marcoulides, &amp; Bentler, 2016), there is little justification for preferring a particular value for the H 0 .minimizing Type I and II errors), which has been met with much criticism (Beauducel &amp; Wittmann, 2005;Fan &amp; Sivo, 2005;Heene, Hilbert, Draxler, Ziegler, &amp; Bühner, 2011;Heene, Hilbert, Freudenthaler, &amp; Bühner, 2012;Marsh, Hau, &amp; Wen, 2004). In our view, there are at least three major limitations preventing any of the previously proposed cutoffs from being generally useful to "test hypotheses" about approximate fit. First, they do not account for sampling variability. When claiming fit indices are relatively unaffected by sample size (e.g., Hu &amp; Bentler, 1998), that only refers to the mean of their sampling distribution; as with any statistic, their sampling variance shrinks with larger N (Marsh et al., 2004). Second, sampling distributions of fit indices (including their means) vary across characteristics of the data (e.g., missing data: Davey, 2005; categorical data: Sass, Schmitt, &amp; Marsh, 2014) and the model (e.g., number of factors and indicators; Jorgensen, Kite, Chen, &amp; Short, 2018). Third, it is rarely clear what the numerical value of an index means in any absolute sense, so even when using a fit index (typically RMSEA's 90% CI) to actually test a less restrictive H 0 (MacCallum et al., 2006;K.-H. Yuan, Chan, Marcoulides, &amp; Bentler, 2016), there is little justification for preferring a particular value for the H 0 .</p>
        <p>By approximating a sampling distribution consistent with M H , "Bayesian variant[s]" (Hoofs et al., 2018, p. 543) of SEM fit indices would provide a viable solution to the first two problems (as resampling methods have; Cheng &amp; Wu, 2017;Jorgensen, Kite, et al., 2018;Zhang &amp; Savalei, 2016). Although we do not set out to resolve the third issue in this article, we later note how Bayesian fit indices could be used to avoid the third major limitation by removing the need for a cutoff value at all. However, these three issues are contingent on applying cutoff guidelines as critical values to test a H 0 of approximate fit. Again, we do not explicitly endorse this interpretation, instead encouraging researchers to consider the fit indices descriptive of model fit. No index will be fully descriptive, so any questionably poor values (including within the 90% CI of RMSEA) should be considered by researchers as an invitation to explore how their model fails using tools for detecting local misspecification.By approximating a sampling distribution consistent with M H , "Bayesian variant[s]" (Hoofs et al., 2018, p. 543) of SEM fit indices would provide a viable solution to the first two problems (as resampling methods have; Cheng &amp; Wu, 2017;Jorgensen, Kite, et al., 2018;Zhang &amp; Savalei, 2016). Although we do not set out to resolve the third issue in this article, we later note how Bayesian fit indices could be used to avoid the third major limitation by removing the need for a cutoff value at all. However, these three issues are contingent on applying cutoff guidelines as critical values to test a H 0 of approximate fit. Again, we do not explicitly endorse this interpretation, instead encouraging researchers to consider the fit indices descriptive of model fit. No index will be fully descriptive, so any questionably poor values (including within the 90% CI of RMSEA) should be considered by researchers as an invitation to explore how their model fails using tools for detecting local misspecification.</p>
        <p>Bayesian Model-Fit Assessment Levy (2011) concluded in his review of BSEM model evaluation approaches that techniques for assessing model fit remain underdeveloped. Thus, although the increased availability of 
            <rs type="software">BSEM</rs> software for applied research holds great promise, particularly for complex modeling situations, the full scope of BSEM's practical application remains limited due to the corresponding lack of general guidelines and best practices for model evaluation for applied users.
        </p>
        <p>Although Hoofs et al. (2018, p. 543) recently proposed a "Bayesian variant of the RMSEA" (BRMSEA), most 
            <rs type="software">BSEM</rs> software currently provides only one measure of overall fit: the posterior predictive p value (PPP; Gelman, Meng, &amp; Stern, 1996) based on the familiar chi-square model-fit statistic. Most 
            <rs type="software">BSEM</rs> software also provides only two indices for model comparisonthe deviance information criterion (DIC; Spiegelhalter, Best, Carlin, &amp; van der Linde, 2002) and Schwarz's (1978) so-called Bayesian information criterion (BIC)-although blavaan also provides additional, more recently developed information criteria (Vehtari, Gelman, &amp; Gabry, 2017;Watanabe, 2010)
        </p>
        <p>PPMC (Gelman et al., 1996) is a flexible method to test whether aspects of a model adequately capture features of the data. When MCMC estimation is used to estimate model parameters, a discrepancy function can be specified to capture the degree to which a meaningful feature of the observed data differs from its expected value given the model parameters at iteration i of a Markov chain that has converged on the posterior distribution. If the model is not an adequate representation of the true data-generating process (or at least cannot make sufficiently similar predictions about observed data), the realized value of the discrepancy function will be large for the observed data (D i obs ). Although D is a function of parameters (and data) rather than an estimated parameter itself, D is evaluated using all samples from the posterior distribution of model parameters, resulting in "an empirical approximation to the posterior distribution of the discrepancy measure . . . [also] referred to as the realized values of the discrepancy measure" (Levy, 2011, pp. 672-673).PPMC (Gelman et al., 1996) is a flexible method to test whether aspects of a model adequately capture features of the data. When MCMC estimation is used to estimate model parameters, a discrepancy function can be specified to capture the degree to which a meaningful feature of the observed data differs from its expected value given the model parameters at iteration i of a Markov chain that has converged on the posterior distribution. If the model is not an adequate representation of the true data-generating process (or at least cannot make sufficiently similar predictions about observed data), the realized value of the discrepancy function will be large for the observed data (D i obs ). Although D is a function of parameters (and data) rather than an estimated parameter itself, D is evaluated using all samples from the posterior distribution of model parameters, resulting in "an empirical approximation to the posterior distribution of the discrepancy measure . . . [also] referred to as the realized values of the discrepancy measure" (Levy, 2011, pp. 672-673).</p>
        <p>To quantify whether the discrepancy is larger than would be expected due to chance sampling fluctuations, a random sample of replicated data is drawn from the population implied by the model parameters (i.e., data that are predicted by the posterior distribution to occur if the model holds) at the same iteration i of the Markov chain. Because the replicated data are consistent with the model, the realized value of the discrepancy function for the replicated data (D i rep ) only reflects sampling error. In contrast to the distribution of D obs , D rep empirically approximates the posterior predictive distribution of the discrepancy function, conditional on the data and estimated model parameters (i.e., the expected values of D if the H 0 of perfect data-model correspondence were true).To quantify whether the discrepancy is larger than would be expected due to chance sampling fluctuations, a random sample of replicated data is drawn from the population implied by the model parameters (i.e., data that are predicted by the posterior distribution to occur if the model holds) at the same iteration i of the Markov chain. Because the replicated data are consistent with the model, the realized value of the discrepancy function for the replicated data (D i rep ) only reflects sampling error. In contrast to the distribution of D obs , D rep empirically approximates the posterior predictive distribution of the discrepancy function, conditional on the data and estimated model parameters (i.e., the expected values of D if the H 0 of perfect data-model correspondence were true).</p>
        <p>If the model is consistent with the population that generated the observed data, then P(D obs Ͼ D rep ) ϭ 50%, but this probability will differ from 50% to the degree that the model over-or underpredicts the feature of the data specified by the chosen discrepancy function. The PPP estimates this probability as the proportion of i ϭ 1, 2, . . . I samples from the posterior (i.e., postadaptation iterations of the Markov chain) for which D i obs Ͼ D i rep . Because the sampling distribution of PPP is not uniform in practice, application of traditional null-hypothesis testing criteria (␣ levels) yields conservative inferences (Levy, 2011); however, many advocate its use as an informative diagnostic for identifying how a model fails (Gelman &amp; Shalizi, 2013) rather than for traditional hypothesis testing.If the model is consistent with the population that generated the observed data, then P(D obs Ͼ D rep ) ϭ 50%, but this probability will differ from 50% to the degree that the model over-or underpredicts the feature of the data specified by the chosen discrepancy function. The PPP estimates this probability as the proportion of i ϭ 1, 2, . . . I samples from the posterior (i.e., postadaptation iterations of the Markov chain) for which D i obs Ͼ D i rep . Because the sampling distribution of PPP is not uniform in practice, application of traditional null-hypothesis testing criteria (␣ levels) yields conservative inferences (Levy, 2011); however, many advocate its use as an informative diagnostic for identifying how a model fails (Gelman &amp; Shalizi, 2013) rather than for traditional hypothesis testing.</p>
        <p>PPMC is flexible regarding the discrepancy function, which can evaluate any feature of the model from which predictions can be derived. For example, in general linear models, the discrepancy function can be summary measures such as the mean, standard deviation, minimum, or maximum values of the model's outcome variable (Gelman et al., 2014). Levy (2011) described some reasonable discrepancy functions for evaluating an SEM, such as the This document is copyrighted by the American Psychological Association or one of its allied publishers.PPMC is flexible regarding the discrepancy function, which can evaluate any feature of the model from which predictions can be derived. For example, in general linear models, the discrepancy function can be summary measures such as the mean, standard deviation, minimum, or maximum values of the model's outcome variable (Gelman et al., 2014). Levy (2011) described some reasonable discrepancy functions for evaluating an SEM, such as the This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>SRMR, the model-implied factor correlations, and the familiar chi-square model-fit statistic (i.e., the likelihood ratio comparing the hypothesized model with a saturated model). Because 
            <rs type="software">BSEM</rs> software packages (Arbuckle, 2012;Merkle &amp; Rosseel, 2018;Muthén &amp; Asparouhov, 2012) uniformly report only PPP based on the chi-square statistic-and because this paper focuses only on chi-square-based fit indices-we confine our discussion of PPP to this special case, denoting it as PPP 2.
        </p>
        <p>Information Criteria Based on the Posterior Distribution Spiegelhalter et al. (2002) proposed DIC as a Bayesian generalization of Akaike's information criterion (AIC), which adjusts a likelihood-based measure of model fit (e.g., the log-likelihood of the model) by taking the model's complexity into account. Complexity can be quantified by the number of estimated parameters (p), although that oversimplifies how models can be parsimonious (Preacher, 2006). In the SEM framework, AIC can be calculated6 as 2 ϩ 2p, but in BSEM, the number of estimated parameters cannot simply be represented as an integer. For example, estimating three parameters with restrictively informative priors effectively limits the parameter space much more than estimating the same three parameters using uninformative priors, so the number of estimated parameters should differ in these situations.Information Criteria Based on the Posterior Distribution Spiegelhalter et al. (2002) proposed DIC as a Bayesian generalization of Akaike's information criterion (AIC), which adjusts a likelihood-based measure of model fit (e.g., the log-likelihood of the model) by taking the model's complexity into account. Complexity can be quantified by the number of estimated parameters (p), although that oversimplifies how models can be parsimonious (Preacher, 2006). In the SEM framework, AIC can be calculated6 as 2 ϩ 2p, but in BSEM, the number of estimated parameters cannot simply be represented as an integer. For example, estimating three parameters with restrictively informative priors effectively limits the parameter space much more than estimating the same three parameters using uninformative priors, so the number of estimated parameters should differ in these situations.</p>
        <p>The effective number of parameters pD (or p ˆ; Vehtari et al., 2017) can be estimated from the posterior distribution by calculating the deviance (i.e., 2 ) using each vector of parameters ( ˜i) sampled from the posterior distribution during MCMC estimation, the same way one would calculate chi square using ML estimates of the model parameters. But sampling ˜i repeatedly from the joint posterior yields a posterior distribution of the deviance, with posterior-mean devianceThe effective number of parameters pD (or p ˆ; Vehtari et al., 2017) can be estimated from the posterior distribution by calculating the deviance (i.e., 2 ) using each vector of parameters ( ˜i) sampled from the posterior distribution during MCMC estimation, the same way one would calculate chi square using ML estimates of the model parameters. But sampling ˜i repeatedly from the joint posterior yields a posterior distribution of the deviance, with posterior-mean deviance</p>
        <p>A different point-estimate for the model's deviance, D( ), can be calculated using ˆand ͚ ˆimplied by the posterior means of the parameters , which is analogous to the ML 2 calculated using the ML point estimates (which are the mode rather than the mean of the likelihood function). The sampled parameters ˜i at a particular iteration i in a Markov chain will rarely (if ever) be identical to the posterior mean , so the discrepancy function D i calculated at iteration i is expected to exceed D( ) because values of ˜i further from7 yield smaller (log-)likelihoods of the data. Spiegelhalter et al. (2002) called the degree to which D exceeds D( ) the effective number of parameters,A different point-estimate for the model's deviance, D( ), can be calculated using ˆand ͚ ˆimplied by the posterior means of the parameters , which is analogous to the ML 2 calculated using the ML point estimates (which are the mode rather than the mean of the likelihood function). The sampled parameters ˜i at a particular iteration i in a Markov chain will rarely (if ever) be identical to the posterior mean , so the discrepancy function D i calculated at iteration i is expected to exceed D( ) because values of ˜i further from7 yield smaller (log-)likelihoods of the data. Spiegelhalter et al. (2002) called the degree to which D exceeds D( ) the effective number of parameters,</p>
        <p>Conceptually, pD quantifies the degree to which the fit of the model improves due to allowing unknown values to vary across iterations of the Markov chain rather than fixing them to hypothesized values. Thus, it is a crude measure of model complexity, but it is not a count of the number of parameters (as in the frequentist framework). Another definition for pD is as a function of the variance of D i across iterations of the Markov chain (Vehtari et al., 2017), which perhaps more clearly illustrates that pD represents the uncertainty about how well a hypothesized model actually fits the data, and that uncertainty increases as we place fewer restrictions (or less restrictive priors) on our fitted model.Conceptually, pD quantifies the degree to which the fit of the model improves due to allowing unknown values to vary across iterations of the Markov chain rather than fixing them to hypothesized values. Thus, it is a crude measure of model complexity, but it is not a count of the number of parameters (as in the frequentist framework). Another definition for pD is as a function of the variance of D i across iterations of the Markov chain (Vehtari et al., 2017), which perhaps more clearly illustrates that pD represents the uncertainty about how well a hypothesized model actually fits the data, and that uncertainty increases as we place fewer restrictions (or less restrictive priors) on our fitted model.</p>
        <p>DIC itself can be calculated analogously to AIC, with the deviance evaluated at the posterior mean D( ) substituted for the analogous ML 2 , and the effective number of parameters pD substituted for the number of parameters in MLE:DIC itself can be calculated analogously to AIC, with the deviance evaluated at the posterior mean D( ) substituted for the analogous ML 2 , and the effective number of parameters pD substituted for the number of parameters in MLE:</p>
        <p>Alternatively, DIC can be calculated8 as a function of the posteriormean deviance D , as shown on the far right-hand side of Equation 15.Alternatively, DIC can be calculated8 as a function of the posteriormean deviance D , as shown on the far right-hand side of Equation 15.</p>
        <p>Although there are not established fit indices in BSEM that are equivalent to their frequentist counterparts (which might not even be possible, given the lack of df or an integer number of estimated parameters in a Bayesian context), a recently proposed Bayesian analog of RMSEA (Hoofs et al., 2018) showed promise for evaluating approximate fit in large samples (N Ͼ 1,000). Hoofs et al. (2018) presented an intriguing approach to BRMSEA that attempts to estimate the same parameter that RMSEA estimates in a frequentist context. They replaced the quantities ML 2 and df in Equation 5 with analogous quantities representing a model's misfit and complexity, as conceptualized in terms of posterior predictive model checking (PPP 2) and the effective number of parameters (pD) in DIC. Specifically, rather than using the discrepancy function evaluated at the posterior mean-D( ), which is analogous to the chi square when priors are noninformative-model misfit is represented by the difference in discrepancies of observed and replicated data (D i obs Ϫ D i rep ) at each iteration in the Markov chain, as in posterior predictive model checks (hence, our "PPMC" superscript in Equation 16 below). Model complexity (as represented by df ϭ p ‫ء‬q in Equation 5) is calculated as p ‫ء‬ -pD,Although there are not established fit indices in BSEM that are equivalent to their frequentist counterparts (which might not even be possible, given the lack of df or an integer number of estimated parameters in a Bayesian context), a recently proposed Bayesian analog of RMSEA (Hoofs et al., 2018) showed promise for evaluating approximate fit in large samples (N Ͼ 1,000). Hoofs et al. (2018) presented an intriguing approach to BRMSEA that attempts to estimate the same parameter that RMSEA estimates in a frequentist context. They replaced the quantities ML 2 and df in Equation 5 with analogous quantities representing a model's misfit and complexity, as conceptualized in terms of posterior predictive model checking (PPP 2) and the effective number of parameters (pD) in DIC. Specifically, rather than using the discrepancy function evaluated at the posterior mean-D( ), which is analogous to the chi square when priors are noninformative-model misfit is represented by the difference in discrepancies of observed and replicated data (D i obs Ϫ D i rep ) at each iteration in the Markov chain, as in posterior predictive model checks (hence, our "PPMC" superscript in Equation 16 below). Model complexity (as represented by df ϭ p ‫ء‬q in Equation 5) is calculated as p ‫ء‬ -pD,</p>
        <p>(16) Hoofs et al. (2018) proposed their BRMSEA PPMC to complement the PPP 2, because in large samples, PPP 2 rejects all models with even minor misspecification (similar to 2 in SEM). The results from their simulation show that under those conditions (large N, minor misfit, "significantly" small PPP 2), their BRMSEA PPMC would indicate approximately well-fitting models are acceptable, according to commonly used cutoffs (i.e., RMSEA Ͻ .08 is acceptable, RMSEA Ͻ .05 indicates close fit; Browne &amp; Cudeck, 1992).(16) Hoofs et al. (2018) proposed their BRMSEA PPMC to complement the PPP 2, because in large samples, PPP 2 rejects all models with even minor misspecification (similar to 2 in SEM). The results from their simulation show that under those conditions (large N, minor misfit, "significantly" small PPP 2), their BRMSEA PPMC would indicate approximately well-fitting models are acceptable, according to commonly used cutoffs (i.e., RMSEA Ͻ .08 is acceptable, RMSEA Ͻ .05 indicates close fit; Browne &amp; Cudeck, 1992).</p>
        <p>To operate as a measure of effect size to accompany the statistical significance test offered by chi square, a fit index should be sensitive only to misspecification (Fan &amp; Sivo, 2007). Although BRMSEA PPMC has been the first attempt to translate a frequentist fit index to BSEM, it does appear sensitive to sample size as well as model size. For example, when Hoofs et al. (2018) simulated data from a six-indicator confirmatory factor analysis (CFA), the BRMSEA 90% credible interval (with noninformative priors) implied a similar amount of sampling variability as implied by the RMSEA 90% CI under MLE. However, the 90% credible intervals for BRMSEA were much narrower than the 90% CIs for RMSEA when fitting larger, 12-indicator CFA models to smaller samples, leading to much lower power of BRMSEA than RMSEA to detect even severe misspecifications. Thus, when fitting models that are large relative to a smaller sample size, neither the BRMSEA nor PPP 2 would have power to detect important problems with the model.To operate as a measure of effect size to accompany the statistical significance test offered by chi square, a fit index should be sensitive only to misspecification (Fan &amp; Sivo, 2007). Although BRMSEA PPMC has been the first attempt to translate a frequentist fit index to BSEM, it does appear sensitive to sample size as well as model size. For example, when Hoofs et al. (2018) simulated data from a six-indicator confirmatory factor analysis (CFA), the BRMSEA 90% credible interval (with noninformative priors) implied a similar amount of sampling variability as implied by the RMSEA 90% CI under MLE. However, the 90% credible intervals for BRMSEA were much narrower than the 90% CIs for RMSEA when fitting larger, 12-indicator CFA models to smaller samples, leading to much lower power of BRMSEA than RMSEA to detect even severe misspecifications. Thus, when fitting models that are large relative to a smaller sample size, neither the BRMSEA nor PPP 2 would have power to detect important problems with the model.</p>
        <p>This has important implications for multidimensional assessment of (approximate) model fit in BSEM, especially in the common case when samples are substantially less than 1,000. When samples are particularly small, the ML 2 might have very little power, yet the 90% CI of RMSEA will be wide enough to prevent rejecting the hypothesis of inadequate or poor fit (RMSEA Ͼ 0.08 or 0.10). An example of this can be found in the SEM textbook by (Kline, 2016, p. 257, Table 9.9), in which a multigroup CFA with full factorial invariance was not rejected by the ML 2 test (p ϭ .229). However, the upper confidence limit of RMSEA was 0.103, prompting Kline (2016) to inspect local sources of misfit, where he found 16 correlation residuals that exceeded .10 (recall this practice is consistent with our recommendation to consider any indication of questionable overall fit as worthy of further attention, not as a reason to outright reject a model).This has important implications for multidimensional assessment of (approximate) model fit in BSEM, especially in the common case when samples are substantially less than 1,000. When samples are particularly small, the ML 2 might have very little power, yet the 90% CI of RMSEA will be wide enough to prevent rejecting the hypothesis of inadequate or poor fit (RMSEA Ͼ 0.08 or 0.10). An example of this can be found in the SEM textbook by (Kline, 2016, p. 257, Table 9.9), in which a multigroup CFA with full factorial invariance was not rejected by the ML 2 test (p ϭ .229). However, the upper confidence limit of RMSEA was 0.103, prompting Kline (2016) to inspect local sources of misfit, where he found 16 correlation residuals that exceeded .10 (recall this practice is consistent with our recommendation to consider any indication of questionable overall fit as worthy of further attention, not as a reason to outright reject a model).</p>
        <p>We therefore argue that there are common situations (e.g., small to moderate sample size) when researchers would want a Bayesian analog of RMSEA to behave as RMSEA would under MLE, as opposed to indicating very certainly (i.e., very narrow credible intervals 9 ) that a model with important misspecifications fits well. In the following section, we explore further methods to translate commonly used fit indices for use in BSEM, which can be expected to behave similarly to their MLE counterparts when noninformative priors are used for BSEM parameters (in which case, the posterior distribution is proportionally equivalent to the likelihood function). We consider a few cases of informative priors in the Illustrative Examples section, on the basis of which we offer important considerations for future research.We therefore argue that there are common situations (e.g., small to moderate sample size) when researchers would want a Bayesian analog of RMSEA to behave as RMSEA would under MLE, as opposed to indicating very certainly (i.e., very narrow credible intervals 9 ) that a model with important misspecifications fits well. In the following section, we explore further methods to translate commonly used fit indices for use in BSEM, which can be expected to behave similarly to their MLE counterparts when noninformative priors are used for BSEM parameters (in which case, the posterior distribution is proportionally equivalent to the likelihood function). We consider a few cases of informative priors in the Illustrative Examples section, on the basis of which we offer important considerations for future research.</p>
        <p>There is no ideal measure of fit in SEM. Rather, different indices evaluate different dimensions of model fit, leading many experts to propose supplementing the ML 2 test statistic with at least two additional fit indices (Brown, 2006;Hu &amp; Bentler, 1998;Kline, 2016). The addition of a BRMSEA index allows Bayesian models to be evaluated relative to their complexity, which complements simply using PPP 2 to evaluate whether the observed data are consistent with the model. To allow for models to be evaluated across additional dimensions of (approximate or relative) fit, we propose how additional non-centrality-based and incremental fit indices from SEM can be incorporated into BSEM. As previous research has shown (see Fan &amp; Sivo, 2007, for a review of some issues), all fit indices have limitations and (dis)advantages in different situations, so an array of indices would allow for more nuanced evaluation of how a model fails than the BRMSEA alone.There is no ideal measure of fit in SEM. Rather, different indices evaluate different dimensions of model fit, leading many experts to propose supplementing the ML 2 test statistic with at least two additional fit indices (Brown, 2006;Hu &amp; Bentler, 1998;Kline, 2016). The addition of a BRMSEA index allows Bayesian models to be evaluated relative to their complexity, which complements simply using PPP 2 to evaluate whether the observed data are consistent with the model. To allow for models to be evaluated across additional dimensions of (approximate or relative) fit, we propose how additional non-centrality-based and incremental fit indices from SEM can be incorporated into BSEM. As previous research has shown (see Fan &amp; Sivo, 2007, for a review of some issues), all fit indices have limitations and (dis)advantages in different situations, so an array of indices would allow for more nuanced evaluation of how a model fails than the BRMSEA alone.</p>
        <p>We propose 
            <rs type="software">BSEM</rs> fit indices developed in a fashion similar to Hoofs et al. (2018), by using p ‫ء‬ -pD in the role of df to represent model complexity (or rather, model parsimony). However, our proposals differ from Hoofs et al. in how we represent model misfit. A Bayesian analog of RMSEA that can be expected to behave like its frequentist counterpart (at least when using noninformative priors) might differ from Equation 16 by using the Bayesian analog of chi square: the deviance evaluated at the posterior mean (hence, the superscript "DevM"),
        </p>
        <p>We posit that compared with BRMSEA PPMC , BRMSEA DevM would more closely estimate the same quantity that RMSEA estimates (i.e., ε; Browne &amp; Cudeck, 1992) in a frequentist framework, using an analogous (though not equivalent) definition. However, using D( ) does not allow for the advantage of obtaining a credible interval for BRMSEA because Equation 17only contains summaries of the data (N and p ‫ء‬ ) and the posterior (D( ) and pD).We posit that compared with BRMSEA PPMC , BRMSEA DevM would more closely estimate the same quantity that RMSEA estimates (i.e., ε; Browne &amp; Cudeck, 1992) in a frequentist framework, using an analogous (though not equivalent) definition. However, using D( ) does not allow for the advantage of obtaining a credible interval for BRMSEA because Equation 17only contains summaries of the data (N and p ‫ء‬ ) and the posterior (D( ) and pD).</p>
        <p>A similar quantity can be obtained that does vary across iterations, by using D͑ ˜i͒ ϭ D i obs instead of D( ). By substituting the right-hand side of Equation 13for D in Equation 14, rearranging terms provides a distribution centered at D( ):A similar quantity can be obtained that does vary across iterations, by using D͑ ˜i͒ ϭ D i obs instead of D( ). By substituting the right-hand side of Equation 13for D in Equation 14, rearranging terms provides a distribution centered at D( ):</p>
        <p>Thus, substituting (D i obs Ϫ pD) for D( ) in Equation 17 yields a distribution of BRMSEA:Thus, substituting (D i obs Ϫ pD) for D( ) in Equation 17 yields a distribution of BRMSEA:</p>
        <p>The subscript i indicates that BRMSEA i DevM in Equation 19 varies across samples of parameters drawn from their posterior distribution. BRMSEA DevM resembles Hoofs et al.'s (2018) BRMSEA PPMC but replaces D i rep by pD. Interestingly, the two occurrences of pD in the numerator of Equation 19 cancel out, resulting in a numerator that expresses misfit simply as the discrepancy at iteration i rescaled by the number of observed sample moments. Two important observations are worth mentioning here. First, the proposed approach in Equation 19 yields neither a posterior distribution (because it is a function not only of the estimated parameters but also of the observed data) nor a posterior predictive distribution (because it is a function only of observed data, not data simulated from model parameters); rather, it yields a distribution of realized values (Levy, 2011) of a chi-square-based discrepancy 9 See Results for the 12-indicator Models C-E in Figure 2, Model C in Figure 3, and Model F2 in Figure 4 of Hoofs et al. (2018). This document is copyrighted by the American Psychological Association or one of its allied publishers.The subscript i indicates that BRMSEA i DevM in Equation 19 varies across samples of parameters drawn from their posterior distribution. BRMSEA DevM resembles Hoofs et al.'s (2018) BRMSEA PPMC but replaces D i rep by pD. Interestingly, the two occurrences of pD in the numerator of Equation 19 cancel out, resulting in a numerator that expresses misfit simply as the discrepancy at iteration i rescaled by the number of observed sample moments. Two important observations are worth mentioning here. First, the proposed approach in Equation 19 yields neither a posterior distribution (because it is a function not only of the estimated parameters but also of the observed data) nor a posterior predictive distribution (because it is a function only of observed data, not data simulated from model parameters); rather, it yields a distribution of realized values (Levy, 2011) of a chi-square-based discrepancy 9 See Results for the 12-indicator Models C-E in Figure 2, Model C in Figure 3, and Model F2 in Figure 4 of Hoofs et al. (2018). This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>measure, which could therefore be used in a posterior predictive model check, unlike BRMSEA PPMC . We provide further details in a later section. Second, it follows from Jensen's inequality10 that the mean of ͙X generally underestimates the square root of X . So although the equivalence in Equation 18 2018), the distribution of BRMSEA i DevM (and other indices in the Appendix) can be summarized using a measure of central tendency, such as the mean (expected a posteriori; EAP), mode (modal a posteriori; MAP), or median of their distribution, which should typically be quite close to the value obtained using D( ) as in Equation 17. Uncertainty about EAP, MAP, or median estimates can be represented with their respective standard deviations and with 5th and 95th percentile from their empirical distributions, similar to defining a 90% credible interval for a model parameter.measure, which could therefore be used in a posterior predictive model check, unlike BRMSEA PPMC . We provide further details in a later section. Second, it follows from Jensen's inequality10 that the mean of ͙X generally underestimates the square root of X . So although the equivalence in Equation 18 2018), the distribution of BRMSEA i DevM (and other indices in the Appendix) can be summarized using a measure of central tendency, such as the mean (expected a posteriori; EAP), mode (modal a posteriori; MAP), or median of their distribution, which should typically be quite close to the value obtained using D( ) as in Equation 17. Uncertainty about EAP, MAP, or median estimates can be represented with their respective standard deviations and with 5th and 95th percentile from their empirical distributions, similar to defining a 90% credible interval for a model parameter.</p>
        <p>Incremental fit indices also require fitting a null model (M 0 ) to the data, which is traditionally an independence model in which means and variances are freely estimated but covariances are constrained to zero. Although Hoofs et al. (2018, p. 26) brought up the concern that an independence model would be hard to justify (even contradictory) when incorporating prior information, we currently only consider the case of noninformative priors. But we recommend defining a meaningful null model appropriate for answering a specific research question, taking into account the functional form of the hypothesized model and characteristics of the data (e.g., Widaman &amp; Thompson, 2003, discussed alternative null models for multigroup and longitudinal data; Lai &amp; Yoon, 2015, proposed a modified version of Rigdon's, 1998, null model specifically for evaluating measurement invariance). Similar to differences between discrepancies in posterior predictive checks, the differences (or ratios) between discrepancies of the hypothesized and null models in Equations 10 to 12 are calculated at each iteration of each model's Markov chain(s). Thus, for computational purposes, the same number of samples should be drawn from the posterior distribution of both models fit to the same data.Incremental fit indices also require fitting a null model (M 0 ) to the data, which is traditionally an independence model in which means and variances are freely estimated but covariances are constrained to zero. Although Hoofs et al. (2018, p. 26) brought up the concern that an independence model would be hard to justify (even contradictory) when incorporating prior information, we currently only consider the case of noninformative priors. But we recommend defining a meaningful null model appropriate for answering a specific research question, taking into account the functional form of the hypothesized model and characteristics of the data (e.g., Widaman &amp; Thompson, 2003, discussed alternative null models for multigroup and longitudinal data; Lai &amp; Yoon, 2015, proposed a modified version of Rigdon's, 1998, null model specifically for evaluating measurement invariance). Similar to differences between discrepancies in posterior predictive checks, the differences (or ratios) between discrepancies of the hypothesized and null models in Equations 10 to 12 are calculated at each iteration of each model's Markov chain(s). Thus, for computational purposes, the same number of samples should be drawn from the posterior distribution of both models fit to the same data.</p>
        <p>Because D obs is evaluated across the estimated posterior distribution of model parameters, uncertainty about fit indices is "borrowed" from uncertainty about the model parameters. Following from Levy (2011), the realized values D i obs approximate the posterior distribution of the discrepancy function, so the posterior distributions of derived quantities like BRMSEA i DevM can be seen as realized values of fit indices defined analogously in Bayesian and frequentist frameworks. Empirical distributions of fit indices help (but may not fully) resolve the first major limitation of interpreting the magnitude of fit indices relative to rule-of-thumb guidelines that we identified previously. And because the distribution of a fit index is derived from the estimated posterior distribution of the model parameters, features of the model are already taken into account, partially resolving the second major limitation (assumptions must still be made about the data distribution).Because D obs is evaluated across the estimated posterior distribution of model parameters, uncertainty about fit indices is "borrowed" from uncertainty about the model parameters. Following from Levy (2011), the realized values D i obs approximate the posterior distribution of the discrepancy function, so the posterior distributions of derived quantities like BRMSEA i DevM can be seen as realized values of fit indices defined analogously in Bayesian and frequentist frameworks. Empirical distributions of fit indices help (but may not fully) resolve the first major limitation of interpreting the magnitude of fit indices relative to rule-of-thumb guidelines that we identified previously. And because the distribution of a fit index is derived from the estimated posterior distribution of the model parameters, features of the model are already taken into account, partially resolving the second major limitation (assumptions must still be made about the data distribution).</p>
        <p>There typically exists some actual discrepancy between the hypothesized model and the true data-generating process, even if the model parameters were to be estimated using the population covariance matrix as input data (ruling out discrepancy due to sampling error; Cudeck &amp; Browne, 1992;MacCallum, 2003). Although, in that sense, there is a population parameter ε (Equation 6) that the sample formula (Equation 5) estimates (likewise for other fit indices), we do not assert that BRMSEA DevM would consistently estimate ε. We only argue that in the special case of noninformative priors being used to estimate the same model parameters with MCMC as with MLE, the Bayesian analog of RMSEA should provide a reasonable approximation of ε by virtue of MCMC and MLE converging on equivalent parameter estimates in this case.There typically exists some actual discrepancy between the hypothesized model and the true data-generating process, even if the model parameters were to be estimated using the population covariance matrix as input data (ruling out discrepancy due to sampling error; Cudeck &amp; Browne, 1992;MacCallum, 2003). Although, in that sense, there is a population parameter ε (Equation 6) that the sample formula (Equation 5) estimates (likewise for other fit indices), we do not assert that BRMSEA DevM would consistently estimate ε. We only argue that in the special case of noninformative priors being used to estimate the same model parameters with MCMC as with MLE, the Bayesian analog of RMSEA should provide a reasonable approximation of ε by virtue of MCMC and MLE converging on equivalent parameter estimates in this case.</p>
        <p>It was brought to our attention during the review process that referring to a "posterior distribution of a fit index" could be construed as misleading given that the discrepancy function D is a function of not only estimated parameters but also the data on which those parameters were conditioned. Although it would be possible to estimate data-model discrepancy as a parameter in a BSEM framework (e.g., as adventitious error; Wu &amp; Browne, 2015), we do not develop such an approach here. Instead, we show how the familiar measures of approximate data-model correspondence in SEM (i.e., fit indices derived from the ML 2 statistic) can be conceptualized in a Bayesian framework by plugging analogous quantities into the same formulas derived in a frequentist framework.It was brought to our attention during the review process that referring to a "posterior distribution of a fit index" could be construed as misleading given that the discrepancy function D is a function of not only estimated parameters but also the data on which those parameters were conditioned. Although it would be possible to estimate data-model discrepancy as a parameter in a BSEM framework (e.g., as adventitious error; Wu &amp; Browne, 2015), we do not develop such an approach here. Instead, we show how the familiar measures of approximate data-model correspondence in SEM (i.e., fit indices derived from the ML 2 statistic) can be conceptualized in a Bayesian framework by plugging analogous quantities into the same formulas derived in a frequentist framework.</p>
        <p>Thus, we tend to refer to our proposed fit indices as having "distributions" with "intervals" around their means. For brevity in some instances (e.g., plots and tables), we sometimes refer to a "posterior distribution" of BRMSEA (or its 5th and 95th percentiles as constituting a 90% "credible interval"), following the existing convention of referring to a "posterior distribution for a discrepancy measure" (Levy, 2011, p. 672). But readers should recall that BRMSEA DevM estimates a quantity only analogous (not equivalent) to the frequentist RMSEA (likewise for other fit indices), and we use the terms "posterior distribution" and "credible interval" only in the sense that BRMSEA is a function of the data and model parameters that is evaluated across the estimated posterior distribution of model parameters.Thus, we tend to refer to our proposed fit indices as having "distributions" with "intervals" around their means. For brevity in some instances (e.g., plots and tables), we sometimes refer to a "posterior distribution" of BRMSEA (or its 5th and 95th percentiles as constituting a 90% "credible interval"), following the existing convention of referring to a "posterior distribution for a discrepancy measure" (Levy, 2011, p. 672). But readers should recall that BRMSEA DevM estimates a quantity only analogous (not equivalent) to the frequentist RMSEA (likewise for other fit indices), and we use the terms "posterior distribution" and "credible interval" only in the sense that BRMSEA is a function of the data and model parameters that is evaluated across the estimated posterior distribution of model parameters.</p>
        <p>The analogous definitions in Equations 5 and 19 yield similar interpretations for BRMSEA DevM and RMSEA, so it is tempting to apply the same rules of thumb for interpreting their magnitudes. However, one should not expect any proposed guidelines for interpreting the magnitude of RMSEA (or of any other fit index) to generalize11 to BRMSEA DevM . Likewise, although a 90% interval estimate could be used to test a hypothesis about a fit index (e.g., similar to MacCallum et al., 2006), the hypothesized value should not be derived from MLE-based guidelines when informative priors are used. Furthermore, researchers should refrain from applying the standard interpretation of a 90% credible interval (i.e., conditional on the data, there is a 90% probability the parameter lies between these limits) until more is understood about the quantities that are consistently estimated by the proposed indices.The analogous definitions in Equations 5 and 19 yield similar interpretations for BRMSEA DevM and RMSEA, so it is tempting to apply the same rules of thumb for interpreting their magnitudes. However, one should not expect any proposed guidelines for interpreting the magnitude of RMSEA (or of any other fit index) to generalize11 to BRMSEA DevM . Likewise, although a 90% interval estimate could be used to test a hypothesis about a fit index (e.g., similar to MacCallum et al., 2006), the hypothesized value should not be derived from MLE-based guidelines when informative priors are used. Furthermore, researchers should refrain from applying the standard interpretation of a 90% credible interval (i.e., conditional on the data, there is a 90% probability the parameter lies between these limits) until more is understood about the quantities that are consistently estimated by the proposed indices.</p>
        <p>This brings us back to the third major limitation of proposed cutoff values for fit indices: It is not immediately apparent how to derive a cutoff value that can be meaningfully interpreted as indicating maximally ignorable or minimally important datamodel misfit. Rather than relying on fixed cutoffs proposed under MLE, the magnitude of the realized values of fit indices could instead be compared with values consistent with the model simulated using PPMC, avoiding (though not resolving) the third major limitation of interpreting fit indices relative to cutoff values. As this issue falls beyond the scope of our current study, we return to this point in the Discussion to provide details as a direction for future research.This brings us back to the third major limitation of proposed cutoff values for fit indices: It is not immediately apparent how to derive a cutoff value that can be meaningfully interpreted as indicating maximally ignorable or minimally important datamodel misfit. Rather than relying on fixed cutoffs proposed under MLE, the magnitude of the realized values of fit indices could instead be compared with values consistent with the model simulated using PPMC, avoiding (though not resolving) the third major limitation of interpreting fit indices relative to cutoff values. As this issue falls beyond the scope of our current study, we return to this point in the Discussion to provide details as a direction for future research.</p>
        <p>We borrowed design factors from Fan and Sivo (2007) to compare the sampling behaviors of fit indices under MLE and MCMC estimation across a range of sample sizes, model types (and sizes), and levels of model misfit. We chose their design elements because they synchronized levels of misspecification across model types and sizes such that the power to detect moderate and large amounts of misfit was always 51% and 88%, respectively, when N ϭ 100 (see Table 1). Similar to Hoofs et al. (2018), we simulated data from CFA models with cross-loadings (CFA-A) and simple structure (CFA-B), but we also simulated data from large (12 indicators, four factors) and small (six indicators, two factors) structural regression models (SEM-A and SEM-B, respectively). Thus, we worked with models that had similar numbers of factors and indicators as Hoofs et al. worked with, but with the advantage of holding the practical impact of misfit constant across types of model and misspecification.We borrowed design factors from Fan and Sivo (2007) to compare the sampling behaviors of fit indices under MLE and MCMC estimation across a range of sample sizes, model types (and sizes), and levels of model misfit. We chose their design elements because they synchronized levels of misspecification across model types and sizes such that the power to detect moderate and large amounts of misfit was always 51% and 88%, respectively, when N ϭ 100 (see Table 1). Similar to Hoofs et al. (2018), we simulated data from CFA models with cross-loadings (CFA-A) and simple structure (CFA-B), but we also simulated data from large (12 indicators, four factors) and small (six indicators, two factors) structural regression models (SEM-A and SEM-B, respectively). Thus, we worked with models that had similar numbers of factors and indicators as Hoofs et al. worked with, but with the advantage of holding the practical impact of misfit constant across types of model and misspecification.</p>
        <p>Figures 1,2, 3, and 4 present path diagrams of the four population models, the population values used to generate multivariatenormal data, and which parameters were fixed to zero in analysis models that were moderately or severely misspecified. Because Hoofs et al. (2018) found their BRMSEA behaved similar to RMSEA when N Ն 1,000, we did not generate samples larger than 1,000. We included five sample-size conditions: N ϭ 75, 100, 250, 500, or 1,000. Our full-factorial 5 (N) ϫ 4 (model types) ϫ 3 (levels of misfit: none, moderate, and severe) design therefore included 60 conditions, and we generated 1,000 replications in each condition. Data were generated using the simulate-Data() function in the 
            <rs type="software">R</rs> (
            <rs type="creator">R Core Team</rs>, 2018) package 
            <rs type="software">lavaan</rs> (Rosseel, 2012). Models were fit to data using MLE in 
            <rs type="software">lavaan</rs> and using MCMC estimation (specifically, Gibbs sampling) available in the 
            <rs type="software">R</rs> package 
            <rs type="software">blavaan</rs> (Merkle &amp; Rosseel, 2018), which optionally uses JAGS (Plummer, 2017) or 
            <rs type="software">Stan</rs> (Carpenter et al., 2017) as the general Bayesian estimation program in the back end.
        </p>
        <p>We used noninformative priors to analyze the simulated data. "Noninformative" indicates the prior distributions for the parameters provided little to no information above and beyond the information provided by the data, so that the posterior distributions are estimated around the information from the data instead of prior knowledge (Gelman, Simpson, &amp; Betancourt, 2017). Priors for factor loadings, indicator intercepts, and regressions were ϳ N ͑ ϭ 0, 2 ϭ 100͒ and priors for indicator residual standard deviations were ϳ half-Cauchy( ϭ 0, 2 ϭ 2.5). For the CFA models, factor variances and covariances were ϳ Wishart Ϫ1 (⌿ ϭ I, ϭ nf ϩ 1), where I represents an identity matrix of dimension equal to the number of factors (nf), and degrees of freedom () equal to number of factors plus 1. For the SEM models, factor covariances followed the same inverse-Wishart distribution as the CFA models, and the factor residual standard deviations followed the same half-Cauchy priors as the indicator residual standard deviations. It is pertinent to mention that these priors are noninformative with respect to the model parameters and the scale of the data in this simulation, but these priors could be considered informative in different conditions (e.g., data with larger scales).We used noninformative priors to analyze the simulated data. "Noninformative" indicates the prior distributions for the parameters provided little to no information above and beyond the information provided by the data, so that the posterior distributions are estimated around the information from the data instead of prior knowledge (Gelman, Simpson, &amp; Betancourt, 2017). Priors for factor loadings, indicator intercepts, and regressions were ϳ N ͑ ϭ 0, 2 ϭ 100͒ and priors for indicator residual standard deviations were ϳ half-Cauchy( ϭ 0, 2 ϭ 2.5). For the CFA models, factor variances and covariances were ϳ Wishart Ϫ1 (⌿ ϭ I, ϭ nf ϩ 1), where I represents an identity matrix of dimension equal to the number of factors (nf), and degrees of freedom () equal to number of factors plus 1. For the SEM models, factor covariances followed the same inverse-Wishart distribution as the CFA models, and the factor residual standard deviations followed the same half-Cauchy priors as the indicator residual standard deviations. It is pertinent to mention that these priors are noninformative with respect to the model parameters and the scale of the data in this simulation, but these priors could be considered informative in different conditions (e.g., data with larger scales).</p>
        <p>Each model started with 30,000 burn-in iterations; if the model did not converge, this was iteratively increased by 5,000 until the model converged. Convergence of each Markov chain to the same posterior distribution was evaluated using the potential scale reduction factor (PSRF), also known as "univariate R-hat" (Gelman &amp; Rubin, 1992). It was determined that the Note. Power ϭ power of ML 2 test when N ϭ 100; F ML ϭ maximum likelihood fit function; df ϭ degrees of freedom; ε ϭ population RMSEA; RMSEA ϭ estimated RMSEA under MLE; BRMSEA ϭ Bayesian variant of RMSEA using "DevM" formulation; ⌬ML ϭ difference between ε and RMSEA (omitted when ε ϭ 0); ⌬MCMC ϭ difference between ε and BRMSEA (omitted when ε ϭ 0); Misfit ϭ level of misspecification. This document is copyrighted by the American Psychological Association or one of its allied publishers.Each model started with 30,000 burn-in iterations; if the model did not converge, this was iteratively increased by 5,000 until the model converged. Convergence of each Markov chain to the same posterior distribution was evaluated using the potential scale reduction factor (PSRF), also known as "univariate R-hat" (Gelman &amp; Rubin, 1992). It was determined that the Note. Power ϭ power of ML 2 test when N ϭ 100; F ML ϭ maximum likelihood fit function; df ϭ degrees of freedom; ε ϭ population RMSEA; RMSEA ϭ estimated RMSEA under MLE; BRMSEA ϭ Bayesian variant of RMSEA using "DevM" formulation; ⌬ML ϭ difference between ε and RMSEA (omitted when ε ϭ 0); ⌬MCMC ϭ difference between ε and BRMSEA (omitted when ε ϭ 0); Misfit ϭ level of misspecification. This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>model converged when R-hat Ͻ 1.10 for each parameter (Brooks &amp; Gelman, 1998). Although the total (including burnin) number of iterations could differ across replications, we always saved the same number of post-burn-in iterations (5,000 from each chain) to estimate the posterior distributions. For every analysis, we used three chains, yielding 15,000 iterations for drawing inferences. We did not "thin" the chains by discarding samples because although thinning decreases the autocorrelation between iterations (thus decreasing the Monte Carlo error of the posterior distribution's sample statistics), it does not affect the posterior distribution itself nor inferences drawn from it (Gelman et al., 2014).model converged when R-hat Ͻ 1.10 for each parameter (Brooks &amp; Gelman, 1998). Although the total (including burnin) number of iterations could differ across replications, we always saved the same number of post-burn-in iterations (5,000 from each chain) to estimate the posterior distributions. For every analysis, we used three chains, yielding 15,000 iterations for drawing inferences. We did not "thin" the chains by discarding samples because although thinning decreases the autocorrelation between iterations (thus decreasing the Monte Carlo error of the posterior distribution's sample statistics), it does not affect the posterior distribution itself nor inferences drawn from it (Gelman et al., 2014).</p>
        <p>Because Hoofs et al. (2018) already showed via Monte Carlo simulations that their BRMSEA PPMC and the RMSEA under MLE do not converge until sample size is quite large (N Ն 1,000), the main goal of our simulation study was to evaluate whether our proposed BRMSEA DevM behaves as expected when priors are uninformative (i.e., similar to the RMSEA under MLE, which converges on ε at much smaller N; Curran, Bollen, Chen, Paxton, &amp; Kirby, 2003). Thus, when comparing SEM fit indices under MLE with our proposed BSEM fit indices in the Results section, we omit the "DevM" label for brevity, referring only to BRMSEA, BCFI, and so forth, except in the case of explicitly comparing the "DevM" and "PPMC" formulations.Because Hoofs et al. (2018) already showed via Monte Carlo simulations that their BRMSEA PPMC and the RMSEA under MLE do not converge until sample size is quite large (N Ն 1,000), the main goal of our simulation study was to evaluate whether our proposed BRMSEA DevM behaves as expected when priors are uninformative (i.e., similar to the RMSEA under MLE, which converges on ε at much smaller N; Curran, Bollen, Chen, Paxton, &amp; Kirby, 2003). Thus, when comparing SEM fit indices under MLE with our proposed BSEM fit indices in the Results section, we omit the "DevM" label for brevity, referring only to BRMSEA, BCFI, and so forth, except in the case of explicitly comparing the "DevM" and "PPMC" formulations.</p>
        <p>For each of the 60 conditions, we had 1,000 replications that converged for both the MLE and Bayesian models (PSRF Ͻ 1.1). The total computation time was 3,175.33 days (8.69 years), distributed across several parallel computers. To calculate BSEM fit indices with p ‫ء‬ -pD in place of df, we chose pD LOO rather than pD WAIC or pD DIC because it is preferred by Vehtari et al. (2017). But because we estimated models with noninformative priors, any pD was expected to be very close to the number of parameters under MLE. Table 2 shows that pD LOO and pD WAIC were both This document is copyrighted by the American Psychological Association or one of its allied publishers.For each of the 60 conditions, we had 1,000 replications that converged for both the MLE and Bayesian models (PSRF Ͻ 1.1). The total computation time was 3,175.33 days (8.69 years), distributed across several parallel computers. To calculate BSEM fit indices with p ‫ء‬ -pD in place of df, we chose pD LOO rather than pD WAIC or pD DIC because it is preferred by Vehtari et al. (2017). But because we estimated models with noninformative priors, any pD was expected to be very close to the number of parameters under MLE. Table 2 shows that pD LOO and pD WAIC were both This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>very similar to the number of parameters under MLE across model types and misspecification levels, whereas pD DIC substantially underestimated the number of parameters for model SEM-A.very similar to the number of parameters under MLE across model types and misspecification levels, whereas pD DIC substantially underestimated the number of parameters for model SEM-A.</p>
        <p>Given this result, we recommend using either pD LOO or pD WAIC in practice.Given this result, we recommend using either pD LOO or pD WAIC in practice.</p>
        <p>Population RMSEA for each analysis model (i.e., ε with varying across levels of misspecification) was calculated by fitting the model to the population covariance matrix implied by its associated population-model parameters. F ML (which is independent of sample size) in each condition was plugged into Equation 6, reported in Table 1 along with the average RMSEA and BRMSEA (averaged across sample size conditions). Compared with (Hoofs et al., 2018, see their Figures 234), Table 1 shows that across population models and levels of misspecification, both RMSEA and BRMSEA closely approximate ε. The largest deviations (ε ˆϪ ε) were when there was no misspecification (ε ϭ 0), which was due to a floor effect because RMSEA is bound below at zero. When this floor effect was not present, the absolute value of ε ˆϪ ε ranged from 0.0008 to 0.0086, showing that, on average, BRMSEA (like RMSEA under MLE) reproduced ε within two to three decimal places. The solid lines in Figure 5 further show that across sample sizes, RMSEA and BRMSEA tended to converge quickly on the same value. The larger deviations between RMSEA and BRMSEA in the small SEM model (SEM-B in Figure 4) may be related to the more erratic sampling variability of RMSEA when either N or df (but especially when both) are small (Kenny, Kaniskan, &amp; McCoach, 2015).Population RMSEA for each analysis model (i.e., ε with varying across levels of misspecification) was calculated by fitting the model to the population covariance matrix implied by its associated population-model parameters. F ML (which is independent of sample size) in each condition was plugged into Equation 6, reported in Table 1 along with the average RMSEA and BRMSEA (averaged across sample size conditions). Compared with (Hoofs et al., 2018, see their Figures 234), Table 1 shows that across population models and levels of misspecification, both RMSEA and BRMSEA closely approximate ε. The largest deviations (ε ˆϪ ε) were when there was no misspecification (ε ϭ 0), which was due to a floor effect because RMSEA is bound below at zero. When this floor effect was not present, the absolute value of ε ˆϪ ε ranged from 0.0008 to 0.0086, showing that, on average, BRMSEA (like RMSEA under MLE) reproduced ε within two to three decimal places. The solid lines in Figure 5 further show that across sample sizes, RMSEA and BRMSEA tended to converge quickly on the same value. The larger deviations between RMSEA and BRMSEA in the small SEM model (SEM-B in Figure 4) may be related to the more erratic sampling variability of RMSEA when either N or df (but especially when both) are small (Kenny, Kaniskan, &amp; McCoach, 2015).</p>
        <p>Table 3 compares each BSEM fit index with its ML counterpart. Recall that D( ) is the Bayesian analog of chi square. The BSEM fit indices consistently indicate slightly worse fit, at least in part because D( ) Ͼ 2 in 93.3% of replications; however, in most cases, this difference is minimal because the values are equal to the second decimal place. Looking at these differences as paired comparisons, we can estimate the Cohen's d as the standardized mean difference, representing the mean difference in units of SD. Following standard guidelines (Cohen, 1992), these are small to medium effect sizes. Table 3 also shows that fit indices have nearly equivalent standard deviations, and Figure 5 shows that for This document is copyrighted by the American Psychological Association or one of its allied publishers.Table 3 compares each BSEM fit index with its ML counterpart. Recall that D( ) is the Bayesian analog of chi square. The BSEM fit indices consistently indicate slightly worse fit, at least in part because D( ) Ͼ 2 in 93.3% of replications; however, in most cases, this difference is minimal because the values are equal to the second decimal place. Looking at these differences as paired comparisons, we can estimate the Cohen's d as the standardized mean difference, representing the mean difference in units of SD. Following standard guidelines (Cohen, 1992), these are small to medium effect sizes. Table 3 also shows that fit indices have nearly equivalent standard deviations, and Figure 5 shows that for This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>(B)RMSEA, these similarities generally hold across simulation conditions. Additionally, Table 3 shows the percent overlap between the Monte Carlo distributions as well as their correlations (r). Overlap was calculated with the 
            <rs type="software">R</rs> package overlap (Meredith &amp; Ridout, 2017). The overlap ranged from 93.44% to 99.76%, indicating high similarity between ML fit indices and posterior means of BSEM fit indices. The correlations also indicate high linear relation between ML and MCMC, ranging between .89 to .99, where the lowest relation is for ⌫ ˆadj . The PPP 2 is comparable with the ML 2 p value. As Table 3 shows, the PPP 2 follows the similar behavior as the ML 2 p value, not only on average value but also in variability, consistently highly correlated (r ϭ .936) and highly overlapping (81.89%).
        </p>
        <p>Comparisons in Table 3 marginalize over all simulation conditions. To evaluate the sensitivity of each index to design factors, we used factorial ANOVA to estimate the percentage of sampling variance ( 2 ) in a fit index attributable to each design factor. Table 4 shows only negligible interaction effects on both MLE and MCMC, with the exception of a misspecification by sample size interaction on chi square, which is of course because chi square is more sensitive to misspecification in larger samples. The main effect of N was only substantial for chi square and (B)NFI, which is also consistent with previous -A). Dotted paths represent nonzero population values that were fixed to zero in the analysis model. x ϭ factor loading matrix for exogenous factors; y ϭ factor loading matrix for endogenous factors; ⌽ ϭ factor covariance matrix for endogenous factors; ⌿ ϭ factor covariance matrix for exogenous factors; ϭ residual covariance matrix; ⌫ ϭ regressions from exogenous factors; B ϭ regressions from endogenous factors. This document is copyrighted by the American Psychological Association or one of its allied publishers.Comparisons in Table 3 marginalize over all simulation conditions. To evaluate the sensitivity of each index to design factors, we used factorial ANOVA to estimate the percentage of sampling variance ( 2 ) in a fit index attributable to each design factor. Table 4 shows only negligible interaction effects on both MLE and MCMC, with the exception of a misspecification by sample size interaction on chi square, which is of course because chi square is more sensitive to misspecification in larger samples. The main effect of N was only substantial for chi square and (B)NFI, which is also consistent with previous -A). Dotted paths represent nonzero population values that were fixed to zero in the analysis model. x ϭ factor loading matrix for exogenous factors; y ϭ factor loading matrix for endogenous factors; ⌽ ϭ factor covariance matrix for endogenous factors; ⌿ ϭ factor covariance matrix for exogenous factors; ϭ residual covariance matrix; ⌫ ϭ regressions from exogenous factors; B ϭ regressions from endogenous factors. This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>research. BSEM fit indices generally tended to be more sensitive to N and model type, and less sensitive to misspecification, than their ML counterparts. The differences tended to be small, but two substantial differences can be seen: BRMSEA seems much more affected by model type than RMSEA, and ⌫ ˆadj appears less sensitive to misspecification under MCMC than MLE. Consistent with past research (Fan &amp; Sivo, 2007), (B)CFI and (B)⌫ ˆwere substantially affected ( 2 Ͼ 10%) only by model misspecification, which is ideal behavior for a fit index (Hu &amp; Bentler, 1998). Finally, PPP 2 was affected by the interaction between misspecification level and sample size. Table 5 presents the Bayesian fit indices across misspecification levels, as misspecification increases the fit indices present worse model fit. Table 6 shows that for models with minor misspecification, PPP 2 tended to reveal model misspecification as sample size increased. With minor misspecification at N ϭ 75, PPP 2 Ͻ .01 for 4.5% of replications compared with 95.3% of replications with N ϭ 500. In the case of severe misspecification, 28.6% of replications had PPP 2 Ͻ .01 at N ϭ 75, compared with 100% of replications when N ϭ 500. This is analogous to increased power of ML 2 when sample size increases, rejecting models even with trivial levels of misspecification, whereas small sample sizes are unable to detect severe levels of misspecification consistently.research. BSEM fit indices generally tended to be more sensitive to N and model type, and less sensitive to misspecification, than their ML counterparts. The differences tended to be small, but two substantial differences can be seen: BRMSEA seems much more affected by model type than RMSEA, and ⌫ ˆadj appears less sensitive to misspecification under MCMC than MLE. Consistent with past research (Fan &amp; Sivo, 2007), (B)CFI and (B)⌫ ˆwere substantially affected ( 2 Ͼ 10%) only by model misspecification, which is ideal behavior for a fit index (Hu &amp; Bentler, 1998). Finally, PPP 2 was affected by the interaction between misspecification level and sample size. Table 5 presents the Bayesian fit indices across misspecification levels, as misspecification increases the fit indices present worse model fit. Table 6 shows that for models with minor misspecification, PPP 2 tended to reveal model misspecification as sample size increased. With minor misspecification at N ϭ 75, PPP 2 Ͻ .01 for 4.5% of replications compared with 95.3% of replications with N ϭ 500. In the case of severe misspecification, 28.6% of replications had PPP 2 Ͻ .01 at N ϭ 75, compared with 100% of replications when N ϭ 500. This is analogous to increased power of ML 2 when sample size increases, rejecting models even with trivial levels of misspecification, whereas small sample sizes are unable to detect severe levels of misspecification consistently.</p>
        <p>To understand the nature of the differences between estimators, Figure 5 shows the average (B)RMSEA across conditions. To evaluate the estimated sampling variability in BRMSEA implied by its posterior distribution, the average of its 5th and of its 95th posterior percentiles (corresponding to 90% credible-interval limits) were plotted along with the average 90% confidence limits of RMSEA. BRM-SEA appears generally less variable (more precisely estimated) than RMSEA, except when SEM-B was perfectly specified. However, recall from Table 3 that BRMSEA is just as variable as RMSEA, so the estimated distribution of BRMSEA actually underestimates its true sampling variability. The means, however, were similar between estimators, showing greater variability at smaller sample sizes, especially for model SEM-B. Compared with BRMSEA PPMC (Hoofs et al., 2018), we see that BRMSEA DevM is less sensitive to sample size and provides similar information about model fit as its MLE counterpart. This contrasts with Hoofs et al. (2018), who showed that their BRMSEA PPMC under noninformative priors provided much lower values than RMSEA under MLE, failing to detect misspecification in smaller samples. In Table 7 we see the linear relation between the PPP 2 and the approximate Bayesian fit indices, it presents the stronger relation with ⌫ ˆ, and the lowest relation with NFI.To understand the nature of the differences between estimators, Figure 5 shows the average (B)RMSEA across conditions. To evaluate the estimated sampling variability in BRMSEA implied by its posterior distribution, the average of its 5th and of its 95th posterior percentiles (corresponding to 90% credible-interval limits) were plotted along with the average 90% confidence limits of RMSEA. BRM-SEA appears generally less variable (more precisely estimated) than RMSEA, except when SEM-B was perfectly specified. However, recall from Table 3 that BRMSEA is just as variable as RMSEA, so the estimated distribution of BRMSEA actually underestimates its true sampling variability. The means, however, were similar between estimators, showing greater variability at smaller sample sizes, especially for model SEM-B. Compared with BRMSEA PPMC (Hoofs et al., 2018), we see that BRMSEA DevM is less sensitive to sample size and provides similar information about model fit as its MLE counterpart. This contrasts with Hoofs et al. (2018), who showed that their BRMSEA PPMC under noninformative priors provided much lower values than RMSEA under MLE, failing to detect misspecification in smaller samples. In Table 7 we see the linear relation between the PPP 2 and the approximate Bayesian fit indices, it presents the stronger relation with ⌫ ˆ, and the lowest relation with NFI.</p>
        <p>We found the same patterns of results for other fit indices, which can be seen in plots available on the Open Science Framework. 12Because only RMSEA has analytically derived confidence limits under MLE, we represented sampling variability of the other MLE fit indices by the 5th and 95th percentiles from their Monte Carlo distributions in each condition.We found the same patterns of results for other fit indices, which can be seen in plots available on the Open Science Framework. 12Because only RMSEA has analytically derived confidence limits under MLE, we represented sampling variability of the other MLE fit indices by the 5th and 95th percentiles from their Monte Carlo distributions in each condition.</p>
        <p>We can conclude from our Monte Carlo study that the BSEM fit indices proposed here can be expected to provide similar information about data-model fit as their frequentist counterparts do, at least in the special case of effectively equivalent models under MLE and MCMC with noninformative priors. But there are some important limitations worth noting.We can conclude from our Monte Carlo study that the BSEM fit indices proposed here can be expected to provide similar information about data-model fit as their frequentist counterparts do, at least in the special case of effectively equivalent models under MLE and MCMC with noninformative priors. But there are some important limitations worth noting.</p>
        <p>First, our Monte Carlo study utilized only D i obs and D( ) as defined by the marginal likelihood of the data, not the conditional likelihood, which is consistent with the chi square (likelihood ratio) test statistic in SEM. Bayesian latent variable models can directly sample latent variables through data augmentation (Merkle, Furr, &amp; Rabe-Hesketh, n.d.;Merkle &amp; Rosseel, 2018;Song &amp; Lee, 2012), and the conditional likelihood treats the latent variable scores as parameters (called person parameters in the item-response theory framework). In contrast, the latent variable scores can be integrated out to yield the marginal likelihood. This document is copyrighted by the American Psychological Association or one of its allied publishers.First, our Monte Carlo study utilized only D i obs and D( ) as defined by the marginal likelihood of the data, not the conditional likelihood, which is consistent with the chi square (likelihood ratio) test statistic in SEM. Bayesian latent variable models can directly sample latent variables through data augmentation (Merkle, Furr, &amp; Rabe-Hesketh, n.d.;Merkle &amp; Rosseel, 2018;Song &amp; Lee, 2012), and the conditional likelihood treats the latent variable scores as parameters (called person parameters in the item-response theory framework). In contrast, the latent variable scores can be integrated out to yield the marginal likelihood. This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>Information criteria based on marginal likelihoods have been shown to behave more consistently with expectations across conditions than those based on conditional likelihoods (Merkle et al., n.d.), and the interpretation applies more generally to any new data point rather than only to new observed data from cases with the same factor scores. We therefore considered marginal likelihood to be preferable (a view shared by developers of 
            <rs type="software">BSEM</rs> software such as 
            <rs type="software">Mplus</rs> and 
            <rs type="software">blavaan</rs>), but future researchers might find it reasonable to investigate the sampling properties of 
            <rs type="software">BSEM</rs> fit indices calculated under conditional likelihoods. Additionally, we simulated only complete data. Missing data are quite common in applied research (Enders, 2010;Little, 2013), so more research is needed about how the proposed fit indices behave with incomplete data. Because 
            <rs type="software">BSEM</rs> software uses data augmentation to deal with missing data (Merkle &amp; Rosseel, 2018;Muthén &amp; Muthén, 1998-2017), the proposed fit indices can still be estimated with incomplete data. Missing values are treated as unknown parameters to be estimated at each iteration of the Markov chain, effectively imputing the data before calculating the likelihood (Merkle, 2011). This would make the effective number of parameters pD larger, but blavaan marginalizes the missing data by integrating it out, just as it does with latent variable scores, so the pD it reports should be relatively unaffected by missing data. The ML 2 statistic and fit indices based on it have been shown to indicate better fit when incomplete data are analyzed using full-information maximum likelihood (FIML; Davey, 2005), but it remains to be seen whether multiple imputation in SEM or data augmentation in 
            <rs type="software">BSEM</rs> affect fit measures the same way. The numerous other factors that could impact 
            <rs type="software">BSEM</rs> fit indices (e.g., proportion missing data, fraction of missing information, mechanisms of missingness) warrant deeper attention than would fit within the scope of the current article, but two of our illustrative examples explore some effects of missing data on fit measures.
        </p>
        <p>Finally, our Monte Carlo study investigated only noninformative priors because the variety of ways that informative priors could effect BSEM fit indices would have required an unwieldy number of design factors (thus, these issues warrant their own investigation). Given how commonly informative priors are applied (van de Schoot et al., 2017), it is important to understand how the proposed indices can be expected to behave. In general, we expect priors would only have a noticeable effect on the posterior distribution in small to moderate samples, because in larger samples, the likelihood overwhelms the prior. This is consistent with results reported by Hoofs et al. (2018). But this also depends on how informative the priors are (Gelman et al., 2017). Greater precision will shrink pD, which can affect fit to different degrees depending on how closely the prior matches the actual parameter (Muthén &amp; Asparouhov, 2012). Equation 19shows that pD cancels out in our "DevM" formulation of BRMSEA, whereas in the "PPMC" formulation (Equation 16) D i obs is rescaled by D i rep . Thus, the shrinking of pD by increasing the precision of prior distributions should have differential effects on the different proposals for BRMSEA (and other 2 -based fit indices). We explore some assumptions using illustrative examples, and we encourage future researchers to take these issues into account when designing Monte Carlo studies to investigate the effect of informative priors on BSEM fit indices, as well as comparing the "PPMC" with "DevM" formulations.Finally, our Monte Carlo study investigated only noninformative priors because the variety of ways that informative priors could effect BSEM fit indices would have required an unwieldy number of design factors (thus, these issues warrant their own investigation). Given how commonly informative priors are applied (van de Schoot et al., 2017), it is important to understand how the proposed indices can be expected to behave. In general, we expect priors would only have a noticeable effect on the posterior distribution in small to moderate samples, because in larger samples, the likelihood overwhelms the prior. This is consistent with results reported by Hoofs et al. (2018). But this also depends on how informative the priors are (Gelman et al., 2017). Greater precision will shrink pD, which can affect fit to different degrees depending on how closely the prior matches the actual parameter (Muthén &amp; Asparouhov, 2012). Equation 19shows that pD cancels out in our "DevM" formulation of BRMSEA, whereas in the "PPMC" formulation (Equation 16) D i obs is rescaled by D i rep . Thus, the shrinking of pD by increasing the precision of prior distributions should have differential effects on the different proposals for BRMSEA (and other 2 -based fit indices). We explore some assumptions using illustrative examples, and we encourage future researchers to take these issues into account when designing Monte Carlo studies to investigate the effect of informative priors on BSEM fit indices, as well as comparing the "PPMC" with "DevM" formulations.</p>
        <p>We encourage future Monte Carlo research into the issues involving informative priors and missing data discussed above, and our illustrative examples serve as preliminary investigations into the details we expect to warrant immediate attention. We utilize the popular Holzinger and Swineford (1939) data set, available 13 in the 
            <rs type="software">R</rs> packages lavaan (Rosseel, 2012) and blavaan (Merkle &amp; Rosseel, 2018). The data set consists of mental ability test scores of N ϭ 301 seventh-and eighth-grade children from two different schools. The CFA consisted of three latent cognitive-ability constructs (visual, textual, and speed), each of which was defined by three indicators.
        </p>
        <p>The model was estimated 14 with MLE in lavaan (Rosseel, 2012), and several Bayesian models were estimated with the No-U-Turn Sampler (NUTS)-an extension to Hamiltonian Monte 13 A description and path diagram of the model can also be found on the lavaan tutorial: http://lavaan.ugent.be/tutorial/cfa.html 14 The 
            <rs type="software">R</rs>
            <rs type="software">scripts</rs> to replicate this analysis are available on 
            <rs type="url">https://osf.io/ afkcw</rs>/ This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.
        </p>
        <p>Carlo (Gelman et al., 2014;Hoffman &amp; Gelman, 2014)-with 
            <rs type="software">Stan</rs> (Carpenter et al., 2017;
            <rs type="creator">Stan Development Team</rs>, 2018) as the general Bayesian software employed by blavaan (Merkle &amp; Rosseel, 2018) in the back end. For both MLE and MCMC (NUTS), we identified the latent scales and locations by fixing factor variances to 1 and factor means to 0. For MCMC estimation, model parameters were sampled using three chains, discarding 10,000 burn-in iterations from each, and retaining 10,000 postburn-in iterations from each chain. Model convergence was as-sessed by inspecting traceplots for adequate mixing chains and verifying R-hat Ͻ 1.10 for every parameter (Brooks &amp; Gelman, 1998). For all models, we calculated 
            <rs type="software">BSEM</rs> fit indices using pD LOO as the measure of effective number of parameters. The number of parameters estimated with MLE was always 30 (df ϭ 24), and different Bayesian models yielded different estimates of the effective number of parameters.
        </p>
        <p>Results using blavaan's default priors (except for residual standard deviations, where the default prior is ϳ ⌫(1, .5) for the This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.Results using blavaan's default priors (except for residual standard deviations, where the default prior is ϳ ⌫(1, .5) for the This document is copyrighted by the American Psychological Association or one of its allied publishers. This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>residual variance) are presented in Table 8, which compares our proposed fit indices with those using the PPMC method proposed by Hoofs et al. (2018) as well as the ML fit indices. These results provide no new information beyond the conclusions of the Monte Carlo study, but they provide users with a real-data example with accompanying syntax in our online OSF materials. They also serve as a baseline to highlight how informative priors and missing data can affect the resulting posterior distributions of the proposed BSEM fit indices in subsequent illustrative examples. We summarize our expectations of these effects below:residual variance) are presented in Table 8, which compares our proposed fit indices with those using the PPMC method proposed by Hoofs et al. (2018) as well as the ML fit indices. These results provide no new information beyond the conclusions of the Monte Carlo study, but they provide users with a real-data example with accompanying syntax in our online OSF materials. They also serve as a baseline to highlight how informative priors and missing data can affect the resulting posterior distributions of the proposed BSEM fit indices in subsequent illustrative examples. We summarize our expectations of these effects below:</p>
        <p>• We expect results not to differ between alternative noninformative prior specifications. • We expect sufficiently informative priors (i.e., informative enough not to be overwhelmed by the likelihood) that are consistent with the data to yield narrower intervals than noninformative priors. When the posterior distributions of estimated parameters vary less, so will D obs . However, when the posterior is influenced by the prior, pD will be lower, which could affect the expected values of some fit indices (see . Holding other quantities constant, smaller pD should yield smaller BRMSEA DevM and BNFI DevM (indicating better and worse fit, respectively) but larger B-⌫ ˆadj DevM (indicating better fit), whereas the posterior means of B-⌫ ˆDevM and BCFI DevM should be unaffected. Given where pD H appears twice in Equation 23, how BTLI DevM might be affected would probably depend on the average magnitude of D H obs relative to p ‫ء‬ . • We expect that if an informative prior is not consistent with the data, the BSEM fit indices will indicate poorer This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.• We expect results not to differ between alternative noninformative prior specifications. • We expect sufficiently informative priors (i.e., informative enough not to be overwhelmed by the likelihood) that are consistent with the data to yield narrower intervals than noninformative priors. When the posterior distributions of estimated parameters vary less, so will D obs . However, when the posterior is influenced by the prior, pD will be lower, which could affect the expected values of some fit indices (see . Holding other quantities constant, smaller pD should yield smaller BRMSEA DevM and BNFI DevM (indicating better and worse fit, respectively) but larger B-⌫ ˆadj DevM (indicating better fit), whereas the posterior means of B-⌫ ˆDevM and BCFI DevM should be unaffected. Given where pD H appears twice in Equation 23, how BTLI DevM might be affected would probably depend on the average magnitude of D H obs relative to p ‫ء‬ . • We expect that if an informative prior is not consistent with the data, the BSEM fit indices will indicate poorer This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>data-model correspondence (e.g., higher BRMSEA and lower BCFI). This follows from priors being part of a Bayesian model, and sufficiently informative priors place restrictions on the parameter space sampled during MCMC estimation, so invalid restrictions should be seen as model misspecifications that yield poorer fit measures. • We expect a model with small-variance priors for crossloadings hypothesized to be approximately zero (Muthén &amp; Asparouhov, 2012) to yield better fit than a model that fixes those parameters to zero. This is becoming a common practice in BSEM, raising some concerns about making poor-fitting models appear acceptable (Stromeyer, Miller, Sriramachandramurthy, &amp; DeMartino, 2015), prompting Asparouhov, Muthén, and Morin (2015) to advocate sensitivity analyses in combination with small-variance priors. • Because blavaan marginalizes over imputed missing values, we expect missing data to have negligible impact on the effective number of parameters pD, but we expect D( ) to be smaller with greater proportions of missing data, as observed by Davey (2005). This phenomenon also held for the independence model (Davey, 2005), making it more difficult to predict the behavior of incremental fit indices.data-model correspondence (e.g., higher BRMSEA and lower BCFI). This follows from priors being part of a Bayesian model, and sufficiently informative priors place restrictions on the parameter space sampled during MCMC estimation, so invalid restrictions should be seen as model misspecifications that yield poorer fit measures. • We expect a model with small-variance priors for crossloadings hypothesized to be approximately zero (Muthén &amp; Asparouhov, 2012) to yield better fit than a model that fixes those parameters to zero. This is becoming a common practice in BSEM, raising some concerns about making poor-fitting models appear acceptable (Stromeyer, Miller, Sriramachandramurthy, &amp; DeMartino, 2015), prompting Asparouhov, Muthén, and Morin (2015) to advocate sensitivity analyses in combination with small-variance priors. • Because blavaan marginalizes over imputed missing values, we expect missing data to have negligible impact on the effective number of parameters pD, but we expect D( ) to be smaller with greater proportions of missing data, as observed by Davey (2005). This phenomenon also held for the independence model (Davey, 2005), making it more difficult to predict the behavior of incremental fit indices.</p>
        <p>To verify our first assumption about the effects of priors-that different specifications of noninformative priors yield approximately the same results-we compared the following noninformative priors:To verify our first assumption about the effects of priors-that different specifications of noninformative priors yield approximately the same results-we compared the following noninformative priors:</p>
        <p>• Default priors in blavaan (Merkle &amp; Rosseel, 2018):• Default priors in blavaan (Merkle &amp; Rosseel, 2018):</p>
        <p>Factor loadings and indicator intercepts were distributed as ϳ N͑ ϭ 0, 2 ϭ 100͒; indicator residual standard deviations were distributed as ϳ half-Cauchy( ϭ 0, 2 ϭ 2.5); and factor correlations were distributed as ϳ U(Ϫ1, 1).Factor loadings and indicator intercepts were distributed as ϳ N͑ ϭ 0, 2 ϭ 100͒; indicator residual standard deviations were distributed as ϳ half-Cauchy( ϭ 0, 2 ϭ 2.5); and factor correlations were distributed as ϳ U(Ϫ1, 1).</p>
        <p>• Alternative noninformative priors: Factor loadings and indicator intercepts were distributed as ϳ N͑ ϭ 0, 2 ϭ 1,000,000͒; indicator residual standard deviations were distributed as ϳ U(0.01, 1000); and factor correlations were distributed as ϳ U(Ϫ1, 1). These priors are even less informative, to a degree similar to the default priors in Mplus (Muthén &amp; Muthén, 1998-2017). However, we could not exactly replicate all the default priors of Mplus because they place priors on factor covariances, whereas blavaan places priors on factor correlations, which are rescaled to covariances implied by the estimated correlations and (fixed or free) variances (Merkle &amp; Rosseel, 2018).• Alternative noninformative priors: Factor loadings and indicator intercepts were distributed as ϳ N͑ ϭ 0, 2 ϭ 1,000,000͒; indicator residual standard deviations were distributed as ϳ U(0.01, 1000); and factor correlations were distributed as ϳ U(Ϫ1, 1). These priors are even less informative, to a degree similar to the default priors in Mplus (Muthén &amp; Muthén, 1998-2017). However, we could not exactly replicate all the default priors of Mplus because they place priors on factor covariances, whereas blavaan places priors on factor correlations, which are rescaled to covariances implied by the estimated correlations and (fixed or free) variances (Merkle &amp; Rosseel, 2018).</p>
        <p>The default priors yielded pD LOO ϭ 32.263 (similar to 30 estimated parameters under MLE) and p ‫ء‬ Ϫ pD ϭ 21.737 (similar to df ϭ 24 under MLE). The alternative priors (the "Wide" column in Table 9) yielded nearly identical results: pD LOO ϭ 32.451 and p ‫ء‬ Ϫ pD ϭ 21.549.The default priors yielded pD LOO ϭ 32.263 (similar to 30 estimated parameters under MLE) and p ‫ء‬ Ϫ pD ϭ 21.737 (similar to df ϭ 24 under MLE). The alternative priors (the "Wide" column in Table 9) yielded nearly identical results: pD LOO ϭ 32.451 and p ‫ء‬ Ϫ pD ϭ 21.549.</p>
        <p>We calculated Bayesian fit indices using both the "DevM" formulation proposed here (columns labeled MCMC DevM in Table 8) and the "PPMC" formulation (columns labeled MCMC PPMC in Table 8) based on the BRMSEA proposed by Hoofs et al. (2018). Each index's distribution is summarized in Table 8 by its mean, standard deviation, and percentiles corresponding to a 90% credible interval. The means using default priors can be compared with their MLE counterparts (also presented in Table 8), and the upper and lower bounds of BRMSEA can be compared with the 90% CI of RMSEA.We calculated Bayesian fit indices using both the "DevM" formulation proposed here (columns labeled MCMC DevM in Table 8) and the "PPMC" formulation (columns labeled MCMC PPMC in Table 8) based on the BRMSEA proposed by Hoofs et al. (2018). Each index's distribution is summarized in Table 8 by its mean, standard deviation, and percentiles corresponding to a 90% credible interval. The means using default priors can be compared with their MLE counterparts (also presented in Table 8), and the upper and lower bounds of BRMSEA can be compared with the 90% CI of RMSEA.</p>
        <p>Posterior means of MCMC DevM -based fit indices were close to the MLE fit indices, whereas the MCMC PPMC -based fit indices indicated better data-model fit. This is due to the difference in how model discrepancy is rescaled between the "DevM" and "PPMC" formulations (compare Equation 19 with Equation 16), as shown in the row of Table 8 labeled " 2 ." The MLE chi square and MCMC DevM chi-square analog were 85.31 and 83.56, respectively, whereas the average of the difference (D obs Ϫ D rep ) used by MCMC PPMC was 61.28. Chi-square-based approximate fit indices using the "PPMC" formulation therefore indicated better datamodel fit. If one were to test a H 0 of approximate fit using the guideline RMSEA Ͻ 0.08 (MacCallum et al., 1996(MacCallum et al., , 2006)), this model would have been rejected using both MLE and This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.Posterior means of MCMC DevM -based fit indices were close to the MLE fit indices, whereas the MCMC PPMC -based fit indices indicated better data-model fit. This is due to the difference in how model discrepancy is rescaled between the "DevM" and "PPMC" formulations (compare Equation 19 with Equation 16), as shown in the row of Table 8 labeled " 2 ." The MLE chi square and MCMC DevM chi-square analog were 85.31 and 83.56, respectively, whereas the average of the difference (D obs Ϫ D rep ) used by MCMC PPMC was 61.28. Chi-square-based approximate fit indices using the "PPMC" formulation therefore indicated better datamodel fit. If one were to test a H 0 of approximate fit using the guideline RMSEA Ͻ 0.08 (MacCallum et al., 1996(MacCallum et al., , 2006)), this model would have been rejected using both MLE and This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>BRMSEA DevM , yet accepted using BRMSEA PPMC . Using the 90% CI, a H 0 of poor fit (RMSEA Ͼ 10) would be rejected using BRMSEA PPMC but not by RMSEA or BRMSEA DevM , so the latter should prompt researchers to further investigate whether their model fails in specific ways. These results with our nine-indicator model were consistent with the simulation results in Hoofs et al. (2018), which showed that in small to moderate samples, BRMSEA PPMC was similar to RMSEA in six-indicator models but was much smaller than RMSEA in 12-indicator models. Specifying alternative noninformative priors yielded nearly identical results. These examples further support that MCMC DevM fit indices are reasonable approximations of their MLE counterparts, but with narrower intervals (see Table 8, Figure 5). In contrast, MCMC PPMC fit indices indicate a more positive (perhaps misleading) impression of data-model fit.BRMSEA DevM , yet accepted using BRMSEA PPMC . Using the 90% CI, a H 0 of poor fit (RMSEA Ͼ 10) would be rejected using BRMSEA PPMC but not by RMSEA or BRMSEA DevM , so the latter should prompt researchers to further investigate whether their model fails in specific ways. These results with our nine-indicator model were consistent with the simulation results in Hoofs et al. (2018), which showed that in small to moderate samples, BRMSEA PPMC was similar to RMSEA in six-indicator models but was much smaller than RMSEA in 12-indicator models. Specifying alternative noninformative priors yielded nearly identical results. These examples further support that MCMC DevM fit indices are reasonable approximations of their MLE counterparts, but with narrower intervals (see Table 8, Figure 5). In contrast, MCMC PPMC fit indices indicate a more positive (perhaps misleading) impression of data-model fit.</p>
        <p>Small-Variance Priors for Cross-Loadings Holzinger and Swineford (1939) posited a bifactor solution from their exploratory factor analysis results, in which the three factors of our example model would be interpreted as orthogonal method factors, and all indicators would also load on a single generalintelligence factor. However, a hypothetical researcher might be interested in retaining approximately simple structure by specifying a Bayesian model that more flexibly represents theoretical expectations. Following Muthén and Asparouhov (2012), we specified the same model with default priors but also added all possible cross-loadings with the constraint that they were approximately zero (i.e., specifying a normal prior with ϭ 0 and 2 ϭ 0.01). The standard deviations of the observed variables ranged from 1.01 to 1.29, so (similar to Muthén &amp; Asparouhov, 2012) these priors represents a hypothesized 95% probability that approximately standardized cross-loadings are within Ϯ0.2 of zero.Small-Variance Priors for Cross-Loadings Holzinger and Swineford (1939) posited a bifactor solution from their exploratory factor analysis results, in which the three factors of our example model would be interpreted as orthogonal method factors, and all indicators would also load on a single generalintelligence factor. However, a hypothetical researcher might be interested in retaining approximately simple structure by specifying a Bayesian model that more flexibly represents theoretical expectations. Following Muthén and Asparouhov (2012), we specified the same model with default priors but also added all possible cross-loadings with the constraint that they were approximately zero (i.e., specifying a normal prior with ϭ 0 and 2 ϭ 0.01). The standard deviations of the observed variables ranged from 1.01 to 1.29, so (similar to Muthén &amp; Asparouhov, 2012) these priors represents a hypothesized 95% probability that approximately standardized cross-loadings are within Ϯ0.2 of zero.</p>
        <p>As expected, adding small-variance priors for cross-loadings hypothesized to be approximately zero yielded fit measures that indicated better data-model correspondence. Comparing the "Cross" column in Table 9 with the "Wide" column (the latter of which is essentially equivalent to the MCMC DevM results in Table 8), PPP 2 ϭ 0.17 (compared with Ͻ.001) and BRMSEA DevM ϭAs expected, adding small-variance priors for cross-loadings hypothesized to be approximately zero yielded fit measures that indicated better data-model correspondence. Comparing the "Cross" column in Table 9 with the "Wide" column (the latter of which is essentially equivalent to the MCMC DevM results in Table 8), PPP 2 ϭ 0.17 (compared with Ͻ.001) and BRMSEA DevM ϭ</p>
        <p>The estimated factor correlations were in the approximate range of .30 to .50 (medium to large; Cohen, 1992), so constraining them to zero would be a gross model misspecification. Thus, if we specified strongly informative priors that constrained the factor correlations to be approximately (but not exactly) zero, those priors would be an aspect of the model that is misspecified, which should be reflected by fit measures. We would expect (a) pD to decrease by no more than three parameters, and (b) PPP 2 and approximate fit indices to show worse model fit than an otherwise equivalent model with uninformative priors for factor correlations (i.e., the results from Table 8).The estimated factor correlations were in the approximate range of .30 to .50 (medium to large; Cohen, 1992), so constraining them to zero would be a gross model misspecification. Thus, if we specified strongly informative priors that constrained the factor correlations to be approximately (but not exactly) zero, those priors would be an aspect of the model that is misspecified, which should be reflected by fit measures. We would expect (a) pD to decrease by no more than three parameters, and (b) PPP 2 and approximate fit indices to show worse model fit than an otherwise equivalent model with uninformative priors for factor correlations (i.e., the results from Table 8).</p>
        <p>To verify this expectation, we fit a model with default priors for all parameters except factor correlations, for which we specified informative priors following a rescaled Beta(200,200) distribution. The standard boundaries of the Beta distribution from 0 to 1 are rescaled by multiplying by 2 and subtracting 1, effectively setting new boundaries from Ϫ1 to 1 (same as correlations). These priors reflect the belief that these factor correlations have a 95% chance of being within Ϯ0.1 of 0.To verify this expectation, we fit a model with default priors for all parameters except factor correlations, for which we specified informative priors following a rescaled Beta(200,200) distribution. The standard boundaries of the Beta distribution from 0 to 1 are rescaled by multiplying by 2 and subtracting 1, effectively setting new boundaries from Ϫ1 to 1 (same as correlations). These priors reflect the belief that these factor correlations have a 95% chance of being within Ϯ0.1 of 0.</p>
        <p>The "Orth" column of Table 9 shows the results for the model with orthogonality constraints. As expected, pD decreases from 31.77 with default priors to 29.61 (about 0.72 for each of three constrained correlations). All approximate fit indices indicated worse fit after constraining correlations, for example, BRMSEA DevM ϭ 0.116 for approximate orthogonality compared with BRMSEA DevM ϭ 0.097 with default priors. Their intervals were not noticeably more precise after specifying informative priors for factor correlations, so misspecified priors do not appear to translate to increased precision for fit indices in this case, but future simulation research could test whether and under what conditions this could be expected.The "Orth" column of Table 9 shows the results for the model with orthogonality constraints. As expected, pD decreases from 31.77 with default priors to 29.61 (about 0.72 for each of three constrained correlations). All approximate fit indices indicated worse fit after constraining correlations, for example, BRMSEA DevM ϭ 0.116 for approximate orthogonality compared with BRMSEA DevM ϭ 0.097 with default priors. Their intervals were not noticeably more precise after specifying informative priors for factor correlations, so misspecified priors do not appear to translate to increased precision for fit indices in this case, but future simulation research could test whether and under what conditions this could be expected.</p>
        <p>The previous examples investigated informative priors for nuisance parameters and for a substantive hypothesis that was not consistent with the data. We also consider the case of informative priors based on previously obtained estimates. To imitate a situation in which the priors are maximally consistent with the observed data, we specified priors based on the posterior distribution from the model with default priors. Priors for factor loadings and item intercepts were ϳ N( ϭ , ϭ 0.1), and for residual standard deviations were ϳ Log-N( ϭ , ϭ 0.1), where is the estimated posterior mean from the model with default priors, and the standard deviations specified similar precision as for the model with near-zero cross-loadings. Thus, the model was specified with high certainty that the parameters would be consistent with estimates from the model with default priors (which we treated as results from a "previous" sample). In practice, this is what Gelman et al. (2017) would consider "cheating" because the ϭ was specified after seeing the data instead of reflecting prior theoretical/probabilistic beliefs about the expected distribution.The previous examples investigated informative priors for nuisance parameters and for a substantive hypothesis that was not consistent with the data. We also consider the case of informative priors based on previously obtained estimates. To imitate a situation in which the priors are maximally consistent with the observed data, we specified priors based on the posterior distribution from the model with default priors. Priors for factor loadings and item intercepts were ϳ N( ϭ , ϭ 0.1), and for residual standard deviations were ϳ Log-N( ϭ , ϭ 0.1), where is the estimated posterior mean from the model with default priors, and the standard deviations specified similar precision as for the model with near-zero cross-loadings. Thus, the model was specified with high certainty that the parameters would be consistent with estimates from the model with default priors (which we treated as results from a "previous" sample). In practice, this is what Gelman et al. (2017) would consider "cheating" because the ϭ was specified after seeing the data instead of reflecting prior theoretical/probabilistic beliefs about the expected distribution.</p>
        <p>The column "Strict" in Table 9 presents the results. As expected, pD decreases substantially, from 31.7 (default priors) to 22.8. Note that this decrease results solely from estimating all the same parameters but with greater precision (i.e., prior variances 2 ϭ 0.01 vs. 2 ϭ 100). On average, pD decreased by 0.33 per estimated parameter. Contrary to expectation, all fit indices presented worse model fit. So despite the priors being set around the previously estimated posterior means, D( ) was also affected, possibly because the priors did not allow the MCMC sampler to sufficiently explore the parameter space. However, the posterior distributions of model parameters estimated with default and strict priors still overlapped substantially: between 85.7% and 99.5% across parameters, with an average overlap of 95.2% (SD ϭ 3.5%). It is also notable that contrary to expectation, the strict priors did not yield narrower intervals for fit indices than noninformative priors did.The column "Strict" in Table 9 presents the results. As expected, pD decreases substantially, from 31.7 (default priors) to 22.8. Note that this decrease results solely from estimating all the same parameters but with greater precision (i.e., prior variances 2 ϭ 0.01 vs. 2 ϭ 100). On average, pD decreased by 0.33 per estimated parameter. Contrary to expectation, all fit indices presented worse model fit. So despite the priors being set around the previously estimated posterior means, D( ) was also affected, possibly because the priors did not allow the MCMC sampler to sufficiently explore the parameter space. However, the posterior distributions of model parameters estimated with default and strict priors still overlapped substantially: between 85.7% and 99.5% across parameters, with an average overlap of 95.2% (SD ϭ 3.5%). It is also notable that contrary to expectation, the strict priors did not yield narrower intervals for fit indices than noninformative priors did.</p>
        <p>We are unsure whether these results would be observed in a model that fit the data well; given the mediocre fit of this model, placing great certainty in the wrong parameters might merely exacerbate evidence against data-model fit. This would be consistent with the expected behavior of a marginal likelihood with constrained priors, which yield an inappropriate marginal likelihood. This is reflected in a marginal likelihood sensitive to aspects of the prior distribution that have minimal effect on the posterior inferences (Gelman et al., 2017), which could imply the approximate fit indices have the potential to diagnose if prior distributions are affecting the marginal likelihood in such a way that would make it unreliable.We are unsure whether these results would be observed in a model that fit the data well; given the mediocre fit of this model, placing great certainty in the wrong parameters might merely exacerbate evidence against data-model fit. This would be consistent with the expected behavior of a marginal likelihood with constrained priors, which yield an inappropriate marginal likelihood. This is reflected in a marginal likelihood sensitive to aspects of the prior distribution that have minimal effect on the posterior inferences (Gelman et al., 2017), which could imply the approximate fit indices have the potential to diagnose if prior distributions are affecting the marginal likelihood in such a way that would make it unreliable.</p>
        <p>The effects of informative priors should be even more pronounced when they have a greater relative influence on the posterior, as would be the case with small samples. We drew a random sample of N ϭ 75 from the full data set and fit the model with default priors, wide priors, and strict priors to the subsample. Consistent with our simulation results, Table 10 shows wider intervals for all models (reflecting less information from data), and the different noninformative priors yielded effectively the same posterior means (consistent with MLE across sample sizes). Again, the strict priors yielded worse fit, but the effect of informative priors appears even more extreme than with the full sample. Unfortunately, the data-model correspondence already appears much poorer for this subsample than the full N ϭ 301 in Table 9, which might be due (at least in part) to small-sample bias (Jiang &amp; Yuan, 2017;Nevitt &amp; Hancock, 2004). Future simulation studies could verify whether our original expectation (same average fit with smaller intervals) holds when the model accurately represents the population.The effects of informative priors should be even more pronounced when they have a greater relative influence on the posterior, as would be the case with small samples. We drew a random sample of N ϭ 75 from the full data set and fit the model with default priors, wide priors, and strict priors to the subsample. Consistent with our simulation results, Table 10 shows wider intervals for all models (reflecting less information from data), and the different noninformative priors yielded effectively the same posterior means (consistent with MLE across sample sizes). Again, the strict priors yielded worse fit, but the effect of informative priors appears even more extreme than with the full sample. Unfortunately, the data-model correspondence already appears much poorer for this subsample than the full N ϭ 301 in Table 9, which might be due (at least in part) to small-sample bias (Jiang &amp; Yuan, 2017;Nevitt &amp; Hancock, 2004). Future simulation studies could verify whether our original expectation (same average fit with smaller intervals) holds when the model accurately represents the population.</p>
        <p>We used the 
            <rs type="software">R</rs> package 
            <rs type="software">simsem</rs> (Jorgensen, Pornprasertmanit, Miller, &amp; Schoemann, 2018) to set 20% and 50% of values on all the indicators in the Holzinger and Swineford (1939) data to be missing completely at random (Enders, 2010). We fit the model This document is copyrighted by the American Psychological Association or one of its allied publishers.
        </p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>using FIML and MCMC using default noninformative priors.using FIML and MCMC using default noninformative priors.</p>
        <p>Results are presented in Table 11, which repeats the complete-data MCMC results from Table 8 to ease comparison. As expected, missing data have minimal impact on pD given that imputed values are integrated out to yield a marginal likelihood (Merkle &amp; Rosseel, 2018;Muthén &amp; Muthén, 1998-2017). Consistent with Davey (2005), absolute fit measures indicated better fit with greater proportions of missing data. Contrary to Davey (2005), we observed that incremental fit indices indicated better fit for 20% missing than for complete data yet worse fit for 50% missing than for 20% missing. Our model was similar to the simulated models of Davey's Monte Carlo study, but we imposed missing values on all indicators rather than only three, and our model fit the Holzinger and Swineford (1939) data worse than most of the conditions that Davey simulated. Table 11 includes 0 2 for the independence model M 0 to help make sense of the results for incremental fit indices. With greater proportions of missing data, the fit for M 0 improves proportionally more so than the fit for the hypothesized model, so the hypothesized model's fit relative to M 0 is not as high with 50% missing data.Results are presented in Table 11, which repeats the complete-data MCMC results from Table 8 to ease comparison. As expected, missing data have minimal impact on pD given that imputed values are integrated out to yield a marginal likelihood (Merkle &amp; Rosseel, 2018;Muthén &amp; Muthén, 1998-2017). Consistent with Davey (2005), absolute fit measures indicated better fit with greater proportions of missing data. Contrary to Davey (2005), we observed that incremental fit indices indicated better fit for 20% missing than for complete data yet worse fit for 50% missing than for 20% missing. Our model was similar to the simulated models of Davey's Monte Carlo study, but we imposed missing values on all indicators rather than only three, and our model fit the Holzinger and Swineford (1939) data worse than most of the conditions that Davey simulated. Table 11 includes 0 2 for the independence model M 0 to help make sense of the results for incremental fit indices. With greater proportions of missing data, the fit for M 0 improves proportionally more so than the fit for the hypothesized model, so the hypothesized model's fit relative to M 0 is not as high with 50% missing data.</p>
        <p>We proposed how chi-square-based SEM fit indices can be calculated in BSEM using a Bayesian analog of the chi-square statistic: D( ). Because D( ) Ϸ 2 , it can be used to calculate measures of approximate fit that are commonly used in SEM to complement the chi-square test of exact fit of a model to data. We compared these BSEM fit indices with their frequentist counterparts through a Monte Carlo simulation study, which verified our expectation that MCMC with noninformative priors yields similar results to MLE across levels of misspecification, sample sizes, and This document is copyrighted by the American Psychological Association or one of its allied publishers.We proposed how chi-square-based SEM fit indices can be calculated in BSEM using a Bayesian analog of the chi-square statistic: D( ). Because D( ) Ϸ 2 , it can be used to calculate measures of approximate fit that are commonly used in SEM to complement the chi-square test of exact fit of a model to data. We compared these BSEM fit indices with their frequentist counterparts through a Monte Carlo simulation study, which verified our expectation that MCMC with noninformative priors yields similar results to MLE across levels of misspecification, sample sizes, and This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>model types. Because we apply the same formulas to analogous Bayesian and frequentist quantities, we opine that traditional guidelines proposed for interpreting the magnitude of SEM fit indices based on intuition and experience (Bentler &amp; Bonett, 1980;Browne &amp; Cudeck, 1992) would be no less valid to apply to BSEM fit indices applied using the "DevM" formulation, even when informative priors are specified. An advantage of BSEM fit indices over most of their frequentist counterparts is that the posterior distribution allows uncertainty to be quantified for any fit index, although the BRMSEA DevM intervals were narrower than the CIs of RMSEA under MLE. So researchers who insist on interpreting fit indices from a hypothesis-testing perspective (MacCallum et al., 1996(MacCallum et al., , 2006) ) should not expect previous simulation results that provided guidelines based on Type I and II error rates (Hu &amp; Bentler, 1998, 1999; and for invariance testing: Chen, 2007;Cheung &amp; Rensvold, 2002;Meade, Johnson, &amp; Braddy, 2008) to generalize to the BSEM fit indices proposed here. Recall, though, that much research has already revealed that fixed cutoffs do not generalize well in frequentist SEM (Beauducel &amp; Wittmann, 2005;Davey, 2005;Fan &amp; Sivo, 2005;Heene et al., 2011Heene et al., , 2012;;Jorgensen, Kite, et al., 2018;Marsh et al., 2004;Pornprasertmanit, 2014;Pornprasertmanit et al., 2013;Sass et al., 2014), so we do not advocate their use (see below for discussion of alternatives).model types. Because we apply the same formulas to analogous Bayesian and frequentist quantities, we opine that traditional guidelines proposed for interpreting the magnitude of SEM fit indices based on intuition and experience (Bentler &amp; Bonett, 1980;Browne &amp; Cudeck, 1992) would be no less valid to apply to BSEM fit indices applied using the "DevM" formulation, even when informative priors are specified. An advantage of BSEM fit indices over most of their frequentist counterparts is that the posterior distribution allows uncertainty to be quantified for any fit index, although the BRMSEA DevM intervals were narrower than the CIs of RMSEA under MLE. So researchers who insist on interpreting fit indices from a hypothesis-testing perspective (MacCallum et al., 1996(MacCallum et al., , 2006) ) should not expect previous simulation results that provided guidelines based on Type I and II error rates (Hu &amp; Bentler, 1998, 1999; and for invariance testing: Chen, 2007;Cheung &amp; Rensvold, 2002;Meade, Johnson, &amp; Braddy, 2008) to generalize to the BSEM fit indices proposed here. Recall, though, that much research has already revealed that fixed cutoffs do not generalize well in frequentist SEM (Beauducel &amp; Wittmann, 2005;Davey, 2005;Fan &amp; Sivo, 2005;Heene et al., 2011Heene et al., , 2012;;Jorgensen, Kite, et al., 2018;Marsh et al., 2004;Pornprasertmanit, 2014;Pornprasertmanit et al., 2013;Sass et al., 2014), so we do not advocate their use (see below for discussion of alternatives).</p>
        <p>Consistent with previous research (Fan &amp; Sivo, 2007), ⌫ ˆand CFI were substantially affected ( 2 Ͼ 10%) only by the level of misspecification, whereas other indices were also affected by model type (and NFI was also affected by sample size). Based on these simulation results, we would therefore recommended the use of CFI and ⌫ ˆin applied research as well as their continued investigation in simulation research. Also, we recommend applied researchers use either pD LOO or pD WAIC to calculate 
            <rs type="software">BSEM</rs> fit indices because pD DIC appeared sensitive to model type.
        </p>
        <p>Our illustrative examples provide preliminary support for some expectations based on intuition and past results. Different noninformative priors yielded similar results. Informative priors for nuisance parameters (Asparouhov et al., 2015;Muthén &amp; Asparouhov, 2012;Stromeyer et al., 2015) yielded fit indices that indicated better fit. Informative priors that were inconsistent with the data yielded fit indices that indicated worse fit. However, informative priors that were consistent with the data also yielded fit indices that indicated worse fit, which might be due to the lack of data-model correspondence in our Holzinger and Swineford (1939) example or a diagnosis of the priors provoking the marginal likelihood to become inappropriate due to the strong constraints (Gelman et al., 2017). Finally, missing data yielded absolute fit indices that indicated better fit, but incremental fit indices only indicated better fit when the majority of data were observed (20% missing) because the M 0 's fit seemed to improve proportionally more than M H 's fit when 50% of the values were set to missing (a result not observed with FIML; Davey, 2005). Before more general guidelines can be provided for their use in evaluating a wider array of BSEM models, future Monte Carlo research must be designed to investigate whether these assumptions hold under various condi-tions and to further probe the causes of some unexpected results. Gelman et al. (2017) classified the relevance and use of priors accordingly: minimalist (or "noninformative"), reference, structural, weakly informative, and regularizing. The minimalist and reference types do little more than fulfill the requirement of specifying a prior for Bayesian inference, but with no intention of priors providing information about the model parameters. Structural priors set a structural form for the related parameters, without guiding the expected values. Regularizing priors represent a strong assumption about the model, where the researcher intends to yield smoother and more stable inferences. Between structural and regularizing is weakly informative, which provides information that applies to a general type of problems without taking full advantage of problem-specific knowledge. In BSEM, the priors are usually in the categories of minimalist, reference, or structural, the latter of which can yield unexpected results regarding model comparison15 when priors add no information about the model, only varying levels of uncertainty about them (e.g., 2 ). The priors are included in the calculation of the marginal likelihood, which can be affected by priors without affecting the posterior distribution inferences-this is a known issue with multivariate models (Gelman et al., 2017). More research is needed to determine the effects of different types of priors and ways to quantify the effect of priors on the marginal likelihood and the posterior distribution.Our illustrative examples provide preliminary support for some expectations based on intuition and past results. Different noninformative priors yielded similar results. Informative priors for nuisance parameters (Asparouhov et al., 2015;Muthén &amp; Asparouhov, 2012;Stromeyer et al., 2015) yielded fit indices that indicated better fit. Informative priors that were inconsistent with the data yielded fit indices that indicated worse fit. However, informative priors that were consistent with the data also yielded fit indices that indicated worse fit, which might be due to the lack of data-model correspondence in our Holzinger and Swineford (1939) example or a diagnosis of the priors provoking the marginal likelihood to become inappropriate due to the strong constraints (Gelman et al., 2017). Finally, missing data yielded absolute fit indices that indicated better fit, but incremental fit indices only indicated better fit when the majority of data were observed (20% missing) because the M 0 's fit seemed to improve proportionally more than M H 's fit when 50% of the values were set to missing (a result not observed with FIML; Davey, 2005). Before more general guidelines can be provided for their use in evaluating a wider array of BSEM models, future Monte Carlo research must be designed to investigate whether these assumptions hold under various condi-tions and to further probe the causes of some unexpected results. Gelman et al. (2017) classified the relevance and use of priors accordingly: minimalist (or "noninformative"), reference, structural, weakly informative, and regularizing. The minimalist and reference types do little more than fulfill the requirement of specifying a prior for Bayesian inference, but with no intention of priors providing information about the model parameters. Structural priors set a structural form for the related parameters, without guiding the expected values. Regularizing priors represent a strong assumption about the model, where the researcher intends to yield smoother and more stable inferences. Between structural and regularizing is weakly informative, which provides information that applies to a general type of problems without taking full advantage of problem-specific knowledge. In BSEM, the priors are usually in the categories of minimalist, reference, or structural, the latter of which can yield unexpected results regarding model comparison15 when priors add no information about the model, only varying levels of uncertainty about them (e.g., 2 ). The priors are included in the calculation of the marginal likelihood, which can be affected by priors without affecting the posterior distribution inferences-this is a known issue with multivariate models (Gelman et al., 2017). More research is needed to determine the effects of different types of priors and ways to quantify the effect of priors on the marginal likelihood and the posterior distribution.</p>
        <p>We considered chi-square-based fit indices only to assess the practical fit of the model (consistent with their use in MLE). Because their true sampling variability is underestimated by the interval derived from the posterior percentiles, using 90% intervals to test a H 0 about the model would yield inflated Type I errors. Instead, researchers interested in using a fit index to test the model could use it as the discrepancy function for the observed data in a PPMC framework. Because Hoofs et al. (2018) proposed calculating BRMSEA i PPMC using the posterior predictive distribution of D i rep , one could not subsequently use BRMSEA i PPMC as the discrepancy function in a posterior predictive model check (e.g., the way Levy, 2011, used SRMR as a discrepancy function). On the other hand, BRMSEA i DevM (or other "DevM" fit indices Equations 20 -25) could serve as a discrepancy function in a PPMC. For example, the posterior distribution of BRMSEA DevM could be approximated using their realized values by plugging D i obs into Equation 17, whereas plugging D i rep into Equation 17 in place of D i obs would approximate its posterior predictive distribution (i.e., expected values given the model parameters). However, because BRMSEA i DevM is simply a function of D H,i obs and other quantities that do not differ for the observed (or replicated) data (i.e., N, df, p ‫ء‬ , and pD), we can expect PPMC to yield the same conclusions when using BRMSEA i DevM (or any indices based solely on D H obs ) as when using H 2 itself. So we would not expect applying PPMC with a fit index based only on H 2 to add any extra information beyond what PPP 2 provides.We considered chi-square-based fit indices only to assess the practical fit of the model (consistent with their use in MLE). Because their true sampling variability is underestimated by the interval derived from the posterior percentiles, using 90% intervals to test a H 0 about the model would yield inflated Type I errors. Instead, researchers interested in using a fit index to test the model could use it as the discrepancy function for the observed data in a PPMC framework. Because Hoofs et al. (2018) proposed calculating BRMSEA i PPMC using the posterior predictive distribution of D i rep , one could not subsequently use BRMSEA i PPMC as the discrepancy function in a posterior predictive model check (e.g., the way Levy, 2011, used SRMR as a discrepancy function). On the other hand, BRMSEA i DevM (or other "DevM" fit indices Equations 20 -25) could serve as a discrepancy function in a PPMC. For example, the posterior distribution of BRMSEA DevM could be approximated using their realized values by plugging D i obs into Equation 17, whereas plugging D i rep into Equation 17 in place of D i obs would approximate its posterior predictive distribution (i.e., expected values given the model parameters). However, because BRMSEA i DevM is simply a function of D H,i obs and other quantities that do not differ for the observed (or replicated) data (i.e., N, df, p ‫ء‬ , and pD), we can expect PPMC to yield the same conclusions when using BRMSEA i DevM (or any indices based solely on D H obs ) as when using H 2 itself. So we would not expect applying PPMC with a fit index based only on H 2 to add any extra information beyond what PPP 2 provides.</p>
        <p>In contrast, incremental fit indices are based not only on H 2 but also on 0 2 , so a PPMC using Equation 25 could provide distinct information from PPMC using H 2 (i.e., How well does M H fit relative to a meaningfully specified M 0 ; Bentler &amp; Bonett, 1980). Beyond the complication of specifying informative priors for a null model M 0 that is known not to be true (Hoofs et al., 2018), a second complication involves estimating a posterior predictive distribution of incremental fit indices. Calculating a posterior distribution of realized values (e.g., of BCFI i DevM ) would require 0 2 , which only requires fitting an appropriate M 0 (e.g., an independence model) to the observed data. PPMC would also require fitting the hypothesized model M H to both observed data and replicated data generated from the sampled estimates (at that iteration) of the model being fitted to the observed data. However, whereas M 0 is also fitted to observed data, M 0 should be fitted to replicated data generated from M H , so that the model would be tested assuming M H is true, not M 0 . We expect a deeper exploration of how to conduct PPMC with incremental fit indices (e.g., whether to use the same replicated data generated when fitting M H ) to be a valuable extension of the current proposal. We conclude by reminding readers that fit indices were designed merely to be descriptive measures meant to help researchers evaluate the degree to which their model fails to reproduce the observed data; they were not originally developed to be test statistics (although they have been applied as such). Global fit indices are meant to complement, not replace, informative tests of the model. Researchers who find values of their fit indices questionable should complement these global measures with more informative local measures of model (mis)fit, such as described in Levy (2011). We are hopeful that fit indices can be conscientiously applied in (B)SEM in conjunction with other model-evaluation tools that summarize different dimensions of data-model correspondence (or lack thereof).In contrast, incremental fit indices are based not only on H 2 but also on 0 2 , so a PPMC using Equation 25 could provide distinct information from PPMC using H 2 (i.e., How well does M H fit relative to a meaningfully specified M 0 ; Bentler &amp; Bonett, 1980). Beyond the complication of specifying informative priors for a null model M 0 that is known not to be true (Hoofs et al., 2018), a second complication involves estimating a posterior predictive distribution of incremental fit indices. Calculating a posterior distribution of realized values (e.g., of BCFI i DevM ) would require 0 2 , which only requires fitting an appropriate M 0 (e.g., an independence model) to the observed data. PPMC would also require fitting the hypothesized model M H to both observed data and replicated data generated from the sampled estimates (at that iteration) of the model being fitted to the observed data. However, whereas M 0 is also fitted to observed data, M 0 should be fitted to replicated data generated from M H , so that the model would be tested assuming M H is true, not M 0 . We expect a deeper exploration of how to conduct PPMC with incremental fit indices (e.g., whether to use the same replicated data generated when fitting M H ) to be a valuable extension of the current proposal. We conclude by reminding readers that fit indices were designed merely to be descriptive measures meant to help researchers evaluate the degree to which their model fails to reproduce the observed data; they were not originally developed to be test statistics (although they have been applied as such). Global fit indices are meant to complement, not replace, informative tests of the model. Researchers who find values of their fit indices questionable should complement these global measures with more informative local measures of model (mis)fit, such as described in Levy (2011). We are hopeful that fit indices can be conscientiously applied in (B)SEM in conjunction with other model-evaluation tools that summarize different dimensions of data-model correspondence (or lack thereof).</p>
        <p>These formulas use the same quantities defined for Equation 19and are expected (when using noninformative priors) to be reasonable approximations of fit indices under MLE. We designate these with a superscript "DevM" to indicate the observed deviance at iteration i in the Markov chain is rescaled using pD to make its expectation equal to the deviance evaluated at the posterior mean. Note, however, that because the mean of a nonlinear function of X does not generally equal the same nonlinear function of X , the recentering of D i obs does not imply that the posterior mean of any of these indices will equal the values of the indices calculated using D( ). The simulation study shows that they are nonetheless reasonable approximations of their ML counterparts.These formulas use the same quantities defined for Equation 19and are expected (when using noninformative priors) to be reasonable approximations of fit indices under MLE. We designate these with a superscript "DevM" to indicate the observed deviance at iteration i in the Markov chain is rescaled using pD to make its expectation equal to the deviance evaluated at the posterior mean. Note, however, that because the mean of a nonlinear function of X does not generally equal the same nonlinear function of X , the recentering of D i obs does not imply that the posterior mean of any of these indices will equal the values of the indices calculated using D( ). The simulation study shows that they are nonetheless reasonable approximations of their ML counterparts.</p>
        <p>For all incremental fit indices, quantities for the hypothesized model (M H ) have an "H" subscript, whereas a "0" indicates the same quantity from the null model (M 0 ). (25)For all incremental fit indices, quantities for the hypothesized model (M H ) have an "H" subscript, whereas a "0" indicates the same quantity from the null model (M 0 ). (25)</p>
        <p>Following from Hoofs et al. (2018), indices can be derived using similar principles, rescaling D i obs not by pD but by D i rep . We designate these with a superscript "PPMC" to indicate the observed deviance at iteration i in the Markov chain is rescaled using posterior predictive model checks. (31)Following from Hoofs et al. (2018), indices can be derived using similar principles, rescaling D i obs not by pD but by D i rep . We designate these with a superscript "PPMC" to indicate the observed deviance at iteration i in the Markov chain is rescaled using posterior predictive model checks. (31)</p>
        <p>Note. ⌬Mean ϭ MLE Ϫ MCMC; % Overlap ϭ percentage of the MLE sampling distribution that overlaps with the MCMC sampling distribution; d ϭ standardized ⌬Mean (Cohen's d); r ϭ the Pearson correlation between indices under MLE and MCMC; MLE ϭ maximum likelihood estimation; SD ϭ Standard deviation; MCMC ϭ Markov Chain Monte Carlo estimation; 2 ϭ chi-square; PPP 2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index.Note. ⌬Mean ϭ MLE Ϫ MCMC; % Overlap ϭ percentage of the MLE sampling distribution that overlaps with the MCMC sampling distribution; d ϭ standardized ⌬Mean (Cohen's d); r ϭ the Pearson correlation between indices under MLE and MCMC; MLE ϭ maximum likelihood estimation; SD ϭ Standard deviation; MCMC ϭ Markov Chain Monte Carlo estimation; 2 ϭ chi-square; PPP 2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index.</p>
        <p>2 ϭ chi-square; PPP 2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index; MI ϭ misspecification; MT ϭ model type; N ϭ sample size.2 ϭ chi-square; PPP 2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index; MI ϭ misspecification; MT ϭ model type; N ϭ sample size.</p>
        <p>Note. These results marginalize over sample-size and model-type conditions. 90% CI ϭ the average lower and upper bounds. PPP 2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index; CI ϭ credible interval.Note. These results marginalize over sample-size and model-type conditions. 90% CI ϭ the average lower and upper bounds. PPP 2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index; CI ϭ credible interval.</p>
        <p>2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index.2 ϭ chi-square based posterior predictive p-value; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ McDonald's centrality index; CFI ϭ comparative fit index; TLI ϭ Tuker-Lewis index; NFI ϭ normed fit index.</p>
        <p>Note. Row names under the Index column should be understood as representing their Bayesian analogs (e.g., D( ) in place of 2 , PPP 2 in place of the p value, BRMSEA in place of RMSEA, and so on). 90% CI ϭ the average lower and upper bounds; MCAR ϭ percentage of data points missing completely at random; MCMC ϭ "DevM"-based Bayesian fit indices; FIML ϭ full-information maximum likelihood based fit indices; 2 ϭ chi-square; 0 2 ϭ null model chi-square; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭNote. Row names under the Index column should be understood as representing their Bayesian analogs (e.g., D( ) in place of 2 , PPP 2 in place of the p value, BRMSEA in place of RMSEA, and so on). 90% CI ϭ the average lower and upper bounds; MCAR ϭ percentage of data points missing completely at random; MCMC ϭ "DevM"-based Bayesian fit indices; FIML ϭ full-information maximum likelihood based fit indices; 2 ϭ chi-square; 0 2 ϭ null model chi-square; RMSEA ϭ root mean square error of approximation; ⌫ ˆϭ gamma-hat; ⌫ ˆadj ϭ adjusted gamma-hat; Mc ϭ</p>
        <p>Some restricted cases of nonlinearity in variables have been proposed, such as latent moderated structures for normally distributed latent variables(Klein &amp; Moosbrugger,Some restricted cases of nonlinearity in variables have been proposed, such as latent moderated structures for normally distributed latent variables(Klein &amp; Moosbrugger,</p>
        <p>2000), but are rarely implemented in 
            <rs type="software">SEM</rs> software packages(Muthén &amp; Muthén, 1998-2017).2 Rare exceptions can be fitted with MLE by placing nonlinear constraints on certain estimated parameters. SeeGrimm and Ram (2009) for an example of nonlinear latent growth models.3 Some resampling methods have been proposed for obtaining confidence intervals for SEM fit indices in a frequentist framework, such as bootstrapping(Cheng &amp; Wu, 2017;Zhang &amp; Savalei, 2016), permutation(Jorgensen, Kite, et al., 2018), and Monte Carlo simulation(Millsap, 2007; Pornprasertmanit, 2014;Pornprasertmanit, Wu, &amp; Little, 2013). This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.
        </p>
        <p>FIT INDICES FOR BSEMFIT INDICES FOR BSEM</p>
        <p>Online materials can be found at https://osf.io/afkcw/.Online materials can be found at https://osf.io/afkcw/.</p>
        <p>Early software such as 
            <rs type="software">LISREL</rs>(Jöreskog &amp; Sörbom, 2006) and EQS(Bentler, 2006) applied Wishart-theory likelihood to analyses of covariance structure only, so they used N -1 instead of N. More recently developed software such as 
            <rs type="software">Mplus</rs>(Muthén &amp; Muthén, 1998-2017) and lavaan(Rosseel, 2012) include a mean structure by default, so they apply normaltheory likelihood, which requires N as the multiplier(Widaman &amp; Thompson, 2003). This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.
        </p>
        <p>Some SEM software, such as 
            <rs type="software">Mplus</rs> and 
            <rs type="software">lavaan</rs>, calculate AIC as Ϫ2 ϫ log͑likelihood͒ ϩ 2p, which yields equivalent rankings of competing models because each model's chi square is calculated relative to the same saturated model.
        </p>
        <p>Or more generally, further from the posterior mode of , which is the same as the posterior mean when the posterior distribution is symmetric and unimodal (e.g., normal).Or more generally, further from the posterior mode of , which is the same as the posterior mean when the posterior distribution is symmetric and unimodal (e.g., normal).</p>
        <p>Vehtari et al. (2017) compared pD as developed for DIC with an expression developed forWatanabe's (2010) widely applicable information criterion (WAIC)-which Vehtari et al. called more "fully Bayesian in that it uses the entire posterior distribution" (p. 1414)-as well as a Pareto-smoothed importance sampling (PSIS) approximation of leave-one-out (LOO) crossvalidation. We do not discuss the more complex calculations of pD presented by Vehtari et al., but we compare them in our simulation study. This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.Vehtari et al. (2017) compared pD as developed for DIC with an expression developed forWatanabe's (2010) widely applicable information criterion (WAIC)-which Vehtari et al. called more "fully Bayesian in that it uses the entire posterior distribution" (p. 1414)-as well as a Pareto-smoothed importance sampling (PSIS) approximation of leave-one-out (LOO) crossvalidation. We do not discuss the more complex calculations of pD presented by Vehtari et al., but we compare them in our simulation study. This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>An example proof can be found here: https://math.stackexchange.com/ questions/1204484/average-of-square-rootss-sum-vs-square-root-of-anaverageAn example proof can be found here: https://math.stackexchange.com/ questions/1204484/average-of-square-rootss-sum-vs-square-root-of-anaverage</p>
        <p>Guidelines proposed for fit indices under MLE have also been shown not to generalize to other frequentist estimators, such as DWLS with a mean-and variance-adjusted chi square for ordinal data(Sass et al., 2014). This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.Guidelines proposed for fit indices under MLE have also been shown not to generalize to other frequentist estimators, such as DWLS with a mean-and variance-adjusted chi square for ordinal data(Sass et al., 2014). This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>Online files associated with this project are available at https://osf.io/ afkcw/Online files associated with this project are available at https://osf.io/ afkcw/</p>
        <p>Recall that an SEM's chi-square statistic compares M H with a saturated M S . This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.Recall that an SEM's chi-square statistic compares M H with a saturated M S . This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This document is copyrighted by the American Psychological Association or one of its allied publishers.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
        <p>We thank Marquette University for the use of the high-performance computing cluster, without which our simulation study would not have been possible, partly funded by National Science Foundation awards OCI-0923037 "MRI: Acquisition of a Parallel Computing Cluster and Storage for the Marquette University Grid (
            <rs type="software">MUGrid</rs>)" and CBET-0521602 "Acquisition of a Linux Cluster to Support College-Wide Research &amp; Teaching Activities." We also thank Ed Merkle for his assistance in updating his blavaan package to implement our pro-posal, as well as Hao Wu for his feedback on earlier versions of this article. These results were presented as a paper in July 2018 at the 83rd annual International Meeting of the Psychometric Society in New York, New York. Data and syntax files associated with this project, as well as online supplementary figures, are available on the Open Science Framework (https://osf.io/afkcw/).
        </p>
        <p>The data are available at https://osf.io/afkcw/ The experiment materials are available at https://osf.io/afkcw/The data are available at https://osf.io/afkcw/ The experiment materials are available at https://osf.io/afkcw/</p>
        <p>Note. Wide ϭ wider-than-default noninformative priors; Cross ϭ strongly informative priors for near-zero cross-loadings; Orth ϭ strongly informative priors for near-zero factor correlations; Strict ϭ strongly informative priors for estimated parameters; 90% CI ϭ the average lower and upper bounds; PPP 2 ϭ chi-square based posterior predictive p-value; BRMSEA ϭ Bayesian root mean square error of approximation; B⌫ ˆϭ Bayesian gamma-hat; B⌫ ˆadj ϭ Bayesian adjusted gamma-hat; BMc ϭ Bayesian McDonald's centrality index; BCFI ϭ Bayesian comparative fit index; BTLI ϭ Bayesian Tuker-Lewis index; BNFI ϭ Bayesian normed fit index; pD LOO ϭ leave-one-out based effective number of parameters; D() ϭ deviance. This document is copyrighted by the American Psychological Association or one of its allied publishers.Note. Wide ϭ wider-than-default noninformative priors; Cross ϭ strongly informative priors for near-zero cross-loadings; Orth ϭ strongly informative priors for near-zero factor correlations; Strict ϭ strongly informative priors for estimated parameters; 90% CI ϭ the average lower and upper bounds; PPP 2 ϭ chi-square based posterior predictive p-value; BRMSEA ϭ Bayesian root mean square error of approximation; B⌫ ˆϭ Bayesian gamma-hat; B⌫ ˆadj ϭ Bayesian adjusted gamma-hat; BMc ϭ Bayesian McDonald's centrality index; BCFI ϭ Bayesian comparative fit index; BTLI ϭ Bayesian Tuker-Lewis index; BNFI ϭ Bayesian normed fit index; pD LOO ϭ leave-one-out based effective number of parameters; D() ϭ deviance. This document is copyrighted by the American Psychological Association or one of its allied publishers.</p>
        <p>This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.This article is intended solely for the personal use of the individual user and is not to be disseminated broadly.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T15:44+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made. The images or other third party material in this book are included in the book's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the book's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
        <p>A Wasserstein distance is a metric between probability distributions μ and ν on a ground space X , induced by the problem of optimal mass transportation or simply optimal transport. It reflects the minimal effort that is required in order to reconfigure the mass of μ to produce the mass distribution of ν. The 'effort' corresponds to the total work needed to achieve this reconfiguration, where work equals the amount of mass at the origin times the distance to the prescribed destination of this mass. The distance between origin and destination can be raised to some power other than 1 when defining the notion of work, giving rise to correspondingly different Wasserstein distances. When viewing the space of probability measures on X as a metric space endowed with a Wasserstein distance, we speak of a Wassertein Space.</p>
        <p>Mass transportation and the associated Wasserstein metrics/spaces are ubiquitous in mathematics, with a long history that has seen them catalyse core developments in analysis, optimisation, and probability. Beyond their intrinsic mathematical richness, they possess attractive features that make them a versatile tool for the statistician. They frequently appear in the development of statistical theory and inferential methodology, sometimes as a technical tool in asymptotic theory, due to the useful topology they induce and their easy majorisation; and other times as a methodological tool, for example, in structural modelling and goodnessof-fit testing. A more recent trend in statistics is to consider Wasserstein spaces themselves as a sample and/or parameter space and treat inference problems in such spaces. It is this more recent trend that is the topic of this book and is coming to be known as 'statistics in Wasserstein spaces' or 'statistical optimal transport'.</p>
        <p>From the theoretical point of view, statistics in Wasserstein spaces represents an emerging topic in mathematical statistics, situated at the interface between functional data analysis (where the data are functions, seen as random elements of an infinite-dimensional Hilbert space) and non-Euclidean statistics (where the data satisfy non-linear constraints, thus lying on non-Euclidean manifolds). Wasservii viii Preface stein spaces provide the natural mathematical formalism to describe data collections that are best modelled as random measures on R d (e.g. images and point processes). Such random measures carry the infinite-dimensional traits of functional data, but are intrinsically non-linear due to positivity and integrability restrictions. Indeed, contrarily to functional data, their dominating statistical variation arises through random (non-linear) deformations of an underlying template, rather than the additive (linear) perturbation of an underlying template. This shows optimal transport to be a canonical framework for dealing with problems involving the so-called phase variation (also known as registration, multi-reference alignment, or synchronisation problems). This connection is pursued in detail in this book and linked with the so-called problem of optimal multitransport (or optimal multicoupling).</p>
        <p>In writing our monograph, we had two aims in mind:</p>
        <p>1. To present the key aspects of optimal transportation and Wasserstein spaces (Chaps. 1 and 2) relevant to statistical inference, tailored to the interests and background of the (mathematical) statistician. There are, of course, classic texts comprehensively covering this background. 1 But their choice of topics and style of exposition are usually adapted to the analyst and/or probabilist, with aspects most relevant for statisticians scattered among (much) other material. 2. To make use of the 'Wasserstein background' to present some of the fundamentals of statistical estimation in Wasserstein spaces, and its connection to the problem of phase variation (registration) and optimal multicoupling. In doing so, we highlight connections with classical topics in statistical shape theory, such as Procrustes analysis. On these topics, no book/monograph appears to yet exist.</p>
        <p>The book focusses on the theory of statistics in Wasserstein spaces. It does not cover the associated computational/numerical aspects. This is partially due to space restrictions, but also due to the fact that a reference entirely dedicated to such issues can be found in the very recent monograph of Peyré and Cuturi [103]. Moreover, since this book is meant to be a rapid introduction for non-specialists, we have made no attempt to give a complete bibliography. We have added some bibliographic remarks at the end of each chapter, but these are in no way meant to be exhaustive. For those seeking reference works, Rachev [106] is an excellent overview of optimal transport up to 1985. Other recent reviews are Bogachev and Kolesnikov [26] and Panaretos and Zemel [101]. The latter review can be thought of as complementary to the present book and surveys some of the applications of optimal transport methods to statistics and probability theory.</p>
        <p>The material is organised into five chapters.</p>
        <p>• Chapter 1 presents the necessary background in optimal transportation. Starting with Monge's original formulation, it presents Kantorovich's probabilistic relaxation and the associated duality theory. It then focusses on quadratic cost functions (squared normed cost) and gives a more detailed treatment of certain important special cases. Topics of statistical concern such as the regularity of transport maps and their stability under weak convergence of the origin/destination measures are also presented. The chapter concludes with a consideration of more general cost functions and the characterisation of optimal transport plans via cyclical monotonicity. • Chapter 2 presents the salient features of ( 2 -)Wasserstein space starting with topological properties of statistical importance, as well as metric properties such as covering numbers. It continues with geometrical features of the space, reviewing the tangent bundle structure of the space, the characterisation of geodesics, and the log and exponential maps as related to transport maps. Finally, it reviews the relationship between the curvature and the so-called compatibility of transport maps, roughly speaking when can one expect optimal transport maps to form a group. • Chapter 3 starts to shift attention to issues more statistical and treats the problem of existence, uniqueness, characterisation, and regularity of Fréchet means (barycenters) for collections of measures in Wasserstein space. This is done by means of the so-called multimarginal transport problem (a.k.a. optimal multitransport or optimal multicoupling problem). The treatment starts with finite collections of measures, and then considers Fréchet means for (potentially uncountably supported) probability distributions on Wasserstein space and associated measurability concerns. • Chapter 4 considers the problem of estimation of the Fréchet mean of a probability distribution in Wasserstein space, on the basis of a finite collection of i.i.d. elements from this law observed with 'sampling noise'. It is shown that this problem is inextricably linked to the problem of separation of amplitude and phase variation (a.k.a. registration) of random point patters, where the focus is on estimating the maps yielding the optimal multicoupling rather than the Fréchet mean itself. Nonparametric methodology for solving either problem is reviewed, coupled with associated asymptotic theory and several illustrative examples. • Chapter 5 focusses on the problem of actually constructing the Fréchet mean and/or optimal multicoupling of a collection of measures, which is a necessary step when using the methods of Chap. 4 in practice. It presents the steepest descent algorithm based on the geometrical features reviewed in Chap. 2 and a convergence analysis thereof. Interestingly, it is seen that the algorithm is closely related to Procrustes algorithms in shape theory, and this connection is discussed in depth. Several special cases are reviewed in more detail.</p>
        <p>In this preliminary chapter, we introduce the problem of optimal transport, which is the main concept behind Wasserstein spaces. General references on this topic are the books by Rachev and Rüschendorf [107], Villani [124,125], Ambrosio et al. [12], Ambrosio and Gigli [10], and Santambrogio [119]. This chapter includes only few proofs, when they are simple, informative, or are not easily found in one of the cited references.</p>
        <p>In 1781, Monge [95] asked the following question: given a pile of sand and a pit of equal volume, how can one optimally transport the sand into the pit? In modern mathematical terms, the problem can be formulated as follows. There is a sand space X , a pit space Y , and a cost function c : X × Y → R that encapsulates how costly it is to move a unit of sand at x ∈ X to a location y ∈ Y in the pit. The sand distribution is represented by a measure μ on X , and the shape of the pit is described by a measure ν on Y . Our decision where to put each unit of sand can be thought of as a function T : X → Y , and it incurs a total transport cost of</p>
        <p>Moreover, one cannot put all the sand at a single point y in the pit; it is not allowed to shrink or expand the sand. The map T must be mass-preserving: for any subset B ⊆</p>
        <p>Y representing a region of the pit of volume ν(B), exactly that same volume of sand must go into B. The amount of sand allocated to B is {x ∈ X : T (x) ∈ B} = T -1 (B), so the mass preservation requirement is that μ(T -1 (B)) = ν(B) for all B ⊆ Y . This condition will be denoted by T #μ = ν and in words: ν is the push-forward of μ under T , or T pushes μ forward to ν. To make the discussion mathematically rigorous, we must assume that c and T are measurable maps, and that μ(T -1 (B)) = ν(B) for all measurable subsets of Y . When the underlying measures are understood from the context, we call T a transport map. Specifying B = Y , we see that no such T can exist unless μ(X ) = ν(Y ); we shall assume that this quantity is finite, and by means of normalisation, that μ and ν are probability measures. In this setting, the Monge problem is to find the optimal transport map, that is, to solve inf</p>
        <p>We assume throughout this book that X and Y are complete and separable metric spaces, 1 endowed with their Borel σ -algebra, which, we recall, is defined as the smallest σ -algebra containing the open sets. Measures defined on the Borel σalgebra of X are called Borel measures. Thus, if μ is a Borel measure on X , then μ(A) is defined for any A that is open, or closed, or a countable union of closed sets, etc., and any continuous map on X is measurable. Similarly, we endow Y with its Borel σ -algebra. The product space X × Y is also complete and separable when endowed with its product topology; its Borel σ -algebra is generated by the product σ -algebra of those of X and Y ; thus, any continuous cost function c : X × Y → R is measurable. It will henceforth always be assumed, without explicit further notice, that μ and ν are Borel measures on X and Y , respectively, and that the cost function is continuous and nonnegative.</p>
        <p>It is quite natural to assume that the cost is an increasing function of the distance between x and y, such as a power function. More precisely, that Y = X is a complete and separable metric space with metric d, and c(x, y) = d p (x, y), p ≥ 0, x, y ∈ X .</p>
        <p>(1.1)</p>
        <p>In particular, c is continuous, hence measurable, if p &gt; 0. The limit case p = 0 yields the discontinuous function c(x, y) = 1{x = y}, which nevertheless remains measurable because the diagonal {(x, x) : x ∈ X } is measurable in X × X . Particular focus will be put on the quadratic case p = 2 (Sect. 1.6) and the linear case p = 1 (Sect. 1.8.2). The problem introduced by Monge [95] is very difficult, mainly because the set of transport maps {T : T #μ = ν} is intractable. And, it may very well be empty: this will be the case if μ is a Dirac measure at some x 0 ∈ X (meaning that μ(A) = 1 if x 0 ∈ A and 0 otherwise) but ν is not. Indeed, in that case the set B = {T (x 0 )} satisfies μ(T -1 (B)) = 1 &gt; ν(B), so no such T can exist. This also shows that the problem is asymmetric in μ and ν: in the Dirac example, there always exists a map T such that T #ν = μ-the constant map T (x) = x 0 for all x is the unique such map. A less 1 But see the bibliographical notes for some literature on more general spaces.</p>
        <p>1.1 The Monge and the Kantorovich Problems 3 extreme situation occurs in the case of absolutely continuous measures. If μ and ν have densities f and g on R d and T is continuously differentiable, then T #μ = ν if and only if for μ-almost all x f (x) = g(T (x))| det ∇T (x)|. This is a highly non-linear equation in T , nowadays known as a particular case of a family of partial differential equations called Monge-Ampère equations. More than two centuries after the work of Monge, Caffarelli [32] cleverly used the theory of Monge-Ampère equations to show smoothness of transport maps (see Sect. 1.6.4).</p>
        <p>As mentioned above, if μ = δ {x 0 } is a Dirac measure and ν is not, then no transport maps from μ to ν can exist, because the mass at x 0 must be sent to a unique point x 0 . In 1942, Kantorovich [77] proposed a relaxation of Monge's problem in which mass can be split. In other words, for each point x ∈ X one constructs a probability measure μ x that describes how the mass at x is split among different destinations. If μ x is a Dirac measure at some y, then all the mass at x is sent to y. The formal mathematical object to represent this idea is a probability measure π on the product space X × Y (which is X 2 in our particular setting). Here π(A × B) is the amount of sand transported from the subset A ⊆ X into the part of the pit represented by B ⊆ Y . The total mass sent from A is π(A × Y ), and the total mass sent into B is π(X × B). Thus, π is mass-preserving if and only if</p>
        <p>Probability measures satisfying (1.2) will be called transference plans, and the set of those will be denoted by Π (μ, ν). We also say that π is a coupling of μ and ν, and that μ and ν are the first and second marginal distributions, or simply marginals, of π. The total cost associated with π ∈ Π (μ, ν) is</p>
        <p>c(x, y) dπ(x, y).</p>
        <p>In our setting of a complete separable metric space X , one can represent π as a collection of probability measures {π x } x∈X on Y , in the sense that for all measurable nonnegative g X ×Y g(x, y) dπ(x, y) = X Y g(x, y) dπ x (y) dμ(x).</p>
        <p>The collection {π x } is that of the conditional distributions, and the iteration of integrals is called disintegration. For proofs of existence of conditional distributions, one can consult Dudley [47,Section 10.2] or Kallenberg [76,Chapter 5]. Conversely, the measure μ and the collection {π x } determine π uniquely by choosing g to be indicator functions. An interpretation of these notions in terms of random variables will be given in Sect. 1.2.</p>
        <p>The Kantorovich problem is to find the best transference plan, that is, to solve inf π∈Π (μ,ν)</p>
        <p>The Kantorovich problem is a relaxation of the Monge problem, because to each transport map T one can associate a transference plan π = π T of the same total cost. To see this, choose the conditional distribution π x to be a Dirac at T (x). Disintegration then yields</p>
        <p>This choice of π satisfies (1.2) because π(A × B) = μ(A ∩ T -foot_1 (B)) and ν(B) = μ(T -1 (B)) for all Borel A ⊆ X and B ⊆ Y .</p>
        <p>Compared to the Monge problem, the relaxed problem has considerable advantages. Firstly, the set of transference plans is never empty: it always contains the product measure μ ⊗ ν defined by [μ ⊗ ν](A) = μ(A)ν(B). Secondly, both the objective function C(π) and the constraints (1.foot_2) are linear in π, so the problem can be seen as infinite-dimensional linear programming. To be precise, we need to endow the space of measures with a linear structure, and this is done in the standard way: define the space M(X ) of all finite signed Borel measures on X . This is a vector space with (μ 1 +αμ 2 )(A) = μ 1 (A)+αμ 2 (A) for α ∈ R, μ 1 , μ 2 ∈ M(X ) and A ⊆ X Borel. The set of probability measures on X is denoted by P(X ), and is a convex subset of M(X ). The set Π (μ, ν) is then a convex subset of P(X ×Y ), and as C(π) is linear in π, the set of minimisers is a convex subset of Π (μ, ν). Thirdly, there is a natural symmetry between Π (μ, ν) and Π (ν, μ). If π belongs to the former and we define π(B × A) = π(A × B), then π ∈ Π (ν, μ). In particular, when X = Y and c = c is symmetric (as in (1.1)), inf π∈Π (μ,ν)</p>
        <p>and π ∈ Π (μ, ν) is optimal if and only if its natural counterpart π is optimal in Π (ν, μ). This symmetry will be fundamental in the definition of the Wasserstein distances in Chap. 2.</p>
        <p>Perhaps most importantly, a minimiser for the Kantorovich problem exists under weak conditions. In order to show this, we first recall some definitions. Let C b (X ) be the space of real-valued, continuous bounded functions on X . A sequence of probability measures {μ n } ∈ M(X ) is said to converge weakly 2 to μ ∈ M(X ) if for all f ∈ C b (X ), f dμ n → f dμ. To avoid confusion with other types of convergence, we will usually write μ n → μ weakly; in the rare cases where a symbol 1.2 Probabilistic Interpretation 5 is needed we shall use the notation μ n w → μ. Of course, if μ n → μ weakly and μ n ∈ P(X ), then μ must be in P(X ) too (this is seen by taking f ≡ 1 and by observing that f dμ ≥ 0 if f ≥ 0).</p>
        <p>A collection of probability measures K is tight if for all ε &gt; 0 there exists a compact set K such that inf μ∈K μ(K) &gt; 1 -ε. If K is represented by a sequence {μ n }, then Prokhorov's theorem (Billingsley [24,Theorem 5.1]) states that a subsequence of {μ n } must converge weakly to some probability measure μ.</p>
        <p>We are now ready to show that the Kantorovich problem admits a solution when c is continuous and nonnegative and X and Y are complete separable metric spaces. Let {π n } be a minimising sequence for C. Then, according to [24,Theorem 1.3], μ and ν must be tight. If K 1 and K 2 are compact with μ(K 1 ), ν(K 2 ) &gt; 1 -ε, then K 1 × K 2 is compact and for all π ∈ Π (μ, ν), π(K 1 × K 2 ) &gt; 1 -2ε. It follows that the entire collection Π (μ, ν) is tight, and by Prokhorov's theorem π n has a weak limit π after extraction of a subsequence. For any integer K, c K (x, y) = min(c(x, y), K) is a continuous bounded function, and C(π n ) = c(x, y) dπ n (x, y) ≥ c K (x, y) dπ n (x, y) → c K (x, y) dπ(x, y), n → ∞.</p>
        <p>By the monotone convergence theorem</p>
        <p>Since {π n } was chosen as a minimising sequence for C, π must be a minimiser, and existence is established.</p>
        <p>As we have seen, the Kantorovich problem is a relaxation of the Monge problem, in the sense that inf</p>
        <p>for some optimal π * . If π * = π T for some transport map T , then we say that the solution is induced from a transport map. This will happen in two different and important cases that are discussed in Sects. 1.3 and 1.6.1.</p>
        <p>A remark about terminology is in order. Many authors talk about the Monge-Kantorovich problem or the optimal transport(ation) problem. More often than not, they refer to what we call here the Kantorovich problem. When one of the scenarios presented in Sects. 1.3 and 1.6.1 is considered, this does not result in ambiguity.</p>
        <p>The preceding section was an analytic presentation of the Monge and the Kantorovich problems. It is illuminating, however, to also recast things in probabilistic terms, and this is the topic of this section.</p>
        <p>A random element on a complete separable metric space (or any topological space) X is simply a measurable function X from some (generic) probability space (Ω , F , P) to X (with its Borel σ -algebra). The probability law (or probability distribution) is the probability measure μ X = X#P defined on the space X ; this is the Borel measure satisfying μ X (A) = P(X ∈ A) for all Borel sets A.</p>
        <p>Suppose that one is given two random elements X and Y taking values in X and Y , respectively, and a cost function c : X × Y → R. The Monge problem is to find a measurable function T such that T (X) has the same distribution as Y , and such that the expectation</p>
        <p>The Kantorovich problem is to find a joint distribution for the pair (X,Y ) whose marginals are the original distributions of X and Y , respectively, and such that the probability law π = (X,Y )#P minimises the expectation</p>
        <p>Any such joint distribution is called a coupling of X and Y . Of course, (X, T (X)) is a coupling when T (X) has the same distribution as Y . The measures π x in the previous section are then interpreted as the conditional distribution of Y given X = x.</p>
        <p>Consider now the important case where X = Y = R d , c(x, y) = xy 2 , and X and Y are square integrable random vectors (E X 2 + E Y 2 &lt; ∞). Let A and B be the covariance matrices of X and Y , respectively, and notice that the covariance matrix of a coupling π must have the form C = A V V t B for a d × d matrix V . The covariance matrix of the difference X -Y is</p>
        <p>Since only V depends on the coupling π, the problem is equivalent to that of maximising the trace of V , the cross-covariance matrix between X and Y . This must be done subject to the constraint that a coupling π with covariance matrix C exists; in particular, C has to be positive semidefinite.</p>
        <p>1. 3 The Discrete Uniform Case 7</p>
        <p>There is a special case in which the Monge-Kantorovich problem reduces to a finite combinatorial problem. Although it may seem at first hand as an oversimplification of the original problem, it is of importance in practice because arbitrary measures can be approximated by discrete measures by means of the strong law of large numbers. Moreover, the discrete case is important in theory as well, as a motivating example for the Kantorovich duality (Sect. 1.4) and the property of cyclical monotonicity (Sect. 1.7). Suppose that μ and ν are each uniform on n distinct points:</p>
        <p>The only relevant costs are c i j = c(x i , y j ), the collection of which can be represented by an n × n matrix C. Transport maps T are associated with permutations in S n , the set of all bijective functions from {1, . . . , n} to itself: given σ ∈ S n , a transport map can be constructed by defining T (x i ) = y σ (i) . If σ is not a permutation, then T will not be a transport map from μ to ν. Transference plans π are equivalent to n × n matrices M with coordinates M i j = π({(x i , y j )}) = M i j ; this is the amount of mass sent from x i to y j . In order for π to a be a transference plan, it must be that ∑ j M i j = 1/n for all i and ∑ i M i j = 1/n for all j, and in addition M must be nonnegative. In other words, the matrix M = nM belongs to B n , the set of bistochastic matrices of order n, defined as n × n matrices M satisfying c i j M i j = inf</p>
        <p>If σ is a permutation, then one can define M = M(σ ) by M i j = 1/n if j = σ (i) and 0 otherwise. Then M ∈ B n /n and C(M) = C(σ ). Such M (or, more precisely, nM) is called a permutation matrix.</p>
        <p>The Kantorovich problem is a linear program with n 2 variables and 2n constraints. It must have a solution because B n (hence B n /n) is a compact (nonempty) set in R n 2 and the objective function is linear in the matrix elements, hence continuous. (This property is independent of the possibly infinite-dimensional spaces X and Y in which the points lie.) The Monge problem also admits a solution because S n is a finite set. To see that the two problems are essentially the same, we need to introduce the following notion. If B is a convex set, then x ∈ B is an extremal point of B if it cannot be written as a convex combination tz + (1t)y for some distinct points y, z ∈ B. It is well known (Luenberger and Ye [89,Section 2.5]) that there exists an optimal solution that is extremal, so that it becomes relevant to identify the extremal points of B n . It is fairly clear that each permutation matrix is extremal in B n ; the less obvious converse is known as Birkhoff's theorem, a proof of which can be found, for instance, at the end of the introduction in Villani [124] or (in a different terminology) in Luenberger and Ye [89,Section 6.5]. Thus, we have:</p>
        <p>There exists σ ∈ S n such that M(σ ) minimises C(M) over B n /n. Furthermore, if {σ 1 , . . . , σ k } is the set of optimal permutations, then the set of optimal matrices is the convex hull of {M(σ 1 ),..., M(σ k )}. In particular, if σ is the unique optimal permutation, then M(σ ) is the unique optimal matrix. Thus, in the discrete case, the Monge and the Kantorovich problems coincide. One can of course use the simplex method [89,Chapter 3] to solve the linear program, but there are n! vertices, and there is in principle no guarantee that the simplex method solves the problem efficiently. However, the constraints matrix has a very specific form (it contains only zeroes and ones, and is totally unimodular), so specialised algorithms for this problem exist. One of them is the Hungarian algorithm of Kuhn [85] or its variant of Munkres [96] that has a worst-case computational complexity of at most O(n 4 ). Another alternative is the class of net flow algorithms described in [89,Chapter 6]. In particular, the algorithm of Edmonds and Karp [50] has a complexity of at most O(n 3 log n). This monograph does not focus on computational aspects for optimal transport. This is a fascinating and very active area of contemporary research, and readers are directed to Peyré and Cuturi [103].</p>
        <p>The special case described here could have been more precisely called "the discrete uniform case on the same number of points", as "the discrete case" could refer to any two finitely supported measures μ and ν. In the Monge context, the setup discussed here is the most interesting case, see page 8 in the supplement for more details.</p>
        <p>The discrete case of Sect. 1.3 is an example of a linear program and thus enjoys a rich duality theory (Luenberger and Ye [89,Chapter 4]). The general Kantorovich problem is an infinite-dimensional linear program, and under mild assumptions admits similar duality.</p>
        <p>We can represent any matrix M as a vector in R n 2 , say M, by enumeration of the elements row by row. If nM is bistochastic, i.e., M ∈ B n /n, then the 2n constraints can be represented in a (2n) × n 2 matrix A. For instance, if n = 3, then</p>
        <p>For general n, the constraints read AM = n -1 (1, . . . , 1) ∈ R 2n and A takes the form</p>
        <p>with I n the n × n identity matrix. Thus, the problem can be written min</p>
        <p>The last constraint is to be interpreted coordinate-wise; all the elements of M must be nonnegative. The dual problem is constructed by introducing one variable for each row of A, transposing the constraint matrix and interchanging the roles of the objective vector C and the constraints vector b = n -1 (1,...,1). Call the new variables p 1 ,..., p n and q 1 ,...,q n , and notice that each column of A corresponds to exactly one p i and one q j , and that the n 2 columns exhaust all possibilities. Hence, the dual problem is</p>
        <p>(1.4) In the context of duality, one uses the terminology primal problem for the original optimisation problem. Weak duality states that if M and (p, q) satisfy the respective constraints, then</p>
        <p>In particular, if equality holds, then M is primal optimal and (p, q) is dual optimal.</p>
        <p>Strong duality is the nontrivial assertion that there exist M * and (p * , q * ) satisfying</p>
        <p>The vectors C and M were obtained from the cost function c and the transference plan π as C i j = c(x i , y j ) and M i j = π({(x i , y j )}). Similarly, we can view the vectors p and q as restrictions of functions ϕ : X → R and ψ : Y → R of the form p i = ϕ(x i ) and q j = ψ(y j ). The constraint vector b = (1 n , 1 n ) can be written as b i = μ({x i }) and b n+ j = ν({y j }). In this formulation, the constraint p i + q j ≤ c i j writes (ϕ, ψ) ∈ Φ c with</p>
        <p>and the dual problem (1.4) becomes sup</p>
        <p>Simple measure theory shows that the set constraints (1.</p>
        <p>The proof follows from the fact that (1.2) yields the above equality when ϕ and ψ are indicator functions. One then uses linearity and approximations to deduce the result.</p>
        <p>Weak duality follows immediately from Lemma 1.4.1.</p>
        <p>Strong duality can be stated in the following form: Theorem 1.4.2 (Kantorovich Duality) Let μ and ν be probability measures on complete separable metric spaces X and Y , respectively, and let c : X × Y → R + be a measurable function. Then</p>
        <p>See the Bibliographical Notes for other versions of the duality. When the cost function is continuous, or more generally, a countable supremum of continuous functions, the infimum is attained (see (1.3)). The existence of maximisers (ϕ, ψ) is more delicate and requires a finiteness condition, as formulated in Proposition 1.8.1 below.</p>
        <p>The next sections are dedicated to more concrete examples that will be used through the rest of the book.</p>
        <p>When X = Y = R, the Monge-Kantorovich problem has a particularly simple structure, because the class of "nice" transport maps contains at most a single element. Identify μ, ν ∈ P(R) with their cumulative distribution functions F and G defined by</p>
        <p>Let the cost function be (momentarily) quadratic: c(x, y) = |x -y| 2 /2. Since for</p>
        <p>it seems natural to expect the optimal transport map to be monotonically increasing.</p>
        <p>It turns out that, on the real line, there is at most one such transport map: if T is increasing and</p>
        <p>If t = T (x), then the above equation reduces to T (x) = G -1 (F(x)). This formula determines T uniquely, and has an interesting probabilistic interpretation: it is wellknown that if X is a random variable with continuous distribution function F, then F(X) follows a uniform distribution on (0, 1). Conversely, if U follows a uniform distribution, G is any distribution function, and</p>
        <p>is the quantile function of X, then the random variable G -1 (U) has distribution function G. We say that G -1 is the left-continuous inverse of G. In terms of push-forward maps, we can write Using the change of variables formula, we see that the total cost of T is</p>
        <p>If F is discontinuous, then F#μ is not Lebesgue measure, and T is not necessarily defined. But there will exist an optimal transference plan π ∈ Π (μ, ν) that is monotone in the following sense: there exists a set Γ ⊂ R 2 such that π(Γ ) = 1 and whenever (x i , y i ) ∈ Γ ,</p>
        <p>Thus, mass at x 1 and x 2 can be split if need be, but in a monotone way. For example, if μ puts mass 1/2 at x 1 = -1 and at x 2 = 1 and ν is uniform on [-1, 1]. Then the transference plan spreads the mass of x 1 uniformly on [-1, 0], and the mass of x 2 uniformly on [0, 1]. This is a particular case of the cyclical monotonicity that will be discussed in Sect. 1.7. Elementary calculations show that the inequality</p>
        <p>holds more generally than the quadratic cost c(x, y) = |x-y| 2 . Specifically, it suffices that c(x, y) = h(|x -y|) with h convex on R + . Since any distribution can be approximated by continuous distributions, in view of the above discussion, the following result from Villani [124,Theorem 2.18] should not be too surprising.</p>
        <p>If the infimum is finite and h is strictly convex, then the optimal transference plan is unique. Furthermore, if F is continuous, then the infimum is attained by the trans-</p>
        <p>The prototypical choice for h is h(z) = |z| p with p &gt; 1. This result allows in particular a direct evaluation of the Wasserstein distances for measures on the real line (see Chap. 2).</p>
        <p>Note that no regularity is needed in order that the optimal transference plan be unique, unlike in higher dimensions (compare Theorem 1.8.2). The structure of solutions in the concave case (0 &lt; p &lt; 1) is more complicated, see McCann [94]. When p = 1, the cost function is convex but not strictly so, and solutions will not be unique. However, the total cost in Theorem 1.5.1 admits another representation that is often more convenient. Proposition 1.5.2 (Quantiles and Distribution Functions) If F and G are distribution functions, then</p>
        <p>The proof is a simple application of Fubini's theorem; see page 13 in the supplement.</p>
        <p>This section is devoted to the specific cost function</p>
        <p>where X is a separable Hilbert space. This cost is popular in applications, and leads to a lucid and elegant theory. The factor of 1/2 does not affect the minimising coupling π and leads to cleaner expressions. (It does affect the optimal dual pair, but in an obvious way.)</p>
        <p>We begin with the Euclidean case, where</p>
        <p>) is endowed with the Euclidean metric, and use the Kantorovich duality to obtain characterisations of optimal maps. Since the dual objective function to be maximised</p>
        <p>is increasing in ϕ and ψ, one should seek functions that take values as large as possible subject to the constraint ϕ(x)+ψ(y) ≤ xy 2 /2. Suppose that an oracle tells us that some ϕ ∈ L 1 (μ) is a good candidate. Then the largest possible ψ satisfying</p>
        <p>In other words,</p>
        <p>As a supremum over affine functions (in y), ψ enjoys some useful properties. We remind the reader that a function f :</p>
        <p>Affine functions are convex and lower semicontinuous, and it straightforward from the definitions that both convexity and lower semicontinuity are preserved under the supremum operation. Thus, the function ψ is convex and lower semicontinuous. In particular, it is Borel measurable due to the following characterisation: f is lower semicontinuous if and only if {x : f (x) ≤ α} is a closed set for all α ∈ R.</p>
        <p>From the preceding subsection, we now know that optimal dual functions ϕ and ψ must take the form of the difference between • 2 /2 and a convex function. Given the vast wealth of knowledge on convex functions (Rockafellar [113]), it will be convenient to work with ϕ and ψ, and to assume that ψ = ( ϕ) * , where</p>
        <p>is the Legendre transform of f ([113, Chapter 26]; [124,Chapter 2]), and is of fundamental importance in convex analysis. Now by symmetry, one can also replace ϕ by ( ψ) * = ( ϕ) * * , so it is reasonable to expect that an optimal dual pair should take the form</p>
        <p>, with ϕ convex and lower semicontinuous.</p>
        <p>The alternative representation of the dual objective value as</p>
        <p>that μ and ν have finite second moments. This condition also guarantees that an optimal ϕ exists, as the conditions of Proposition 1.8.1 are satisfied. An alternative direct proof for the quadratic case can be found in Villani [124,Theorem 2.9]. Suppose that an optimal ϕ is found. What can we say about optimal transference plans π? According to the duality, a necessary and sufficient condition is that</p>
        <p>Since we have ϕ(x) + ( ϕ) * (y) ≥ x, y everywhere, the integrand is nonnegative. Hence, the integral vanishes if and only if π is concentrated on the set of (x, y) such that ϕ(x) + ϕ * (y) = x, y . By definition of the Legendre transform as a supremum, this happens if and only if the supremum defining ϕ * (y) is attained at x; equivalently</p>
        <p>This condition is precisely the definition of y being a subgradient of ϕ at x [113, Chapter 23]. When ϕ is differentiable at x, its unique subgradient is the gradient y = ∇ ϕ(x) [113,Theorem 25.1]. If we are fortunate and ϕ is differentiable everywhere, or even μ-almost everywhere, then the optimal transference plan π is unique, and in fact induced from the transport map ∇ ϕ. The problem, of course, is that ϕ may fail to be differentiable μ-almost surely. This is remedied by assuming some regularity on the source measure μ in order to make sure that any convex function be differentiable μ-almost</p>
        <p>Another issue that might arise is that optimal ϕ's might not exist. This is easily dealt with using Proposition 1.8.1. If we assume that μ and ν have finite second moments:</p>
        <p>then any transference plan π ∈ Π (μ, ν) has a finite cost, as is seen from integrating the elementary inequality xy 2 ≤ 2 x 2 + 2 y 2 and using Lemma 1.4.1:</p>
        <p>With these tools, we can now prove a fundamental existence and uniqueness result for the Monge-Kantorovich problem. It has been proven independently by several authors, including Brenier [31], Cuesta-Albertos and Matrán [37], Knott and Smith [83], and Rachev and Rüschendorf [117].</p>
        <p>Theorem 1.6.2 (Quadratic Cost in Euclidean Spaces) Let μ and ν be probability measures on R d with finite second moments, and suppose that μ is absolutely continuous with respect to Lebesgue measure. Then the solution to the Kantorovich problem is unique, and is induced from a transport map T that equals μ-almost surely the gradient of a convex function φ . Furthermore, the pair</p>
        <p>is optimal for the dual problem.</p>
        <p>Proof. To alleviate the notation we write φ instead of ϕ. By Proposition 1.8.1, there exists an optimal dual pair (ϕ, ψ) such that φ (x) = x 2 /2 -ϕ(x) is convex and lower semicontinuous, and by the discussion in Sect. 1.1, there exists an optimal π. Since φ is μ-integrable, it must be finite almost everywhere, i.e., μ(domφ ) = 1. By Theorem 1.6.1, if we define N as the set of nondifferentiability points of φ , then Leb(N ∩ domφ ) = 0; as μ is absolutely continuous, the same holds for μ. (Here Leb denotes Lebesgue measure.)</p>
        <p>We conclude that μ(int(domφ</p>
        <p>In other words, φ is differentiable μalmost everywhere, and so for μ-almost any x, there exists a unique y such that φ (x) + φ * (y) = x, y , and y = ∇φ (x). This shows that π is unique and induced from the transport map ∇φ (x). The gradient ∇φ is Borel measurable, since each of its coordinates can be written as lim sup q→0,q∈Q q -1 (φ (x + qv) -φ (x)) for some vector v (the canonical basis of R d ), which is Borel measurable because the limit superior is taken on countably many functions (and φ is measurable because it is lower semicontinuous).</p>
        <p>The finite-dimensionality of R d in the previous subsection was only used in order to apply Theorem 1.6.1, so one could hope to extend the results to infinite-dimensional separable Hilbert spaces.</p>
        <p>Although there is no obvious parallel for Lebesgue measure (i.e., translation invariant) on infinite-dimensional Banach spaces, one can still define absolute continuity via Gaussian measures. Indeed, μ ∈ P(R d ) is absolutely continuous with respect to Lebesgue measure if and only if the following holds: if N ⊂ R d is such that ν(N ) = 0 for any nondegenerate Gaussian measure ν, then μ(N ) = 0. This definition can be extended to any separable Banach space X via projections, as follows. Let X * be the (topological) dual of X , consisting of all real-valued, continuous linear functionals on X . Definition 1.6.3 (Gaussian Measures) A probability measure μ ∈ P(X ) is a nondegenerate Gaussian measure if for any ∈ X * \ {0}, #μ ∈ P(R) is a Gaussian measure with positive variance.</p>
        <p>Apart from the one-dimensional case of Sect. 1.5, there is another special case in which there is a unique and explicit solution to the Monge-Kantorovich problem.</p>
        <p>Suppose that μ and ν are Gaussian measures on R d with zero means and nonsingular covariance matrices A and B. By Theorem 1.6.2, we know that there exists a unique optimal map T such that T #μ = ν. Since linear push-forwards of Gaussians are Gaussian, it seems natural to guess that T should be linear, and this is indeed the case.</p>
        <p>Since T is a linear map that should be the gradient of a convex function φ , it must be that φ is quadratic, i.e., φ (x) -φ (0) = x, Qx for x ∈ R d and some matrix Q. The gradient of φ at x is (Q + Q t )x and the Hessian matrix is Q + Q t . Thus, T = Q + Q t and since φ is convex, T must be positive semidefinite.</p>
        <p>Viewing T as a matrix leads to the Riccati equation TAT = B (since T is symmetric). This is a quadratic equation in T , and so we wish to take square roots in a way that would isolate T . This is done by multiplying the equation from both sides with A 1/2 :</p>
        <p>All matrices in brackets are positive semidefinite. By taking square roots and multiplying with A -1/2 , we finally find</p>
        <p>A straightforward calculation shows that TAT = B indeed, and T is positive definite, hence optimal. To calculate the transport cost C(T ), observe that (T -I)#μ is a centred Gaussian measure with covariance matrix</p>
        <p>If Y ∼ N (0,C), then E Y 2 equals the trace of C, denoted trC. Hence, by properties of the trace,</p>
        <p>(1.6)</p>
        <p>By continuity arguments, (1.6) is the total transport cost between any two Gaussian distributions with zero means, even if A is singular.</p>
        <p>If AB = BA, the above formulae simplify to</p>
        <p>with F the Frobenius norm.</p>
        <p>If the means of μ and ν are m and n, one simply needs to translate the measures. The optimal map and the total cost are then</p>
        <p>From this, we can deduce a lower bound on the total cost between any two measures in R d in terms of their second order structure. This is worth mentioning, because such lower bounds are not very common (the Monge-Kantorovich problem is defined by an infimum, and thus typically easier to bound from above). Proposition 1.6.5 (Lower Bound for Quadratic Cost) Let μ, ν ∈ P(R d ) have means m and n and covariance matrices A and B and let π be the optimal map. Then</p>
        <p>Proof. It will be convenient here to use the probabilistic terminology of Sect. 1.2. Let X and Y be random variables with distributions μ and ν. Any coupling of X and Y will have covariance matrix of the form C = A V V t B ∈ R 2d×2d for some matrix V ∈ R d×d , constrained so that C is positive semidefinite. This gives the lower bound inf</p>
        <p>As we know from the Gaussian case, the last infimum is given by (1.6).</p>
        <p>The optimal transport map T between Gaussian measures on R d is linear, so it is of course very smooth (analytic). The densities of Gaussian measures are analytic too, so that T inherits the regularity of μ and ν. Using the formula for T , one can show that a similar phenomenon takes place in the one-dimensional case. Though we do not have a formula for T at our disposal when μ and ν are general absolutely continuous measures on R d , d ≥ 2, it turns out that even in that case, T inherits the regularity of μ and ν if some convexity conditions are satisfied.</p>
        <p>To guess what kind of results can be hoped for, let us first examine the case d = 1. Let F and G denote the distribution functions of μ and ν, respectively. Suppose that G is continuously differentiable and that G &gt; 0 on some open interval (finite or not) I such that ν(I) = 1. Then the inverse function theorem says that G -1 is also continuously differentiable. Recall that the support of a (Borel) probability measure μ (denoted suppμ) is the smallest closed set K such that μ(K) = 1. A simple application of the chain rule (see page 19 in the supplement) gives: Theorem 1.6.6 (Regularity in R) Let μ, ν ∈ P(R) possess distribution functions F and G of class C k , k ≥ 1. Suppose further that suppν is an interval I (possibly unbounded) and that G &gt; 0 on the interior of I. Then the optimal map is of class C k as well. If F, G ∈ C 0 are merely continuous, then so is the optimal map.</p>
        <p>The assumption on the support of ν is important: if μ is Lebesgue measure on [0, 1] and the support of ν is disconnected, then T cannot even be continuous, no matter how smooth ν is.</p>
        <p>The argument above cannot be easily extended to measures on R d , d ≥ 2, because there is no explicit formula available for the optimal maps. As before, we cannot expect the optimal map to be continuous if the support of ν is disconnected. It turns out that the condition on the support of ν is not connectedness, but rather convexity. This was shown by Caffarelli, who was able to prove ( [32] and the references within) that the optimal maps have the same smoothness as the measures. To state the result, we recall the following notation for an open Ω ⊆ R d , k ≥ 0 and α ∈ (0, 1]. We say that f ∈ C k,α (Ω ) if all the partial derivatives of order k of f are locally α-Hölder on Ω . For example, if k = 1, this means that for any x ∈ Ω there exists a constant L and an open ball B containing x such that</p>
        <p>Let φ be such that ∇φ #μ = ν.</p>
        <p>1. If Ω 1 and Ω 2 are bounded and f , g are bounded below, then φ is strictly convex and of class C 1,α (Ω 1 ) for some α &gt; 0.</p>
        <p>In other words, the optimal map T = ∇φ ∈ C k+1,α (Ω 1 ) is one derivative smoother than the densities, so has the same smoothness as the measures μ, ν.</p>
        <p>Theorem 1.6.7 will be used in two ways in this book. Firstly, it is used to derive criteria for a Karcher mean of a collection of measures to be the Fréchet mean of that collection (Theorem 3.1.15). Secondly, it allows one to obtain very smooth estimates for the transport maps. Indeed, any two measures μ and ν can be approximated by measures satisfying the second condition: one can approximate them by discrete measures using the law of large numbers and then employ a convolution with, e.g., a Gaussian measure (see, for instance, Theorem 2.2.7). It is not obvious that the transport maps between the approximations converge to the transport maps between the original measures, but we will see this to be true in the next section.</p>
        <p>In this section, we discuss the behaviour of the solution to the Monge-Kantorovich problem when the measures μ and ν are replaced by approximations μ n and ν n . Since any measure can be approximated by discrete measures or by smooth measures, this allows us to benefit from both worlds. On the one hand, approximating μ and ν with discrete measures leads to the finite discrete problem of Sect. 1.3 that can be solved exactly. On the other hand, approximating μ and ν with Gaussian convolutions thereof leads to very smooth measures (at least on R d ), and so the regularity results of the previous section imply that the respective optimal maps will also be smooth. Finally, in applications, one would almost always observe the measures of interest μ and ν with a certain amount of noise, and it is therefore of interest to control the error introduced by the noise. In image analysis, μ can represent an image that has undergone blurring, or some other perturbation (Amit et al. [13]). In other applications, the noise could be due to sampling variation, where instead of μ one observes a discrete measure μ N obtained from realisations X 1 , . . . , X N of random elements with distribution μ as μ N = N -1 ∑ N i=1 δ {X i } (see Chap. 4). In Sect. 1.7.1, we will see that the optimal transference plan π depends continuously on μ and ν. With this result under one's belt, one can then deduce an analogous property for the optimal map T from μ to ν given some regularity of μ, as will be seen in Sect. 1.7.2.</p>
        <p>We shall assume throughout this section that μ n → μ and ν n → ν weakly, which, we recall, means that X f dμ n → X f dμ for all continuous bounded f : X → R. The following equivalent definitions for weak convergence will be used not only in this section, but elsewhere as well.</p>
        <p>Lemma 1.7.1 (Portmanteau) Let X be a complete separable metric space and let μ, μ n ∈ P(X ). Then the following are equivalent:</p>
        <p>In this subsection, we state and sketch the proof of the fact that if μ n → μ and ν n → ν weakly, then the optimal transference plans π n ∈ Π (μ n , ν n ) converge to an optimal π ∈ Π (μ, ν). The result, as stated in Villani [125,Theorem 5.20], is valid on complete separate metric spaces with general cost functions, and reads as follows.</p>
        <p>Theorem 1.7.2 (Weak Convergence and Optimal Plans) Let μ n and ν n converge weakly to μ and ν, respectively, in P(X ) and let c :</p>
        <p>are optimal transference plans and</p>
        <p>then (π n ) is a tight sequence and each of its weak limits π ∈ Π (μ, ν) is optimal.</p>
        <p>One can even let c vary with n under some conditions. Let c(x, y) = xy 2 /2. We prefer to keep the notation c(•, •) in order to stress the generality of the arguments. A key idea in the proof is the replacement of optimality by another property called cyclical monotonicity, which behaves nicely with respect to weak convergence. To motivate this property, we recall the discrete case of Sect. 1.3 where μ = N -1 ∑ N i=1 δ {x i } and ν = N -1 ∑ N i=1 δ {y i }. There exists an optimal transference plan π induced from a permutation σ 0 ∈ S N . Since the ordering of {x i } and {y i } is irrelevant in the representations of μ and ν, we may assume without loss of generality that σ 0 is the identity permutation. Then, by definition of optimality,</p>
        <p>If σ is the identity except for a subset i 1 ,...,i n , n ≤ N, then in particular</p>
        <p>and if we choose σ</p>
        <p>By decomposing a permutation σ ∈ S N to disjoint cycles, one can verify that (1.8) implies (1.7). This will be useful since, as it turns out, a variant of (1.8) holds for arbitrary measures μ and ν for which there is no relevant finite N as in (1.7).</p>
        <p>A set Γ ⊆ X 2 is cyclically monotone if for any n and any (x 1 , y 1 ),...,(x n ,</p>
        <p>), (y 0 = y n ).</p>
        <p>(1.9)</p>
        <p>A probability measure π on X 2 is cyclically monotone if there exists a monotone Borel set Γ such that π(Γ ) = 1.</p>
        <p>The relevance of cyclical monotonicity becomes clear from the following observation. If μ and ν are discrete uniform measures on N points and σ is an optimal permutation for the Monge-Kantorovich problem, then the coupling π = (1/N) ∑ N i=1 δ {(x i , y σ (i) )} is cyclically monotone. In fact, even if the optimal permutation is not unique, the set</p>
        <p>is cyclically monotone. Furthermore, π ∈ Π (μ, ν) is optimal if and only if it is cyclically monotone, if and only if π(Γ ) = 1. It is heuristically easy to see that cyclical monotonicity is a necessary condition for optimality: Proposition 1.7.4 (Optimal Plans Are Cyclically Monotone) Let μ, ν ∈ P(X ) and suppose that the cost function c is nonnegative and continuous. Assume that the optimal π ∈ Π (μ, ν) has a finite total cost. Then suppπ is cyclically monotone. In particular, π is cyclically monotone.</p>
        <p>The idea of the proof is that if for some (x 1 , y 1 ),...,(x n , y n ) in the support of π,</p>
        <p>then by continuity of c, the same inequality holds on some balls of positive measure. One can then replace π by a measure having (x i , y i-1 ) rather than (x i , y i ) in its support, and this measure will incur a lower cost than π. A rigorous proof can be found in Gangbo and McCann [59,Theorem 2.3].</p>
        <p>Thus, optimal transference plans π solve infinitely many discrete Monge-Kantorovich problems emanating from their support. More precisely, for any finite collection (x i , y i ) ∈ suppπ, i = 1,...,N and any permutation σ ∈ S N , (1.7) is satisfied. Therefore, the identity permutation is optimal between the measures (1/N) ∑ δ {x i } and (1/N) ∑ δ {y j }.</p>
        <p>In the same spirit as Γ defined above for the discrete case, one can strengthen Proposition 1.7.4 and prove existence of a cyclically monotone set Γ that includes the support of any optimal transference plan π: take Γ = ∪supp(π) for π optimal.</p>
        <p>The converse of Proposition 1.7.4 also holds.</p>
        <p>Proposition 1.7.5 (Cyclically Monotone Plans Are Optimal) Let μ, ν ∈ P(X ), c : X 2 → R + continuous and π ∈ Π (μ, ν) a cyclically monotone measure with C(π) finite. Then π is optimal in Π (μ, ν).</p>
        <p>Let us sketch the proof in the quadratic case c(x, y) = xy 2 /2 and see how convexity comes into play. Straightforward algebra shows that (1.9) is equivalent, in the quadratic case, to n ∑ i=1 y i , x i+1x i ≤ 0, (x n+1 = x 1 ).</p>
        <p>(1.10) Fix (x 0 , y 0 ) ∈ Γ = suppπ and define φ :</p>
        <p>This function is defined as a supremum of affine functions, and is therefore convex and lower semicontinuous. Cyclical monotonicity of Γ implies that φ (x 0 ) = 0, so φ is not identically infinite (it would have been so if Γ were not cyclically monotone). Straightforward computations show that Γ is included in the subdifferential of φ : y is a subgradient of φ at x when (x, y) ∈ Γ . Optimality of π then follows by weak duality, since π assigns full measure to the set of (x, y) such that φ (x) + φ * (y) = x, y ; see (1.5) and the discussion around it.</p>
        <p>The argument for more general costs follows similar lines and is sketched at the end of this subsection.</p>
        <p>Given these intermediary results, it is now instructive to prove Theorem 1.7.2.</p>
        <p>Proof (Proof of Theorem 1.7.2). Since μ n → μ weakly, it is a tight sequence, and similarly for ν n . Consequently, the entire set of plans ∪ n Π (μ n , ν n ) is tight too (see the discussion before deriving (1.3)). Therefore, up to a subsequence, (π n ) has a weak limit π. We need to show that π is cyclically monotone and that C(π) is finite. The latter is easy, since c M (x, y) = min(M, c(x, y)) is continuous and bounded:</p>
        <p>To show that π is cyclically monotone, fix (x 1 , y 1 ),..., (x N , y N ) ∈ suppπ. We show that there exist (x n k , y n k ) ∈ suppπ n that converge to (x k , y k ). Once this is established, we conclude from the cyclical monotonicity of suppπ n and the continuity of c that</p>
        <p>The existence proof for the sequence is standard. For ε &gt; 0, let B = B ε (x k , y k ) be an open ball around (x k , y k ). Then π(B) &gt; 0 and by the portmanteau Lemma 1.7.1, π n (B) &gt; 0 for sufficiently large n. It follows that there exist (x n k , y n k ) ∈ B ∩ suppπ n . Let ε = 1/m, say, then for all n ≥ N m we can find (x n k , y n k ) ∈ suppμ n of distance 2/m from (x k , y k ). We can choose N m+1 &gt; N m without loss of generality in order to complete the proof.</p>
        <p>A few remarks are in order. Firstly, quadratic cyclically monotone sets (with respect to xy 2 /2) are included in the subdifferential of convex functions. The converse is also true, as can be easily deduced from summing up the subgradient inequalities</p>
        <p>where y i is a subgradient of φ at x i . For future reference, we state this characterisation as a theorem (which is valid in infinite dimensions too).</p>
        <p>Theorem 1.7.6 (Rockafellar [112]) A nonempty Γ ⊆ X 2 is quadratic cyclically monotone if and only if it is included in the graph of the subdifferential of a lower semicontinuous convex function that is not identically infinite.</p>
        <p>Secondly, we have not used at all the Kantorovich duality, merely its weak form.</p>
        <p>The machinery of cyclical monotonicity can be used in order to prove the duality Theorem 1.4.2. This is indeed the strategy of Villani [125,Chapter 5], who explains its advantage with respect to Hahn-Banach-type duality proofs. Lastly, the idea of the proof of Proposition 1.7.5 generalises to other costs in a natural way. Given a cyclically monotone (with respect to a cost function c) set Γ and a fixed pair (x 0 , y 0 ) ∈ Γ , define (Rüschendorf [116])</p>
        <p>Then under some conditions, (ϕ, ψ) is dual optimal for some ψ. As explained in Sect. 1.8, ψ can be chosen to be essentially ϕ c (as defined in that section).</p>
        <p>We now extend the weak convergence of π n to π of the previous subsection to convergence of optimal maps. Because of the applications we have in mind, we shall work exclusively in the Euclidean space X = R d with the quadratic cost function; our results can most likely be extended to more general situations.</p>
        <p>In this setting, we know that optimal plans are supported on graphs of subdifferentials of convex functions. Suppose that π n is induced by T n and π is induced by T . Then in some sense, the weak convergence of π n to π yields convergence of the graphs of T n to the graph of T . Our goal is to strengthen this to uniform convergence of T n to T . Roughly speaking, we show the following: there exists a set A with μ(A) = 1 and such that T n converge uniformly to T on every compact subset of A. For the reader's convenience, we give a user-friendly version here; a more general statement is given in Proposition 1.7.11 below.</p>
        <p>This property (which is weaker than cyclical monotonicity) is important enough to have its own name. Following the notation of Alberti and Ambrosio [6], we call a set-valued function (or multifunction) u :</p>
        <p>If d = 1, this simply means that u is a nondecreasing (set-valued) function. For example, one can define</p>
        <p>Next, u is said to be maximally monotone if no points can be added to its graph while preserving monotonicity:</p>
        <p>=⇒ y ∈ u(x ).</p>
        <p>It will be convenient to identify u with its graph; we will often write (x, y) ∈ u to mean y ∈ u(x). Note that u(x) can be empty, even when u is maximally monotone.</p>
        <p>The previous example for u is not maximally monotone, but it will be if we modify u(0) to be (-∞, 0] and u(1) to be [0, ∞).</p>
        <p>Of course, if φ : R d → R ∪ {∞} is convex, then u = ∂ φ is monotone. It follows from Theorem 1.7.6 that u is maximally cyclically monotone (no points can be added to its graph while preserving cyclical monotonicity). It can actually be shown that u is maximally monotone [6,Section 7]. In what follows, we will always work with subdifferentials of convex functions, so unless stated otherwise, u will always be assumed maximally monotone.</p>
        <p>Maximally monotone functions enjoy the following very useful continuity property. It is proven in [6, Corollary 1.3] and will be used extensively below. Proposition 1.7.8 (Continuity at Singletons) Let x ∈ R d such that u(x) = {y} is a singleton. Then u is nonempty on some neighbourhood of x and it is continuous at x: if x n → x and y n ∈ u(x n ), then y n → y.</p>
        <p>Notice that this result implies that if a convex function φ is differentiable on some open set E ⊆ R d , then it is continuously differentiable there (Rockafellar [113,Corollary 25.5.1]).</p>
        <p>If f : R d → R ∪ {∞} is any function, one can define its subgradient at x locally as</p>
        <p>(See the discussion after Theorem 1.8.2.) When f is convex, one can remove the o( zx ) term and the inequality holds for all z, i.e., globally and not locally. Since monotonicity is related to convexity, it should not be surprising that monotonicity is in some sense a local property. Suppose that u(x 0 ) = {y 0 } is a singleton and that for some y * ∈ R d , yy * , xx 0 ≥ 0 for all x ∈ R d and y ∈ u(x). Then by maximality, y * must equal y 0 . By "local property", we mean that the conclusion y * = y 0 holds if the above inequality holds for x in a small neighbourhood of x 0 (an open set that includes x 0 ). We will need a more general version of this result, replacing neighbourhoods by a weaker condition that can be related to Lebesgue points. The strengthening is somewhat technical; the reader can skip directly to Lemma 1.7.10 and assume that G is open without losing much intuition. Let B r (x 0 ) = {x : x-x 0 &lt; r} for r ≥ 0 and x 0 ∈ R d . The interior of a set G ⊆ R d is denoted by intG and the closure by G. If G is measurable, then LebG denotes the Lebesgue measure of G. Finally, convG denotes the convex hull of G.</p>
        <p>A point x 0 is a Lebesgue point (or of Lebesgue density) of a measurable set G ⊆ R d if for any ε &gt; 0 there exists t ε &gt; 0 such that</p>
        <p>An illuminating example is the set {y ≤ |x|} in R 2 (see Fig. 1.1). Since the "slope" of the square root is infinite, x 0 = (0, 0) is a Lebesgue point, but the fraction above is strictly smaller than one, for all t &gt; 0. 5] show that Leb(G \ G den ) = 0 (and Leb(G den \ G) = 0, so G den is very close to G). By the Hahn-Banach theorem, G den ⊆ int(conv(G)): indeed, if x is not in int(convG), then there is a separating hyperplane between x and convG ⊇ G, so the fraction above is at most 1/2 for all t &gt; 0.</p>
        <p>The "denseness" of Lebesgue points is materialised in the following result. It is given as exercise in [121] when d = 1, and the proof can be found on page 27 in the supplement.</p>
        <p>Lemma 1.7.9 (Density Points and Distance) Let x 0 be a point of Lebesgue density of a measurable set G ⊆ R d . Then</p>
        <p>Of course, this result holds for any x 0 ∈ G if the little o is replaced by big O, since δ is Lipschitz. When x 0 ∈ intG, this is trivial because δ vanishes on intG.</p>
        <p>The important part here is the following corollary: for almost all x ∈ G, δ (z) = o( zx ) as z → x. This can be seen in other ways: since δ is Lipschitz, it is differentiable almost everywhere. If x ∈ G and δ is differentiable at x, then ∇δ (x) must be 0 (because δ is minimised there), and then δ (z) = o( zx ). We just showed that δ is differentiable with vanishing derivative at all Lebesgue points of x. The converse is not true: G = {±1/n} ∞ n=1 has no Lebesgue points, but δ (y) ≤ 4y 2 as y → 0.</p>
        <p>The locality of monotone functions can now be stated as follows. It is proven on page 27 of the supplement. These continuity properties cannot be of much use unless u(x) is a singleton for reasonably many values of x. Fortunately, this is indeed the case: the set of points x such that u(x) contains more than one element has Lebesgue measure 0 (see Alberti and Ambrosio [6, Remark 2.3] for a stronger result). Another issue is that u may be empty, and convexity comes into play here again. Let domu = {x : u(x) = / 0}. Then there exists a convex closed set K such that intK ⊆ domu ⊆ K. [6,Corollary 1.3(2)]. Although domu itself may fail to be convex, it is almost convex in the above sense. By convexity, K \ intK has Lebesgue measure 0 (see the discussion after Theorem 1.6.1) and so the set of points in K where u is not a singleton, {x ∈ K : u(x) = / 0} ∪ {x ∈ K : u(x) contains more than one point}, has Lebesgue measure 0, and u(x) is empty for all x / ∈ K. (It is in fact not difficult to show that if x ∈ ∂ K, then u(x) cannot be a singleton, by the Hahn-Banach theorem.)</p>
        <p>With this background on monotone functions at our disposal, we are now ready to state the stability result for the optimal maps. We assume the following.</p>
        <p>Assumptions 1 Let μ n , μ, ν n , ν ∈ P(R d ) with optimal couplings (with respect to quadratic cost) π n ∈ Π (μ n , ν n ), π ∈ Π (μ, ν) and convex potentials φ n and φ , respectively, such that</p>
        <p>xy 2 dπ n (x, y) &lt; ∞;</p>
        <p>• (unique limit) the optimal π ∈ Π (μ, ν) is unique.</p>
        <p>We further denote the subgradients ∂ φ n and ∂ φ by u n and u, respectively.</p>
        <p>These assumptions imply that π has a finite total cost. This can be shown by the lim inf argument in the proof of Theorem 1.7.2 but also from the uniqueness of π.</p>
        <p>As a corollary of the uniqueness of π, it follows that π n → π weakly; notice that this holds even if π n is not unique for any n. We will now translate this weak convergence to convergence of the maximal monotone maps u n to u, in the following form.</p>
        <p>In particular, if u is univalued throughout int(E) (so that φ ∈ C 1 there), then uniform convergence holds for any compact Ω ⊂ int(E).</p>
        <p>The proof of Proposition 1.7.11, given on page 28 of the supplement, follows two separate steps:</p>
        <p>• if a sequence in the graph of u n converges, then the limit is in the graph of u;</p>
        <p>• sequences in the graph of u n are bounded if the domain is bounded.</p>
        <p>Corollary 1.7.12 (Pointwise Convergence μ-Almost Surely) If in addition μ is absolutely continuous, then u n (x) → u(x) μ-almost surely.</p>
        <p>Proof. We first claim that E ⊆ domu. Indeed, for any x ∈ E and any ε &gt; 0, the ball B = B ε (x) has positive measure. Consequently, u cannot be empty on the entire ball, because otherwise μ(B) = π(B × R d ) would be 0. Since domu is almost convex (see the discussion before Assumptions 1), this implies that actually int(convE) ⊆ domu. The rest is now easy: the set of points x ∈ E for which Ω = {x} fails to satisfy the conditions of Proposition 1.7.11 is included in</p>
        <p>which is μ-negligible because μ is absolutely continuous and both sets have Lebesgue measure 0.</p>
        <p>It is well-known (Luenberger and Ye [89,Section 4.4]) that the solutions to the primal and dual problems are related to each other via complementary slackness. In other words, solution of one problem provides a lot of information about the solution of the other problem. Here, we show that this idea remains true for the Kantorovich primal and dual problems, extending the discussion in Sect. 1.6.1 to more general cost functions.</p>
        <p>Let X and Y be complete separable metric spaces, μ ∈ P(X ), ν ∈ P(Y ), and c : X × Y → R + be a measurable cost function.</p>
        <p>If one finds functions (ϕ, ψ) ∈ Φ c and a transference plan π ∈ Π (μ, ν) having the same objective values, then by weak duality (ϕ, ψ) is optimal in Φ c and π is optimal in Π (μ, ν). Having the same objective values is equivalent to</p>
        <p>π-almost surely.</p>
        <p>It has already been established that there exists an optimal transference plan π * . Assuming that C(π * ) &lt; ∞ (otherwise all transference plans are optimal), a pair (ϕ, ψ) ∈ Φ c is optimal if and only if</p>
        <p>Conversely, if (ϕ 0 , ψ 0 ) is an optimal pair, then π is optimal if and only if it is concentrated on the set {(x, y) : ϕ 0 (x) + ψ 0 (y) = c(x, y)}.</p>
        <p>In particular, if for a given x there exists a unique y such that ϕ 0 (x)+ψ 0 (y) = c(x, y), then the mass at x must be sent entirely to y and not be split; if this is the case for μalmost all x, then this relation defines y as a function of x and the resulting optimal π is in fact induced from a transport map. This idea provides a criterion for solvability of the Monge problem (Villani [125,Theorem 5.30]).</p>
        <p>It turns out that the dual Kantorovich problem can be recast as an unconstrained optimisation problem of only one function ϕ. The new formulation is not only conceptually simpler than the original one, but also sheds light on the properties of the optimal dual variables. Since the dual objective function to be maximised,</p>
        <p>is increasing in ϕ and ψ, one should seek functions that take values as large as possible subject to the constraint ϕ(x) + ψ(y) ≤ c(x, y). Suppose that an oracle tells us that some ϕ ∈ L 1 (μ) is a good candidate. Then the largest possible ψ satisfying (ϕ, ψ) ∈ Φ c is defined as</p>
        <p>A function taking this form is called c-concave [124, Chapter 2]; we say that ψ is the c-transform of ϕ. It is not necessarily true that ϕ c is integrable or even measurable, but if we neglect this difficulty, then it is obvious that sup</p>
        <p>The dual problem can thus be formulated as the unconstrained problem</p>
        <p>One can apply this c-transform again and replace ϕ by</p>
        <p>so that ϕ cc has a better objective value yet still (ϕ cc , ϕ c ) ∈ Φ c (modulo measurability issues). An elementary calculation shows that in general ϕ ccc = ϕ c . Thus, for any function ϕ 1 , the pair of functions (ϕ, ψ) = (ϕ cc 1 , ϕ c 1 ) has a better objective value than (ϕ 1 , ψ 1 ), and satisfies (ϕ, ψ) ∈ Φ c . Moreover, ϕ c = ψ and ψ c = ϕ; in words, ϕ and ψ are c-conjugate. An optimal dual pair (ϕ, ψ) can be expected to be c-conjugate; this is indeed true almost surely: Proposition 1.8.1 (Existence of an Optimal Pair) Let μ and ν be probability measures on X and Y such that the independent coupling with respect to the nonnegative and lower semicontinuous cost function is finite: X ×Y c(x, y) dμ(x)dν(y) &lt; ∞. Then there exists an optimal pair (ϕ, ψ) for the dual Kantorovich problem. Furthermore, the pair can be chosen in a way that μ-almost surely, ϕ = ψ c and ν-almost surely, ψ = ϕ c . Proposition 1.8.1 is established (under weaker conditions) by Ambrosio and Pratelli [11,Theorem 3.2]. It is clear from the discussion above that once existence of an optimal pair (ϕ 1 , ψ 1 ) is established, the pair (ϕ, ψ) = (ϕ cc 1 , ϕ c 1 ) should be optimal. Combining Proposition 1.8.1 with the preceding subsection, we see that if ϕ is optimal (for the unconstrained dual problem), then any optimal transference plan π * must be concentrated on the set</p>
        <p>If for μ-almost every x this equation defines y uniquely as a (measurable) function of x, then π * is induced by a transport map. Indeed, we have seen how this is the case, in the quadratic case c(x, y) = xy 2 /2, when μ is absolutely continuous. An extension to p &gt; 1 (instead of p = 2) is sketched in Sect. 1.8.3.</p>
        <p>We remark that at the level of generality of Proposition 1.8.1, the function ϕ c may fail to be Borel measurable; Ambrosio and Pratelli show that this pair can be modified up to null sets in order to be Borel measurable. If c is continuous, however, then ϕ c is an infimum of a collection of continuous functions (in y). Hence -ϕ c is lower semicontinuous, which yields that ϕ c is measurable. When c is uniformly continuous, measurability of ϕ c is established in a more lucid way, as exemplified in the next subsection.</p>
        <p>Whether ϕ c (y) is tractable to evaluate depends on the structure of c. We have seen an example where c was the quadratic Euclidean distance. Here, we shall consider another useful case, where c is a metric. Assume that X = Y , denote their metric by d, and let c(x, y) = d(x, y). If ϕ = ψ c is c-concave, then it is 1-Lipschitz. Indeed, by definition and the triangle inequality</p>
        <p>Interchanging x and z yields |ϕ(x) -ϕ(z)| ≤ d(x, z). 3Next, we claim that if ϕ is Lipschitz, then ϕ c (y) = -ϕ(y). Indeed, choosing x = y in the infimum shows that ϕ c (y) ≤ d(y, y) -ϕ(y) = -ϕ(y). But the Lipschitz condition on ϕ implies that for all x, d(x, y) -ϕ(x) ≥ -ϕ(y). In view of that, we can take in the dual problem ϕ to be Lipschitz and ψ = -ϕ, and the duality formula</p>
        <p>1 Optimal Transport This is known as the Kantorovich-Rubinstein theorem [124,Theorem 1.14]. (We have been a bit sloppy because ϕ may not be integrable. But if for some</p>
        <p>, then any Lipschitz function is μ-integrable. Otherwise one needs to restrict the supremum to, e.g., bounded Lipschitz ϕ.)</p>
        <p>We now return to the Euclidean case X = Y = R d and explore the structure of c-transforms. When c is different than xy 2 /2, we can no longer "open up the square" and relate the Monge-Kantorovich problem to convexity. However, we can still apply the idea that ϕ(x) + ϕ c (y) = c(x, y) if and only if the infimum is attained at x. Indeed, recall that</p>
        <p>Notice the similarity to the subgradient inequality for convex functions, with the sign being reversed. In analogy, we call the collection of y's satisfying the above in equality the c-superdifferential of ϕ at x, and we denote it by</p>
        <p>The following result generalises Theorem 1.6.2 to other powers p &gt; 1 of the Euclidean norm. These cost functions define the Wasserstein distances of the next chapter.</p>
        <p>v p /p for some p &gt; 1 and let μ and ν be probability measures on R d with finite pth moments such that μ is absolutely continuous with respect to Lebesgue measure. Then the solution to the Kantorovich problem with cost function c is unique and induced from a transport map T . Furthermore, there exists an optimal pair (ϕ, ϕ c ) of the dual problem, with ϕ c-concave. The solutions are related by</p>
        <p>Proof (Assuming ν has Compact Support). The existence of the optimal pair (ϕ, ϕ c ) with the desired properties follows from Proposition 1.8.1 (they are Borel measurable because c is continuous). We shall now show that ϕ has a unique csupergradient μ-almost surely.</p>
        <p>Step 1: ϕ is c-superdifferentiable. Let π * be an optimal coupling. By duality arguments, π is concentrated on the set of (x, y) such that y ∈ ∂ c ϕ(x). Consequently, for μ-almost any x, the c-superdifferential of ϕ at x is nonempty.</p>
        <p>Step 2: ϕ is differentiable. Here, we impose the additional condition that ν is compactly supported. Then ϕ can be taken as a c-transform on the compact support of ν. Since h is locally Lipschitz (it is C 1 because p &gt; 1) this implies that ϕ is locally Lipschitz. Hence, it is differentiable Lebesgue almost surely, and consequently μalmost surely.</p>
        <p>Step 3: Conclusion. For μ-almost every x there exists y ∈ ∂ c ϕ(x) and a gradient u = ∇ϕ(x). In particular, u is a subgradient of ϕ:</p>
        <p>Here and more generally, o( zx ) denotes a function r(z) (defined in a neighbourhood of x) such that r(z)/ zx → 0 as z → x. (If ϕ were convex, then we could take r ≡ 0, so the definition for convex functions is equivalent, and then the inequality holds globally and not only locally.) But</p>
        <p>which defines y as a (measurable) function of x. 4 Hence, the optimal transference plan π is unique and induced from the transport map T .</p>
        <p>The general result, without assuming compact support for ν, can be found in Gangbo and McCann [59]. It holds for a larger class of functions h, those that are strictly convex on R d (this yields that ∇h is invertible), have superlinear growth</p>
        <p>)), then μ does not need to be absolutely continuous; it suffices that it not give positive measure to any set of Hausdorff dimension smaller or equal than d -1. When d = 1 this means that Theorem 1.8.2 is still valid as long as μ has no atoms (μ({x}) = 0 for all x ∈ R), which is a weaker condition than μ being absolutely continuous.</p>
        <p>It is also noteworthy that for strictly concave cost functions (e.g., p ∈ (0, 1)), the situation is similar when the supports of μ and ν are disjoint. The reason is that h may fail to be differentiable at 0, but it only needs to be differentiated at xy with x ∈ suppμ and y ∈ suppν. If the supports are not disjoint, then one needs to leave all common mass in place until the supports become disjoint (Villani [124,Chapter 2]) and then the result of [59] applies. As a simple example, let μ be uniform on [0, 1] and ν be uniform on [0, 2]. After leaving common mass in place, we are left with uniforms on [0, 1] and [1,2] (with total mass 1/2) with essentially disjoint supports, for which the optimal transport map is the decreasing map T (x) = 2x. Thus, the unique optimal π is not induced from a map, but rather from an equal weight mixture of T and the identity. Informally, each point x in the support of μ needs to be split; half stays and x and the other half transported to 2x. The optimal coupling from ν to μ is unique and induced from the map S(x) = x if x ≤ 1 and 2x if x ≥ 1, which is neither increasing nor decreasing. Theorem 1.5.1 for the one-dimensional case is taken from [124], where it is proven using the general duality theorem. For direct proofs and the history of this result, one may consult Rachev [106] or Rachev and Rüschendorf [107, Section 3.1]. The concave case is carefully treated by McCann [94].</p>
        <p>The results in the Gaussian case were obtained independently by Olkin and Pukelsheim [98] and Givens and Shortt [65]. The proof given here is from Bhatia [20,Exercise 1.2.13]. An extension to separable Hilbert spaces can be found in Gelbrich [62] or Cuesta-Albertos et al. [39].</p>
        <p>The regularity theory of Sect. 1.6.4 is very delicate. Caffarelli [32] showed the first part of Theorem 1.6.7; the proof can also be found in Figalli's book [52,Theorem 4.23]. Villani [124,Theorem 4.14] states the result without proof and refers to Alesker et al. [7] for a sketch of the second part of Theorem 1.6.7. Other regularity results exist, Villani [125,Chapter 12]; Santambrogio [119, Section 1.7.6]; Figalli [52].</p>
        <p>Cuesta-Albertos et al. [40,Theorem 3.2] prove Theorem 1.7.2 for the quadratic case; the form given here is from Schachermayer and Teichmann [120, Theorem 3].</p>
        <p>The definition of cyclical monotonicity depends on the cost function. It is typically referred to as c-cyclical monotonicity, with "cyclical monotonicity" reserved to the special case of quadratic cost. Since we focus on the quadratic case and for readability, we slightly deviate from the standard jargon. That cyclical monotonicity implies optimality (Proposition 1.7.5) was shown independently by Pratelli [105] (finite lower semicontinuous cost) and Schachermayer and Teichmann [120] (possibly infinite continuous cost). A joint generalisation is given by Beiglböck et al. [18].</p>
        <p>Section 1.7.2 is taken from Zemel and Panaretos [134, Section 7.5]; a slightly weaker version was shown independently by Chernozhukov et al. [35]. Heinich and Lootgieter [68] establish almost sure pointwise convergence. If μ n = μ, then the optimal maps converge in μ-measure [125,Corollary 5.23] in a very general setup, but there are simple examples where this fails if μ n = μ [125,Remark 5.25]. In the quadratic case, further stability results of a weaker flavour (focussing on the convex potential φ , rather than its derivative, which is the optimal map) can be found in del Barrio and Loubes [42].</p>
        <p>The idea of using the c-transform (Sect. 1.8) is from Rüschendorf [116].</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>The Kantorovich problem described in the previous chapter gives rise to a metric structure, the Wasserstein distance, in the space of probability measures P(X ) on a space X . The resulting metric space, a subspace of P(X ), is commonly known as the Wasserstein space W (although, as Villani [125, pages 118-119] puts it, this terminology is "very questionable"; see also Bobkov and Ledoux [25,page 4]). In Chap. 4, we shall see that this metric is in a sense canonical when dealing with warpings, that is, deformations of the space X (for example, in Theorem 4.2.4). In this chapter, we give the fundamental properties of the Wasserstein space. After some basic definitions, we describe the topological properties of that space in Sect. 2.2. It is then explained in Sect. 2.3 how W can be endowed with a sort of infinite-dimensional Riemannian structure. Measurability issues are dealt with in the somewhat technical Sect. 2.4.</p>
        <p>Let X be a separable Banach space. The p-Wasserstein space on X is defined as</p>
        <p>We will sometimes abbreviate and write simply W p instead of W p (X ).</p>
        <p>Recall that if μ, ν ∈ P(X ), then Π (μ, ν) is defined to be the set of measures π ∈ P(X 2 ) having μ and ν as marginals in the sense of (1.2). The p-Wasserstein distance between μ and ν is defined as the minimal total transportation cost between Electronic Supplementary Material The online version of this chapter (https://doi.org/10.1007/ 978-3-030-38438-8 2) contains supplementary material.</p>
        <p>μ and ν in the Kantorovich problem with respect to the cost function c p (x, y) =</p>
        <p>xy p :</p>
        <p>The Wasserstein distance between μ and ν is finite when both measures are in W p (X ), because</p>
        <p>; it is nonnegative and symmetric and it is easy to see that W p (μ, ν) = 0 if and only if μ = ν. A proof that W p is a metric (satisfies the triangle inequality) on W p can be found in Villani [124,Chapter 7]. The aforementioned setting is by no means the most general one can consider. Firstly, one can define W p and W p for 0 &lt; p &lt; 1 by removing the power 1/p from the infimum and the limit case p = 0 yields the total variation distance. Another limit case can be defined as W ∞ (μ, ν) = lim p→∞ W p (μ, ν). Moreover, W p and W p can be defined whenever X is a complete and separable metric space (or even only separable; see Clément and Desch [36]): one fixes some x 0 in X and replaces</p>
        <p>x by d(x, x 0 ). Although the topological properties below still hold at that level of generality (except when p = 0 or p = ∞), for the sake of simplifying the notation we restrict the discussion to Banach spaces. It will always be assumed without explicit mention that 1 ≤ p &lt; ∞.</p>
        <p>The space W p (X ) is defined as the collection of measures μ such that W p (μ, δ 0 ) &lt; ∞ with δ x being a Dirac measure at x. Of course, W p (μ, ν) can be finite even if μ, ν / ∈ W p (X ). But if μ ∈ W p (X ) and ν / ∈ W p (X ), then W p (μ, ν) is always infinite. This can be seen from the triangle inequality</p>
        <p>In the sequel, we shall almost exclusively deal with measures in W p (X ).</p>
        <p>The Wasserstein spaces are ordered in the sense that if q ≥ p, then W q (X ) ⊆ W p (X ). This property extends to the distances in the form:</p>
        <p>To see this, let π ∈ Π (μ, ν) be optimal with respect to q. Jensen's inequality for the convex function z → z q/p gives</p>
        <p>xy q dπ(x, y) ≥ Xfoot_7</p>
        <p>xy p dπ(x, y) q/p ≥ W q p (μ, ν).</p>
        <p>The converse of (2.1) fails to hold in general, since it is possible that W p be finite while W q is infinite. A converse can be established, however, if μ and ν are bounded:</p>
        <p>(2.2) Indeed, if we denote the supremum by d K and let π be now optimal with respect to p, then π(K × K) = 1 and</p>
        <p>xy p dπ(x, y) = d q-p K W p p (μ, ν).</p>
        <p>Another useful property of the Wasserstein distance is the upper bound</p>
        <p>for any pair of measurable functions t, s : X → X . Situations where this inequality holds as equality and t and s are optimal maps are related to compatibility of the measures μ, ν = t#μ and ρ = s#μ (see Sect. 2.3.2) and will be of conceptual importance in the context of Fréchet means (see Sect. 3.1).</p>
        <p>We also recall the notation</p>
        <p>The topology of a space is determined by the collection of its closed sets. Since W p (X ) is a metric space, whether a set is closed or not depends on which sequences in W p (X ) converge. The following characterisation from Villani [124,Theorem 7.12] will be very useful.</p>
        <p>Then the following are equivalent:</p>
        <p>4. for any C &gt; 0 and any continuous f : [87,Lemma 14]) μ n → μ weakly and there exists ν ∈ W p (X ) such that W p (μ n , ν) → W p (μ, ν).</p>
        <p>Consequently, the Wasserstein topology is finer than the weak topology induced on W p (X ) from P(X ). Indeed, let A ⊆ W p (X ) be weakly closed. If μ n ∈ A converge to μ in W p (X ), then μ n → μ weakly, so μ ∈ A . In other words, the Wasserstein topology has more closed sets than the induced weak topology. Moreover, each W p (X ) is a weakly closed subset of P(X ) by the same arguments that lead to (1.3). In view of Theorem 2.2.1, a common strategy to establish Wasserstein convergence is to first show tightness and obtain weak convergence, hence a candidate limit, and then show that the stronger Wasserstein convergence actually holds.</p>
        <p>In some situations, the last part is automatic:</p>
        <p>Proof. This is immediate from (2.4).</p>
        <p>The fact that convergence in W p is stronger than weak convergence is exemplified in the following result. If</p>
        <p>But if the convergence is only weak, then the Wasserstein distance is still lower semicontinuous:</p>
        <p>This follows from Theorem 1.7.2 and (1.3). Before giving some examples, it will be convenient to formulate Theorem 2.2.1 in probabilistic terms. Let X, X n be random elements on X with laws μ, μ n ∈ W p (X ). Assume without loss of generality that X, X n are defined on the same probability space (Ω , F , P) and write W p (X n , X) to denote W p (μ n , μ). Then W p (X n , X) → 0 if and only if X n → X weakly and E X n p → E X p . An early example of the use of Wasserstein metric in statistics is due to Bickel and Freedman [21]. Let X n be independent and identically distributed random variables with mean zero and variance 1 and let Z be a standard normal random variable. Then</p>
        <p>n has the same asymptotic distribution as Z n . Another consequence of Theorem 2.2.1 is that (in the presence of weak convergence) convergence of moments automatically yields convergence of smaller moments (there are, however, more elementary ways to see this). In the previous example, for instance, one can also conclude that E|Z n | p → E|Z| p for any p ≤ 2 by the last condition of the theorem. If in addition EX 4 1 &lt; ∞, then</p>
        <p>(see Durrett [49, Theorem 2.3.5]) so W 4 (Z n , Z) → 0 and all moments up to order 4 converge. Condition (2.4) is called uniform integrability of the function x → x p with respect to the collection (μ n ). Of course, it holds for a single measure μ ∈ W p (X ) by the dominated convergence theorem. This condition allows us to characterise compact sets in the Wasserstein space. One should beware that when X is infinitedimensional, (2.4) alone is not sufficient in order to conclude that μ n has a convergent subsequence: take μ n to be Dirac measures at e n with (e n ) an orthonormal basis of a Hilbert space X (or any sequence with e n = 1 that has no convergent subsequence, if X is a Banach space). The uniform integrability (2.4) must be accompanied with tightness, which is a consequence of (2.4) only when X = R d .</p>
        <p>Moreover, (2.6) is equivalent to the existence of a monotonically divergent function g :</p>
        <p>The proof is on page 41 of the supplement.</p>
        <p>For any sequence (μ n ) in W p (tight or not) there exists a monotonically divergent g with X x p g( x ) dμ n (x) &lt; ∞ for all n.</p>
        <p>Proof. This is immediate, since K is weakly tight and the supremum in (2.6) vanishes when R is larger than the finite quantity sup x∈K x . Finally, K is closed, so K is weakly closed, hence Wasserstein closed, by the portmanteau Lemma 1.7.1.</p>
        <p>For future reference, we give another consequence of uniform integrability, called uniform absolute continuity ∀ε ∃δ ∀n ∀A ⊆ X Borel :</p>
        <p>To show that (2.4) implies (2.7), let ε &gt; 0, choose R = R ε &gt; 0 such that the supremum in (2.4) is smaller than ε/2, and set</p>
        <p>If we identify a measure μ ∈ W p (X ) with a random variable X (having distribution μ), then X has a finite p-th moment in the sense that the real-valued random variable X is in L p . In view of that, it should not come as a surprise that W p (X ) enjoys topological properties similar to L p spaces. In this subsection, we give some examples of useful dense subsets of W p (X ) and then "show" that like X itself, it is a complete separable metric space. In the next subsection, we describe some of the negative properties that W p (X ) has, again in similarity with L p spaces. We first show that W p (X ) is separable. The core idea of the proof is the feasibility of approximating any measure with discrete measures as follows.</p>
        <p>Let μ be a probability measure on X , and let X 1 , X 2 , . . . be a sequence of independent random elements in X with probability distribution μ. Then the empirical measure μ n is defined as the random measure</p>
        <p>The law of large numbers shows that for any (measurable) bounded or nonnegative f :</p>
        <p>In particular when f (x) = x p , we obtain convergence of moments of order p.</p>
        <p>Hence by Theorem 2.2.1, if μ ∈ W p (X ), then μ n → μ in W p (X ) if and only if μ n → μ weakly. We know that integrals of bounded functions converge with probability one, but the null set where convergence fails may depend on the chosen function and there are uncountably many such functions. When X = R d , by the portmanteau Lemma 1.7.1 we can replace the collection C b (X ) by indicator functions of rectangles of the form (-∞,</p>
        <p>It turns out that the countable collection provided by rational vectors a suffices (see the proof of Theorem 4.4.1 where this is done in a more complicated setting). For more general spaces X , we need to find another countable collection { f j } such that convergence of the integrals of f j for all j suffices for weak convergence.</p>
        <p>In the previous subsection, we have shown that W p (X ) is separable and complete like L p spaces. Just like them, however, the Wasserstein space is neither locally compact nor σ -compact. For this reason, existence proofs of Fréchet means in W p (X ) require tools that are more specific to this space, and do not rely upon local compactness (see Sect. 3.1).</p>
        <p>Proposition 2.2.9 (W p is Not Locally Compact) Let μ ∈ W p (X ) and let ε &gt; 0.</p>
        <p>Then the Wasserstein ball</p>
        <p>Ambrosio et al. [12,Remark 7.1.9] show this when μ is a Dirac measure, and we extend their argument on page 43 of the supplement. From this, we deduce:</p>
        <p>Corollary 2.2.10 The Wasserstein space W p (X ) is not σ -compact.</p>
        <p>Proof. If K is a compact set in W p (X ), then its interior is empty by Proposition 2.2.9. A countable union of compact sets has an empty interior (hence cannot equal the entire space W p (X )) by the Baire property, which holds on the complete metric space W p (X ) by the Baire category theorem (Dudley [47, Theorem 2.5.2]).</p>
        <p>Let K ⊂ W p (X ) be compact and assume that X = R d . Then for any ε &gt; 0 the covering number</p>
        <p>is finite. These numbers appear in statistics in various ways, particularly in empirical processes (see, for instance, Wainwright [126,Chapter 5]) and the goal of this subsection is to give an upper bound for N(ε; K ).</p>
        <p>The function f provides a certain measure of how compact K is. If K = W p (K) is the set of measures supported on a compact K ⊆ R d , then f (L) can be taken infinite for L large, and L can be treated as a constant in the theorem. Otherwise L increases as ε 0, at a speed that depends on f : the faster f diverges, the slower L grows with decreasing ε and the better the bound becomes.</p>
        <p>Since ε &gt; 0 is small and L is increasing in ε, the restriction that dε ≤ L is typically not binding. We provide some examples before giving the proof. Example 1: if all the measures are supported on the d-dimensional unit ball, then L can be taken equal to one, independently of ε. We obtain</p>
        <p>Example 2: if all the measures in K have uniform exponential moments, then</p>
        <p>up to smaller order terms. Here (when 0 &lt; δ &lt; ∞) the behaviour of N(ε) depends more strongly upon p: if p &lt; p, then we can replace δ by δ = δ + pp &gt; δ , leading to a smaller magnitude of N(ε).</p>
        <p>Example 4: if f (L) is only log L, then N behaves like ε -(d+p) exp(ε -pd ), so p has a very dominant effect.</p>
        <p>Proof. The proof is divided into four steps.</p>
        <p>Step 1: Compact support. Let P L : R d → R d be the projection onto</p>
        <p>and this vanishes as L → ∞.</p>
        <p>Step 2: n-Point measures. Let n = N(ε; B L (0)) be the covering number of the Euclidean ball in R d . There exists a set x 1 ,...,</p>
        <p>), there exists a measure μ n supported on the x i 's and such that</p>
        <p>The transport map defined by t(x) = x i for x ∈ C i pushes μ forward to μ n and</p>
        <p>According to Rogers [114], we have the bound</p>
        <p>whenever ε ≤ L/d.</p>
        <p>Step 3: Common weights.</p>
        <p>This set contains fewer than (2 + 1/δ ) n-1 elements, and any measure supported on {x 1 , . . . , x n } can be approximated by a measure in μ n,ε,δ with error 2L(nδ ) 1/p .</p>
        <p>Step 4:</p>
        <p>) and δ = [ε/(2L)] p /n. Combining the previous three steps, we obtain in the case L ≥ εd that</p>
        <p>Although the Wasserstein space W p (X ) is non-linear in terms of measures, it is linear in terms of maps. Indeed, if μ ∈ W p (X ) and T i : X → X are such that</p>
        <p>Later, in Sect. 2.4, we shall see that W p (X ) is in fact homeomorphic to a subset of the space of such functions. The goal of this section is to exploit the linearity of the latter in order to define the tangent bundle of W p . This in particular will be used for deriving differentiability properties of the Wasserstein distance in Sect. 3.1.6. However, the latter can be understood at a purely analytic level, and readers uncomfortable with differential geometry can access most of the rest of the monograph without reference to this section. We assume here that X is a Hilbert space and that p = 2; the results extend to any p &gt; 1. Absolutely continuous measures are assumed to be so with respect to Lebesgue measure if X = R d and otherwise refer to Definition 1.6.4.</p>
        <p>Let γ ∈ W 2 (X ) be absolutely continuous and μ ∈ W 2 (X ) arbitrary. From Sect. 1.6.1, we know that there exists a unique solution to the Monge-Kantorovich problem, and that solution is given by a transport map that we denote by t μ γ . Recalling that i : X → X is the identity map, we can define a curve</p>
        <p>This curve is known as McCann's [93] interpolant. As hinted in the introduction to this section, it is constructed via classical linear interpolation of the transport maps t μ γ and the identity. Clearly γ 0 = γ, γ 1 = μ and from (2.3),</p>
        <p>It follows from the triangle inequality in W 2 that these inequalities must hold as equalities. Taking this one step further, we see that</p>
        <p>In other words, McCann's interpolant is a constant-speed geodesic in W 2 (X ).</p>
        <p>In view of this, it seems reasonable to define the tangent space of W 2 (X ) at μ as (Ambrosio et al. [12,Definition 8.5.1])</p>
        <p>It follows from the definition that Tan μ ⊆ L 2 (μ). (Strictly speaking, Tan μ is a subset of the space of functions f : X → X such that f ∈ L 2 (μ) rather than L 2 (μ) itself, as in Definition 2.4.3, but we will write L 2 for simplicity.)</p>
        <p>Although not obvious from the definition, this is a linear space. The reason is that, in R d , Lipschitz functions are dense in L 2 (μ), and for t Lipschitz the negative of a tangent element</p>
        <p>lies in the tangent space, since s can be seen to belong to the subgradient of a convex function by definition of s. This also shows that Tan μ can be seen to be the L 2 (μ)-closure of all gradients of C ∞ c functions. We refer to [12, Definition 8.4.1 and Theorem 8.5.1] for the proof and extensions to other values of p &gt; 1 and to infinite dimensions, using cylindrical functions that depend on finitely many coordinates [12,Definition 5.1.11]. The alternative definition highlights that it is essentially the inner product in Tan μ , but not the elements themselves, that depends on μ.</p>
        <p>The tangent space definition is valid for arbitrary measures in W 2 (X ). The exponential map at γ ∈ W 2 (X ) is the restriction to Tan γ of the mapping that sends</p>
        <p>Thus, when γ is absolutely continuous, exp γ is surjective, as can be seen from its right inverse, the log map</p>
        <p>defined throughout W 2 (by virtue of Theorem 1.6.2). In symbols,</p>
        <p>because convex combinations of optimal maps are optimal maps as well. In particular, McCann's interpolant i + t(t μ γi) #γ is mapped bijectively to the line segment t(t μ γi) ∈ Tan γ through the log map. It is also worth mentioning that McCann's interpolant can also be defined as</p>
        <p>where p 1 , p 2 : X 2 → X are projections and π is any optimal transport plan between γ and μ. This is defined for arbitrary measures γ, μ ∈ W 2 , and reduces to the previous definition if γ is absolutely continuous. It is shown in Ambrosio et al. [12,Chapter 7] or Santambrogio [119,Proposition 5.32] that these are the only constant-speed geodesics in W 2 .</p>
        <p>Let γ, μ, ν ∈ W 2 (X ) be absolutely continuous measures. Then by (2.3)</p>
        <p>In other words, the distance between μ and ν is smaller in W 2 (X ) than the distance between the corresponding vectors log γ (μ) and log γ (ν) in the tangent space Tan γ .</p>
        <p>In the terminology of differential geometry, this means that the Wasserstein space has nonnegative sectional curvature at any absolutely continuous γ.</p>
        <p>It is instructive to see when equality holds. As t</p>
        <p>Since the map t</p>
        <p>. This motivates the following definition.</p>
        <p>The absolute continuity is not necessary and was introduced for notational simplicity. A more general definition that applies to general measures is the following: every finite subcollection of C admits an optimal multicoupling whose relevant projections are simultaneously pairwise optimal; see the paragraph preceding Theorem 3.1.9.</p>
        <p>A collection of two (absolutely continuous) measures is always compatible. More interestingly, if X = R, then the entire collection of absolutely continuous (or even just continuous) measures is compatible. This is because of the simple geometry of convex functions in R: gradients of convex functions are nondecreasing, and this property is stable under composition. In a more probabilistic way of thinking, one can always push-forward μ to ν via the uniform distribution Leb| [0,1] (see Sect. 1.5). Letting F -1 μ and F -1 ν denote the quantile functions, we have seen that 1) .</p>
        <p>(As a matter of fact, in this specific case, the equality holds for all p ≥ 1 and not only for p = 2.) In other words, μ → F -1 μ is an isometry from W 2 (R) to the subset of L 2 (0, 1) formed by (equivalence classes of) left-continuous nondecreasing functions on (0, 1). Since this is a convex subset of a Hilbert space, this property provides a very simple way to evaluate Fréchet means in W 2 (R) (see Sect. 3.1). If γ = Leb| [0,1] , then F -1 μ = t μ γ for all μ, so we can write the above equality as</p>
        <p>so that if X = R, the Wasserstein space is essentially flat (has zero sectional curvature).</p>
        <p>The importance of compatibility can be seen as mimicking the simple onedimensional case in terms of a Hilbert space embedding. Let C ⊆ W 2 (X ) be compatible and fix γ ∈ C . Then for all μ, ν ∈ C</p>
        <p>Consequently, once again, μ → t μ γ is an isometric embedding of C into L 2 (γ). Generalising the one-dimensional case, we shall see that this allows for easy calculations of Fréchet means by means of averaging transport maps (Theorem 3.1.9).</p>
        <p>Example: Gaussian compatible measures. The Gaussian case presented in Sect. 1.6.3 is helpful in shedding light on the structure imposed by the compatibility condition. Let γ ∈ W 2 (R d ) be a standard Gaussian distribution with identity covariance matrix. Let Σ μ denote the covariance matrix of a measure μ ∈ W 2 (R d ). When μ and ν are centred nondegenerate Gaussian measures,</p>
        <p>, so that γ, μ, and ν are compatible if and only if</p>
        <p>Since the matrix on the left-hand side must be symmetric, it must necessarily be that Σ</p>
        <p>1/2 ν and Σ -1/2 μ commute (if A and B are symmetric, then AB is symmetric if and only if AB = BA), or equivalently, if and only if Σ ν and Σ μ commute. We see that a collection C of Gaussian measures on R d that includes the standard Gaussian distribution is compatible if and only if all the covariance matrices of the measures in C are simultaneously diagonalisable. In other words, there exists an orthogonal matrix U such that D μ = UΣ μ U t is diagonal for all μ ∈ C . In that case, formula (1.6)</p>
        <p>and identifying the (nonnegative) number a ∈ R with the map x → ax on R, the optimal maps take the "orthogonal separable" form</p>
        <p>In other words, up to an orthogonal change of coordinates, the optimal maps take the form of d nondecreasing real-valued functions. This is yet another crystallisation of the one-dimensional-like structure of compatible measures.</p>
        <p>With the intuition of the Gaussian case at our disposal, we can discuss a more general case. Suppose that the optimal maps are continuously differentiable. Then differentiating the equation</p>
        <p>Since optimal maps are gradients of convex functions, their derivatives must be symmetric and positive semidefinite matrices. A product of such matrices stays symmetric if and only if they commute, so in this differentiable setting, compatibility is equivalent to commutativity of the matrices ∇t ν γ (t γ μ (x)) and ∇t γ μ (x) for μ-almost all x. In the Gaussian case, the optimal maps are linear functions, so x does not appear in the matrices.</p>
        <p>Here are some examples of compatible measures. It will be convenient to describe them using the optimal maps from a reference measure γ ∈ W 2 (R d ). Define C = t#γ with t belonging to one of the following families. The first imposes the one-dimensional structure by varying only the behaviour of the norm of x, while the second allows for separation of variables that splits the d-dimensional problem into d one-dimensional ones.</p>
        <p>Radial transformations. Consider the collection of functions t :</p>
        <p>Since both I and xx t are positive semidefinite, the above matrix is so if both G and G are nonnegative. If s(x) = xH( x ) is a function of the same form, then s(t(x)) = xG( x )H( x G( x )) which belongs to that family of functions (since G is nonnegative). Clearly</p>
        <p>commutes with ∇t(x), since both matrices are of the form aI + bxx t with a, b scalars (that depend on x). In order to be able to change the base measure γ, we need to check that the inverses belong to the family. But if y = t(x), then x = ay for some scalar a that solves the equation</p>
        <p>Such a is guaranteed to be unique if a → aG(a) is strictly increasing and it will exist (for y in the range of t) if it is continuous. As a matter of fact, since the eigenvalues of ∇t(x) are G(a) and</p>
        <p>the condition that a → aG(a) is strictly increasing is sufficient (this is weaker than G itself increasing). Finally, differentiability of G is not required, so it is enough if G is continuous and aG(a) is strictly increasing. Separable variables. Consider the collection of functions t : R d → R d of the form</p>
        <p>with T i continuous and strictly increasing. This is a generalisation of the compatible Gaussian case discussed above in which all the T i 's were linear. Here, it is obvious that elements in this family are optimal maps and that the family is closed under inverses and composition, so that compatibility follows immediately. This family is characterised by measures having a common dependence structure. More precisely, we say that C :</p>
        <p>a distribution function of a random vector having uniform margins. In other words, if there is a random vector V = (V 1 ,...,V d ) with P(V i ≤ a) = a for all a ∈ [0, 1] and all j = 1, . . . , d, and</p>
        <p>Nelsen [97] provides an overview on copulae. To any d-dimensional probability measure μ, one can assign a copula C = C μ in terms of the distribution function G of μ and its marginals G j as</p>
        <p>If each G j is surjective on (0, 1), which is equivalent to it being continuous, then this equation defines C uniquely on (0,</p>
        <p>Suppose ψ is another convex function with gradient s and that compatibility holds, i.e., ∇s(t(x)) commutes with ∇t(x) for all x. Then in order for ∇s A (t A (x)) = A t ∇s(AA t t(Ax))A and ∇t A (x) = A t ∇t(Ax)A to commute, it suffices that AA t = I, i.e., that A be orthogonal. Consequently, if {t#μ} t∈T are compatible, then so are {t U #μ} t∈T for any orthogonal matrix U.</p>
        <p>Let μ be a fixed absolutely continuous probability measure in W 2 (X ). If Λ ∈ W 2 (X ) is another probability measure, then the transport map t Λ μ and the convex potential are functions of Λ . If Λ is now random, then we would like to be able to make probability statements about them. To this end, it needs to be shown that t Λ μ and the convex potential are measurable functions of Λ . The goal of this section is to develop a rigorous mathematical framework that justifies such probability statements. We show that all the relevant quantities are indeed measurable, and in particular establish a Fubini-type result in Proposition 2.4.9. This technical section may be skipped at first reading.</p>
        <p>Here is an example of a measurability result (Villani [125,Corollary 5.22]). Recall that P(X ) is the space of Borel probability measures on X , endowed with the topology of weak convergence that makes it a metric space. Let X be a complete separable metric space and c : X 2 → R + a continuous cost function. Let (Ω , F , P) be a probability space and Λ , κ : Ω → P(X ) be measurable maps. Then there exists a measurable selection of optimal transference plans. That is, a measurable π : Ω → P(X 2 ) such that π(ω) ∈ Π (Λ (ω), κ(ω)) is optimal for all ω ∈ Ω .</p>
        <p>Although this result is very general, it only provides information about π. If π is induced from a map T , it is not obvious how to construct T from π in a measurable way; we will therefore follow a different path. In order to (almost) have a self-contained exposition, we work in a somewhat simplified setting that nevertheless suffices for the sequel. At least in the Euclidean case X = R d , more general measurability results in the flavour of this section can be found in Fontbona et al. [53]. On the other hand, we will not need to appeal to abstract measurable selection theorems as in [53,125].</p>
        <p>Let X be a separable Banach space. (Most of the results below hold for any complete separable metric space but we will avoid this generality for brevity and simpler notation). The Wasserstein space W p (X ) is a metric space for any p ≥ 1. We can thus define: Definition 2.4.1 (Random Measure) A random measure Λ is any measurable map from a probability space (Ω , F , P) to W p (X ), endowed with its Borel σ -algebra.</p>
        <p>In what follows, whenever we call something random, we mean that it is measurable as a map from some generic unspecified probability space. Lemma 2.4.2 A random measure Λ is measurable if and only if it is measurable with respect to the induced weak topology.</p>
        <p>Since both topologies are Polish, this follows from abstract measure-theoretic results (Fremlin [57,Paragraph 423F]). We give an elementary proof on page 53 of the supplement.</p>
        <p>Optimal maps are functions from X to itself. In order to define random optimal maps, we need to define a topology and a σ -algebra on the space of such functions. Definition 2.4.3 (The Space L p (μ)) Let X be a Banach space and μ a Borel measure on X . Then the space L p (μ) is the space of measurable functions f : X → X such that</p>
        <p>When X is separable, L p (μ) is an example of a Bochner space, though we will not use this terminology. It follows from the definition that f L p (μ) is the L p norm of the map x → f (x) X from X to R:</p>
        <p>As usual we identify functions that coincide almost everywhere. Clearly, L p (μ) is a normed vector space. It enjoys another property shared by L p spaces-completeness:</p>
        <p>Theorem 2.4.4 (Riesz-Fischer) The space L p (μ) is a Banach space.</p>
        <p>The proof, a simple variant of the classical one, is given on page 53 of the supplement.</p>
        <p>Random maps lead naturally to random measures:</p>
        <p>Lemma 2.4.5 (Push-Forward with Random Maps) Let μ ∈ W p (X ) and let t be a random map in L p (μ). Then Λ = t#μ is a continuous mapping from L p (μ) to W p (X ), hence a random measure.</p>
        <p>Proof. That Λ takes values in W p follows from a change of variables</p>
        <p>Conversely, t is a continuous function of Λ : Lemma 2.4.6 (Measurability of Transport Maps) Let Λ be a random measure in W p (X ) and let μ ∈ W p (X ) such that (i, t Λ μ )#μ is the unique optimal coupling of μ and Λ . Then Λ → t Λ μ is a continuous mapping from W p (X ) to L p (μ), so t Λ μ is a random element in L p (μ). In particular, the result holds if X is a separable Hilbert space, p &gt; 1, and μ is absolutely continuous.</p>
        <p>Proof. This result is more subtle than Lemma 2.4.5, since Λ → t Λ μ is not necessarily Lipschitz. We give here a self-contained proof for the Euclidean case with quadratic cost and μ absolutely continuous. The general case builds on Villani [125, Corollary 5.23] and is given on page 54 of the supplement.</p>
        <p>Suppose that</p>
        <p>Since ab p ≤ 2 p a p + 2 p b p , the last integral is no larger than 4</p>
        <p>x 2 dΛ (x).</p>
        <p>Since (Λ n ) and Λ are tight in the Wasserstein space, they must satisfy the absolute uniform continuity (2.7). Let δ = δ ε as in (2.7), and notice that by the measure preserving property of the optimal maps, the last two integrals are taken on sets of measures 1 -μ(S). Since μ is absolutely continuous, we can find a compact set S of μ-measure at least 1 -δ and on which Proposition 1.7.11 applies (see Corollary 1.7.12), yielding</p>
        <p>and this completes the proof upon letting ε → 0.</p>
        <p>In Proposition 5.3.7, we show under some conditions that t Λ μ L 2 (μ) is a continuous function of μ.</p>
        <p>From now on, we assume that X is a separable Hilbert space and that p = 2. The results can most likely be generalised to all p &gt; 1 (see Ambrosio et al. [12, Section 10.2]), but we restrict to the quadratic case for simplicity. Theorem 3.2.13 below requires the application of Fubini's theorem in the form</p>
        <p>In order for this to even make sense, we need to have a meaning for "expectation" in the spaces L 2 (θ 0 ) and L 2 (θ 0 ), both of which are Banach spaces. There are several (nonequivalent) definitions for integrals in such spaces (Hildebrant [69]); the one which will be the most convenient for our needs is the Bochner integral.</p>
        <p>Definition 2.4.7 (Bochner Integral) Let B be a Banach space and let f : (Ω , F , P) → B be a simple random element taking values in B:</p>
        <p>Then the Bochner integral (or expectation) of f is defined by</p>
        <p>If f is measurable and there exists a sequence f n of simple random elements such that f nf → 0 almost surely and E f nf → 0, then the Bochner integral of f is defined as the limit</p>
        <p>The space of functions for which the Bochner integral is defined is the Bochner space L 1 (Ω ; B), but we will use neither this terminology nor the notation. It is not difficult to see that Bochner integrals are well-defined: the expectations do not depend on the representation of the simple functions nor on the approximating sequence, and the limit exists in B (because it is complete). More on Bochner integrals can be found in Then there exists a sequence of simple functions f n such that f n (ω)f (ω) → 0 for almost all ω if and only if f (Ω \ N ) is separable for some N ⊆ Ω of probability zero. In that case, f n can be chosen so that f n (ω) ≤ 2 f (ω) for all ω ∈ Ω .</p>
        <p>A proof can be found in [48, Lemma III.6.9], or on page 55 of the supplement. Functions satisfying this approximation condition are sometimes called strongly measurable or Bochner measurable. In view of the lemma, we will call them separately valued, since this is the condition that will need to be checked in order to define their integrals. Two remarks are in order. Firstly, if B itself is separable, then f (Ω ) will obviously be separable. Secondly, the set N ⊂ Ω \N on which (g n k ) does not converge to f may fail to be measurable, but must have outer probability zero (it is included in a measurable set of measure zero) [48,Lemma III.6.9]. This can be remedied by assuming that the probability space (Ω , F , P) is complete. It will not, however, be necessary to do so, since this measurability issue will not alter the Bochner expectation of f . Proposition 2.4.9 (Fubini for Optimal Maps) Let Λ be a random measure in W 2 (X ) such that EW 2 (δ 0 ,Λ ) &lt; ∞ and let θ 0 , θ ∈ W 2 (X ) such that t Λ θ 0 and t θ θ 0 exist (and are unique) with probability one. (For example, if θ 0 is absolutely continuous.) Then</p>
        <p>This holds by linearity when Λ is a simple random measure. The general case follows by approximation: the Wasserstein space is separable and so is the space of optimal maps, by Lemma 2.4.6, so we may apply Lemma 2.4.8 and approximate t Λ θ 0 by simple maps for which the equality holds by linearity. On page 56 of the supplement, we show that these simple maps can be assumed optimal, and give the full details.</p>
        <p>Our proof of Theorem 2.2.11 borrows heavily from Bolley et al. [29]. A similar result was obtained by Kloeckner [81], who also provides a lower bound of a similar order.</p>
        <p>The origins of Sect. 2.3 can be traced back to the seminal work of Jordan et al. [74], who interpret the Fokker-Planck equation as a gradient flow (where functionals defined on W 2 can be differentiated) with respect to the 2-Wasserstein metric. The Riemannian interpretation was (formally) introduced by Otto [99], and rigorously established by Ambrosio et al. [12] and others; see Villani [125,Chapter 15] for further bibliography and more details.</p>
        <p>Compatible measures (Definition 2.3.1) were implicitly introduced by Boissard et al. [28] in the context of admissible optimal maps where one defines families of gradients of convex functions (T i ) such that T -1 j • T i is a gradient of a convex function for any i and j. For (any) fixed measure γ ∈ C , compatibility of C is then equivalent to admissibility of the collection of maps {t μ γ } μ∈C . The examples we gave are also taken from [28].</p>
        <p>Lemma 2.3.3 is from Cuesta-Albertos et al. [38,Theorem 2.9] (see also Zemel and Panaretos [135]).</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>If H is a Hilbert space (or a closed convex subspace thereof) and x 1 ,...,x N ∈ H, then the empirical mean x N = N -foot_8 ∑ x i is the unique element of H that minimises the sum of squared distances from the x i 's. 1 That is, if we define</p>
        <p>is the unique minimiser of F. This is easily seen by "opening the squares" and writing</p>
        <p>The concept of a Fréchet mean (Fréchet [55]) generalises the notion of mean to a more general metric space by replacing the usual "sum of squares" with a "sum of squared distances", giving rise to the so-called Fréchet functional. A closely related notion is that of a Karcher mean (Karcher [78]), a term that describes stationary points of the sum of squares functional, when the latter is differentiable (see Sect. 3.1.6). Population versions of Fréchet means, assuming the space is endowed with a probability law, can also be defined, replacing summation by expectation with respect to that law.</p>
        <p>The online version of this chapter (https://doi.org/10.1007/ 978-3-030-38438-8 3) contains supplementary material.</p>
        <p>Fréchet means are perhaps the most basic object of statistical interest, and this chapter studies such means when the underlying space is the Wasserstein space W 2 . In general, existence and uniqueness of a Fréchet mean can be subtle, but we will see that the nature of optimal transport allows for rather clean statements in the case of Wasserstein space.</p>
        <p>As foretold in the preceding paragraph, the definition of a Fréchet mean requires the definition of an appropriate sum-of-squares functional, the Fréchet functional: Definition 3.1.1 (Empirical Fréchet Functional and Mean) The Fréchet functional associated with measures μ 1 , . . . ,</p>
        <p>In analysis, a Fréchet mean is often called a barycentre. We shall use the terminology of "Fréchet mean" that is arguably more popular in statistics. 2The factor 1/(2N) is irrelevant for the definition of Fréchet mean. It is introduced in order to have simpler expressions for the derivatives (Theorems 3.1.14 and 3.2.13) and to be compatible with a population version EW 2 2 (γ,Λ )/2 (see (3.3)). The first reference that deals with empirical Fréchet means in W 2 (R d ) is the seminal paper of Agueh and Carlier [2]. They treat the more general weighted Fréchet functional</p>
        <p>but, for simplicity, we shall focus on the case of equal weights. (If all the w i 's are rational, then the weighted functional can be encompassed in (3.1) by taking some of the μ i 's to be the same. The case of irrational w i 's is then treated with continuity arguments. Moreover, (3.3) encapsulates (3.1) as well as the weighted version when Λ can take finitely many values.)</p>
        <p>In [60], Gangbo and Świe ¸ch consider the following multimarginal Monge-Kantorovich problem. Let μ 1 ,..., μ N be N measures in W 2 (X ) and let Π (μ 1 ,..., μ N ) be the set of probability measures in X N having {μ i } N i=1 as marginals. The problem is to minimise</p>
        <p>The factor 1/(2N 2 ) is of course irrelevant for the minimisation and its purpose will be clarified shortly. If N = 2, we obtain the Kantorovich problem with quadratic cost. The probabilistic interpretation (as in Sect. 1.2) is that one is given random variables X 1 ,...,X N with marginal probability laws μ 1 ,..., μ N and seeks to construct a random vector</p>
        <p>for any other random vector Z = (Z 1 , . . . , Z N ) such that X i d = Z i . Intuitively, we seek a random vector with prescribed marginals but maximally correlated entries.</p>
        <p>We refer to elements of Π (μ 1 , . . . , μ N ) (equivalently, joint laws of X 1 , . . . , X N ) as multicouplings (of μ 1 , . . . , μ N ). Just like in the Kantorovich problem, there always exists an optimal multicoupling π.</p>
        <p>Let us now show how the multimarginal problem is equivalent to the problem of finding the Fréchet mean of μ 1 , . . . , μ N . The first thing to observe is that the objective function can be written as</p>
        <p>The next result shows that the Fréchet mean and the multicoupling problems are essentially the same.</p>
        <p>Then μ is a Fréchet mean of (μ 1 , . . . , μ N ) if and only if there exists an optimal multicoupling π ∈ W (X N ) of (μ 1 ,..., μ N ) such that μ = M#π, and furthermore</p>
        <p>Proof. Let π be an arbitrary multicoupling of (μ 1 ,..., μ N ) and set μ = M#π. Then (x → x i , M)#π is a coupling of μ i and μ, and therefore</p>
        <p>Summation over i gives F(μ) ≤ G(π) and so inf F ≤ inf G.</p>
        <p>For the other inequality, let μ ∈ W (X ) be arbitrary. For each i, let π i be an optimal coupling between μ and μ i . Invoking the gluing lemma (Ambrosio and Gigli [10, Lemma 2.1]), we may glue all π i 's using their common marginal μ. This procedure constructs a measure η on X N+1 with marginals μ 1 , . . . , μ N , μ and its relevant projection π is then a multicoupling of μ 1 ,..., μ N .</p>
        <p>Since X is a Hilbert space, the minimiser of y → ∑ x iy 2 is y = M(x). Thus</p>
        <p>In particular, inf F ≥ inf G and combining this with the established converse inequality we see that inf F = inf G. Observe also that the last displayed inequality holds as equality if and only if</p>
        <p>, and μ cannot be optimal. Finally, if π is optimal, then</p>
        <p>establishing optimality of μ = M#π and completing the proof.</p>
        <p>Since optimal couplings exist, we deduce that so do Fréchet means.</p>
        <p>x p dμ i (x), and when p &gt; 1 equality holds if and only if</p>
        <p>Proof. Let π be a multicoupling of μ 1 ,..., μ N such that μ = M N #π (Proposition 3.1.2). Then</p>
        <p>x p dμ i (x).</p>
        <p>The statement about equality follows from strict convexity of x → x p if p &gt; 1.</p>
        <p>A further corollary of Proposition 3.1.2 is a bound on the support: 4 The support of any Fréchet mean is included in the set</p>
        <p>In particular, if all the μ i 's are supported on a common convex set K, then so is any of their Fréchet means.</p>
        <p>The multimarginal formulation also yields a continuity property for the empirical Fréchet mean. Conditions for uniqueness will be given in the next subsection. Theorem 3.1.5 (Continuity of Fréchet Means) Suppose that W 2 (μ i k , μ i ) → 0 for i = 1, . . . , N and let μ k denote any Fréchet mean of (μ 1 k , . . . , μ N k ). Then (μ k ) stays in a compact set of W 2 (X ), and any limit point is a Fréchet mean of (μ 1 , . . . , μ N ).</p>
        <p>In particular, if μ 1 ,..., μ N have a unique Fréchet mean μ, then μ k → μ in W 2 (X ).</p>
        <p>Proof. We sketch the steps of the proof here, with the full details given on page 63 of the supplement.</p>
        <p>Step 1: tightness of (μ k ). This is true because the collection of multicouplings is tight, and the mean function M is continuous.</p>
        <p>Step 2: weak limits are limits in W 2 (X ). This holds because the mean function has linear growth.</p>
        <p>Step 3: the limit is a Fréchet mean of (μ 1 ,..., μ N ). From Corollary 3.1.3, it follows that μ k must be sought on some fixed bounded set in W 2 (X ). On such sets, the Fréchet functionals are uniformly Lipschitz, so their minimisers converge as well.</p>
        <p>A general situation in which Fréchet means are unique is when the Fréchet functional is strictly convex. In the Wasserstein space, this requires some regularity, but weak convexity holds in general. Absolutely continuous measures on infinitedimensional X are defined in Definition 1.6.4.</p>
        <p>When Λ is absolutely continuous, the inequality is strict unless t ∈ {0, 1} or γ 1 = γ 2 .</p>
        <p>Remark 3.1.7 The Wasserstein distance is not convex along geodesics. That is, if we replace the linear interpolant tγ Proof. Let π i ∈ Π (γ i ,Λ ) be optimal and notice that the linear interpolant As a corollary, we deduce that the Fréchet mean is unique if one of the measures μ i is absolutely continuous, and this extends to the population version (see Proposition 3.2.7).</p>
        <p>We conclude this subsection by stating an important regularity property in the Euclidean case. See Agueh and Carlier [2, Proposition 5.1] for a proof.</p>
        <p>When X = R, there is a simple expression for the Fréchet mean because W 2 (R) can be imbedded in a Hilbert space. Indeed, recall that</p>
        <p>(see Sect.</p>
        <p>The Fréchet mean is thus unique. This is no longer true in higher dimension, unless some regularity is imposed on the measures (Proposition 3.2.7). Boissard et al. [28] noticed that compatibility of μ 1 ,..., μ N according to Definition 2.3.1 allows for a simple solution to the Fréchet mean problem, as in the one-dimensional case. Recall from Proposition 3.1.2 that this is equivalent to the multimarginal problem. Returning to the original form of G, we obtain an easy lower bound for any π ∈ Π (μ 1 ,..., μ N ):</p>
        <p>because the (i, j)-th marginal of π is a coupling of μ i and μ j . Thus, if equality above holds for π, then π is optimal and M#π is the Fréchet mean by Proposition 3.1.2. This is indeed the case for π = (i, t</p>
        <p>μ 1 )#μ 1 because the compatibility gives:</p>
        <p>We may thus conclude, in a slightly more general form (γ was μ 1 above):</p>
        <p>Theorem 3.1.9 (Fréchet Mean of Compatible Measures) Suppose that {γ, μ 1 , . . . , μ N } are compatible measures. Then</p>
        <p>is the Fréchet mean of (μ 1 ,..., μ N ).</p>
        <p>A population version is given in Theorem 5.5.3.</p>
        <p>Agueh and Carlier [2] provide a useful sufficient condition for γ to be the Fréchet mean. When</p>
        <p>Summation over i gives the result.</p>
        <p>A population version of this result, based on similar calculations, is given in Theorem 4.2.4.</p>
        <p>The next two results are formulated in R d because then the converse of Proposition 3.1.10 is proven to be true. If one could extend [2, Proposition 3.8] to any separable Hilbert X , then the two lemmata below will hold with R d replaced by X . The simple proofs are given on page 66 of the supplement. Lemma 3.1.11 (Independent Fréchet Means) Let μ 1 , . . . , μ N and ν 1 , . . . , ν N be absolutely continuous measures in W 2 (R d 1 ) and W 2 (R d 2 ) with Fréchet means μ and ν, respectively. Then the independent coupling μ ⊗ ν is the Fréchet mean of</p>
        <p>By induction (or a straightforward modification of the proof), one can show that the Fréchet mean of (μ i ⊗ ν i ⊗ ρ i ) is μ ⊗ ν ⊗ ρ, and so on. Lemma 3.1.12 (Rotated Fréchet Means) If μ is the Fréchet mean of the absolutely continuous measures μ 1 , . . . , μ N and U is orthogonal, then U#μ is the Fréchet mean of U#μ 1 ,...,U#μ N .</p>
        <p>Since we seek to minimise the Fréchet functional F, it would be helpful if F were differentiable, because we could then find at least local minima by solving the equation F = 0. This observation of Karcher [78] leads to the notion of Karcher mean.</p>
        <p>Definition 3.1.13 (Karcher Mean) Let F be a Fréchet functional associated with some random measure Λ in W 2 (X ). Then γ is a Karcher mean for Λ if F is differentiable at γ and F (γ) = 0.</p>
        <p>Of course, if γ is a Fréchet mean for the random measure Λ and F is differentiable at γ, then F (γ) must vanish. In this subsection, we build upon the work of Ambrosio et al. [12] and determine the derivative of the Fréchet functional. This will not only allow for a simple characterisation of Karcher means in terms of the optimal maps t Λ γ (Proposition 3.2.14), but will also be the cornerstone of the construction of a steepest descent algorithm for empirical calculation of Fréchet means. The differentiability holds at the population level too (Theorem 3.2.13).</p>
        <p>It turns out that the tangent bundle structure described in Sect. 2.3 gives rise to a differentiable structure in the Wasserstein space. Fix μ 0 ∈ W 2 (X ) and consider the function</p>
        <p>Ambrosio et al. [12,Corollary 10.2.7] show that when γ is absolutely continuous, lim</p>
        <p>Parts of the proof of this result (the limit superior above is ≤ 0; the limit inferior is bounded below) are reproduced in Proposition 3.2.12. The integral above can be seen as the inner product t</p>
        <p>in the space L 2 (γ) that includes as a (closed) subspace the tangent space Tan γ . In terms of this inner product and the log map, we can write</p>
        <p>so that F 0 is Fréchet-differentiablefoot_13 at γ with derivative</p>
        <p>By linearity, one immediately obtains: Theorem 3.1.14 (Gradient of the Fréchet Functional) Fix a collection of measures μ 1 , . . . , μ N ∈ W 2 (X ). When γ ∈ W 2 (X ) is absolutely continuous, the Fréchet functional</p>
        <p>is Fréchet-differentiable and</p>
        <p>It follows from this that an absolutely continuous γ ∈ W 2 (X ) is a Karcher mean if and only if the average of the optimal maps is the identity. If in addition one μ i is absolutely continuous with bounded density, then the Fréchet mean μ is absolutely continuous by Proposition 3.1.8, so it is a Karcher mean. The result extends to the population version; see Proposition 3.2.14.</p>
        <p>It may happen that a collection μ 1 ,..., μ N of absolutely continuous measures have a Karcher mean that is not a Fréchet mean; see Álvarez-Esteban et al. [9, Example 3.1] for an example in R 2 . But a Karcher mean γ is "almost" a Fréchet mean in the following sense. By Proposition 3.2.14, N -1 ∑ t</p>
        <p>If, on the other hand, the equality holds for all x ∈ X , then γ is the Fréchet mean by taking integrals and applying Proposition 3.1.10. One can hope that under regularity conditions, the γ-almost sure equality can be upgraded to equality everywhere. Indeed, this is the case: Theorem 3.1.15 (Optimality Criterion for Karcher Means) Let U ⊆ R d be an open convex set and let μ 1 ,..., μ N ∈ W 2 (R d ) be probability measures on U with bounded strictly positive densities g 1 , . . . , g N . Suppose that an absolutely continuous Karcher mean γ is supported on U with bounded strictly positive density f there. Then γ is the Fréchet mean of μ 1 , . . . , μ N if one of the following holds:</p>
        <p>1. U = R d and the densities f , g 1 ,...,g N are of class C 0,α for some α &gt; 0; 2. U is bounded and the densities f , g 1 ,...,g N are bounded below on U.</p>
        <p>Proof. The result exploits Caffarelli's regularity theory for Monge-Ampère equations in the form of Theorem 1.6.7. In the first case, there exist C 1 (in fact, C 2,α ) convex potentials ϕ i on R d with t</p>
        <p>and open by continuity. It is therefore empty, so F (γ) = 0 everywhere, and γ is the Fréchet mean (see the discussion before the theorem).</p>
        <p>In the second case, by the same argument we have ∑ t μ i γ (x)/N = x for all x ∈ U. Since U is convex, there must exist a constant C such that ∑ ϕ i (x) = C +N x 2 /2 for all x ∈ U, and we may assume without loss of generality that C = 0. If one repeats the proof of Proposition 3.1.10, then F(γ) ≤ F(θ ) for all θ ∈ P(U). By continuity considerations, the inequality holds for all θ ∈ P(U) (Theorem 2.2.7) and since U is closed and convex, γ is the Fréchet mean by Corollary 3.1.3.</p>
        <p>In this section, we extend the notion of empirical Fréchet mean to the population level, where Λ is a random element in W 2 (X ) (a measurable mapping from a probability space to W 2 (X )). This requires a different strategy, since it is not clear how to define the analogue of the multicouplings at that level of abstraction. However, it is important to point out that when there is more structure in Λ , multicouplings can be defined as laws of stochastic processes; see Pass [102] for a detailed account of the problem in this case.</p>
        <p>In analogy with (3.1), we define: Definition 3.2.1 (Population Fréchet Mean) Let Λ be a random measure in W 2 (X ). The Fréchet mean of Λ is the minimiser (if it exists and is unique) of the Fréchet functional</p>
        <p>Since W 2 is continuous and nonnegative, the expectation is well-defined.</p>
        <p>Existence and uniqueness of Fréchet means on a general metric space M are rather delicate questions. Usually, existence proofs are easier: for example, since the Fréchet functional F is continuous on M (as we show below), one often invokes local compactness of M in order to establish existence of a minimiser. Unfortunately, a different strategy is needed when M = W 2 (X ), because the Wasserstein space is not locally compact (Proposition 2.2.9). The first thing to notice is that F is indeed continuous (this is clear for the empirical version). This is a consequence of the triangle inequality and holds when W 2 (X ) is replaced by any metric space.</p>
        <p>Proof. Assume that F is finite at γ. If θ is any other measure in W 2 (X ), write</p>
        <p>Since F(γ) &lt; ∞, this shows that F is finite everywhere and the right-hand side vanishes as θ → γ in W 2 (X ). Now that we know that F is continuous, the same upper bound shows that it is in fact locally Lipschitz.</p>
        <p>Example: let (a n ) be a sequence of positive numbers that sum up to one. Let x n = 1/a n and suppose that Λ equals δ {x n } ∈ W 2 (R) with probability a n . Then</p>
        <p>and by Lemma 3.2.2 F is identically infinite. Henceforth, we say that F is finite when the condition in Lemma 3.2.2 holds.</p>
        <p>Using the lower semicontinuity (2.5), one can prove existence on R d rather easily. (The empirical means exist even in infinite dimensions by Corollary 3.1.3.)</p>
        <p>The Fréchet functional associated with any random measure Λ in W 2 (R d ) admits a minimiser.</p>
        <p>Proof. The assertion is clear if F is identically infinite. Otherwise, let (γ n ) be a minimising sequence. We wish to show that the sequence is tight. Define L = sup n F(γ n ) &lt; ∞ and observe that since x ≤ 1 + x 2 for all x ∈ R,</p>
        <p>Since closed and bounded sets in R d are compact, it follows that (γ n ) is a tight sequence. We may assume that γ n → γ weakly, then use (2.5) and Fatou's lemma to obtain</p>
        <p>Thus, γ is a minimiser of F, and existence is established.</p>
        <p>When X is an infinite-dimensional Hilbert space, existence still holds under a compactness assumption. We first prove a result about the support of the Fréchet mean. At the empirical level, one can say more about the support (see Corollary 3.1.4).</p>
        <p>Let Λ be a random measure in W 2 (X ) and let K ⊆ X be a convex closed set such that P[Λ (K) = 1] = 1. If γ minimises F, then γ(K) = 1.</p>
        <p>The proof amounts to a simple projection argument; see page 70 in the supplement.</p>
        <p>If there exists a compact convex K satisfying the hypothesis of Proposition 3.2.4, then the Fréchet functional admits a minimiser supported on K.</p>
        <p>Proof. Proposition 3.2.4 allows us to restrict the domain of F to W 2 (K), the collection of probability measures supported on K. Since this set is compact in W 2 (X ) (Corollary 2.2.5), the result follows from continuity of F.</p>
        <p>From the convexity (3.2), one obtains a simple criterion for uniqueness. See Definition 1.6.4 for absolute continuity in infinite dimensions.</p>
        <p>Let Λ be a random measure in W 2 (X ) with finite Fréchet functional. If Λ is absolutely continuous with positive (inner) probability, then the Fréchet mean of Λ is unique (if it exists). Remark 3.2.8 It is not obvious that the set of absolutely continuous measures is measurable in W 2 (X ). We assume that there exists a Borel set A ⊂ W 2 (X ) such that P(Λ ∈ A) &gt; 0 and all measures in A are absolutely continuous.</p>
        <p>Proof. By taking expectations in (3.2), one sees that F is convex on W 2 (X ) with respect to linear interpolants. From Proposition 3.1.6, we conclude that</p>
        <p>As F was already shown to be weakly convex in any case, it follows that P(Λ absolutely continuous) &gt; 0 =⇒ F strictly convex.</p>
        <p>Since strictly convex functionals have at most one minimiser, this completes the proof.</p>
        <p>We state without proof an important consistency result (Le Gouic and Loubes [87, Theorem 3]). Since W 2 (X ) is a complete and separable metric space, we can define the "second degree" Wasserstein space W 2 (W 2 (X )). The law of a random measure Λ is in W 2 (W 2 (X )) if and only if the corresponding Fréchet functional is finite. Theorem 3.2.9 (Consistency of Fréchet Means) Let Λ n ,Λ be random measures in W 2 (R d ) with finite Fréchet functionals and laws</p>
        <p>, then any sequence λ n of Fréchet means of Λ n has a W 2 -limit point λ , which is a Fréchet mean of Λ .</p>
        <p>See the Bibliographical Notes for a more general formulation.</p>
        <p>Let Λ be a random measure in W 2 (R d ) with finite Fréchet functional and let Λ 1 , . . . be a sample from Λ . Assume λ is the unique Fréchet mean of Λ (see Proposition 3.2.7). Then almost surely, the sequence of empirical Fréchet means of Λ 1 ,...,Λ n converges to λ . Proof. Let P be the law of Λ and let P n be its empirical counterpart (a random element in W 2 (W 2 (R d )). Like in the proof of Proposition 2.2.6 (with X replaced by the complete separable metric space W 2 (R d )), almost surely</p>
        <p>and Theorem 3.2.9 applies.</p>
        <p>Under a compactness assumption, one can give a direct proof for the law of large numbers as in Theorem 3.1.5. This is done on page 71 in the supplement.</p>
        <p>As a generalisation of the empirical version, we have: Theorem 3.2.11 (Fréchet Means in W 2 (R)) Let Λ be a random measure in W 2 (R) with finite Fréchet functional. Then the Fréchet mean of Λ is the unique measure λ with quantile function F -1 λ (t) = EF -1 Λ (t), t ∈ (0, 1).</p>
        <p>Proof. Since L 2 (0, 1) is a Hilbert space, the random element F -1 Λ ∈ L 2 (0, 1) has a unique Fréchet mean g ∈ L 2 (0, 1), defined by the relations g, f = E F -1 Λ , f for all f ∈ L 2 (0, 1). On page 72 of the supplement, we show that g can be identified with F -1 λ .</p>
        <p>Interestingly, no regularity is needed in order for the Fréchet mean to be unique. This is not the case for higher dimensions, see Proposition 3.2.7. If there is some regularity, then one can state Theorem 3.2.11 in terms of optimal maps, because F -1 Λ is the optimal map from Leb| [0,1] to Λ . If γ ∈ W 2 (R) is any absolutely continuous (or even just continuous) measure, then Theorem 3.2.11 can be stated as follows: the Fréchet mean of Λ is the measure [Et Λ γ ]#γ. A generalisation of this result to compatible measures (Definition 2.3.1) can be carried out in the same way, since compatible measures are imbedded in a Hilbert space, using the Bochner integrals for the definition of the expected optimal maps (see Sect. 2.4).</p>
        <p>We now use the Fubini result (Proposition 2.4.9) in order to extend the differentiability of the Fréchet functional to the population version. This will follow immediately if we can interchange the expectation and the derivative in the form</p>
        <p>In order to do this, we will use dominated convergence in conjunction with uniform bounds on the slopes</p>
        <p>(3.4) Proposition 3.2.12 (Slope Bounds) Let θ 0 , Λ , and θ be probability measures with θ 0 absolutely continuous, and set δ = W 2 (θ , θ 0 ). Then</p>
        <p>where u is defined by (3.4)</p>
        <p>Thus, the Fréchet derivative of F can be identified with the map -(Et Λ θ 0i) in the tangent space at θ 0 , a subspace of L 2 (θ 0 ).</p>
        <p>Proof. Introduce the slopes u(θ ,Λ ) defined by (3.4). Then for all Λ ,u(θ ,Λ ) → 0 as W 2 (θ , θ 0 ) → 0, by the differentiability properties established above. Let us show that Eu(θ ,Λ ) → 0 as well. By Proposition 3.2.12, the expectation of u is bounded above by a constant that does not depend on Λ , and below by the negative of</p>
        <p>Both expectations are finite by the hypothesis on Λ because the Fréchet functional is finite. The dominated convergence theorem yields</p>
        <p>The measurability of the integral and the result then follow from Fubini's theorem (see Proposition 2.4.9).</p>
        <p>The characterisation of Karcher means follows immediately from Theorem 3.2.13. The other statement is that the derivative vanishes at the minimum, which is fairly obvious intuitively; see page 73 in the supplement.</p>
        <p>Proposition 3.1.2 is essentially due to Agueh and Carlier [2, Proposition 4.2], who show it on R d (see also Zemel and Panaretos [134,Theorem 2]). An earlier result in a compact setting can be found in Carlier and Ekeland [33]. The formulation given here is from Masarotto et al. [91]. A more general version is provided by Le Gouic and Loubes [87,Theorem 8].</p>
        <p>Lemmata 3.1.11 and 3.1.12 are from [135], but were known earlier (e.g., Bonneel et al. [30]).</p>
        <p>Proposition [22], who also show the law of large numbers (Corollary 3.2.10) and deal with the one-dimensional setup (Theorem 3.2.11) in a compact setting. Section 2.4 appears to be new, but see the discussion in its beginning for other measurability results.</p>
        <p>Barycentres can be defined for any p ≥ 1 as the measures minimising μ → EW p p (Λ , μ). (Strictly speaking, these are not Fréchet means unless p = 2.) Le Gouic and Loubes [87] show Proposition 3.2.3 and Theorem 3.2.9 in this more general setup, where R d can be replaced by any separable locally compact geodesic space.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>Why is it relevant to construct the Fréchet mean of a collection of measures with respect to the Wasserstein metric? A simple answer is that this kind of average will often express a more natural notion of "typical" realisation of a random probability distribution than an arithmetic average. 1 Much more can be said, however, in that the Wasserstein-Fréchet mean and the closely related notion of an optimal multicoupling arise canonically as the appropriate framework for the formulation and solution to the problem of separation of amplitude and phase variation of a point process. It would almost seem that Wasserstein-Fréchet means were "made" for precisely this problem.</p>
        <p>When analysing the (co)variation of a real-valued stochastic process {Y (x) : x ∈ K} over a convex compact domain K, it can be broadly said that one may distinguish two layers of variation:</p>
        <p>• Amplitude variation. This is the "classical" variation that one would also encounter in multivariate analysis, and refers to the stochastic fluctuations around a mean level, usually encoded in the covariance kernel, at least up to second order.</p>
        <p>In short, this is variation "in the y-axis" (ordinate).</p>
        <p>The online version of this chapter (https://doi.org/10.1007/ 978-3-030-38438-8 4) contains supplementary material.</p>
        <p>• Phase variation. This is a second layer of non-linear variation peculiar to continuous domain stochastic processes, and is rarely-if ever-encountered in multivariate analysis. It arises as the result of random changes (or deformations) in the time scale (or the spatial domain) of definition of the process. It can be conceptualised as a composition of the stochastic process with a random transformation (warp map) acting on its domain. This is variation "in the x-axis" (abscissa).</p>
        <p>The terminology on amplitude/phase variation is adapted from random trigonometric functions, which may vary in amplitude (oscillations in the range of the function) or phase (oscillations in the domain of the function). Failing to properly account for the superposition of these two forms of variation may entirely distort the findings of a statistical analysis of the random function (see Sect. 4.1.1). Consequently, it is an important problem to be able to separate the two, thus correctly accounting for the distinct contribution of each. The problem of separation is also known as that of registration (Ramsay and Li [108]), synchronisation (Wang and Gasser [129]), or multireference alignment (Bandeira et al. [16]), though in some cases these terms refer to a simpler problem where there is no amplitude variation at all. Phase variation naturally arises in the study of random phenomena where there is no absolute notion of time or space, but every realisation of the phenomenon evolves according to a time scale that is intrinsic to the phenomenon itself, and (unfortunately) unobservable. Processes related to physiological measurements, such as growth curves and neuronal signals, are usual suspects. Growth curves can be modelled as continuous random functions (functional data), whereas neuronal signals are better modelled as discrete random measures (point processes). We first describe amplitude/phase variation in the former 2 case, as that is easier to appreciate, before moving on to the latter case, which is the main subject of this chapter.</p>
        <p>can, broadly speaking, have two types of variation. The first, amplitude variation, results from Y (x) being a random variable for every x and describes its fluctuations around the mean level m(x) = EY (x), usually encoded by the variance varY (x). For this reason, it can be referred to as "variation in the y-axis". More 2 As the functional case will only serve as a motivation, our treatment of this case will mostly be heuristic and superficial. Rigorous proofs and more precise details can be found in the books by Ferraty and Vieu [51], Horváth and Kokoszka [70], or Hsing and Eubank [71]. The notion of amplitude and phase variation is discussed in the books by Ramsay and Silverman [109,110] that are of a more applied flavour. One can also consult the review by Wang et al. [127], where amplitude and phase variation are discussed in Sect. 5.2.</p>
        <p>generally, for any finite set x 1 , . . . , x n , the n × n covariance matrix with entries κ(x i , x j ) = cov[Y (x i ),Y (x j )] encapsulates (up to second order) the stochastic deviations of the random vector (Y (x 1 ),...,Y (x n )) from its mean, in analogy with the multivariate case. Heuristically, one then views amplitude variation as the collection κ(x, y) for x, y ∈ K in a sense we discuss next.</p>
        <p>One typically views Y as a random element in the separable Hilbert space L 2 (K), assumed to have E Y 2 &lt; ∞ and continuous sample paths, so that in particular Y (x) is a random variable for all x ∈ K. Then the mean function m(x) = EY (x), x ∈ K and the covariance kernel</p>
        <p>x, y ∈ K are well-defined and finite; we shall assume that they are continuous, which is equivalent to Y being mean-square continuous:</p>
        <p>The covariance kernel κ gives rise to the covariance operator R :</p>
        <p>a self-adjoint positive semidefinite Hilbert-Schmidt operator on L 2 (K). The justification to this terminology is the observation that when m = 0, for all bounded</p>
        <p>and so, without the restriction to</p>
        <p>The covariance operator admits an eigendecomposition</p>
        <p>A major feature in this expansion is the separation of the functional part from the stochastic part: the functions φ k (x) are deterministic; the random variables ξ k are scalars. This separation actually holds for any orthonormal basis; the role of choosing the eigenbasis of R is making ξ k uncorrelated:</p>
        <p>vanishes when k = l and equals r k otherwise. For this reason, it is not surprising that using as φ k the eigenfunctions yields the optimal representation of Y . Here, optimality is with respect to truncations: for any other basis (ψ k ) and any M,</p>
        <p>so that (φ k ) provides the best finite-dimensional approximation to Y . The approximation error on the right-hand side equals</p>
        <p>and depends on how quickly the eigenvalues of R decay.</p>
        <p>One carries out inference for m and κ on the basis of a sample Y 1 ,...,Y n by</p>
        <p>from which one proceeds to estimate R and its eigendecomposition.</p>
        <p>We have seen that amplitude variation in the sense described above is linear and dealt with using linear operations. There is another, qualitatively different type of variation, phase variation, that is non-linear and does not have an obvious finitedimensional analogue. It arises when in addition to the randomness in the values Y (x) itself, an extra layer of stochasticity is present in its domain of definition. In mathematical terms, there is a random invertible warp function (sometimes called deformation or warping) T : K → K and instead of Y (x), one observes realisations from</p>
        <p>For this reason, phase variation can be viewed as "variation in the x-axis". When d = 1, the set K is usually interpreted as a time interval, and then the model stipulates that each individual has its own time scale. Typically, the warp function is assumed to be a homeomorphism of K independent of Y and often some additional smoothness is imposed, say T ∈ C 2 . One of the classical examples is growth curves of children, of which a dataset from the Berkeley growth study (Jones and Bayley [73]) is shown in Fig. 4.1. The curves are the derivatives of the height of a sample of ten girls as a function of time, from birth until age 18. One clearly notices the presence of the two types of variation in the figure. The initial velocity for all children is the highest immediately or shortly after birth, and in most cases decreases sharply during the first 2 years. Then follows a period of acceleration for another year or so, and so on. Despite presenting qualitatively similar behaviour, the curves differ substantially not only in the magnitude of the peaks but also in their location. For instance, one red curve has a local minimum at the age of three, while a green one has a local maximum at almost that same time point. It is apparent that if one tries to estimate the mean function by averaging the curves at each time x, the shape of the resulting estimate would look very different from each of the curves. Thus, this pointwise averaging (known as the cross-sectional mean) fails to represent the typical behaviour. This phenomenon is seen more explicitly in the next example. (Strictly speaking, x → x + B is not from [0, 1] to itself; for illustration purposes, we assume in this example that K = R.) The random variable A generates the amplitude variation, while B represents the phase variation. In Fig. 4.2, we plot four realisations and the resulting empirical means for the two extreme scenarios where B = 0 (no phase variation) or A = 1 (no amplitude variation). In the left panel of the figure, we see that the sample mean (in thick blue) lies between the observations and has a similar form, so can be viewed as the curve representing the typical realisation of the random curve. This is in contrast to the right panel, where the mean is qualitatively different from all curves in the sample: though periodicity is still present, the peaks and troughs have been flattened, and the sample mean is much more diffuse than any of the observations. The phenomenon illustrated in Fig. 4.2 is hardly surprising, since as mentioned earlier amplitude variation is linear while phase variation is not, and taking sample means is a linear operation. Let us see in formulae how this phenomenon occurs. When A = 1 we have</p>
        <p>Since B is symmetric the second term vanishes, and unless B is trivial the expectation of the cosine is smaller than one in absolute value. Consequently, the expectation of Ỹ (x) is the original function sin 8πx multiplied by a constant of magnitude strictly less than one, resulting in peaks of smaller magnitude.</p>
        <p>In the general case, where Ỹ (x) = Y (T -1 (x)) and Y and T are independent, we have</p>
        <p>From this, several conclusions can be drawn. Let μ = μ(T -1 (x)) be the conditional mean function given T . Then the value of the mean function itself, E μ, at x 0 is determined not by a single point, say x, but rather by all the values of m at the possible outcomes of T -1 (x). In particular, if x 0 was a local maximum for m, then E[ μ(x 0 )] will typically be strictly smaller than m(x 0 ); the phase variation results in smearing m.</p>
        <p>At this point an important remark should be made. Whether or not phase variation is problematic depends on the specific application. If one is interested indeed in the mean and covariance functions of Ỹ , then the standard empirical estimators will be consistent, since Ỹ itself is a random function. But if it is rather m, the mean of Y , that is of interest, then the confounding of the amplitude and phase variation will lead to inconsistency. This can also be seen from the formula</p>
        <p>The above series is not the Karhunen-Loève expansion of Ỹ ; the simplest way to notice this is the observation that φ k (T -1 (x)) includes both the functional component φ k and the random component T -1 (x). The true Karhunen-Loève expansion of Ỹ will in general be qualitatively very different from that of Y , not only in terms of the mean function but also in terms of the covariance operator and, consequently, its eigenfunctions and eigenvalues. As illustrated in the trigonometric example, the typical situation is that the mean E Ỹ is more diffuse than m, and the decay of the eigenvalues rk of the covariance operator is slower than that of r k ; as a result, one needs to truncate the sum at high threshold in order to capture a substantial enough part of the variability. In the toy example (4.1), the Karhunen-Loève expansion has a single term besides the mean if B = 0, while having two terms if A = 1.</p>
        <p>When one is indeed interested in the mean m and the covariance κ, the random function T pertaining to the phase variation is a nuisance parameter. Given a sample Ỹi = Y i • T -1 i , i = 1, . . . , n, there is no point in taking pointwise means of Ỹi , because the curves are misaligned;</p>
        <p>To overcome this difficulty, one seeks estimators T i such that</p>
        <p>is approximately Y i (x). In other words, one tries to align the curves in the sample to have a common time scale. Such a procedure is called curve registration. Once registration has been carried out, one proceeds the analysis on Y i (x) assuming only amplitude variation is now present: estimate the mean m by</p>
        <p>and the covariance κ by its analogous counterpart. Put differently, registering the curves amounts to separating the two types of variation. This step is crucial regardless of whether the warp functions are considered as nuisance or an analysis of the warp functions is of interest in the particular application.</p>
        <p>There is an obvious identifiability problem in the model Ỹ = Y • T -1 . If S is any (deterministic) invertible function, then the model with (Y, T ) is statistically indistinguishable from the model with (Y • S, T • S). It is therefore often assumed that ET = i is the identity and in addition, in nearly all application, that T is monotonically increasing (if d = 1).</p>
        <p>Discretely observed data. One cannot measure the height of person at every single instant of her life. In other words, it is rare in practice that one has access to the entire curve. A far more common situation is that one observes the curves discretely, i.e., at a finite number of points. The conceptually simplest setting is that one has access to a grid x 1 ,...,x J ∈ K, and the data come in the form ỹij = Ỹi (t j ), possibly with measurement error. The problem is to find, given ỹij , consistent estimators of T i and of the original, aligned functions Y i .</p>
        <p>In the bibliographic notes, we review some methods for carrying out this separation of amplitude and phase variation. It is fair to say that no single registration method arises as the canonical solution to the functional registration problem. Indeed, most need to make additional structural and/or smoothness assumptions on the warp maps, further to the basic identifiability conditions requiring that T be increasing and that ET equal the identity. We will eventually see that, in contrast, the case of point processes (viewed as discretely observed random measures) admits a canonical framework, without needing additional assumptions.</p>
        <p>A point process is the mathematical object that represents the intuitive notion of a random collection of points in a space X . It is formally defined as a measurable map Π from a generic probability space into the space of (possibly infinite) Borel integer-valued measures of X in such a way that Π (B) is a measurable real-valued random variable for all Borel subsets B of X . The quantity Π (B) represents the random number of points observed in the set B. Among the plethora of books on point processes, let us mention Daley and Vere-Jones [41] and Karr [79]. Kallenberg [75] treats more general objects, random measures, of which point processes are a peculiar special case. We will assume for convenience that Π is a measure on a compact subset K ⊂ R d .</p>
        <p>4.1 Amplitude and Phase Variation 83 Amplitude variation of Π can be understood in analogy with the functional case. One defines the mean measure</p>
        <p>the latter being a finite signed Borel measure on K. Just like in the functional case, these two objects encapsulate the (second-order) amplitude variation 3 properties of the law of Π . Given a sample Π 1 , . . . , Π n of independent point processes distributed as Π , the natural estimators</p>
        <p>are consistent and the former asymptotically normal [79,Proposition 4.8]. Phase variation then pertains to a random warp function T : K → K (independent of Π ) that deforms Π : if we denote the points of Π by x 1 , . . . , x K (with K random), then instead of (x i ), one observes T (x 1 ),...,T (x K ). In symbols, this means that the data arise as Π = T #Π . We refer to Π as the original point processes, and Π as the warped point processes. An example of 30 warped and unwarped point processes is shown in Fig. 4.3. The point patterns in both panels present a qualitatively similar structure: there are two peaks of high concentration of points, while few points appear between these peaks. The difference between the two panels is in the position and concentration of those peaks. In the left panel, only amplitude variation is present, and the location/concentration of the peaks is the same across all observations. In contrast, phase variation results in shifting the peaks to different places for each of the observations, while also smearing or sharpening them. Clearly, estimation of the mean measure of a subset A by averaging the number of observed points in A would not be satisfactory as an estimator of λ when carried out with the warped data. As in the functional case, it will only be consistent for the measure λ defined by</p>
        <p>A ⊆ X , and λ = E[T #λ ] misses most (or at least a significant part) of the bimodal structure of λ and is far more diffuse.</p>
        <p>Since Π and T are independent, the conditional expectation of Π given T is</p>
        <p>Consequently, we refer to Λ = T #λ as the conditional mean measure. The problem of separation of amplitude and phase variation can now be stated as follows. On the basis of a sample Π 1 ,..., Π n , find estimators of (T i ) and (Π i ). Registering the point processes amounts to constructing estimators, registration maps T -1 i , such that the aligned points</p>
        <p>are close to the original points Π i . As in the functional case there are problems with identifiability: the model (Π , T ) cannot be distinguished from the model (S#Π , T • S -1 ) for any invertible S : K → K. It is thus natural to assume that ET is the identity map 4 (otherwise set S = ET , i.e., replace Π by [ET ]#Π and T by T • [ET ] -1 ).</p>
        <p>Constraining T to have mean identity is nevertheless not sufficient for the model Π = T #Π to be identifiable. The reason is that given the two point sets Π and Π , there are many functions that push forward the latter to the former. This ambiguity can be dealt with by assuming some sort of regularity or parsimony for T . For example, when K = [a, b] is a subset of the real line, imposing T to be monotonically increasing guarantees its uniqueness. In multiple dimensions, there is no obvious analogue for increasing functions. One possible definition is the monotonicity described in Sect. 1.7.2:</p>
        <p>This property is rather weak in a sense we describe now. Let K ⊆ R 2 and write y ≥ x if and only if y i ≥ x i for i = 1, 2. It is natural to expect the deformations to maintain the lexicographic order in R 2 :</p>
        <p>If we require in addition that the ordering must be preserved for all quadrants: for z = T (x) and w = T (y)</p>
        <p>then monotonicity is automatically satisfied. In that sense, it is arguably not very restrictive.</p>
        <p>Monotonicity is weaker than cyclical monotonicity (see (1.10) with y i = T (x i )), which is itself equivalent to the property of being the subgradient of a convex function. But if extra smoothness is present and T is a gradient of some function φ : K → R, then φ must be convex and T is then cyclically monotone. Consequently, we will make the following assumptions:</p>
        <p>• the expected value of T is the identity; • T is a gradient of a convex function.</p>
        <p>In the functional case, at least on the real line, these two conditions are imposed on the warp functions in virtually all applications, often accompanied with additional assumptions about smoothness of T , its structural properties, or its distance from the identity. In the next section, we show how these two conditions alone lead to the Wasserstein geometry and open the door to consistent, fully nonparametric separation of the amplitude and phase variation.</p>
        <p>A first hint to the relevance of Wasserstein metrics in W p (X ) for deformations of the space X is that for all p ≥ 1 and all x, y ∈ X , W p (δ x , δ y ) = xy , where δ x is as usual the Dirac measure at x ∈ X . This is in contrast to metrics such as the bounded Lipschitz distance (that metrises weak convergence) or the total variation distance on P(X ). Recall that these are defined by</p>
        <p>In words, the total variation metric "does not see the geometry" of the space X . This is less so for the bounded Lipschitz distance that does take small distances into account but not large ones. Another property (shared by BL and TV) is equivariance with respect to translations. It is more convenient to state it using the probabilistic formalism of Sect. 1.2. Let X ∼ μ and Y ∼ ν be random elements in X , a be a fixed point in X , X = X + a and Y = Y + a. Joint couplings Z = (X ,Y ) are precisely those that take the form (a, a) + Z for a joint coupling Z = (X,Y ). Thus</p>
        <p>where δ a is a Dirac measure at a and * denotes convolution.</p>
        <p>This carries over to Fréchet means in an obvious way.</p>
        <p>Let Λ be a random measure in W 2 (X ) with finite Fréchet functional and a ∈ X . Then γ is a Fréchet mean of Λ if and only if γ * δ a is a Fréchet mean of Λ * δ a .</p>
        <p>The result holds for other values of p, in the formulation sketched in the bibliographic notes of Chap. 2. In the quadratic case, one has a simple extension to the case where only one measure is translated. Denote the first moment (mean</p>
        <p>4.2 Wasserstein Geometry and Phase Variation 87 (When X is infinite-dimensional, this can be defined as the unique element m ∈ X satisfying m, y = X x, y dμ(x), y ∈ X .)</p>
        <p>By an equivalence of couplings similar to above, we obtain</p>
        <p>which is minimised at a = m(μ)m(ν). This leads to the following conclusion: Proposition 4.2.2 (First Moment of Fréchet Mean) Let Λ be a random measure in W 2 (X ) with finite Fréchet functional and Fréchet mean γ.</p>
        <p>The purpose of this subsection is to show that the standard functional data analysis assumptions on the warp function T , having mean identity and being increasing, are equivalent to purely geometric conditions on T and the conditional mean measure Λ = T #λ . Put differently, if one is willing to assume that ET = i and that T is increasing, then one is led unequivocally to the problem of estimation of Fréchet means in the Wasserstein space W 2 (X ). When X = R, "increasing" is interpreted as being the gradient of a convex function, as explained at the end of Sect. 4.1.2. The total mass λ (X ) is invariant under the push-forward operation, and when it is finite, we may assume without loss of generality that it is equal to one, because all the relevant quantities scale with the total mass. Indeed, if λ = τμ with μ probability measure and τ &gt; 0, then T #λ = τ × T #μ, and the Wasserstein distance (defined as the infimum-over-coupling integrated cost) between τμ and τν is τW p (μ, ν) for μ, ν probabilities.</p>
        <p>We begin with the one-dimensional case, where the explicit formulae allow for a more transparent argument, and for simplicity we will assume some regularity.</p>
        <p>The relevance of the Wasserstein geometry to phase variation becomes clear in the following proposition that shows that Assumptions 2 are equivalent to geometric assumptions on the Wasserstein space W 2 (R).</p>
        <p>These assumptions have a clear interpretation: (B1) stipulates that λ is a Fréchet mean of the random measure Λ = T #λ , while (B2) states that T must be the optimal map from λ to Λ , that is,</p>
        <p>Proof. If T satisfies (B2) then, as an optimal map, it must be nondecreasing λalmost surely. Since λ is arbitrary, T must be nondecreasing on the entire domain K. Conversely, if T is nondecreasing, then it is optimal for any λ . Hence (A2) and (B2) are equivalent. Assuming (A2), we now show that (A1) and (B1) are equivalent. Condition (B1) is equivalent to the assertion that for all θ ∈ W 2 (R),</p>
        <p>λ (see Sect. 3.1.4). Condition (A2) and the assumptions on T imply that F Λ (x) = F λ (T -1 (x)). Suppose that F λ is invertible (i.e., continuous and strictly increasing on K). Then F -1 Λ (u) = T (F -1 λ (u)). Thus (B1) is equivalent to ET (x) = x for all x in the range of F -1 λ , which is K. The assertion that (A1) implies (B1), even if F λ is not invertible, is proven in the next theorem (Theorem 4.2.4) in a more general context.</p>
        <p>The situation in more than one dimension is similar but the proof is less transparent. To avoid compactness assumptions, we introduce the following power growth condition (taken from Agueh and Carlier [2]) of continuous functions that grow like</p>
        <p>• q (q ≥ 0):</p>
        <p>The space G q (X , X ) is defined similarly, with f taking values in X instead of R, and the norm will be denoted in the same way. These are nonseparable Banach spaces. Theorem 4.2.4 (Mean Identity Warp Functions and Fréchet Means) Fix λ ∈ P(X ) and let t ∈ G 1 (X , X ) be a (Bochner measurable) random optimal map with (Bochner) mean identity and such that</p>
        <p>The generalisation with respect to the one-dimensional result is threefold. Firstly, since our main interest is the implication (A1-A2) ⇒ (B1-B2), we need not assume T to be injective. Secondly, the support of λ is not required to be compact. Lastly, the result holds in arbitrary dimension, including infinite-dimensional separable Hilbert spaces X . In particular, if t is a linear map, then t G 1 coincides with the operator norm of t, so the assumption is that t be a bounded self-adjoint nonnegative operator with mean identity and finite expected operator norm.</p>
        <p>Proof. Optimality of t ensures that it has a convex potential φ , and strong and weak duality give</p>
        <p>Formally taking expectations, using Fubini's theorem and that Eφ = • 2 /2 (since Et is the identity) yields</p>
        <p>as required. The rigorous mathematical justification for this is given on page 88 in the supplement.</p>
        <p>The "natural" space for t would be L 2 (λ ), but without the continuity assumption, the result may fail ( Álvarez-Esteban et al. [9,Example 3.1]). A simple argument shows that the growth condition imposed by the G 1 assumption is minimal; see page 89 in the supplement or Galasso et al. [58].</p>
        <p>The same statement holds if X is replaced by a (Borel) convex subset K thereof. The integrals will then be taken on K, showing that λ minimises the Fréchet functional among measures supported on K, and, by continuity, on K. By Proposition 3.2.4, λ is a Fréchet mean.</p>
        <p>In view of the canonicity of the Wasserstein geometry in Sect. 4.2.2, separation of amplitude and phase variation of the point processes Π i essentially requires comput-ing Fréchet means in the 2-Wasserstein space. It is both conceptually important and technically convenient to introduce the case where an oracle reveals the conditional mean measures Λ = T #λ entirely. Thus, assuming that λ ∈ W 2 (X ) is the unique Fréchet mean of a random measure Λ , the goal is to estimate the structural mean λ on the basis of independent and identically distributed realisations Λ 1 ,...,Λ n of λ . Given that λ is defined as the minimiser of the Fréchet functional</p>
        <p>it is natural to estimate λ by a minimiser, say λ n , of the empirical Fréchet functional</p>
        <p>A minimiser λ n exists by Corollary 3.1.3. When X = R, λ n can be seen to be an unbiased estimator of λ in a generalised sense of Lehmann [88] (see Sect. 4.3.5).</p>
        <p>The warp maps (and their inverses) can then be estimated as the optimal maps from λ n to each Λ i (see Sect. 4.3.4).</p>
        <p>In practice, one does not have the fortune of fully observing the inherently infinitedimensional objects Λ 1 ,...,Λ n . A far more realistic scenario is that one only has access to a discrete version of Λ i , say Λ i . The simplest situation is when Λ i arises as an empirical measure of the form τ -1 ∑ τ i=1 δ {Y j }, where Y j are independent with distribution Λ i . More generally, Λ i can be a normalised point process Π i with mean measure τΛ i , i.e.</p>
        <p>This encapsulates the case of empirical measure when τ is an integer and Π i is a binomial point process. The parameter τ is the expected number of observed points over the entire space X ; the larger τ is, the more information Π i gives on Λ i . Except if Λ i is an empirical measure, there is one difficulty in the above setting that needs to be addressed. Unless Π i is binomial, there is a positive probability that Π i (X ) = 0 and no points pertaining to Λ i are observed. In the asymptotic setup below, conditions will be imposed to ensure that this probability becomes negligible as n → ∞. For concreteness we define Λ i = λ (0) for some fixed measure λ (0) that will be of minor importance. This can be a Dirac measure at 0, a certain fixed Gaussian measure, or (normalised) Lebesgue measure on some bounded set in case X = R d . We can now replace the estimator λ n by λ n , defined as any minimiser of</p>
        <p>which exists by Corollary 3.1.3. As a generalisation of the discrete case discussed in Sect. 1.3, the Fréchet mean of discrete measures can be computed exactly. Suppose that N i = Π i (X ) is nonzero for all i. Then each Λ i is a discrete measure supported on N i points. One can then recast the multimarginal formulation (see Sect. 3.1.2) as a finite linear program, solve it, and "average" the solution as in Proposition 3.1.2 in order to obtain λ n (an alternative linear programming formulation for finding a Fréchet mean is given by Anderes et al. [14]). Thus, λ n can be computed in finite time, even when X is infinite-dimensional.</p>
        <p>Finally, a remark about measurability is in order. Point processes can be viewed as random elements in M + (X ) endowed with the vague topology induced from convergence of integrals of continuous functions with compact support. If μ n converge to μ vaguely, and a n are numbers that converge to a, then a n μ n → aμ vaguely. Thus, Λi is a continuous function of the pair ( Π i , Π i (X )) and can be viewed as a random measure with respect to the vague topology. The restriction of the vague topology to probability measures is equivalent to the weak topology, 5 and therefore vague, weak, and Wasserstein measurability are all equivalent.</p>
        <p>Even when the computational complexity involved in calculating λ n is tractable, there is another reason not to use it as an estimator for λ . If one has a-priori knowledge that λ is smooth, it is often desirable to estimate it by a smooth measure. One way to achieve this would be to apply some smoothing technique to λ n using, e.g., kernel density estimation. However, unless the number of observed points from each measure is the same N 1 = • • • = N n = N, λ n will usually be concentrated on many points, essentially N 1 +• • •+N n of them. In other words, the Fréchet mean is concentrated on many more points than each of the measures Λ i , thus potentially hindering its usefulness as a mean because it will not be a representative of the sample. This is most easily seen when X = R, in which case each Λ i is a discrete uniform measure on points</p>
        <p>, where we assume for simplicity that the points are not repeated (this will happen with probability one if Λ i is diffuse). If we now set G i to be the distribution function of Λ i , then the quantile function G -1 i is piecewise constant on each interval (k, k + 1]/N i with jumps at</p>
        <p>The Fréchet mean has quantile function G -1 (u) = n -1 ∑ G -1 i (u) and will have jumps at every point of the form k/N i for k ≤ N i and i = 1, . . . , n. In the worst-case scenario, when no pair from N i has a common divisor, there will be</p>
        <p>which is the number of points on which the Fréchet mean will be supported. (All the G -1 i 's have a jump at one which thus needs to be counted once rather than n times.)</p>
        <p>By counting the number of redundancies in the constraints matrix of the linear program, one can show that this is in general an upper bound on the number of support points of the Fréchet mean.</p>
        <p>An alternative approach is to first smooth each observation λ n and then calculate the Fréchet mean. Since it is easy to bound the Wasserstein distances when dealing with convolutions, we will employ kernel density estimation, although other smoothing approaches could be used as well.</p>
        <p>To simplify the exposition, we provide the technical details only when X = R d , but a similar construction will work when the dimension of X is infinite. Let ψ : R d → (0, ∞) be a continuous, bounded, strictly positive isotropic density function with unit variance: ψ(x) = ψ 1 ( x ) with ψ 1 nonincreasing and</p>
        <p>(Besides the boundedness all these properties can be relaxed, and if X = R even boundedness is not necessary.) A classical example for ψ is the standard Gaussian density in R d . Define the rescaled version ψ σ (x) = σ -d ψ(x/σ ) for all σ &gt; 0. We can then replace Λ i by a smooth proxy Λ i * ψ σ . If Λ i is a sum of Dirac masses at x 1 ,...,x N i , then</p>
        <p>If N i = 0, one can either use λ (0) or λ (0) * ψ σ ; this event will have negligible probability anyway.</p>
        <p>For the purpose of approximating Λ i , this convolution is an acceptable estimator, because as was seen in the proof of Theorem 2.2.7,</p>
        <p>But the measure Λ i has a strictly positive density throughout R d . If we know that Λ is supported on a convex compact K ⊂ R d , it is desirable to construct an estimator that has the same support K. The first idea that comes to mind is to project Λ i * ψ σ to K (see Proposition 3.2.4), as this will further decrease the Wasserstein distance, but the resulting measure will then have positive mass on the boundary of K, and will not be absolutely continuous. We will therefore use a different strategy: eliminate all the mass outside K and redistribute it on K. The simplest way to do this is to restrict Λ i * ψ σ to K and renormalise the restriction to be a probability measure.</p>
        <p>For technical reasons, it will be more convenient to bound the Wasserstein distance when the restriction and renormalisation is done separately on each point of Λ i . This yields the measure</p>
        <p>Lemma 4.4.2 below shows that W 2 2 ( Λ i , Λ i ) ≤ Cσ 2 for some finite constant C. It is apparent that Λ i is a continuous function of Λ i and σ , so Λ i is measurable; in any case this is not particularly important because σ will vanish, so Λ i = Λ i asymptotically and the latter is measurable.</p>
        <p>Our final estimator λ n for λ is defined as the minimiser of</p>
        <p>Since the measures Λ i are absolutely continuous, λ n is unique. We refer to λ n as the regularised Fréchet-Wasserstein estimator, where the regularisation comes from the smoothing and the possible restriction to K.</p>
        <p>In the case X = R, λ n can be constructed via averaging of quantile functions. Let G i be the distribution function of Λ i . Then λ n is the measure with quantile function</p>
        <p>and distribution function</p>
        <p>By construction, the G i are continuous and strictly increasing, so the inverses are proper inverses and one does not to use the right-continuous inverse as in Sect. 3.1.4. If X = R d and d ≥ 2, then there is no explicit expression for λ n , although it exists and is unique. In the next chapter, we present a steepest descent algorithm that approximately constructs λ n by taking advantage of the differentiability properties of the Fréchet functional F n in Sect. 3.1.6.</p>
        <p>Once estimators Λ i , i = 1, . . . , n and λ n are constructed, it is natural to estimate the map T i = t Λ i λ and its inverse T -1 i = t λ Λ i (when Λ i are absolutely continuous; see the discussion after Assumptions 3 below) by the plug-in estimators</p>
        <p>The latter, the registration maps, can then be used in order to register the points Π i via</p>
        <p>It is thus reasonable to expect that if T -1 i is a good estimator, then its composition with T i should be close to the identity and Π i should be close to Π i .</p>
        <p>In the same way, Fréchet means extend the notion of mean to non-Hilbertian spaces, they also extend the definition of unbiased estimators. Let H be a separable Hilbert space (or a convex subset thereof) and suppose that θ is a random element in H whose distribution μ θ depends on a parameter θ ∈ H. Then θ is unbiased for θ if for all θ ∈ H</p>
        <p>In view of that, one can define unbiased estimators of λ ∈ W 2 as measurable functions δ = δ (Λ 1 , . . . ,Λ n ) for which</p>
        <p>This definition was introduced by Lehmann [88].</p>
        <p>Unbiased estimators allow us to avoid the problem of over-registering (the socalled pinching effect; Kneip and Ramsay [82, Section 2.4]; Marron et al. [90, p. 476]). An extreme example of over-registration is if one "aligns" all the observed patterns into a single fixed point x 0 . The registration will then seem "successful" in the sense of having no residual phase variation, but the estimation is clearly biased because the points are not registered to the correct reference measure. Thus, requiring the estimator to be unbiased is an alternative to penalising the registration maps.</p>
        <p>Due to the Hilbert space embedding of W 2 (R), it is possible to characterise unbiased estimators in terms of a simple condition on their quantile functions. As a corollary, λ n , the Fréchet mean of {Λ 1 , . . . ,Λ n }, is unbiased. Our regularised Fréchet-Wasserstein estimator λn can then be interpreted as approximately unbiased, since it approximates the unobservable λ n . Proposition 4.3.1 (Unbiased Estimators in W 2 (R)) Let Λ be a random measure in W 2 (R) with finite Fréchet functional and let λ be the unique Fréchet mean of Λ (Theorem 3.2.11). An estimator δ constructed as a function of a sample (Λ 1 , . . . ,Λ n ) is unbiased for λ if and only if the left-continuous representatives (in L 2 (0, 1)) satisfy E[F -1 δ (x)] = F -1 λ (x) for all x ∈ (0, 1).</p>
        <p>Proof. The proof is straightforward from the definition: δ is unbiased if and only if for all λ and all γ,</p>
        <p>In other words, these two functions must equal almost everywhere on (0, 1), and their left-continuous representatives must equal everywhere (the fact that E λ [F -1 δ ] has such a representative was established in Sect. 3.1.4).</p>
        <p>To show that δ = λ n is unbiased, we simply invoke Theorem 3.2.11 twice to see that</p>
        <p>which proves unbiasedness of δ .</p>
        <p>In functional data analysis, one often assumes that the number of curves n and the number of observed points per curve m both diverge to infinity. An analogous framework for point processes would similarly require the number of point processes n as well as the expected number of points τ per processes to diverge. A technical complication arises, however, because the mean measures do not suffice to characterise the distribution of the processes. Indeed, if one is given a point processes Π with mean measure λ (not necessarily a probability measure), and τ is an integer, there is no unique way to define a process Π (τ) with mean measure τλ . One can define Π (τ) = τΠ , so that every point in Π will be counted τ times. Such a construction, however, can never yield a consistent estimator of λ , even when τ → ∞.</p>
        <p>Another way to generate a point process with mean measure τλ is to take a superposition of τ independent copies of Π . In symbols, this means</p>
        <p>with (Π i ) independent, each having the same distribution as Π . This superposition scheme gives the possibility to use the law of large numbers. If τ is not an integer, then this construction is not well-defined but can be made so by assuming that the distribution of Π is infinitely divisible. The reader willing to assume that τ is always an integer can safely skip to Sect. 4.4.1; all the main ideas are developed first for integer values of τ and then extended to the general case.</p>
        <p>A point process Π is infinitely divisible if for every integer m there exists a collection of m independent and identically distributed Π</p>
        <p>If Π is infinitely divisible and τ = k/m is rational, then can define π (τ) using km independent copies of Π (1/m) :</p>
        <p>One then deals with irrational τ via duality and continuity arguments, as follows. Define the Laplace functional of Π by</p>
        <p>The Laplace functional characterises the distribution of the point process, generalising the notion of Laplace transform of a random variable or vector (Karr [79, Theorem 1.12]). By definition, it translates convolutions into products. When Π = Π (1) is infinitely divisible, the Laplace functional L 1 of Π takes the form (</p>
        <p>)</p>
        <p>(1-e -μ f ) dρ(μ) for some ρ ∈ M + (M + (X )).</p>
        <p>The Laplace functional of Π (τ) is L τ ( f ) = [L 1 ( f )] τ for any rational τ, which simply amounts to multiplying the measure ρ by the scalar τ. One can then do the same for an irrational τ, and the resulting Laplace functional determines the distribution of Π (τ) for all τ &gt; 0.</p>
        <p>We are now ready to define our asymptotic setup. The following assumptions will be made. Notice that the Wasserstein geometry does not appear explicitly in these assumptions, but is rather derived from them in view of Theorem 4.2.4. The compactness requirement can be relaxed under further moment conditions on λ and the point process Π ; we focus on the compact case for the simplicity and because in practice the point patterns will be observed on a bounded observation window.</p>
        <p>Assumptions 3 Let K ⊂ R d be a compact convex nonempty set, λ an absolutely continuous probability measure on K, and τ n a sequence of positive numbers. Let Π be a point processes on K with mean measure λ . Finally, define U = intK.</p>
        <p>• For every n, let {Π</p>
        <p>n } be independent point processes, each having the same distribution as a superposition of τ n copies of Π . • Let T be a random injective function on K (viewed as a random element in C b (K, K) endowed with the supremum norm) such that T (x) ∈ U for x ∈ U (that is, T ∈ C b (U,U)) with nonsingular derivative ∇T (x) ∈ R d×d for almost all x ∈ U, that is a gradient of a convex function. Let {T 1 , . . . , T n } be independent and identically distributed as T .</p>
        <p>i be the warped point processes, having conditional mean measures</p>
        <p>• Define Λ i by the smoothing procedure (4.2), using bandwidth σ</p>
        <p>The dependence of the estimators on n will sometimes be tacit. But Λ i does not depend on n.</p>
        <p>By virtue of Theorem 4.2.4, λ is a Fréchet mean of the random measure Λ = T #λ . Uniqueness of this Fréchet mean will follow from Proposition 3.2.7 if we show that Λ is absolutely continuous with positive probability. This is indeed the case, since T is injective and has a nonsingular Jacobian matrix; see Ambrosio et al. [12,Lemma 5.5.3]. The Jacobian assumption can be removed when X = R, because Fréchet means are always unique by Theorem 3.2.11.</p>
        <p>Notice that there is no assumption about the dependence between rows. Assumptions 3 thus cover, in particular, two different scenarios:</p>
        <p>• Full independence: here the point processes are independent across rows, that is, Π</p>
        <p>(n+1) i are also independent. • Nested observations: here Π (n+1) i includes the same points as Π (n) i and additional points, that is, Π</p>
        <p>and another point process distributed as (τ n+1 -τ n )Π .</p>
        <p>Needless to say, Assumptions 3 also encompass binomial processes when τ n are integers, as well as Poisson processes or, more generally, Poisson cluster processes.</p>
        <p>We now state and prove the consistency result for the estimators of the conditional mean measures Λ i and the structural mean measure λ .</p>
        <p>2. The regularised Fréchet-Wasserstein estimator of the structural mean measure (as described in Sect. 4.3) is strongly Wasserstein-consistent,</p>
        <p>-→ 0, as n → ∞.</p>
        <p>Convergence in 1. holds almost surely under the additional conditions that</p>
        <p>If σ n → 0 only in probability, then convergence in 2. still holds in probability.</p>
        <p>Theorem 4.4.1 still holds without smoothing (σ n = 0). In that case, λ n = λn is possibly not unique, and the theorem should be interpreted in a set-valued sense (as in Proposition 1.7.8): almost surely, any choice of minimisers λn converges to λ as n → ∞.</p>
        <p>The preceding paragraph notwithstanding, we will usually assume that some smoothing is present, in which case λ n is unique and absolutely continuous by Proposition 3.1.8. The uniform Lipschitz bounds for the objective function show that if we restrict the relevant measures to be absolutely continuous, then λ n is a continuous function of ( Λ 1 ,..., Λ n ) and hence λ n : (Ω , F , P) → W 2 (K) is measurable; this is again a minor issue because many arguments in the proof hold for each ω ∈ Ω separately. Thus, even if λ n is not measurable, the proof shows that the convergence holds outer almost surely or in outer probability.</p>
        <p>The first step in proving consistency is to show that the Wasserstein distance between the unsmoothed and the smoothed estimators of Λ i vanishes with the smoothing parameter. The exact rate of decay will be important to later establish the rate of convergence of λ n to λ , and is determined next.</p>
        <p>There exists a finite constant C ψ,K , depending only on ψ and on K, such that</p>
        <p>Since the smoothing parameter will anyway vanish, this restriction to small values of σ is not binding. The constant C ψ,K is explicit. When X = R, a more refined construction allows to improve this constant in some situations, see Panaretos and Zemel [100, Lemma 1].</p>
        <p>Proof. The idea is that (4.2) is a sum of measures with mass 1/N i that can be all sent to the relevant point x j , and we refer to page 98 in the supplement for the precise details.</p>
        <p>Proof (Proof of Theorem 4.4.1). The proof, detailed on page 97 of the supplement, follows the following steps: firstly, one shows the convergence in probability of Λ i to Λ i . This is basically a corollary of Karr [79,Proposition 4.8] and the smoothing bound from Lemma 4.4.2.</p>
        <p>To prove claim (2) one considers the functionals, defined on W 2 (K):</p>
        <p>Since K is compact, they are all locally Lipschitz, so their differences can be controlled by the distances between Λ i , Λ i , and Λ i . The first distance vanishes since the intensity τ → ∞, and the second by the smoothing bound. Another compactness argument yields that F n → F uniformly on W 2 (K), and so the minimisers converge.</p>
        <p>The almost sure convergence in ( 1) is proven as follows. Under the stronger conditions at the end of the theorem's statement, for any fixed a = (a 1 ,...,</p>
        <p>by the law of large numbers. This extends to all rational a's, then to all a by approximation. The smoothing error is again controlled by Lemma 4.4.2.</p>
        <p>We next discuss the consistency of the warp and registration function estimators. These are key elements in order to align the observed point patterns Π i . Recall that we have consistent estimators Λ i for Λ i and λ n for λ . Then</p>
        <p>. We will make the following extra assumptions that lead to more transparent statements (otherwise one needs to replace K with the set of Lebesgue points of the supports of λ and Λ i ).</p>
        <p>1. λ has a positive density on K (in particular, suppλ = K); 2. T is almost surely surjective on U = intK (thus a homeomorphism of U).</p>
        <p>As a consequence suppΛ = supp(T #λ ) = T (suppλ ) = K almost surely.</p>
        <p>Almost sure convergence can be obtained under the same provisions made at the end of the statement of Theorem 4.4.1.</p>
        <p>A few technical remarks are in order. First and foremost, it is not clear that the two suprema are measurable. Even though T i and T -1 i are random elements in C b (U, R d ), their estimators are only defined in an L 2 sense. The proof of Theorem 4.4.3 is done ω-wise. That is, for any ω in the probability space such that Theorem 4.4.1 holds, the two suprema vanish as n → ∞. In other words, the convergence holds in outer probability or outer almost surely.</p>
        <p>Secondly, assuming positive smoothing, the random measures Λ i are smooth with densities bounded below on K, so T -1 i are defined on the whole of U (possibly as set-valued functions on a Λ i -null set). But the only known regularity result for λ n is an upper bound on its density (Proposition 3.1.8), so it is unclear what is its support and consequently what is the domain of definition of T i .</p>
        <p>Lastly, when the smoothing parameter σ is zero, T i and T -1 i are not defined. Nevertheless, Theorem 4.4.3 still holds in the set-valued formulation of Proposition 1.7.11, of which it is a rather simple corollary: Theorem 4.4.3). The proof amounts to setting the scene in order to apply Proposition 1.7.11 of stability of optimal maps. We define</p>
        <p>and verify the conditions of the proposition. The weak convergence of μ n to μ and ν n to ν is the conclusion of Theorem 4.4.1; the finiteness is apparent because K is compact and the uniqueness follows from the assumed absolute continuity of Λ i . Since in addition T -1 i is uniquely defined on U = intK which is an open convex set, the restrictions on Ω in Proposition 1.7.11 are redundant. Uniform convergence of T i to T i is proven in the same way.</p>
        <p>The division by the number of observed points ensures that the resulting measures are probability measures; the relevant information is contained in the point patterns themselves, and is invariant under this normalisation. Proof. The law of large numbers entails that</p>
        <p>i , we have the upper bound</p>
        <p>Fix a compact Ω ⊆ intK and split the integral to Ω and its complement.</p>
        <p>by the law of large numbers, where d K is the diameter of K. By writing intK as a countable union of compact sets (and since λ is absolutely continuous), this can be made arbitrarily small by choice of Ω . We can easily bound the integral on Ω by</p>
        <p>The right-hand side therefore vanishes as n → ∞ by Theorem 4.4.3, and this completes the proof.</p>
        <p>Possible extensions pertaining to the boundary of K are discussed on page 33 of the supplement.</p>
        <p>In this section, we illustrate the estimation framework put forth in this chapter by considering an example of a structural mean λ with a bimodal density on the real line. The unwarped point patterns Π originate from Poisson processes with mean measure λ and, consequently, the warped points Π are Cox processes (see Sect. 4.1.2). Another scenario involving triangular densities can be found in Panaretos and Zemel [100].</p>
        <p>As a first step, we introduce a class of random warp maps satisfying Assumptions 2, that is, increasing maps that have as mean the identity function. The construction is a mixture version of similar maps considered by Wang and Gasser [128,129].</p>
        <p>By means of mixtures, we replace this discrete family by a continuous one: let J &gt; 1 be an integer and V = (V 1 ,...,V J ) be a random vector following the flat Dirichlet distribution (uniform on the set of nonnegative vectors with</p>
        <p>. Take independently k j following the same distribution as k and define</p>
        <p>Since V j is positive, T is increasing and as (V j ) sums up to unity T has mean identity. Realisations of these warp functions are given in Fig. 4.4b and c for J = 2 and J = 10, respectively. The parameters (k j ) were chosen as symmetrised Poisson random variables: each k j has the law of XY with X Poisson with mean 3 and P(Y = 1) = P(Y = -1) = 1/2 for Y and X independent. When J = 10 is large, the function T deviates only mildly from the identity, since a law of large numbers begins to take effect. In contrast, J = 2 yields functions that are quite different from the identity. Thus, it can be said that the parameter J controls the variance of the random warp function T .</p>
        <p>Let the structural mean measure λ be a mixture of a bimodal Gaussian distribution (restricted to K = [-16, 16]) and a beta background on the interval [-12, 12], so that mass is added at the centre of K but not near the boundary. In symbols this is given as follows. Let ϕ be the standard Gaussian density and let β α,β denote the density of a the beta distribution with parameters α and β . Then λ is chosen as the measure with density The main criterion for the quality of our regularised Fréchet-Wasserstein estimator will be its success in discerning the two modes at ±8; these will be smeared by the phase variation arising from the warp functions.</p>
        <p>We next simulated 30 independent Poisson processes with mean measure λ , ε = 0.1, and total intensity (expected number of points) τ = 93. In addition, we generated warp functions as in (4.5) but rescaled to [-16, 16]; that is, having the same law as the functions x → 32T</p>
        <p>x + 16 32 -16</p>
        <p>from K to K. These cause rather violent phase variation, as can be seen by the plots of the densities and distribution functions of the conditional measures Λ = T #λ presented in Fig. 4.6a and b; the warped points themselves are displayed in Fig. 4.6c. Using these warped point patterns, we construct the regularised Fréchet-Wasserstein estimator employing the procedure described in Sect. 4.3. Each Π i was smoothed with a Gaussian kernel and bandwidth chosen by unbiased cross validation. We deviate slightly from the recipe presented in Sect. 4.3 by not restricting the resulting estimates to the interval [-16, 16], but this has no essential effect on the finite sample performance. The regularised Fréchet-Wasserstein estimator λ n serves as the estimator of the structural mean λ and is shown in Fig. 4.7a. It is contrasted with λ at the level of distribution functions, as well as with the empirical arithmetic mean; the latter, the naive estimator, is calculated by ignoring the warping and simply averaging linearly the (smoothed) empirical distribution functions across the observations. We notice that λ n is rather successful at locating the two modes of λ , in contrast with the naive estimator that is more diffuse. In fact, its distribution function increases approximately linearly, suggesting a nearly constant density instead of the correct bimodal one. Estimators of the warp maps T i , depicted in Fig. 4.7b, and their inverses, are defined as the optimal maps between λ n and the estimated conditional mean measures, as explained in Sect. 4.3.4. Then we register the point patterns by applying to them the inverse estimators T -1 i (Fig. 4.8). Figure 4.7c gives two kernel estimators of the density of λ constructed from a superposition of all the warped points and all the registered ones. Notice that the estimator that uses the registered points is much more successful than the one using the warped ones in discerning the two density peaks. This is not surprising after a brief look at Fig. 4.8, where the unwarped, warped, and registered points are displayed. Indeed, there is very high concentration of registered points around the true location of the peaks, ±8. This is not the case for the warped points because of the phase variation that translates the centres of concentration for each individual observation. It is important to remark that the fluctuations in the density estimator in Fig. 4.7c are not related to the registration procedure, and could be reduced by a better choice of bandwidth. Indeed, our procedure does not attempt to estimate the density, but rather the distribution function. .9 presents a superposition of the regularised Fréchet-Wasserstein estimators for 20 independent replications of the experiment, contrasted with a similar superposition for the naive estimator. The latter is clearly seen to be biased around the two peaks, while the regularised Fréchet-Wasserstein seems approximately unbiased, despite presenting fluctuations. It always captures the bimodal nature of the density, as is seen from the two clear elbows in each realisation.</p>
        <p>To illustrate the consistency of the regularised Fréchet-Wasserstein estimator λ n for λ as shown in Theorem 4.4.1, we let the number of processes n as well as the expected number of observed point per process τ to vary. Figures 4.10 and 4.11 show the sampling variation of λ n for different values of n and τ. We observe that as either of these increases, the realisations λ n indeed approach λ . The figures suggest that, in this scenario, the amplitude variation plays a stronger role than the phase variation, as the effect of τ is more substantial. (c) Fig. 4.9: (a) Sampling variation of the regularised Fréchet-Wasserstein mean λ n and the true mean measure λ for 20 independent replications of the experiment; (b) sampling variation of the arithmetic mean, and the true mean measure λ for the same 20 replications; (c) superposition of (a) and (b). For ease of comparison, all three panels include residual curves centred at y = 3/4</p>
        <p>In order to work with measures of strictly positive density, the observed point patterns have been smoothed using a kernel function. This necessarily incurs an additional bias that depends on the bandwidth σ i . The asymptotics (Theorem 4.4.1) guarantee the consistency of the estimators, in particular the regularised Fréchet-Wasserstein estimator λ n , provided that max n i=1 σ i → 0. In our simulations, we choose σ i in a data-driven way by employing unbiased cross validation. To gauge for the effect of the smoothing, we carry out the same estimation procedure but with σ i multiplied by a parameter s. Figure 4.12 presents the distribution function of λ n as a function of s. Interestingly, the curves are nearly identical as long as s ≤ 1, whereas when s &gt; 1, the bias becomes more substantial. In particular, there are no empty point processes, so the i by the more transparent order τ n . As in the consistency proof, the idea is to write</p>
        <p>and control each term separately. The first term corresponds to the phase variation, and comes from the approximation of the theoretical expectation F by a sample mean F n . The second term is associated with the amplitude variation resulting from observing Λ i discretely. The third term can be viewed as the bias incurred by the smoothing procedure. Accordingly, the rate at which λ n converges to λ is a sum of three separate terms. We recall the standard O P terminology: if X n and Y n are random variables, then X n = O P (Y n ) means that the sequence (X n /Y n ) is bounded in probability, which by definition is the condition</p>
        <p>Instead of X n = O P (Y n ), we will sometimes write Y n ≥ O P (X n ). The former notation emphasises the condition that X n grows no faster than Y n , while the latter stresses that Y n grows at least as fast as X n (which is of course the same assertion). Finally,</p>
        <p>Theorem 4.6.3 (Convergence Rates on R) Suppose in addition to Assumptions 3 that d = 1, τ n / log n → ∞ and that Π is either a Poisson process or a binomial process. Then</p>
        <p>where all the constants in the O P terms are explicit.</p>
        <p>Remark 4.6.4 Unlike classical density estimation, no assumptions on the rate of decay of σ n are required, because we only need to estimate the distribution function and not the derivative. If the smoothing parameter is chosen to be σ</p>
        <p>i ] -α for some α &gt; 0 and τ n / log n → ∞, then by Lemma 4.6.1 σ n ≤ max 1≤i≤n σ</p>
        <p>For example, if Rosenblatt's rule α = 1/5 is employed, then the O P (σ n ) term can be replaced by O P (1/ 5 √ τ n ).</p>
        <p>One can think about the parameter τ as separating the sparse and dense regimes as in classical functional data analysis (see also Wu et al. [132]). If τ is bounded, then the setting is ultra sparse and consistency cannot be achieved. A sparse regime can be defined as the case where τ n → ∞ but slower than log n. In that case, consistency is guaranteed, but some point patterns will be empty. The dense regime can be defined as τ n n 2 , in which case the amplitude variation is negligible asymptotically when compared with the phase variation. The exponent -1/4 of τ n can be shown to be optimal without further assumptions, but it can be improved to -1/2 if P( f Λ ≥ ε on K) = 1 for some ε &gt; 0, where f Λ is the density of Λ (see Sect. 4.7). In terms of T , the condition is that P(T ≥ ε) = 1 for some ε and λ has a density bounded below. When this is the case, τ n needs to compared with n rather than n 2 in the next paragraph and the next theorem.</p>
        <p>Theorem 4.6.3 provides conditions for the optimal parametric rate √ n to be achieved: this happens if we set σ n to be of the order O P (n -1/2 ) or less and if τ n is of the order n 2 or more. But if the last two terms in Theorem 4.6.3 are negligible with respect to n -1/2 , then a sort of central limit theorem holds for λ n : Theorem 4.6.5 (Asymptotic Normality) In addition to the conditions of Theorem 4.6.3, assume that τ n /n 2 → ∞, σ n = o P (n -1/2 ) and λ possesses an invertible distribution function F λ on K. Then</p>
        <p>for a zero-mean Gaussian process Z with the same covariance operator of T (the latter viewed as a random element in L 2 (λ )), namely with covariance kernel κ(x, y) = cov T (x), T (y) .</p>
        <p>If the density f λ exists and is (piecewise) continuous and bounded below on K, then the weak convergence also holds in L 2 (K).</p>
        <p>In view of Sect. 2.3, Theorem 4.6.5 can be interpreted as asymptotic normality of λ n in the tangential sense: √ n log λ ( λ n ) converges to a Gaussian random element in the tangent space Tan λ , which is a subset of L 2 (λ ). The additional smoothness conditions allow to switch to the space L 2 (K), which is independent of the unknown template measure λ .</p>
        <p>See pages 109 and 110 in the supplement for detailed proofs of these theorems. Below we sketch the main ideas only. Theorem 4.6.3). The quantile formula</p>
        <p>from Sect. 1.5 and the average quantile formula for the Fréchet mean (Sect. 3.1.4) show that the oracle empirical mean F -1 λ n follows a central limit theorem in L 2 (0, 1). Since we work in the Hilbert space L 2 (0, 1), Fréchet means are simple averages, so the errors in the Fréchet mean have the same rate as the errors in the Fréchet functionals. The smoothing term is easily controlled by Lemma 4.4.2.</p>
        <p>Controlling the amplitude term is more difficult. Bounds can be given using the machinery sketched in Sect. 4.7, but we give a more elementary proof by reducing to the 1-Wasserstein case (using (2.2)), which can be more easily handled in terms of distributions functions (Corollary 1.5.3). Theorem 4.6.5). The hypotheses guarantee that the amplitude and smoothing errors are negligible and</p>
        <p>where GP is the Gaussian process defined in the proof of Theorem 4.6.3. One then employs a composition with F λ .</p>
        <p>One may find the term O P (1/ 4 √ τ n ) in Theorem 4.6.3 to be somewhat surprising, and expect that it ought to be O P (1/ √ τ n ). The goal of this section is to show why the rate 1/ 4 √ τ n is optimal without further assumptions and discuss conditions under which it can be improved to the optimal rate 1/ √ τ n . For simplicity, we concentrate on the case τ n = n and assume that the point process Π is binomial; the Poisson case being easily obtained from the simplified one (using Lemma 4.6.1). We are thus led to study rates of convergence of empirical measures in the Wasserstein space. That is to say, for a fixed exponent p ≥ 1 and a fixed measure μ ∈ W p (X ), we consider independent random variables X 1 , . . . with law μ and the empirical measure μ n = n -1 ∑ n i=1 δ {X i }. The first observation is that EW p (μ, μ n ) → 0: Lemma 4.7.1 Let μ ∈ P(X ) be any measure. Then</p>
        <p>Proof. This result has been established in an almost sure sense in Proposition 2.2.6. To extend to convergence in expectation observe that</p>
        <p>Thus, the random variable 0 ≤ Y n = W p p (μ, μ n ) is bounded by the sample average Z n of a random variable V = X x -X 1 p dμ(x) that has a finite expectation. A version of the dominated converge theorem (given on page 111 in the supplement) implies that EY n → 0. Now invoke Jensen's inequality. The next question is how quickly EW p (μ, μ n ) vanishes when μ ∈ W p (X ). We shall begin with two simple general lower bounds, then discuss upper bounds in the one-dimensional case, put them in the context of Theorem 4.6.3, and finally briefly touch the d-dimensional case.</p>
        <p>√ n Lower Bound) Let μ ∈ P(X ) be nondegenerate. Then there exists a constant c(μ) &gt; 0 such that for all p ≥ 1 and all n</p>
        <p>Proof. Let X ∼ μ and let a = b be two points in the support μ.</p>
        <p>by the central limit theorem and the Kantorovich-Rubinstein theorem (1.11).</p>
        <p>For discrete measures, the rates scale badly with p. More generally: Lemma 4.7.4 (Separated Support) Suppose that there exist Borel sets A, B ⊂ X such that μ(A</p>
        <p>xy &gt; 0.</p>
        <p>Then for any p ≥ 1 there exists c p (μ) &gt; 0 such that EW p (μ n , μ) ≥ c p (μ)n -1/(2p) .</p>
        <p>Any nondegenerate finitely discrete measure μ satisfies this condition, and so do "non-pathological" countably discrete ones. (An example of a "pathological" measure is one assigning positive mass to any rational number.)</p>
        <p>Proof. Let k ∼ B(n, q = μ(A)) denote the number of points from the sample (X 1 , . . . , X n ) that fall in A. Then a mass of |k/n -q| must travel between A and B, a distance of at least d min . Thus, W p p (μ n , μ) ≥ d p min |k/n -q|, and the result follows from the central limit theorem for k; see page 112 in the supplement for the full details.</p>
        <p>These lower bounds are valid on any separable metric space. On the real line, it is easy to obtain a sufficient condition for the optimal rate n -1/2 to be attained for</p>
        <p>/n, we have (by Fubini's theorem and Jensen's inequality)</p>
        <p>Since the integrand is bounded by 1/2, this is certainly satisfied if μ is compactly supported. The J 1 condition is essentially a moment condition, since for any δ &gt; 0, we have for X ∼ μ that E|X| 2+δ &lt; ∞ =⇒ J 1 (μ) &lt; ∞ =⇒ E|X| 2 &lt; ∞. It turns out that this condition is necessary, and has a more subtle counterpart for any p ≥ 1. Let f denote the density of the absolutely continuous part of μ (so f ≡ 0 if μ is discrete).</p>
        <p>is necessary and sufficient for EW p (μ n , μ) = O(n -1/2 ).</p>
        <p>See Bobkov and Ledoux [25, Theorem 5.10] for a proof for the J p condition, and Theorems 5.1 and 5.3 for the values of the constants and a stronger result. When p &gt; 1, for J p (μ) to be finite, the support of μ must be connected; this is not needed when p = 1. Moreover, the J p condition is satisfied when f is bounded below (in which case the support of μ must be compact). However, smoothness alone does not suffice, even for measures with positive density on a compact support. More precisely, we have: Proposition 4.7.6 For any rate ε n → 0 there exists a measure μ on [-1, 1] with positive C ∞ density there, and such that for all n</p>
        <p>The rate n -1/(2p) from Lemma 4.7.4 is the worst among compactly supported measures on R. Indeed, by Jensen's inequality and (2.2), for any μ ∈ P([0, 1]),</p>
        <p>The proof of Proposition 4.7.6 is done by "smoothing" the construction in Lemma 4.7.4, and is given on page 113 in the supplement.</p>
        <p>Let us now put this in the context of Theorem 4.6.3. In the binomial case, since each Π (n) i and each Λ i are independent, we have</p>
        <p>(In the Poisson case, we need to condition on N (n) i and then estimate its inverse square root as is done in the proof of Theorem 4.6.3.) Therefore, a sufficient condition for the rate 1/ √ τ n to hold is that E J 2 (Λ ) &lt; ∞ and a necessary condition is that P( J 2 (Λ ) &lt; ∞) = 1. These hold if there exists δ &gt; 0 such that with probability one Λ has a density bounded below by δ . Since Λ = T #λ , this will happen provided that λ itself has a bounded below density and T has a bounded below derivative. Bigot et al. [23] show that the rate √ τ n cannot be improved. We conclude by proving a lower bound for absolutely continuous measures and stating, without proof, an upper bound. Proposition 4.7.7 Let μ ∈ W 1 (R d ) have an absolutely continuous part with respect to Lebesgue measure, and let ν n be any discrete measure supported on n points (or less). Then there exists a constant C(μ) &gt; 0 such that</p>
        <p>Proof. Let f be the density of the absolutely continuous part μ c , and observe that for some finite number M,</p>
        <p>Let x 1 ,...,x n be the support points of ν n and ε &gt; 0. Let μ c,M be the restriction of μ c to the set where the density is smaller than M. The union of balls B ε (x i ) has μ c,M -measure of at most</p>
        <p>. Thus, a mass 2δ -δ = δ must travel more than ε from ν n to μ in order to cover μ c,M . Hence</p>
        <p>The lower bound holds because we need ε -d balls of radius ε in order to cover a sufficiently large fraction of the mass of μ. The determining quantity for upper bounds on the empirical Wasserstein distance is the covering numbers N(μ, ε, τ) = minimal number of balls whose union has μ mass ≥ 1 -τ.</p>
        <p>Since μ is tight, these are finite for all ε, τ &gt; 0, and they increase as ε and τ approach zero. To put the following bound in context, notice that if μ is compactly supported</p>
        <p>Comparing this with the lower bound in Lemma 4.7.4, we see that in the highdimensional regime d &gt; 2p, absolutely continuous measures have a worse rate than discrete ones. In the low-dimensional regime d &lt; 2p, the situation is opposite. We also obtain that for d &gt; 2 and a compactly supported absolutely continuous μ ∈</p>
        <p>Our exposition in this chapter closely follows the papers Panaretos and Zemel [100] and Zemel and Panaretos [134].</p>
        <p>Books on functional data analysis include Ramsay and Silverman [109,110], Ferraty and Vieu [51], Horváth and Kokoszka [70], and Hsing and Eubank [71], and a recent review is also available (Wang et al. [127]). The specific topic of amplitude and phase variation is discussed in [110,Chapter 7] and [127,Section 5.2]. The next paragraph gives some selective references.</p>
        <p>One of the first functional registration techniques employed dynamic programming (Wang and Gasser [128]) and dates back to Sakoe and Chiba [118]. Landmark registration consists of identifying salient features for each curve, called landmarks, and aligning them (Gasser and Kneip [61]; Gervini and Gasser [63]). In pairwise synchronisation (Tang and Müller [122]) one aligns each pair of curves and then derives an estimator of the warp functions by linear averaging of the pairwise registration maps. Another class of methods involves a template curve, to which each observation is registered, minimising a discrepancy criterion; the template is then iteratively updated (Wang and Gasser [129]; Ramsay and Li [108]). James [72] defines a "feature function" for each curve and uses the moments of the feature function to guarantee identifiability. Elastic registration employs the Fisher-Rao metric that is invariant to warpings and calculates averages in the resulting quotient space (Tucker et al. [123]). Other techniques include semiparametric modelling (Rønn [115]; Gervini and Gasser [64]) and principal components registration (Kneip and Ramsay [82]). More details can be found in the review article by Marron et al. [90]. Wrobel et al. [131] have recently developed a registration method for functional data with a discrete flavour. It is also noteworthy that a version of the Wasserstein metric can also be used in the functional case (Chakraborty and Panaretos [34]).</p>
        <p>The literature on the point processes case is more scarce; see the review by Wu and Srivastava [133].</p>
        <p>A parametric version of Theorem 4.2.4 was first established by Bigot and Klein [22,Theorem 5.1] in R d , extended to a compact nonparametric formulation in Zemel and Panaretos [134]. There is an infinite-dimensional linear version in Masarotto et al. [91]. The current level of generality appears to be new. Theorem 4.4.1 is a stronger version of Panaretos and Zemel [100, Theorem 1] where it was assumed that τ n must diverge to infinity faster than log n. An analogous construction under the Bayesian paradigm can be found in Galasso et al. [58]. Optimality of the rates of convergence in Theorem 4.6.3 is discussed in detail by Bigot et al. [23], where finiteness of the functional J 2 (see Sect. 4.7) is assumed and consequently O P (τ</p>
        <p>As far as we know, Theorem 4.6.5 (taken from [100]) is the first central limit theorem for Fréchet means in Wasserstein space. When the measures Λ i are observed exactly (no amplitude variation: τ n = ∞ and σ = 0) Kroshnin et al. [84] have recently proven a central limit theorem for random Gaussian measures in arbitrary dimension, extending a previous result of Agueh and Carlier [3]. It seems likely that in a fully nonparametric setting, the rates of convergence (compare Theorem 4.6.3) might be slower than √ n; see Ahidar-Coutrix et al. [4]. The magnitude of the amplitude variation in Theorem 4.6.3 pertains to the rates of convergence of EW p (μ n , μ) to zero (Sect. 4.7). This is a topic of intense research, dating back to the seminal paper by Dudley [46], where a version of Theorem 4.7.8 with p = 1 is shown for the bounded Lipschitz metric. The lower bounds proven in this section were adapted from [46], Fournier and Guillin [54], and Weed and Bach [130].</p>
        <p>The version of Theorem 4.7.8 given here can be found in [130] and extends Boissard and Le Gouic [27]. Both papers [27,130] work in a general setting of complete separable metric spaces. An additional log n term appears in the limiting case d = 2p, as already noted (for p = 1) by [46], and the classical work of Ajtai et al. [5] for μ uniform on [0, 1] 2 . More general results are available in [54]. A longer (but far from being complete) bibliography is given in the recent review by Panaretos and Zemel [101, Subsection 3.3.1], including works by Barthe, Dobrić, Talagrand, and coauthors on almost sure results and deviation bounds for the empirical Wasserstein distance.</p>
        <p>The J 1 condition is due to del Barrio et al. [43], who showed it to be necessary and sufficient for the empirical process √ n(F n -F) to converge in distribution to B • F, with B Brownian bridge. The extension to 1 ≤ p ≤ ∞ (and a lot more) can be found in Bobkov and Ledoux [25], employing order statistics and beta distributions to reduce to the uniform case. Alternatively, one may consult Mason [92], who uses weighted approximations to Brownian bridges.</p>
        <p>An important aspect that was not covered here is that of statistical inference of the Wasserstein distance on the basis of the empirical measure. This is a challenging question and results by del Barrio, Munk, and coauthors are available for onedimensional, elliptical, or discrete measures, as explained in [101, Section 3].</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made.</p>
        <p>The images or other third party material in this chapter are included in the chapter's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
        <p>When given measures μ 1 , . . . , μ N are supported on the real line, computing their Fréchet mean μ is straightforward (Sect. 3.1.4). This is in contrast to the multivariate case, where, apart from the important yet special case of compatible measures, closed-form formulae are not available. This chapter presents an iterative procedure that provably approximates at least a Karcher mean with mild restrictions on the measures μ 1 ,..., μ N . The algorithm is based on the differentiability properties of the Fréchet functional developed in Sect. 3.1.6 and can be interpreted as classical steepest descent in the Wasserstein space W 2 (R d ). It reduces the problem of finding the Fréchet mean to a succession of pairwise transport problems, involving only the Monge-Kantorovich problem between two measures. In the Gaussian case (or any location-scatter family), the latter can be done explicitly, rendering the algorithm particularly appealing (see Sect. 5.4.1).</p>
        <p>This chapter can be seen as a complementary to Chap. 4. On the one hand, one can use the proposed algorithm to construct the regularised Fréchet-Wasserstein estimator λ n that approximates a population version (see Sect. 4.3). On the other hand, it could be that the object of interest is the sample μ 1 , . . . , μ N itself, but that the latter is observed with some amount of noise. If one only has access to proxies μ 1 , . . . , μ N , then it is natural to use their Fréchet mean μ as an estimator of μ. The proposed algorithm can then be used, in principle, in order to construct μ, and the consistency framework of Sect. 4.4 then allows to conclude that if each μ i is consistent, then so is μ.</p>
        <p>After presenting the algorithm in Sect. 5.1, we make some connections to Procrustes analysis in Sect. 5.2. A convergence analysis of the algorithm is carried out in Sect. 5.3, after which examples are given in Sect. 5.4. An extension to infinitely many measures is sketched in Sect. 5.5.</p>
        <p>and the bound on the right-hand side of the last display is minimised when τ = 1.</p>
        <p>Proof. Let S i = t μ i γ 0 be the optimal map from γ 0 to μ i , and set</p>
        <p>Both γ 1 and μ i can be written as push-forwards of γ 0 and (2.3) gives the bound</p>
        <p>.</p>
        <p>For brevity, we omit the subscript L 2 (γ 0 ) from the norms and inner products. Developing the squares, summing over i = 1, . . . , N and using (5.3) gives</p>
        <p>and recalling that W i = S ii yields</p>
        <p>To conclude, observe that ττ 2 /2 is maximised at τ = 1.</p>
        <p>In light of Lemmata 5.1.1 and 5.1.2, we will always take τ j = 1. The resulting iteration is summarised as Algorithm 1. A first step in the convergence analysis is that the sequence (F(γ j )) is nonincreasing and that for any integer k,</p>
        <p>As k → ∞, the infinite sum on the left-hand side converges, so F (γ j ) 2 must vanish as j → ∞. (B) For j = 0, let γ j be an arbitrary absolutely continuous measure.</p>
        <p>(C) For i = 1,...,N solve the (pairwise) Monge problem and find the optimal transport map t</p>
        <p>γ j . (E) Set γ j+1 = T j #γ j , i.e. push-forward γ j via T j to obtain γ j+1 .</p>
        <p>(F) If F (γ j+1 ) &lt; ε, stop, and output γ j+1 as the approximation of μ and t μ i γ j+1 as the approximation of t μ i μ , i = 1,...,N. Otherwise, return to step (C).</p>
        <p>Algorithm 1 is similar in spirit to another procedure, generalised Procrustes analysis, that is used in shape theory. Given a subset B ⊆ R d , most commonly a finite collection of labelled points called landmarks, an interesting question is how to mathematically define the shape of B. One way to reach such a definition is to disregard those properties of B that are deemed irrelevant for what one considers this shape should be; typically, these would include its location, its orientation, and/or its scale. Accordingly, the shape of B can be defined as the equivalence class consisting of all sets obtained as gB, where g belongs to a collection G of transformations of R d containing all combinations of rotations, translations, dilations, and/or reflections (Dryden and Mardia [45,Chapter 4]).</p>
        <p>If B 1 and B 2 are two collections of k landmarks, one may define the distance between their shapes as the infimum of B 1 -gB 2 2 over the group G . In other words, one seeks to register B 2 as close as possible to B 1 by using elements of the group G , with distance being measured as the sum of squared Euclidean distances between the transformed points of B 2 and those of B 1 . In a sense, one can think about the shape problem and the Monge problem as dual to each other. In the former, one is given constraints on how to optimally carry out the registration of the points with the cost being judged by how successful the registration procedure is. In the latter, one imposes that the registration be done exactly, and evaluates the cost by how much the space must be deformed in order to achieve this.</p>
        <p>The optimal g and the resulting distance can be found in closed-form by means of ordinary Procrustes analysis [45,Section 5.2]. Suppose now that we are given N &gt; 2 collections of points, B 1 , . . . , B N , with the goal of minimising the sum of squares g i B ig j B j 2 over g i ∈ G . 1 As in the case of Fréchet means in W 2 (R d ) (Sect. 3.1.2), there is a formulation in terms of sum of squares from the average N -1 ∑ g j B j . Unfortunately, there is no explicit solution for this problem when d ≥ 3. Like Algorithm 1, generalised Procrustes analysis (Gower [66]; Dryden and Mardia [45, p. 90]) tackles this "multimatching" setting by iteratively solving the pairwise problem as follows. Choose one of the configurations as an initial estimate/template, then register every other configuration to the template, employing ordinary Procrustes analysis. The new template is then given by the linear average of the registered configurations, and the process is iterated subsequently.</p>
        <p>Paralleling this framework, Algorithm 1 iterates the two steps of registration and linear averaging given the current template γ j , but in a different manner:</p>
        <p>(1) Registration: by finding the optimal transportation maps t μ i γ j , we identify each μ i with the element t μ i γ j -i = log γ j (μ i ). In this sense, the collection (μ 1 , . . . , μ N ) is viewed in the common coordinate system given by the tangent space at the template γ j and is registered to it.</p>
        <p>(2) Averaging: the registered measures are averaged linearly, using the common coordinate system of the registration step (1), as elements in the linear space Tan γ j . The linear average is then retracted back onto the Wasserstein space via the exponential map to yield the estimate at the ( j + 1)-th step, γ j+1 .</p>
        <p>Notice that in the Procrustes sense, the maps that register each μ i to the template γ j are t γ j μ i , the inverses of t μ i γ j . We will not use the term "registration maps" in the sequel, to avoid possible confusion.</p>
        <p>In order to tackle the issue of convergence, we will use an approach that is specific to the nature of optimal transport. This is because the Hessian-type arguments that are used to prove similar convergence results for steepest descent on Riemannian manifolds (Afsari et al. [1]) or Procrustes algorithms (Le [86]; Groisser [67]) do not apply here, since the Fréchet functional may very well fail to be twice differentiable.</p>
        <p>In fact, even in Euclidean spaces, convergence of steepest descent usually requires a Lipschitz bound on the derivative of F (Bertsekas [19, Subsection 1.2.2]). Unfortunately, F is not known to be differentiable at discrete measures, and these constitute a dense set in W 2 ; consequently, this Lipschitz condition is very unlikely to hold. Still, this specific geometry of the Wasserstein space affords some advantages; for instance, we will place no restriction on the starting point for the iteration, except that it be absolutely continuous; and no assumption on how "spread out" the collection μ 1 ,..., μ N is will be necessary as in, for example, [1,67,86]. Theorem 5.3.1 (Limit Points are Karcher Means) Let μ 1 ,..., μ N ∈ W 2 (R d ) be probability measures and suppose that one of them is absolutely continuous with a bounded density. Then, the sequence generated by Algorithm 1 stays in a compact set of the Wasserstein space W 2 (R d ), and any limit point of the sequence is a Karcher mean of (μ 1 ,..., μ N ).</p>
        <p>Since the Fréchet mean μ is a Karcher mean (Proposition 3.1.8), we obtain immediately: Alternatively, combining Theorem 5.3.1 with the optimality criterion Theorem 3.1.15 shows that the algorithm converges to μ when the appropriate assumptions on {μ i } and the Karcher mean μ = lim γ j are satisfied. This allows to conclude that Algorithm 1 converges to the unique Fréchet mean when μ i are Gaussian measures (see Theorem 5.4.1). The proof of Theorem 5.3.1 is rather elaborate, since we need to use specific methods that are tailored to the Wasserstein space. Before giving the proof, we state two important consequences. The first is the uniform convergence of the optimal maps t μ i γ j to t μ i μ on compacta. This convergence does not immediately follow from the Wasserstein convergence of γ j to μ, and is also established for the inverses. Both the formulation and the proof of this result are similar to those of Theorem 4.4.3. The proof of Theorem 5.3.1 is achieved by establishing the following facts:</p>
        <p>1. The sequence (γ j ) stays in a compact subset of W 2 (R d ) (Lemma 5.3.5). 2. Any limit of (γ j ) is absolutely continuous (Proposition 5.3.6 and the paragraph preceding it). 3. Algorithm 1 acts continuously on its argument (Corollary 5.3.8).</p>
        <p>Since it has already been established that F (γ j ) → 0, these three facts indeed suffice. Proof. For all j ≥ 1, γ j takes the form M n #π, where M N (x 1 , . . . , x N ) = x and π is a multicoupling of μ 1 ,..., μ N . The compactness of this set has been established in Step 2 of the proof of Theorem 3.1.5; see page 63 in the supplement, where this is done in a more complicated setup.</p>
        <p>A closer look at the proof reveals that a more general result holds true. Let A denote the steepest descent iteration, that is, A (γ j ) = γ j+1 . Then the image of A , {A μ : μ ∈ W 2 (R d ) absolutely continuous} has a compact closure in W 2 (R d ). This is also true if R d is replaced by a separable Hilbert space.</p>
        <p>In order to show that a weakly convergent sequence (γ j ) of absolutely continuous measures has an absolutely continuous limit γ, it suffices to show that the densities of γ j are uniformly bounded. Indeed, if C is such a bound, then for any open O ⊆ R d , lim inf γ k (O) ≤ CLeb(O), so γ(O) ≤ CLeb(O) by the portmanteau Lemma 1.7.1. It follows that γ is absolutely continuous with density bounded by C. We now show that such C can be found that applies to all measures in the image of A , hence to all sequences resulting from iterations of Algorithm 1. Proposition 5.3.6 (Uniform Density Bound) For each i = 1, . . . , N denote by g i the density of μ i (if it exists) and g i ∞ its supremum, taken to be infinite if g i does not exist (or if g i is unbounded). Let γ 0 be any absolutely continuous probability measure. Then the density of γ 1 = A (γ 0 ) is bounded by the 1/d-th harmonic mean of g i ∞ ,</p>
        <p>.</p>
        <p>The constant C μ depends only on the measures (μ 1 ,..., μ N ), and is finite as long as one μ i has a bounded density, since C μ ≤ N d g i ∞ for any i.</p>
        <p>Proof. Let h i be the density of γ i . By the change of variables formula, for γ 0 -almost any x h 1 (t From this, we obtain an upper bound for h 1 :</p>
        <p>1</p>
        <p>Let Σ be the set of points where this inequality holds, then γ 0 (Σ ) = 1. Hence γ 1 (t</p>
        <p>γ 0 (Σ ))] ≥ γ 0 (Σ ) = 1.</p>
        <p>Thus, γ 1 -almost surely and for all i, h 1 (y) ≤ C μ .</p>
        <p>The third statement (continuity of A ) is much more subtle to establish, and its rather lengthy proof is given next. In view of Proposition 5.3.6, the uniform bound on the densities is not a hindrance for the proof of convergence of Algorithm 1.</p>
        <p>Proposition 5.3.7 Let (γ n ) be a sequence of absolutely continuous measures with uniformly bounded densities, suppose that W 2 (γ n , γ) → 0, and let</p>
        <p>Proof. As has been established in the discussion before Proposition 5.3.6, the limit γ must be absolutely continuous, so η is well-defined. In view of Theorem 2. γ j (x), x),</p>
        <p>and g defined analogously. The proof, given in full detail on page 124 of the supplement, is sketched here.</p>
        <p>Step 1: Truncation. Since γ n converge in the Wasserstein space, they satisfy the uniform integrability (2.4) and absolute continuity (2.7) by Theorem 2.2.1. Consequently, g n,R = min(g n , 4R) is uniformly close to g n :</p>
        <p>We may thus replace g n by a bounded version g n,R .</p>
        <p>Step 2: Convergence of g n to g. By Proposition 1.7.11, the optimal maps t μ i γ n converge to t μ i γ and (since h is continuous), g n → g uniformly on "nice" sets Ω ⊆ E = suppγ. Write</p>
        <p>Step 3: Bounding the first two integrals. The first integral vanishes as n → ∞, by the portmanteau Lemma 1.7.1, and the second by uniform convergence.</p>
        <p>Step 4: Bounding the third integral. The integrand is bounded by 8R, so it suffices to bound the measures of R d \ Ω . This is a bit technical, and uses the uniform density bound on (γ n ) and the portmanteau lemma. Proof (Proof of Corollary 5.3.4). Choose h in the proof of Proposition 5.3.7 to depend only on t 1 ,...,t n .</p>
        <p>Proof (Proof of Theorem 5.3.3). Let E = supp μ and set A i = E den ∩ {x : t μ i μ (x) is univalued}. As μ is absolutely continuous, μ(A i ) = 1, and the same is true for A = ∩ N i=1 A i . The first assertion then follows from Proposition 1.7.11.</p>
        <p>The second statement is proven similarly. Let E i = suppμ i and notice that by absolute continuity the B i = (E i ) den ∩ {x : t μ μ i (x) is univalued} has measure 1 with respect to μ i . Apply Proposition 1.7.11. If in addition E 1 = • • • = E N , then μ i (B) = 1 for B = ∩B i .</p>
        <p>As an illustration, we implement Algorithm 1 in several scenarios for which pairwise optimal maps can be calculated explicitly at every iteration, allowing for fast computation without error propagation. In each case, we give some theory first, describing how the optimal maps are calculated, and then implement Algorithm 1 on simulated examples.</p>
        <p>No example illustrates the use of Algorithm 1 better than the Gaussian case. This is so because optimal maps between centred nondegenerate Gaussian measures with covariances A and B have the explicit form (see Sect. 1.6.3)</p>
        <p>with the obvious slight abuse of notation. In contrast, the Fréchet mean of a collection of Gaussian measures (one of which nonsingular) does not admit a closed-form formula and is only known to be a Gaussian measure whose covariance matrix Γ is the unique invertible root of the matrix equation</p>
        <p>where S i is the covariance matrix of μ i . Given the formula for t B A , application of Algorithm 1 to Gaussian measures is straightforward. The next result shows that, in the Gaussian case, the iterates must converge to the unique Fréchet mean, and that (5.4) can be derived from the characterisation of Karcher means. Theorem 5.4.1 (Convergence in Gaussian Case) Let μ 1 , . . . , μ N be Gaussian measures with zero means and covariance matrices S i with S 1 nonsingular, and let the initial point γ 0 be N (0,Γ 0 ) with Γ 0 nonsingular. Then the sequence of iterates generated by Algorithm 1 converges to the unique Fréchet mean of (μ 1 , . . . , μ N ).</p>
        <p>Proof. Since the optimal maps are linear, so is their mean and therefore γ k is a Gaussian measure for all k, say N (0,Γ k ) with Γ k nonsingular. Any limit point of γ is a Karcher mean by Theorem 5.3.1. If we knew that γ itself were Gaussian, then it actually must be the Fréchet mean because N -1 ∑ t μ i γ equals the identity everywhere on R d (see the discussion before Theorem 3.1.15).</p>
        <p>Let us show that every limit point γ is indeed Gaussian. It suffices to prove that (Γ k ) is a bounded sequence, because if Γ k → Γ , then N (0,Γ k ) → N (0,Γ ) weakly, as can be seen from either Lehmann-Scheffé's theorem (the densities converge) or Lévy's continuity theorem (the characteristic functions converge).</p>
        <p>To see that (Γ k ) is bounded, observe first that for any centred (Gaussian or not) measure μ with covariance matrix S, W 2 2 (μ, δ 0 ) = tr[S],</p>
        <p>where δ 0 is a Dirac mass at the origin. (This follows from the spectral decomposition of S.) Therefore 0 ≤ tr[Γ k ] = W 2 2 (γ k , δ 0 ) is bounded uniformly, because {γ k } stays in a Wasserstein-compact set by Lemma 5.3.5. If we define C = sup k tr[Γ k ] &lt; ∞, then all the diagonal elements of Γ k are bounded uniformly. When A is symmetric and positive semidefinite, 2|A i j | ≤ A ii + A i j . Consequently, all the entries of Γ k are bounded uniformly by C, which means that (Γ k ) is a bounded sequence.</p>
        <p>From the formula for the optimal maps, we see that if Γ is the covariance of the Fréchet mean, then</p>
        <p>and we recover the fixed point equation (5.4).</p>
        <p>If the means are nonzero, then the optimal maps are affine and the same result applies; the Fréchet mean is still a Gaussian measure with covariance matrix Γ and mean that equals the average of the means of μ i , i = 1, . . . , N.</p>
        <p>Figure 5.1 shows density plots of N = 4 centred Gaussian measures on R 2 with covariances S i ∼ Wishart(I 2 , 2), and Fig. 5.2 shows the density of the resulting Fréchet mean. In this particular example, the algorithm needed 11 iterations starting from the identity matrix. The corresponding optimal maps are displayed in Fig. 5.3. It is apparent from the figure that these maps are linear, and after a more careful reflection one can be convinced that their average is the identity. The four plots in the figure are remarkably different, in accordance with the measures themselves having widely varying condition numbers and orientations; μ 3 and more so μ 4 are very concentrated, so the optimal maps "sweep" the mass towards zero. In contrast, the optimal maps to μ 1 and μ 2 spread the mass out away from the origin.</p>
        <p>We next discuss the behaviour of the algorithm when the measures are compatible. Recall that a collection C ⊆ W 2 (X ) is compatible if for all γ, ρ, μ ∈ C , t ν μ • t μ γ = t ν γ in L 2 (γ) (Definition 2.3.1). Boissard et al. [28] showed that when this condition holds, the Fréchet mean of (μ 1 , . . . , μ N ) can be found by simple computations involving the iterated barycentre. We again denote by γ 0 the initial point of Algorithm 1, which can be any absolutely continuous measure. Proof. By definition, the next iterate is</p>
        <p>which is the Fréchet mean by Theorem 3.1.9.</p>
        <p>In this case, Algorithm 1 requires the calculation of N pairwise optimal maps, and this can be reduced to N -1 if the initial point is chosen to be μ 1 . This is the same computational complexity as the calculation of the iterated barycentre proposed in [28].</p>
        <p>When the measures have a common copula, finding the optimal maps reduces to finding the optimal maps between the one-dimensional marginals (see Lemma 2.3.3) and this can be done using quantile functions as described in Sect. 1.5. The marginal Fréchet means are then plugged into the common copula to yield the joint Fréchet mean. We next illustrate Algorithm 1 in three such scenarios.</p>
        <p>When the measures are supported on the real line, there is no need to use the algorithm since the Fréchet mean admits a closed-form expression in terms of quantile functions (see Sect. 3.1.4). We nevertheless discuss this case briefly because we build upon this construction in subsequent examples. Given that t ν μ = F -1 ν • F μ , we may apply Algorithm 1 starting from one of these measures (or any other measure). Figure 5.4 plots N = 4 univariate densities and the Fréchet mean yielded by the algorithm in two different scenarios. At the left, the densities were generated as</p>
        <p>with φ the standard normal density, and the parameters generated independently as [3,13], σ i 1 , σ i 2 ∼ Gamma (4,4).</p>
        <p>At the right of Fig. 5.4, we used a mixture of a shifted gamma and a Gaussian:</p>
        <p>with β i ∼ Gamma(4, 1), m i 3 ∼ U [1,4], m i 4 ∼ U[-4, -1]. The resulting Fréchet mean density for both settings is shown in thick light blue, and can be seen to capture the bimodal nature of the data. Even though the Fréchet mean of Gaussian mixtures is not a Gaussian mixture itself, it is approximately so, provided that the peaks are separated enough. Figure 5.5 shows the optimal maps pushing the Fréchet mean μ to the measures μ 1 ,..., μ N in each case. If one ignores the "middle part" of the x axis, the maps appear (approximately) affine for small values of x and for large values of x, indicating how the peaks are shifted. In the middle region, the maps need to "bridge the gap" between the different slopes and intercepts of these affine maps.</p>
        <p>We next take measures μ i on R 2 , having independent marginal densities f i X as in (5.5), and f i Y as in (5.6). Figure 5.6 shows the density plot of N = 4 such measures, constructed as the product of the measures from Fig. 5.4. One can distinguish the independence by the "parallel" structure of the figures: for every pair (y 1 , y 2 ), the ratio g(x, y 1 )/g(x, y 2 ) does not depend on x (and vice versa, interchanging x and y). Figure 5.7 plots the density of the resulting Fréchet mean. We observe that the Fréchet mean captures the four peaks and their location. Furthermore, the parallel nature of the figure is preserved in the Fréchet mean. Indeed, by Lemma 3.1.11 the Fréchet mean is a product measure. The optimal maps, in Fig. 5.10, are the same as in the next example, and will be discussed there.</p>
        <p>Let μ i be a measure on R 2 with density</p>
        <p>where f i X and f i Y are random densities on the real line with distribution functions F i X and F i Y , and c is a copula density. Figure 5.8 shows the density plot of N = 4 such measures, with f i X generated as in (5.5), f i Y as in (5.6), and c is the Frank(-8) copula density, while Fig. 5.9 plots the density of the Fréchet mean obtained. (For ease of comparison we use the same realisations of the densities that appear in Fig. 5.4.) The Fréchet mean can be seen to preserve the shape of the density, having four clearly distinguished peaks. Figure 5.10, depicting the resulting optimal maps, allows for a clearer interpretation: for instance, the leftmost plot (in black) shows more clearly that the map splits the mass around x = -2 to a much wider interval; and conversely a very large amount mass is sent to x ≈ 2. This rather extreme behaviour matches the peak of the density of μ 1 located at x = 2.</p>
        <p>We now apply Algorithm 1 in a situation that entangles two of the previous settings. Let U be a fixed 3 × 3 real orthogonal matrix with columns U 1 , U 2 , U 3 and let μ i have density g i (y 1 , y 2 , y 3 ) = g i (y) = f i (U t 3 y)</p>
        <p>with f i bounded density on the real line and S i ∈ R 2×2 positive definite. We simulated N = 4 such densities with f i as in (5.5) and S i ∼ Wishart(I 2 , 2). We apply Algorithm 1 to this collection of measures and find their Fréchet mean (see the end of this subsection for precise details on how the optimal maps were calculated). Figure 5.11 shows level set of the resulting densities for some specific values. The bimodal nature of f i implies that for most values of a, {x : f i (x) = a} has four elements. Hence, the level sets in the figures are unions of four separate parts, with each peak of f i contributing two parts that form together the boundary of an ellipsoid in R 3 (see Fig. 5.12). The principal axes of these ellipsoids and their position in R 3 differ between the measures, but the Fréchet mean can be viewed as an average of those in some sense.</p>
        <p>In terms of orientation (principal axes) of the ellipsoids, the Fréchet mean is most similar to μ 1 and μ 2 , whose orientations are similar to one another. Let us now see how the optimal maps are calculated. If Y = (y 1 , y 2 , y 3 ) ∼ μ i , then the random vector (x 1 , x 2 , x 3 ) = X = U -1 Y has joint density</p>
        <p>so the probability law of X is ρ i ⊗ ν i with ρ i centred Gaussian with covariance matrix Σ i and ν i having density f i on R. By Lemma 3.1.11, the Fréchet mean of (U -1 #μ i ) is the product measure of that of (ρ i ) and that of (ν i ); by Lemma 3.1.12, the Fréchet mean of (μ i ) is therefore</p>
        <p>where Σ is the Fréchet-Wasserstein mean of Σ 1 , . . . , Σ N . Starting at an initial point γ 0 = U#(N (0, Σ 0 ) ⊗ ν 0 ), with ν 0 having continuous distribution F ν 0 , the optimal maps are U • t i 0 •U -1 = ∇(ϕ i 0 •U -1 ) with</p>
        <p>the gradients of the convex function</p>
        <p>where we identify t Σ i γ 0 with the positive definite matrix (Σ i ) 1/2 [(Σ i ) 1/2 Σ 0 (Σ i ) 1/2 ] -1/2 (Σ i ) 1/2 that pushes forward N (0, Σ 0 ) to N (0, Σ i ). Due to the one-dimensionality, the algorithm finds the third component of the rotated measures after one step, but the convergence of the Gaussian component requires further iterations.</p>
        <p>Such a collection exists, by using bounded Lipschitz functions</p>
        <p>(Dudley [47, Theorem 11.4.1]</p>
        <p>); an alternative construction can be found in Ambrosio et al.</p>
        <p>[</p>
        <p>12, Section 5.1]. Thus: Proposition 2.2.6 (Empirical Measures in W p ) For any μ ∈ P(X ) and the corresponding sequence of empirical measures μ n , W p (μ n , μ) → 0 almost surely if and only if μ ∈ W p (X ). Indeed, if μ / ∈ W p (X ), then W p (μ n , μ) is infinite for all n, since μ n is compactly supported, hence in W p (X ). Proposition 2.2.6 is the basis for constructing dense subsets of the Wasserstein space. Proposition 2.2.8 (Completeness) The Wasserstein space W p (X ) is complete. One may find two different proofs in Villani [125, Theorem 6.18] and Ambrosio et al. [12, Proposition 7.1.5]. On page 43 of the supplement, we sketch an alternative argument based on completeness of the weak topology.</p>
        <p>Λ</p>
        <p>this condition is also necessary</p>
        <p>[2, Proposition 3.8]</p>
        <p>, hence characterising Fréchet means in R d . It will allow us to easily deduce some equivariance results for Fréchet means with respect to independence (Lemma 3.1.11) and rotations</p>
        <p>(3.1.12)</p>
        <p>. More importantly, it provides a sufficient condition under which a local minimum of F is a global minimum (Theorem 3.1.15) and the same idea can be used to relate the population Fréchet mean to the expected value of the optimal maps (Theorem 4.2.4). Recall that φ * denotes the Legendre transform of φ , as defined on page 14. Proposition 3.1.10 (Fréchet Means and Potentials) Let μ 1 , . . . , μ N ∈ W 2 (X ) be absolutely continuous, let γ ∈ W 2 (X ) and denote by φ * i the convex potentials of t then γ is the unique Fréchet mean of μ 1 , . . . , μ N .</p>
        <p>Proof.</p>
        <p>n are independent as the sets (T -1 (A k )) are disjoint, and Π (A) follows a Poisson distribution with mean λ (T -1 (A)) = Λ (A). This is precisely the definition of a Cox process: conditional upon the driving measure Λ , Π is a Poisson process with mean measure λ . For this reason, it is also called a doubly stochastic process; in our context, the phase variation is</p>
        <p>E.g. by Rachev and Rüschendorf[107], Villani[124,125], Ambrosio and Gigli[10], Ambrosio et al.[12], and more recently by Santambrogio[119].</p>
        <p>Optimal Transport</p>
        <p>Weak convergence is sometimes called narrow convergence, weak* convergence, or convergence in distribution.</p>
        <p>1.7 Stability of Solutions Under Weak Convergence</p>
        <p>In general, ψ c inherits the modulus of continuity of c, see Santambrogio[119, page 11].</p>
        <p>Gradients of Borel functions are measurable, as the limit can be taken on a countable set. The inverse (∇h) -1 equals the gradient of the Legendre transform h * and is therefore Borel measurable.</p>
        <p>© The Author(s) 2020 V. M. Panaretos, Y. Zemel, An Invitation to Statistics in Wasserstein Space, SpringerBriefs in Probability and Mathematical Statistics, https://doi.org/10.1007/978-3-030-38438-8 2</p>
        <p>The Wasserstein Space</p>
        <p>It should be remarked that this is a Hilbertian property (or at least a property linked to an inner product), not merely a linear property. In other words, it does not extend to Banach spaces. As an example, let H = R</p>
        <p>with the L 1 norm and consider the vertices (0, 0), (0, 1), and (1, 0) of the unit simplex. The mean of these is (1/3, 1/3) but for (x, y) in the triangle,F(x, y) = (x + y) 2 + (x + 1y) 2 + (1x + y) 2 = 2 + x 2 + y 2 + (xy)2 is minimised at (0, 0). © The Author(s) 2020 V. M. Panaretos, Y. Zemel, An Invitation to Statistics in Wasserstein Space, SpringerBriefs in Probability and Mathematical Statistics, https://doi.org/10.1007/978-3-030-38438-8</p>
        <p>Fréchet Means in the Wasserstein Space W 2</p>
        <p>Interestingly, Fréchet himself[56] considered the Wasserstein metric between probability measures on R, and some refer to this as the Fréchet distance (e.g., Dowson and Landau[44]), which is another reason to use this terminology.</p>
        <p>The notion of Fréchet derivative is also named after Maurice Fréchet, but is not directly related to Fréchet means.</p>
        <p>For instance, the arithmetic average of two scalar Gaussians N(μ 1 , 1) and N(μ</p>
        <p>, 1) will be their mixture with equal weights, but their Fréchet-Wasserstein average will be the Gaussian N( 1 2 μ 1 + 1 2 μ 2 , 1) (see Lemma 4.2.1), which is arguably more representative from an intuitive point of view.In much the same way, the Fréchet-Wasserstein average of probability measures representing some type of object (e.g., normalised greyscale images of faces) will also be an object of the same type. This sort of phenomenon is well-known in manifold statistics, more generally, and is arguably one of the key motivations to account for the non-linear geometry of the sample space, rather than imbed it into a larger linear space and use the addition operation.© The Author(s) 2020 V. M. Panaretos, Y. Zemel, An Invitation to Statistics in Wasserstein Space, SpringerBriefs in Probability and Mathematical Statistics, https://doi.org/10.1007/978-3-030-38438-8 4</p>
        <p>Phase Variation and Fréchet Means</p>
        <p>If the cumulative count process Γ (t) = Π [0,t) is mean-square continuous, then the use of the term "amplitude variation" can be seen to remain natural, as Γ (t) will admit a Karhunen-Loève expansion, with all stochasticity being attributable to the random amplitudes in the expansion.</p>
        <p>This can be defined as Bochner integral in the space of measurable bounded T : K → K.</p>
        <p>In finite dimensional (or more generally, locally compact metric) spaces. If X is an infinitedimensional Hilbert space, the vague topology is trivial. This is stated and proved as Lemma 5 on page 27 in the supplement.</p>
        <p>Phase Variation and Fréchet Means For any integer k define ζ k : [0, 1] → [0, 1] by</p>
        <p>Construction of Fréchet Means and Multicouplings</p>
        <p>One needs to add an additional constraint to prevent registering all the collections to the origin.</p>
        <p>© The Author(s) 2020 V. M. Panaretos, Y. Zemel, An Invitation to Statistics in Wasserstein Space, SpringerBriefs in Probability and Mathematical Statistics, https://doi.org/10.1007/978-3-030-38438-8</p>
        <p>We wish to thank three anonymous reviewers for their thoughtful feedback. We are especially indebted to one of them, whose analytical insights were particularly useful. Any errors or omissions are, of course, our own responsibility. Victor M. Panaretos gratefully acknowledges support from a European Research Council Starting Grant. Yoav Zemel was supported by Swiss National Science Foundation Grant # 178220. Finally, we wish to thank Mark Podolskij and Donna Chernyk for their patience and encouragement.</p>
        <p>Victor M. Panaretos Cambridge, UK Yoav Zemel</p>
        <p>These findings are reaffirmed in Fig. 4.13 that show the registered point processes again as a function of s. We see that only minor differences are present as s varies from 0.1 to 1, for example, in the grey (8), black (17), and green (19) processes. When s = 3, the distortion becomes quite more substantial. This phenomenon repeats itself across all combinations of n, τ, and s tested.</p>
        <p>Since the conditional mean measures Λ i are discretely observed, the rate of convergence of our estimators will be affected by the rate at which the number of observed points per process N</p>
        <p>(n) i increases to infinity. The latter is controlled by the next lemma, which is valid for any complete separable metric space X .</p>
        <p>Throughout this section, we assume that N is a fixed integer and consider a fixed collection</p>
        <p>with μ 1 absolutely continuous with bounded density, (5.1) whose unique (Proposition 3.1.8) Fréchet mean μ is sought. It has been established that if γ is absolutely continuous then the associated Fréchet functional</p>
        <p>has Fréchet derivative (Theorem 3.1.14)</p>
        <p>) be an absolutely continuous measure, representing our current estimate of the Fréchet mean at step j. Then it makes sense to introduce a step size τ j &gt; 0, and to follow the steepest descent of F given by the negative of the gradient:</p>
        <p>In order to employ further descent at γ j+1 , it needs to be verified that F is differentiable at γ j+1 , which amounts to showing that the latter stays absolutely continuous. This will happen for all but countably many values of the step size τ j , but necessarily if the latter is contained in [0, 1]:</p>
        <p>The idea is that push-forwards of γ 0 under monotone maps are absolutely continuous if and only if the monotonicity is strict, a property preserved by averaging. See page 118 in the supplement for the details. Lemma 5.1.1 suggests that the step size should be restricted to [0, 1]. The next result suggests that the objective function essentially tells us that the optimal step size, achieving the maximal reduction of the objective function (thus corresponding to an approximate line search), is exactly equal to 1. It does not rely on finite-dimensional arguments and holds when replacing R d by a separable Hilbert space.</p>
        <p>Let Λ ∈ W 2 (R d ) be a random measure with finite Fréchet functional. The population version of (5.1) is q = P(Λ absolutely continuous with density bounded by M) &gt; 0 for some M &lt; ∞, (5.7) which we assume henceforth. This condition is satisfied if and only if P(Λ absolutely continuous with bounded density) &gt; 0.</p>
        <p>These probabilities are well-defined because the set In light of Theorem 3.2.13, we can define a population version of Algorithm 1 with the iteration function</p>
        <p>The (Bochner) expectation is well-defined in L 2 (γ) because the random map t Λ γ is measurable (Lemma 2.4.6). Since L 2 (γ) is a Hilbert space, the law of large numbers applies there, and results for the empirical version carry over to the population version by means of approximations. In particular: Lemma 5.5.1 Any descent iterate γ has density bounded by q -d M, where q and M are as in (5.7).</p>
        <p>Proof. The result is true in the empirical case, by Proposition 5.3.6. The key point (observed by Pass [102,Subsection 3.3]) is that the number of measures does not appear in the bound q -d M.</p>
        <p>Let Λ 1 , . . . be a sample from Λ and let q n be the proportion of measures in (Λ 1 , . . . ,Λ n ) that have density bounded by M. Then both n -1 ∑ n i=1 t Λ i γ → Et Λ γ and q n → q almost surely by the law of large numbers. Pick any ω in the probability space for which this happens and notice that (invoking Lemma 2.4.5)</p>
        <p>Let λ n denote the measure in the last limit. By Proposition 5.3.6, its density is bounded by q -d n M → q -d M almost surely, so for any C &gt; q -d M and n large, λ n has density bounded by C. By the portmanteau Lemma 1.7.1, so does lim Though it follows that every Karcher mean of Λ has a bounded density, we cannot yet conclude that the same bound holds for the Fréchet mean, because we need an apriori knowledge that the latter is absolutely continuous. This again can be achieved by approximations: Theorem 5.5.2 (Bounded Density for Population Fréchet Mean) Let Λ ∈ W 2 (R d ) be a random measure with finite Fréchet functional. If Λ has a bounded density with positive probability, then the Fréchet mean of Λ is absolutely continuous with a bounded density.</p>
        <p>Proof. Let q and M be as in (5.7), Λ 1 , . . . be a sample from Λ , and q n the proportion of (Λ i ) i≤n with density bounded by M. The empirical Fréchet mean λ n of the sample (Λ 1 , . . . ,Λ n ) has a density bounded by q -d n M. The Fréchet mean λ of Λ is unique by Proposition 3.2.7, and consequently λ n → λ in W 2 (R d ) by the law of large numbers (Corollary 3.2.10). For any C &gt; lim sup q -d n M, the density of λ is bounded by C by the portmanteau Lemma 1.7.1, and the limsup is q -d M almost surely. Thus, the density is bounded by q -d M.</p>
        <p>In the same way, one shows the population version of Theorem 3.1.9: It is of course sufficient that {γ} ∪ Λ (Ω \ N ) be compatible for some null set N ⊂ Ω .</p>
        <p>The algorithm outlined in this chapter was suggested independently in this steepest descent form by Zemel and Panaretos [134] and in the form a fixed point equation iteration by Álvarez-Esteban et al. [9]. These two papers provide different alternative proofs of Theorem 5.3.1. The exposition here is based on [134]. Although longer and more technical than the one in [9], the formalism in [134] is amenable to directly treating the optimal maps (Theorem 5.3.3) and the multicouplings (Corollary 5.3.4). On the flip side, it is noteworthy that the proof of the Gaussian case (Theorem 5.4.1) given in [9] is more explicit and quantitative; for instance, it shows the additional property that the traces of the matrix iterates are monotonically increasing.</p>
        <p>Developing numerical schemes for computing Fréchet means in W 2 (R d ) is a very active area of research, and readers are referred to the recent monograph of Peyré and Cuturi [103, Section 9.2] for a survey. In recent work, Backhoff-Varaguas et al. [15] propose a stochastic steepest descent for finding Karcher means of a population Fréchet functional associated with a random measure Λ . At iterate j, one replaces γ j by</p>
        <p>The analogue of Theorem 5.3.1 holds under further conditions.</p>
        <p>Open Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence and indicate if changes were made. The images or other third party material in this chapter are included in the chapter's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</p>
    </text>
</tei>

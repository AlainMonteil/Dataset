<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T16:10+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties.Semantic labelling is highly correlated with geometry and radiance reconstruction, as scene entities with similar shape and appearance are more likely to come from similar classes. Recent implicit neural reconstruction techniques are appealing as they do not require prior training data, but the same fully self-supervised approach is not possible for semantics because labels are human-defined properties.</p>
        <p>We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.We extend neural radiance fields (NeRF) to jointly encode semantics with appearance and geometry, so that complete and accurate 2D semantic labels can be achieved using a small amount of in-place annotations specific to the scene. The intrinsic multi-view consistency and smoothness of NeRF benefit semantics by enabling sparse labels to efficiently propagate. We show the benefit of this approach when labels are either sparse or very noisy in room-scale scenes. We demonstrate its advantageous properties in various interesting applications such as an efficient scene labelling tool, novel semantic view synthesis, label denoising, super-resolution, label interpolation and multi-view semantic label fusion in visual semantic mapping systems.</p>
        <p>Enabling intelligent agents, such as indoor mobile robots, to plan context-sensitive actions in their environment requires both a geometric and semantic understanding of the scene. Machine learning methods have proven to be valuable in both geometric and semantic prediction tasks, but the performance of these methods suffers when the distribution of the training data does not match the scenes observed at test-time. Though the issue can be mitigated by gathering costly annotated data or semi-supervised learning, it is not always feasible in open-set scenarios with various known and unknown classes. For this reason, it is advantageous to have methods that can self-supervise. In particular, there has been recent success in using scene-specific methods (e.g. NeRF [16]) that represent the shape and radiance of a single scene with a neural network trained from scratch using only images and associated camera poses.Enabling intelligent agents, such as indoor mobile robots, to plan context-sensitive actions in their environment requires both a geometric and semantic understanding of the scene. Machine learning methods have proven to be valuable in both geometric and semantic prediction tasks, but the performance of these methods suffers when the distribution of the training data does not match the scenes observed at test-time. Though the issue can be mitigated by gathering costly annotated data or semi-supervised learning, it is not always feasible in open-set scenarios with various known and unknown classes. For this reason, it is advantageous to have methods that can self-supervise. In particular, there has been recent success in using scene-specific methods (e.g. NeRF [16]) that represent the shape and radiance of a single scene with a neural network trained from scratch using only images and associated camera poses.</p>
        <p>Semantic scene understanding means attaching class la-Semantic scene understanding means attaching class la-</p>
        <p>Figure 1: Neural radiance fields (NeRF) jointly encoding appearance and geometry contain strong priors for segmentation and clustering. We build upon this to create a scenespecific 3D semantic representation, Semantic-NeRF, and show that it can be efficiently learned with in-place supervision to perform various potential applications.Figure 1: Neural radiance fields (NeRF) jointly encoding appearance and geometry contain strong priors for segmentation and clustering. We build upon this to create a scenespecific 3D semantic representation, Semantic-NeRF, and show that it can be efficiently learned with in-place supervision to perform various potential applications.</p>
        <p>bels to a geometric model. The tasks of estimating the geometry of a scene and predicting its semantic labels are strongly related, as parts of a scene that have similar shape are more likely to belong to the same semantic category than those which differ greatly. This has been shown in work on multi-task learning [9,33], where networks that simultaneously predict both shape and semantics perform better than when the tasks are tackled separately. Unlike scene geometry, however, semantic classes are a human-defined concept and it is not possible to semantically label a novel scene in a purely self-supervised manner. The best that could be achieved would be to cluster self-similar structures of a scene into categories; but some labelling would always be needed to associate these clusters with human-defined semantic classes.bels to a geometric model. The tasks of estimating the geometry of a scene and predicting its semantic labels are strongly related, as parts of a scene that have similar shape are more likely to belong to the same semantic category than those which differ greatly. This has been shown in work on multi-task learning [9,33], where networks that simultaneously predict both shape and semantics perform better than when the tasks are tackled separately. Unlike scene geometry, however, semantic classes are a human-defined concept and it is not possible to semantically label a novel scene in a purely self-supervised manner. The best that could be achieved would be to cluster self-similar structures of a scene into categories; but some labelling would always be needed to associate these clusters with human-defined semantic classes.</p>
        <p>In this paper, we show how to design a scene-specific network for joint geometric and semantic prediction and train it on images from a single scene with only weak semantic supervision (and no geometric supervision). Because our single network must generate both geometry and semantics, the correlation between these tasks means that semantics prediction can benefit from the smoothness, coherence and self-similarity learned by self-supervision for geometry. In addition, multi-view consistency is inherent to the training process and enables the network to produce accurate semantic labels of the scene, including for views that are substantially different from any in the input set.In this paper, we show how to design a scene-specific network for joint geometric and semantic prediction and train it on images from a single scene with only weak semantic supervision (and no geometric supervision). Because our single network must generate both geometry and semantics, the correlation between these tasks means that semantics prediction can benefit from the smoothness, coherence and self-similarity learned by self-supervision for geometry. In addition, multi-view consistency is inherent to the training process and enables the network to produce accurate semantic labels of the scene, including for views that are substantially different from any in the input set.</p>
        <p>Our system takes as input a set of RGB images with associated known camera poses. We also supply some partial or noisy semantic labels for the images, such as ground truth labels for a small fraction of the images, or noisy or coarse label maps for a higher number of images. We train our network to jointly produce implicit 3D representations of both the geometry and semantics for the whole scene. We evaluate our system both quantitatively and qualitatively on scenes from the Replica dataset [28], and qualitatively on real-world scenes from the ScanNet dataset [3]. Generating dense semantic labels for a whole scene from partial or noisy input labels is important for practical applications, like when a robot encounters a new scene and either only a small amount of in-situ labelling is feasible, or only an imperfect single-view network is available.Our system takes as input a set of RGB images with associated known camera poses. We also supply some partial or noisy semantic labels for the images, such as ground truth labels for a small fraction of the images, or noisy or coarse label maps for a higher number of images. We train our network to jointly produce implicit 3D representations of both the geometry and semantics for the whole scene. We evaluate our system both quantitatively and qualitatively on scenes from the Replica dataset [28], and qualitatively on real-world scenes from the ScanNet dataset [3]. Generating dense semantic labels for a whole scene from partial or noisy input labels is important for practical applications, like when a robot encounters a new scene and either only a small amount of in-situ labelling is feasible, or only an imperfect single-view network is available.</p>
        <p>Most existing 3D semantic mapping and understanding systems work by attaching (fused) semantic labels to a 3D geometric representation created by a standard reconstruction method. For example, [6] uses point clouds, [13] and [22] use surfels, [18] uses voxels, and [12] uses signed distance fields. These classical 3D geometric representations are all limited in their ability to efficiently represent fine details in complex topologies. Volumetric representations, for example, have a convenient structure for parallel processing or use with convolutional neural networks, but suffer from large memory requirements due to discretisation that ultimately limits the resolution it can represent.Most existing 3D semantic mapping and understanding systems work by attaching (fused) semantic labels to a 3D geometric representation created by a standard reconstruction method. For example, [6] uses point clouds, [13] and [22] use surfels, [18] uses voxels, and [12] uses signed distance fields. These classical 3D geometric representations are all limited in their ability to efficiently represent fine details in complex topologies. Volumetric representations, for example, have a convenient structure for parallel processing or use with convolutional neural networks, but suffer from large memory requirements due to discretisation that ultimately limits the resolution it can represent.</p>
        <p>To help overcome these limitations, many learning-based representations have been developed. Code-based representations, for example, use the latent code of an autoencoder as a compact representation of the scene. Generative Query Networks (GQN) [5] can represent simple 3D scenes using a latent scene representation vector. 
            <rs type="software">CodeSLAM</rs> [1] uses a compact and optimisable latent code to represent dense scene geometry in a view-based visual odometry system. The approach of 
            <rs type="software">CodeSLAM</rs> was extended by 
            <rs type="software">SceneCode</rs> [35] to include semantics. 
            <rs type="software">SceneCode</rs> was able to make inference-time refinements of network predictions by optimising for photometric and semantic label consistency between multiple frames. However, although trained with depth maps, 
            <rs type="software">SceneCode</rs> was still a view-based representation and lacked true awareness of the 3D geometry.
        </p>
        <p>There has been much promising recent work on using neural implicit scene representations. As these are continuous representations, they can easily handle complicated topologies and do not suffer from discretisation error, with the actual representative resolution depending on the capacity of the neural network used. The Scene Representation Network (SRN) [25] was one of the first methods to use a multi-layer perceptron (MLP) as the neural representation of a learned scene given a collection of images and associated poses. DeepSDF [19] and DIST [10] used deep decoders to learn implicit signed distance functions of various shape instances of the same class, and Occupancy Networks [15,21] learned an implicit 3D occupancy function for shapes or large scale scenes given 3D supervision.There has been much promising recent work on using neural implicit scene representations. As these are continuous representations, they can easily handle complicated topologies and do not suffer from discretisation error, with the actual representative resolution depending on the capacity of the neural network used. The Scene Representation Network (SRN) [25] was one of the first methods to use a multi-layer perceptron (MLP) as the neural representation of a learned scene given a collection of images and associated poses. DeepSDF [19] and DIST [10] used deep decoders to learn implicit signed distance functions of various shape instances of the same class, and Occupancy Networks [15,21] learned an implicit 3D occupancy function for shapes or large scale scenes given 3D supervision.</p>
        <p>Kohli et al. [8] also proposed to learn a joint implicit representation of appearance and semantics for 3D shapes on top of an SRN using a linear segmentation renderer. After being trained in a two-step semi-supervised manner, the network can synthesise novel view semantic labels from either colour or semantic observations. The methods mentioned above involve extensive pretraining on collections of data to learn priors about the shapes or scenes they are used to represent. Although promising generalisation capability has been shown across different instances or scenes, it is not always possible to get adequate data for various unseen environments. The alternative is a scene-specific representation which requires minimum in-place labelling effort.Kohli et al. [8] also proposed to learn a joint implicit representation of appearance and semantics for 3D shapes on top of an SRN using a linear segmentation renderer. After being trained in a two-step semi-supervised manner, the network can synthesise novel view semantic labels from either colour or semantic observations. The methods mentioned above involve extensive pretraining on collections of data to learn priors about the shapes or scenes they are used to represent. Although promising generalisation capability has been shown across different instances or scenes, it is not always possible to get adequate data for various unseen environments. The alternative is a scene-specific representation which requires minimum in-place labelling effort.</p>
        <p>NeRF [16] and other systems based on it [34,11,31,27] use MLPs to overfit input from a single bounded scene and act as an implicit volumetric representation for realistic view-synthesis. In this paper, we treat NeRF as a powerful scene-specific 3D implicit representation, and extend it to include semantic representation which can be efficiently learned from sparse or noisy annotations (Figure 1).NeRF [16] and other systems based on it [34,11,31,27] use MLPs to overfit input from a single bounded scene and act as an implicit volumetric representation for realistic view-synthesis. In this paper, we treat NeRF as a powerful scene-specific 3D implicit representation, and extend it to include semantic representation which can be efficiently learned from sparse or noisy annotations (Figure 1).</p>
        <p>Given multiple images of a static scene with known camera intrinsics and extrinsics, NeRF [16] uses MLPs to implicitly represent the continuous 3D scene density σ and colour c = (r, g, b) as a function of continuous 5D input vectors of spatial coordinates x = (x, y, z) and viewing directions d = (θ, φ). Specifically, σ(x) is designed to be a function of only 3D position while the radiance c(x, d) is a function of both 3D position and viewing direction.Given multiple images of a static scene with known camera intrinsics and extrinsics, NeRF [16] uses MLPs to implicitly represent the continuous 3D scene density σ and colour c = (r, g, b) as a function of continuous 5D input vectors of spatial coordinates x = (x, y, z) and viewing directions d = (θ, φ). Specifically, σ(x) is designed to be a function of only 3D position while the radiance c(x, d) is a function of both 3D position and viewing direction.</p>
        <p>To compute the colour of a single pixel, NeRF [16] approximates volume rendering by numerical quadrature with hierarchical stratified sampling. Within one hierarchy, if r(t) = o + td is the ray emitted from the centre of projection of camera space through a given pixel, traversing between near and far bounds (t n and t f ), then for selected K random quadrature points {t k } K k=1 between t n and t f , the approximated expected colour is given by:To compute the colour of a single pixel, NeRF [16] approximates volume rendering by numerical quadrature with hierarchical stratified sampling. Within one hierarchy, if r(t) = o + td is the ray emitted from the centre of projection of camera space through a given pixel, traversing between near and far bounds (t n and t f ), then for selected K random quadrature points {t k } K k=1 between t n and t f , the approximated expected colour is given by:</p>
        <p>wherewhere</p>
        <p>where α (x) = 1 -exp(-x), and δ k = t k+1 -t k is the distance between two adjacent quadrature sample points. Given multi-view training images of the observed scene, NeRF uses stochastic gradient descent (SGD) to optimise σ and c by minimising photometric discrepancy.where α (x) = 1 -exp(-x), and δ k = t k+1 -t k is the distance between two adjacent quadrature sample points. Given multi-view training images of the observed scene, NeRF uses stochastic gradient descent (SGD) to optimise σ and c by minimising photometric discrepancy.</p>
        <p>We now show how to extend NeRF to jointly encode appearance, geometry and semantics. As shown in Figure 2, we augment the original NeRF by appending a segmentation renderer before injecting viewing directions into the MLP.We now show how to extend NeRF to jointly encode appearance, geometry and semantics. As shown in Figure 2, we augment the original NeRF by appending a segmentation renderer before injecting viewing directions into the MLP.</p>
        <p>We formalise semantic segmentation as an inherently view-invariant function that maps only a world coordinate x to a distribution over C semantic labels via pre-softmax semantic logits s(x):We formalise semantic segmentation as an inherently view-invariant function that maps only a world coordinate x to a distribution over C semantic labels via pre-softmax semantic logits s(x):</p>
        <p>where F Θ represents the learned MLPs.where F Θ represents the learned MLPs.</p>
        <p>The approximated expected semantic logits Ŝ(r) of a given pixel in the image plane can be written as:The approximated expected semantic logits Ŝ(r) of a given pixel in the image plane can be written as:</p>
        <p>wherewhere</p>
        <p>with α (x) = 1 -exp(-x) and δ k = t k+1 -t k is the distance between adjacent sample points. Semantic logits can then be transformed into multi-class probabilities through a softmax normalisation layer.with α (x) = 1 -exp(-x) and δ k = t k+1 -t k is the distance between adjacent sample points. Semantic logits can then be transformed into multi-class probabilities through a softmax normalisation layer.</p>
        <p>We train the whole network from scratch under photometric loss L p and semantic loss L s :We train the whole network from scratch under photometric loss L p and semantic loss L s :</p>
        <p>where R are the sampled rays within a training batch, and C(r), Ĉc (r) and Ĉf (r) are the ground truth, coarse volume predicted and fine volume predicted RGB colours for ray r, respectively. Similarly, p l , pl c and pl f are the multi-class semantic probability at class l of the ground truth map, coarse volume and fine volume predictions for ray r, respectively. L s is chosen as a multi-class cross-entropy loss to encourage the rendered semantic labels to be consistent with the provided labels, whether these are ground-truth, noisy or partial observations. Hence the total training loss L is:where R are the sampled rays within a training batch, and C(r), Ĉc (r) and Ĉf (r) are the ground truth, coarse volume predicted and fine volume predicted RGB colours for ray r, respectively. Similarly, p l , pl c and pl f are the multi-class semantic probability at class l of the ground truth map, coarse volume and fine volume predictions for ray r, respectively. L s is chosen as a multi-class cross-entropy loss to encourage the rendered semantic labels to be consistent with the provided labels, whether these are ground-truth, noisy or partial observations. Hence the total training loss L is:</p>
        <p>where λ is the weight of the semantic loss and is set to 0.04 to balance the magnitude of both losses [8]. In practice we find that actual performance is not sensitive to λ value and setting λ to 1 gives us similar performance. These photometric and semantic losses naturally encourage the network to generate multi-view consistent 2D renderings from the underlying joint representation.where λ is the weight of the semantic loss and is set to 0.04 to balance the magnitude of both losses [8]. In practice we find that actual performance is not sensitive to λ value and setting λ to 1 gives us similar performance. These photometric and semantic losses naturally encourage the network to generate multi-view consistent 2D renderings from the underlying joint representation.</p>
        <p>A scene-specific semantic representation is obtained by training the network from scratch for each scene individually. We use setup and hyper-parameters similar to [16]. Specifically, we use hierarchical volume sampling to jointly optimise coarse and fine networks, where the former provides importance sampling bias so that the latter can distribute more samples to positions likely to be visible. Positional encoding of length 10 and 4 [32,30] are applied to 3D positions and viewing directions, respectively. In addition, since we have no depth information, we set the bounds of ray sampling to 0.1m and 10m respectively across experiments without careful tuning to span indoor scenes.A scene-specific semantic representation is obtained by training the network from scratch for each scene individually. We use setup and hyper-parameters similar to [16]. Specifically, we use hierarchical volume sampling to jointly optimise coarse and fine networks, where the former provides importance sampling bias so that the latter can distribute more samples to positions likely to be visible. Positional encoding of length 10 and 4 [32,30] are applied to 3D positions and viewing directions, respectively. In addition, since we have no depth information, we set the bounds of ray sampling to 0.1m and 10m respectively across experiments without careful tuning to span indoor scenes.</p>
        <p>Training images are resized to 320x240 for all the experiments. We implement our model in 
            <rs type="software">PyTorch</rs> [20] and train it on a single RTX2080-Ti GPU with 11GB memory. The batch size of rays is set to 1024 due to memory limitations. We train the neural network using the Adam optimiser [7] with a learning rate of 5e-4 for 200,000 iterations.
        </p>
        <p>After training on colour images and semantic labels with associated poses, we obtain a scene-specific implicit 3D semantic representation. We evaluate its effectiveness quantitatively by projecting the 3D representation back into 2D image space where we have direct access to explicit ground truth data. We aim to show the benefits and promising applications of efficiently learning such a joint 3D representation for semantic labelling and understanding. We kindly urge readers to inspect more qualitative results on project page: https://shuaifengzhi.com/Semantic-NeRF/.After training on colour images and semantic labels with associated poses, we obtain a scene-specific implicit 3D semantic representation. We evaluate its effectiveness quantitatively by projecting the 3D representation back into 2D image space where we have direct access to explicit ground truth data. We aim to show the benefits and promising applications of efficiently learning such a joint 3D representation for semantic labelling and understanding. We kindly urge readers to inspect more qualitative results on project page: https://shuaifengzhi.com/Semantic-NeRF/.</p>
        <p>Replica Replica [28] is a reconstruction-based 3D dataset of 18 high fidelity scenes with dense geometry, HDR textures and semantic annotations. We use the Habitat simulator [23] to render RGB colour images, depth maps and semantic labels from randomly generated 6-DOF trajectories similar to hand-held camera motions. We follow the procedure from SceneNet RGB-D [14], and lock the roll angle with the camera up-vector pointing along the y-axis.Replica Replica [28] is a reconstruction-based 3D dataset of 18 high fidelity scenes with dense geometry, HDR textures and semantic annotations. We use the Habitat simulator [23] to render RGB colour images, depth maps and semantic labels from randomly generated 6-DOF trajectories similar to hand-held camera motions. We follow the procedure from SceneNet RGB-D [14], and lock the roll angle with the camera up-vector pointing along the y-axis.</p>
        <p>We use the provided 88 semantic classes from Replica in scene-specific experiments and also manually map these labels to the popular NYUv2-13 definition [24,4] in Section 4.4 for multi-view label fusion, following the mapping convention from ScanNet [3]. For each Replica scene of rooms and offices, we render 900 images at resolution 640x480 using the default pin-hole camera model with 90 degree horizontal field of view. We sample every 5th frame from the sequence to compose the training set and also sample intermediate frames to make the test set.We use the provided 88 semantic classes from Replica in scene-specific experiments and also manually map these labels to the popular NYUv2-13 definition [24,4] in Section 4.4 for multi-view label fusion, following the mapping convention from ScanNet [3]. For each Replica scene of rooms and offices, we render 900 images at resolution 640x480 using the default pin-hole camera model with 90 degree horizontal field of view. We sample every 5th frame from the sequence to compose the training set and also sample intermediate frames to make the test set.</p>
        <p>ScanNet ScanNet [3] is a large-scale real-world indoor RGB-D video dataset of 2.5M views in 1513 scenes with rich annotations including semantic segmentation, camera poses and surface reconstructions. We train Semantic-NeRF on ScanNet scenes using only the provided colour images, camera poses and 2D semantic labels. The sequences in each scene are evenly sampled so that the total amount of training data is roughly 300 frames. During experiments we select several indoor room-scale scenes and train one Semantic-NeRF per scene using posed images and semantic labels from the NYUv2-40 definition.ScanNet ScanNet [3] is a large-scale real-world indoor RGB-D video dataset of 2.5M views in 1513 scenes with rich annotations including semantic segmentation, camera poses and surface reconstructions. We train Semantic-NeRF on ScanNet scenes using only the provided colour images, camera poses and 2D semantic labels. The sequences in each scene are evenly sampled so that the total amount of training data is roughly 300 frames. During experiments we select several indoor room-scale scenes and train one Semantic-NeRF per scene using posed images and semantic labels from the NYUv2-40 definition.</p>
        <p>We check the influence of semantics on appearance and geometry by quantitatively computing the quality of rendered RGB images and depth maps on Replica scenes with and without semantic prediction enabled. Experiments show that there is no clear difference which suggests that the current network has the capacity to learn these tasks jointly.We check the influence of semantics on appearance and geometry by quantitatively computing the quality of rendered RGB images and depth maps on Replica scenes with and without semantic prediction enabled. Experiments show that there is no clear difference which suggests that the current network has the capacity to learn these tasks jointly.</p>
        <p>Note that we might expect that significant high quality semantic labelling information could feasibly improve reconstruction quality, but in this paper we are focused on how geometry can help semantics in the opposite situation where semantic labelling is sparse or noisy.Note that we might expect that significant high quality semantic labelling information could feasibly improve reconstruction quality, but in this paper we are focused on how geometry can help semantics in the opposite situation where semantic labelling is sparse or noisy.</p>
        <p>We first train our semantic-NeRF framework for novel view semantic label synthesis using all available RGB images with camera poses and corresponding semantic labels (i.e., 180 images) from a randomly generated sequence of a certain scene. This fully-supervised setup acts as an upper bound on the semantic segmentation performance of Semantic-NeRF given abundant labelled training data.We first train our semantic-NeRF framework for novel view semantic label synthesis using all available RGB images with camera poses and corresponding semantic labels (i.e., 180 images) from a randomly generated sequence of a certain scene. This fully-supervised setup acts as an upper bound on the semantic segmentation performance of Semantic-NeRF given abundant labelled training data.</p>
        <p>However, in practice it is expensive and time-consuming to acquire accurate dense semantic annotations for all observed images in a scene. Considering the redundancy in semantic labels among overlapping frames, we borrow the idea of key-framing from SLAM systems and hypothesise that providing labels for only selected frames should be enough to train the semantic representation efficiently. We choose key-frames by evenly sampling from sequences and train the networks from scratch with semantic labels only coming from those selected key-frames, while the synthesis performance is always evaluated on all test frames.However, in practice it is expensive and time-consuming to acquire accurate dense semantic annotations for all observed images in a scene. Considering the redundancy in semantic labels among overlapping frames, we borrow the idea of key-framing from SLAM systems and hypothesise that providing labels for only selected frames should be enough to train the semantic representation efficiently. We choose key-frames by evenly sampling from sequences and train the networks from scratch with semantic labels only coming from those selected key-frames, while the synthesis performance is always evaluated on all test frames.</p>
        <p>Figure 3 and 4 validate our assumption that semantics can be efficiently learned from sparse annotations with a sparsity ratio ranging from 0% to 95%, together with corresponding camera motion baselines as a complementary indication. Only marginal performance loss occurs when less than 10% semantic frames are used, and this is mainly caused by renderings of regions which are unobserved or occluded from key-frames. To take this even further, we manually select just two key-frames (99% sparsity ratio) from each scene to cover as much of the scene as possible. It turns out that our network, trained only with two labelled keyframes, can render accurate labels from various viewpoints.Figure 3 and 4 validate our assumption that semantics can be efficiently learned from sparse annotations with a sparsity ratio ranging from 0% to 95%, together with corresponding camera motion baselines as a complementary indication. Only marginal performance loss occurs when less than 10% semantic frames are used, and this is mainly caused by renderings of regions which are unobserved or occluded from key-frames. To take this even further, we manually select just two key-frames (99% sparsity ratio) from each scene to cover as much of the scene as possible. It turns out that our network, trained only with two labelled keyframes, can render accurate labels from various viewpoints.</p>
        <p>In addition to being able to learn the semantic representation with sparse annotations due to the redundancy present in the semantic labels, another important property of Semantic-NeRF is that multi-view consistency between semantic labels is enforced.In addition to being able to learn the semantic representation with sparse annotations due to the redundancy present in the semantic labels, another important property of Semantic-NeRF is that multi-view consistency between semantic labels is enforced.</p>
        <p>In semantic mapping systems (e.g. [29,13,18]), multiple 2D semantic observations are integrated into a 3D map or target frames to produce a more consistent and accurate semantic segmentation. Multi-view consistency is the key concept and motivation in semantic fusion, and the training process of Semantic-NeRF itself can be seen as a multiview label fusion process. Given multiple noisy or partial semantic labels, the network can fuse them into a joint implicit 3D space so that we can extract a denoised label when Baseline Length (cm)In semantic mapping systems (e.g. [29,13,18]), multiple 2D semantic observations are integrated into a 3D map or target frames to produce a more consistent and accurate semantic segmentation. Multi-view consistency is the key concept and motivation in semantic fusion, and the training process of Semantic-NeRF itself can be seen as a multiview label fusion process. Given multiple noisy or partial semantic labels, the network can fuse them into a joint implicit 3D space so that we can extract a denoised label when Baseline Length (cm)</p>
        <p>Figure 4: Quantitative performance of Semantic-NeRF trained on Replica with sparse semantic labels. Sparsity ratio is the percentage of frames dropped compared to full sequence supervision. Three standard metrics are used to evaluate semantic segmentation performance on test poses (higher is better). Performance gracefully degrades with fewer labels due to uncovered or occluded regions, indicating the possibility of efficient dense labelling from fewer annotations. Results with only two labelled key-frames ( ) show remarkably competitive performance.Figure 4: Quantitative performance of Semantic-NeRF trained on Replica with sparse semantic labels. Sparsity ratio is the percentage of frames dropped compared to full sequence supervision. Three standard metrics are used to evaluate semantic segmentation performance on test poses (higher is better). Performance gracefully degrades with fewer labels due to uncovered or occluded regions, indicating the possibility of efficient dense labelling from fewer annotations. Results with only two labelled key-frames ( ) show remarkably competitive performance.</p>
        <p>we re-render the semantic labels from the learned representation back to input training frames. We show the capability of Semantic-NeRF to perform multi-view semantic label fusion under a number of different scenarios: pixel-wise label noise, region-wise label noise, low-resolution dense or sparse labelling, partial labelling, and using the output of an imperfect CNN.we re-render the semantic labels from the learned representation back to input training frames. We show the capability of Semantic-NeRF to perform multi-view semantic label fusion under a number of different scenarios: pixel-wise label noise, region-wise label noise, low-resolution dense or sparse labelling, partial labelling, and using the output of an imperfect CNN.</p>
        <p>Labels with Pixel-wise Noise We corrupt ground-truth training semantic labels by adding independent pixel-wise noise. Specifically, we randomly select a fixed portion of pixels per training frame and randomly flip their labels to arbitrary ones (including the void class). After training using only these noisy labels, we obtain denoised semantic labels by rendering back to the same training poses.Labels with Pixel-wise Noise We corrupt ground-truth training semantic labels by adding independent pixel-wise noise. Specifically, we randomly select a fixed portion of pixels per training frame and randomly flip their labels to arbitrary ones (including the void class). After training using only these noisy labels, we obtain denoised semantic labels by rendering back to the same training poses.</p>
        <p>Figure 5 shows qualitative results from label denoising. When 90% of training pixels are randomly flipped, and it is difficult even for a human to recognise the underlying structure of the scene, the denoised labels still retain accurate boundaries and detail, especially for fine structures. Compared with Figure 3, the entropy in this denoising task is higher because the noisy training labels lack the multi-view consistency of clean ones. In addition, regions with void class tend to have the highest uncertainty since noisy pixels in void regions are not optimised during training. Quantitative results shown in Table 1 also confirm that accurate denoised labels are obtained after training-as-fusion.Figure 5 shows qualitative results from label denoising. When 90% of training pixels are randomly flipped, and it is difficult even for a human to recognise the underlying structure of the scene, the denoised labels still retain accurate boundaries and detail, especially for fine structures. Compared with Figure 3, the entropy in this denoising task is higher because the noisy training labels lack the multi-view consistency of clean ones. In addition, regions with void class tend to have the highest uncertainty since noisy pixels in void regions are not optimised during training. Quantitative results shown in Table 1 also confirm that accurate denoised labels are obtained after training-as-fusion.</p>
        <p>While pixel-wise denoising with such severe corruption is not a realistic application, it is still a very challenging task and, more importantly, highlights our key observation that training itself is a fusion process which enables coherent renderings benefiting from the internal consistency of implicit joint representation.While pixel-wise denoising with such severe corruption is not a realistic application, it is still a very challenging task and, more importantly, highlights our key observation that training itself is a fusion process which enables coherent renderings benefiting from the internal consistency of implicit joint representation.</p>
        <p>We further validate the effectiveness of semantic consistency by randomly flipping the class labels of certain whole instances instead of pixels in the label maps. This is a better simulation of the behaviour of real single-view CNNs because a whole object can easily be labelled as a similar but incorrect class from an obstructed or ambiguous view.We further validate the effectiveness of semantic consistency by randomly flipping the class labels of certain whole instances instead of pixels in the label maps. This is a better simulation of the behaviour of real single-view CNNs because a whole object can easily be labelled as a similar but incorrect class from an obstructed or ambiguous view.</p>
        <p>We choose Replica Room 2 containing 8 instances of chairs as the testing scene. For each chair instance, we compute the occupied area ratio (i.e., ratio of the number of pixels belonging to that instance to the total number of pixels in the image for each ground truth label frame) and then sort the label maps in the sequence based on this occupied area ratio. Two criteria are used for selecting frames in which 1: Quantitative evaluation for label denoising on Replica. Noise ratio is the percentage of changed pixels per frame, and for each instance the percentage of changed frames meeting selected criterion, respectively. mIoU is used for region denoising as it is more sensitive to the incorrect predictions on chair classes within the scene. Both tables are computed against clean training labels.We choose Replica Room 2 containing 8 instances of chairs as the testing scene. For each chair instance, we compute the occupied area ratio (i.e., ratio of the number of pixels belonging to that instance to the total number of pixels in the image for each ground truth label frame) and then sort the label maps in the sequence based on this occupied area ratio. Two criteria are used for selecting frames in which 1: Quantitative evaluation for label denoising on Replica. Noise ratio is the percentage of changed pixels per frame, and for each instance the percentage of changed frames meeting selected criterion, respectively. mIoU is used for region denoising as it is more sensitive to the incorrect predictions on chair classes within the scene. Both tables are computed against clean training labels.</p>
        <p>to randomly perturb each instance: (1) Sort: Select label maps with the least occupied area ratio. The intuition for this is that frames with partial observations are more likely to be mislabelled by semantic label prediction networks due to ambiguous context. (2) Even: Select label maps evenly from the sorted sequences introducing more large inconsis- Figure 6 shows the qualitative results of the re-rendered semantic labels after training. We indeed observe that semantic labels of the chair instances can be corrected due to the enforcement of multi-view consistency during training. Table 1 also shows that there are steady improvements while it becomes much harder to render improved labels when a larger fraction of labels are perturbed.to randomly perturb each instance: (1) Sort: Select label maps with the least occupied area ratio. The intuition for this is that frames with partial observations are more likely to be mislabelled by semantic label prediction networks due to ambiguous context. (2) Even: Select label maps evenly from the sorted sequences introducing more large inconsis- Figure 6 shows the qualitative results of the re-rendered semantic labels after training. We indeed observe that semantic labels of the chair instances can be corrected due to the enforcement of multi-view consistency during training. Table 1 also shows that there are steady improvements while it becomes much harder to render improved labels when a larger fraction of labels are perturbed.</p>
        <p>Semantic label super-resolution is a useful application for scene labelling as well. In an incremental real-time semantic mapping system, a light-weight CNN predicting lowresolution semantic labels might be adopted to reduce computational cost (e.g. [17]). Another possible use case is in a scene labelling tool, since manual annotation in coarse images is much more efficient.Semantic label super-resolution is a useful application for scene labelling as well. In an incremental real-time semantic mapping system, a light-weight CNN predicting lowresolution semantic labels might be adopted to reduce computational cost (e.g. [17]). Another possible use case is in a scene labelling tool, since manual annotation in coarse images is much more efficient.</p>
        <p>Here we show that we can train Semantic-NeRF with only low-resolution semantic information but then render accurate super-resolved semantics for either the input viewpoints or novel views. We test two different strategies to generate low-resolution training labels, with and without interpolation as shown in Figure 7. Given a down-scaling factor S = 8 for instance:Here we show that we can train Semantic-NeRF with only low-resolution semantic information but then render accurate super-resolved semantics for either the input viewpoints or novel views. We test two different strategies to generate low-resolution training labels, with and without interpolation as shown in Figure 7. Given a down-scaling factor S = 8 for instance:</p>
        <p>(1) All ground truth labels are down-scaled from 320×240 to 40 × 30 before being up-scaled back to the original size using nearest neighbour interpolation. (2) All pixels except those from the low-res label map (row and column divisible by 8) are masked by the void class so as not to contribute to the training loss. While method (1) uses interpolated labels to provide 'dense' supervision to a sampled ray batch but will incorrectly interpolate some pixels, method (2) provides sparse but geometrically accurate labels. We report superresolution performance on training poses from all Replica scenes with two scales S = 8 and S = 16 in Table 2. The promising results in semantic label denoising and super-resolution demonstrate one of the main advantages of jointly representing appearance, geometry and semantics: that missing or corrupted semantic labels in any one frame can be corrected through the fusion of many other frames.(1) All ground truth labels are down-scaled from 320×240 to 40 × 30 before being up-scaled back to the original size using nearest neighbour interpolation. (2) All pixels except those from the low-res label map (row and column divisible by 8) are masked by the void class so as not to contribute to the training loss. While method (1) uses interpolated labels to provide 'dense' supervision to a sampled ray batch but will incorrectly interpolate some pixels, method (2) provides sparse but geometrically accurate labels. We report superresolution performance on training poses from all Replica scenes with two scales S = 8 and S = 16 in Table 2. The promising results in semantic label denoising and super-resolution demonstrate one of the main advantages of jointly representing appearance, geometry and semantics: that missing or corrupted semantic labels in any one frame can be corrected through the fusion of many other frames.</p>
        <p>We explore this property in more detail in the next section.We explore this property in more detail in the next section.</p>
        <p>Our super-resolution experiments have shown the ability of Semantic-NeRF to interpolate rich details from lowresolution annotations. For a practical scene-annotation tool, straightforward annotations from a user in the form of clicks or scratches are desirable, and expected that those sparse clicks can expand and propagate to accurately and densely label the scene.Our super-resolution experiments have shown the ability of Semantic-NeRF to interpolate rich details from lowresolution annotations. For a practical scene-annotation tool, straightforward annotations from a user in the form of clicks or scratches are desirable, and expected that those sparse clicks can expand and propagate to accurately and densely label the scene.</p>
        <p>To simulate user annotations, for each class within label maps we randomly select a continuous sub-region with which to apply a ground-truth label while leaving the rest unlabelled. Results in Figure 8 and Table 3 show that supervision from one single pixel per class can lead to surprisingly high quality rendered labels with well preserved global and fine structure. Object boundaries are gradually refined when more supervision is available and the incremental improvements from more labels tend to saturate.To simulate user annotations, for each class within label maps we randomly select a continuous sub-region with which to apply a ground-truth label while leaving the rest unlabelled. Results in Figure 8 and Table 3 show that supervision from one single pixel per class can lead to surprisingly high quality rendered labels with well preserved global and fine structure. Object boundaries are gradually refined when more supervision is available and the incremental improvements from more labels tend to saturate.</p>
        <p>We have shown that a semantic representation can be learned from sparse or noisy/partial supervisions. Here we further validate its practical value in multi-view semantic fusion using CNN predictions.We have shown that a semantic representation can be learned from sparse or noisy/partial supervisions. Here we further validate its practical value in multi-view semantic fusion using CNN predictions.</p>
        <p>There have been several classical pixel-wise semantic fusion approaches [6,13,12] to integrate monocular CNN predictions from multiple viewpoints to refine segmentation. For fair comparison, here we have separated out the multi-view fusion approaches from such systems. Two baseline techniques are: Bayesian fusion, where multiclass label probabilities are multiplied together and then renormalised (e.g. [13]), and average fusion, which simply takes the average of all label distributions (e.g. [12]). Figure 8: Label propagation results using partial annotations of a single-pixel, 1% or 5% of pixels per class within frames, respectively. Accurate labels can be achieved even from single-clicks, which are zoomed-in 9 times for visualisation purposes. To prepare training data in Replica dataset, we render two different sequences per Replica scene to cover various parts of scenes. Each sequence consists of 90 frames evenly sampled from 900 renderings of size 640×480 with semantic labels remapped to NYUv2-13 class convention.There have been several classical pixel-wise semantic fusion approaches [6,13,12] to integrate monocular CNN predictions from multiple viewpoints to refine segmentation. For fair comparison, here we have separated out the multi-view fusion approaches from such systems. Two baseline techniques are: Bayesian fusion, where multiclass label probabilities are multiplied together and then renormalised (e.g. [13]), and average fusion, which simply takes the average of all label distributions (e.g. [12]). Figure 8: Label propagation results using partial annotations of a single-pixel, 1% or 5% of pixels per class within frames, respectively. Accurate labels can be achieved even from single-clicks, which are zoomed-in 9 times for visualisation purposes. To prepare training data in Replica dataset, we render two different sequences per Replica scene to cover various parts of scenes. Each sequence consists of 90 frames evenly sampled from 900 renderings of size 640×480 with semantic labels remapped to NYUv2-13 class convention.</p>
        <p>We choose 
            <rs type="software">DeepLabV3+</rs> [2] with a ResNet-101 backbone as the CNN model for monocular label predictions. To generate decent monocular CNN predictions and avoid over-fitting, we train 
            <rs type="software">DeepLab</rs> on SUN-RGBD [26], and then fine-tune it using data from all Replica scenes except the one chosen for training Semantic-NeRF and label fusion evaluation. We repeat this fine-tuning process and train one individual 
            <rs type="software">DeepLab</rs> CNN model for each test scene.
        </p>
        <p>Monocular CNN predictions of the test scene are used for two purposes: (1) training supervision for our scenespecific Semantic-NeRF model; (2) monocular predictions (per-pixel dense softmax probabilities) for baseline multiview semantic fusion methods. We train Semantic-NeRF using posed colour images together with CNN-predicted labels for 200,000 steps and then re-render the fused semantic labels back to the training poses as fusion results.Monocular CNN predictions of the test scene are used for two purposes: (1) training supervision for our scenespecific Semantic-NeRF model; (2) monocular predictions (per-pixel dense softmax probabilities) for baseline multiview semantic fusion methods. We train Semantic-NeRF using posed colour images together with CNN-predicted labels for 200,000 steps and then re-render the fused semantic labels back to the training poses as fusion results.</p>
        <p>It is important to note that both baseline fusion techniques require depth information to compute the dense correspondences between frames while ours only requires posed images. We report the average performance across all testing scenes in Table 4, in which ground truth depth maps are used for the two baseline approaches to represent a 'best case scenario'. Our method achieves the highest improvement across all metrics, showing the effectiveness of our joint representation in label fusion.It is important to note that both baseline fusion techniques require depth information to compute the dense correspondences between frames while ours only requires posed images. We report the average performance across all testing scenes in Table 4, in which ground truth depth maps are used for the two baseline approaches to represent a 'best case scenario'. Our method achieves the highest improvement across all metrics, showing the effectiveness of our joint representation in label fusion.</p>
        <p>We have shown that adding a semantic output to a scenespecific implicit MLP model of geometry and appearance means that complete and high resolution semantic labels can be generated for a scene when only partial, noisy or lowresolution semantic supervision is available. This method has practical uses in robotics or other applications where scene understanding is required in new scenes where only limited labelling is possible.We have shown that adding a semantic output to a scenespecific implicit MLP model of geometry and appearance means that complete and high resolution semantic labels can be generated for a scene when only partial, noisy or lowresolution semantic supervision is available. This method has practical uses in robotics or other applications where scene understanding is required in new scenes where only limited labelling is possible.</p>
        <p>An interesting direction for future research is interactive labelling, where the continually training network asks for the new labels which will most resolve semantic ambiguity for the whole scene.An interesting direction for future research is interactive labelling, where the continually training network asks for the new labels which will most resolve semantic ambiguity for the whole scene.</p>
        <p>Research presented in this paper has been supported by Dyson Technology Ltd. Shuaifeng Zhi holds a China Scholarship Council-Imperial Scholarship. We are very grateful to Edgar Sucar, Shikun Liu, Kentaro Wada, Binbin Xu and Sotiris Papatheodorou for fruitful discussions.Research presented in this paper has been supported by Dyson Technology Ltd. Shuaifeng Zhi holds a China Scholarship Council-Imperial Scholarship. We are very grateful to Edgar Sucar, Shikun Liu, Kentaro Wada, Binbin Xu and Sotiris Papatheodorou for fruitful discussions.</p>
        <p>In-Place Scene Labelling and Understanding with Implicit Scene Representation Figure 9: Semantic 3D reconstruction obtained using 
            <rs type="software">Semantic-NeRF</rs>. Note that our learned scene-specific 3D representation predicts decent geometry and semantics in occluded regions and fills the holes caused by unobserved regions to some extent.
        </p>
        <p>Abs RelAbs Rel</p>
        <p>Corresponding to Section 4.2 of the main paper, Table 6 shows quantitative results for photometric and geometric reconstruction quality when projected to 2D on Replica scenes with and without semantics enabled. We observe no obvious difference between these two set-ups. Peak signalto-noise ratio (PSNR) is used to measure the quality of the rendered colour images and the metrics used to evaluate the 2D depth maps are shown in Table 5.Corresponding to Section 4.2 of the main paper, Table 6 shows quantitative results for photometric and geometric reconstruction quality when projected to 2D on Replica scenes with and without semantics enabled. We observe no obvious difference between these two set-ups. Peak signalto-noise ratio (PSNR) is used to measure the quality of the rendered colour images and the metrics used to evaluate the 2D depth maps are shown in Table 5.</p>
        <p>After training semantic-NeRF with in-place annotation, we can also extract an explicit 3D scene from the learned MLP to inspect the implicit 3D representation. Geometric meshes are extracted by first querying the MLP on dense 3D grids of the scene and then applying marching cubes. Attached semantic texture is rendered by treating the negative normal direction of vertices in the mesh as the ray marching directions during volume rendering. We show qualitative results for three Replica room scenes in Figure 9.After training semantic-NeRF with in-place annotation, we can also extract an explicit 3D scene from the learned MLP to inspect the implicit 3D representation. Geometric meshes are extracted by first querying the MLP on dense 3D grids of the scene and then applying marching cubes. Attached semantic texture is rendered by treating the negative normal direction of vertices in the mesh as the ray marching directions during volume rendering. We show qualitative results for three Replica room scenes in Figure 9.</p>
        <p>Axis-aligned positional encoding (PE) of 3D positions are fed to both first and intermediate fully-connected (FC) layers with 256 neurons and ReLU activations before predicting volume density. Additional FC layers with 128 neurons are used for view-invariant semantics and viewdependent radiance after merging input viewing directions.Axis-aligned positional encoding (PE) of 3D positions are fed to both first and intermediate fully-connected (FC) layers with 256 neurons and ReLU activations before predicting volume density. Additional FC layers with 128 neurons are used for view-invariant semantics and viewdependent radiance after merging input viewing directions.</p>
        <p>The length of positional encoding L relates to the maximum frequency used and affects the rendering quality. In label propagation task, we find that using only low-frequency components (L = 5) leads to over-smoothed 2D renderings, while using high-frequency ones (L = 40) leads to noisy interpolations, which aligns with findings in recent literature [16,30]. L of 10 empirically performs the best.The length of positional encoding L relates to the maximum frequency used and affects the rendering quality. In label propagation task, we find that using only low-frequency components (L = 5) leads to over-smoothed 2D renderings, while using high-frequency ones (L = 40) leads to noisy interpolations, which aligns with findings in recent literature [16,30]. L of 10 empirically performs the best.</p>
        <p>Here we show more examples of qualitative results in Figure 10, 11, 12 for semantic view synthesis, label denoising and super-resolution, respectively.Here we show more examples of qualitative results in Figure 10, 11, 12 for semantic view synthesis, label denoising and super-resolution, respectively.</p>
        <p>We kindly urge readers to watch our supplementary video on project page https://shuaifengzhi.com/ Semantic-NeRF which highlights the accuracy and consistency of semantic renderings in various situations and applications.We kindly urge readers to watch our supplementary video on project page https://shuaifengzhi.com/ Semantic-NeRF which highlights the accuracy and consistency of semantic renderings in various situations and applications.</p>
    </text>
</tei>

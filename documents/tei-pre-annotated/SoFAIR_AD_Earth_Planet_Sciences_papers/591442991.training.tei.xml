<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:10+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Machine learning (ML) methods have become an important tool for modelling and forecasting complex highdimensional spatiotemporal datasets such as those found in environmental and climate modelling applications. ML approaches can offer a fast, low-cost alternative to short-term forecasting than expensive numerical simulation while addressing a significant outstanding limitation of numerical modelling by being able to robustly and dynamically quantify predictive uncertainty. Low-cost and near-instantaneous forecasting of high-level climate variables has clear applications in early warning systems, nowcasting, and parameterising small-scale locally relevant simulations. This paper presents a novel approach for multi-task spatiotemporal regression by combining data-driven autoencoders with Gaussian Processes (GP) to produce a probabilistic tensor-based regression model. The proposed method is demonstrated for forecasting one-step-ahead temperature and pressure on a global scale simultaneously. By conducting probabilistic regression in the learned latent space, samples can be propagated back to the original feature space to produce uncertainty estimates at a vastly reduced computational cost. The composite GP-autoencoder model was able to simultaneously forecast global temperature and pressure values with average errors of 3.82 • C and 638 hPa, respectively. Further, on average the true values were within the proposed posterior distribution 95.6% of the time illustrating that the model produces a well-calibrated predictive posterior distribution.</p>
        <p>Many scientific and engineering problems focus on dynamical systems, complex phenomena evolving in time and space, often characterised by non-linear interactions of underlying mechanisms on varying scales. Forecasting the behaviour of these complex and multi-faceted systems is challenging yet critical and relied upon every day, i.e., such as weather forecasts. In order to forecast these complex dynamical systems, sophisticated computer models have been developed that aim to simulate the underlying physics and processes governing the behaviour of the system. These models typically represent the system in terms of a set of partial differential equations (PDEs). For instance, numerical weather prediction (NWP) models may rely on finite difference implementations of the Navier-Stokes equations for incompressible fluid flow (Bauer et al., 2015;Giraldo and Restelli, 2008).</p>
        <p>In recent decades, there has been significant advancement in computer models designed to solve systems of PDEs, aiming to predict the behaviour of such systems. This progress has been driven largely by breakthroughs in both software engineering and hardware development, enabling parallel simulations that leverage the immense forecasting dynamical systems, uncertainty can represent variability in data, incomplete knowledge of the underlying physics, errors as a result of discretisations and simplifications, and general measurement errors (Xiu, 2009). Therefore, in order to generate robust forecasts for the behaviour of complex dynamical systems, it is imperative to incorporate a systematic approach for quantifying and propagating uncertainty in the forecasts. Monte Carlo sampling (Eigel et al., 2019;Lozanovski et al., 2020) and Bayesian methods (Haukaas and Gardoni, 2011;Behmanesh and Moaveni, 2015;Cockayne et al., 2019;Esmaeilbeigi et al., 2023) have been implemented into numerical computer models to address the issue of uncertainty. However, robust incorporation of uncertainty and probabilistic methods into numerical algorithms remains an active area of research, with ongoing debates regarding the reliable incorporation of various sources of uncertainty, such as uncertain inputs, into complex numerical models (Chakraborty and Chowdhury, 2016).</p>
        <p>The integration of ML approaches to forecasting dynamical systems serves several benefits. Primarily, it presents an alternative to the computationally intensive process of simulation when making new inferences about a system. ML models generally operate in a ''slow to train, fast to evaluate'' paradigm. Once a model is trained, making new inferences is generally very low-cost and occurs in real-time. Further, ML models also offer the benefit of being able to straightforwardly leverage the growing abundance of empirical data in scientific and engineering domains, such as sensor and satellite data, resolving some of the questions surrounding data assimilation in numerical modelling applications such as ocean modelling (Wilkin et al., 2017;Fringer et al., 2019;Fanous et al., 2023a,b).</p>
        <p>ML-based approaches offer significant advantages in terms of reducing computational demands for inference and optimising data utilisation. However, some algorithms also have the capability to facilitate probabilistic inference, producing distributions of outputs and allowing for the quantification of uncertainty associated with a forecast. Despite these capabilities, many existing applications of ML to forecasting tasks still rely on deterministic algorithms, especially when dealing with complex high-dimensional and multi-task data like the spatiotemporal data characteristic of dynamical systems. This is because there is generally a trade-off between scalability and having the ability to quantify uncertainty or introduce probabilistic aspects to a model. This study explores efficient and scalable approaches to forecasting highdimensional multi-task data in a robust and probabilistic approach. The capacity to manage uncertainty is of paramount importance as it enables a more nuanced comprehension of potential system outcomes and variabilities.</p>
        <p>To demonstrate the appropriateness and robustness of the proposed methodology, it is applied to a case study of forecasting global climate data. Global and local climatic patterns are examples of a well-studied dynamical system, where forecasts can have significant, far-reaching implications, influencing global economic policy and politics which ultimately impacts the lives of the majority of the population. In this study, a data-driven surrogate model (Donnelly et al., 2023) is developed for probabilistic modelling of global climate data from the ERA5 global reanalysis datasets (Hersbach et al., 2020). The dataset consists of accurate hindcast data for a wide collection of meteorological and environmental measurements across the planet dating back to 1940. Despite not being strictly empirical data, the ERA5 dataset underscores the need and significance of ML methods in facilitating easy scalability and leveraging of vast datasets. The ERA5 dataset, which is already several petabytes in size and continuously updated, serves as a prime example of the importance of efficient handling and analysis of voluminous data.</p>
        <p>This study leverages the abundant empirical and quasi-empirical (simulation of historical data) climatic data to develop a data-driven reduced order model (ROM). This ROM is constructed using deep convolutional neural network (CNN)-based autoencoders and GP regression models to simultaneously forecast temperature and pressure observations on a global scale. By integrating these methodologies, this study outlines an advanced and novel approach to quantify predictive uncertainty in high-dimensional feature spaces parameterised by multi-channel tensor data. This is achieved via a latent-space sampling approach whereby uncertainty in the latent space can be robustly decoded back to the original feature space to provide dynamic estimates of uncertainty for multi-task regression problems.</p>
        <p>The proposed method is demonstrated to be highly flexible, scalable, and applicable to general dynamical systems based on spatiotemporal data and can provide accurate point-wise one-step-ahead forecasts as well as robust and dynamic estimates of predictive uncertainty. This ROM is demonstrated as a viable alternative to resource-intensive numerical simulations, facilitating rapid inference capabilities essential for downstream applications such as nowcasting, early warning systems (EWS), and producing realistic input for locally relevant lower-cost simulations. The advantages of a model such as this are clear when considering both the costs of constant re-evaluation using expensive multi-core supercomputers and the existing limitations with respect to uncertainty quantification.</p>
        <p>The proposed model extends the current state-of-the-art (SOTA) with respect to probabilistic regression in high-dimensional multi-task settings and demonstrates how advances in computer vision can be effectively applied to address problems in dynamical systems and climate forecasting. Furthermore, this study outlines the significance of employing a probabilistic strategy in dynamical system regression, as opposed to traditional point-wise forecasting methods, and its potential applicability to real-world challenges like global climate modelling.</p>
        <p>The intractability of many approaches to ML on very highdimensional datasets has led to dimensionality reduction often being an integral part of any ML modelling framework (Ayesha et al., 2020). The recent advances in deep learning have led to the development and adoption of autoencoders as a flexible, non-parametric approach to dimensionality reduction (Tschannen et al., 2018;Sewak et al., 2020). Autoencoders offer increased scalability and complexity over well-known parametric approaches to dimensionality reduction such as principal component analysis (PCA).</p>
        <p>Within the dynamical systems literature, the ROM is often employed similarly, aiming to provide low-dimensional descriptions of the underlying behaviour of a complex system (Loiseau et al., 2018). Historically, approaches to ROM were parametric and involved projecting data onto lower-dimensional linear subspaces, such as proper orthogonal decomposition (Lee and Carlberg, 2020). In recent years, data-driven non-parametric approaches to model reduction have been adopted within the dynamical systems literature, leveraging models such as convolutional autoencoders for a variety of applications such as fluid dynamics (Agostini, 2020;Lee and Carlberg, 2020;Otto and Rowley, 2019;Xu and Duraisamy, 2020;Daneshkhah et al., 2020).</p>
        <p>Learning low-dimensional latent representations can facilitate fast regression, as an alternative to time-consuming numerical simulations. This type of ROM-enabled regression has been applied to a variety of problems such as energy forecasting (Mudunuru et al., 2017), fluid flow predictions (Wiewel et al., 2019;Daneshkhah et al., 2020) and wildfire forecasting (Cheng et al., 2022). However, the approaches outlined are flexible and generally not context-dependent, advanced regression models such as long short-term memory (LSTM) networks have been shown to effectively learn the latent features of general dynamical systems (Maulik et al., 2020). Maulik et al. (2021) used convolutional autoencoders to produce latent features of a dynamical system governed by the Navier-Stokes equations. A GP-based regression model was then developed to learn the temporal evolution of the latent space features. The data was applied to a single task, i.e., a single-channel (1×64×64) tensor, regression problem, and found that the model was able to accurately reproduce the underlying data. However, this is a toy example and does not reflect representative real-world problems and does not demonstrate the scalability of the proposed method. Furthermore, the proposed methodology is not fully utilised by quantifying original feature space after reconstructing the latent representations. Similarly, Guo and Hesthaven (2018) uses GPs for latent space modelling in structural analysis, however, the authors relied on existing parametric model-reduction techniques rather than more flexible data-driven approaches.</p>
        <p>Quilodrán-Casas and Arcucci (2023) propose a sophisticated datadriven approach to the development of a surrogate model to forecast latent space dynamics for 3D unstructured computational fluid dynamics (CFD) simulations. The authors construct a ROM by utilising both PCA and a data-driven autoencoder to produce latent features from their high-dimensional simulation data before training an LSTM onestep-ahead regression model on their latent features. The application to unstructured 3D domains is novel and the results highlighted the robustness of the proposed approach.</p>
        <p>PCA involves a linear transformation of the original data onto a new basis. A fully-connected dense autoencoder with a linear activation function is capable of retrieving principal components when trained to minimise the squared distance between the original data and its reconstruction. Therefore, the two-step encoding process can be reinterpreted as including an additional pre-trained fully-connected layer, parameterised by a weight vector 𝑊 * 0 , located at the beginning of the autoencoder. Although PCA is a widely used technique for dimensionality reduction, it does not easily extend to the case of multi-channel data and can suffer from computational costs in very high-dimensional spaces. Additionally, compared to a GP model, an LSTM does not easily extend to probabilistic inference, which can lead to a lack of consideration for uncertainty in the predictions.</p>
        <p>There are many examples of attempts to forecast environmental and climatic variables using ML and neural network-based approaches. Tran et al. (2021) outline a review of the existing literature for using neural networks to forecast air temperature. The studies outlined utilise a variety of neural network architectures, mostly feed-forward networks, and LSTM models, however, in more recent years CNNs have become increasingly adopted for spatiotemporal forecasts. Wang et al. (2021) similarly outlines a review of neural network-based approaches to forecasting wind speed, and consequently wind power, and found growing adoption of CNNs and autoencoders for feature extraction.</p>
        <p>Zhang and Dong (2020) explore a convolutional recurrent neural network (CRNN) model for a temperature forecasting case study in China. By discretising the domain into a 32 × 62 matrix the authors produce a neural network operating on 4 × 32 × 64 tensors by aggregating historical data to produce the next temperature map in the sequence. The authors manage to obtain a Root Mean Squared Error (RMSE) of 1.7 • C for their proposed model.</p>
        <p>Similarly, Kreuzer et al. (2020) propose convolutional LSTM for temperature forecasting in Germany and obtain an RMSE of 2.1 • C, however, this study only forecasted measurements at five weather stations rather than forecasting a full grid of values across a spatial domain. However, these studies explore deterministic models producing point-wise estimates and forecasts making no considerations for predictive uncertainty. Moreover, these studies focus on the forecasting of single-level measurements and the approaches outlined do not attempt multi-task regression with robust quantification of uncertainty.</p>
        <p>Despite some successful applications thus far of data-driven approaches to ROM used in conjunction with advanced regression models like GPs, the existing literature fails to tackle a truly high-dimensional and multi-task (i.e., predicting multiple spatially varying measurements) problem, generally modelling relatively small-scale data with shallow autoencoders. Existing neural network-based approaches to forecasting environmental and climate data generally make no consideration for uncertainty and aim to forecast single levels over a 1D grid-based output or for discrete locations aggregated into vectors.</p>
        <p>The existing literature does not tackle scenarios of high-dimensional multi-task data with robust consideration of predictive uncertainty. Furthermore, while GP models have been adopted, the existing literature fails to focus on the probabilistic aspects of GP regression. The existing literature primarily emphasises GP as a flexible non-parametric regression model, but it fails to acknowledge the benefits of generating full predictive posterior distributions or quantifying uncertainty in complex regression problems.</p>
        <p>Tensor data is ubiquitous in scientific and engineering applications, e.g., a single-channel 2D tensor describing a scalar field for some spatially varying measurement, or a three-channel tensor representing RGB images. This study is concerned with time-indexed tensors, 𝐑 (𝑡) ∈ R 𝐶×𝐻×𝑊 , describing the state of some dynamical system at time 𝑡, i.e. at each point in time there are 𝐶 variables being measured, each varying over a rectangular 𝐻 × 𝑊 grid. Given a collection of timeindexed measurements, {𝐑 (𝑡) } 𝑇 𝑡=1 , the aim is to construct a probabilistic regression model,</p>
        <p>(1) such that given the state of the system at time 𝑡, a probabilistic forecast for the state of the system at time 𝑡 + 1 can be made using some model, 𝑓 . The regression model will be constructed in order to provide both point-wise estimates of the system at time</p>
        <p>To construct the probabilistic regression model, 𝑓 , a GP is utilised. A GP is a stochastic process defined as a collection of random variables, any finite number of which have a joint Gaussian distribution (Williams and Rasmussen, 2006). A GP can be considered as a generalisation of the multivariate Gaussian distribution, retaining many convenient mathematical properties of the multivariate Gaussian distribution (Donnelly et al., 2022). However, instead of being defined over finite-length vectors and parameterised by a mean vector and covariance matrix, a GP is defined over functions and parameterised by mean and covariance functions. A GP is therefore a (possibly infinite) distribution that is fully specified by its mean function 𝑚(𝐱 (𝑖) ) and its kernel-covariance function 𝑘(𝐱 (𝑖) , 𝐱 (𝑗) ).</p>
        <p>GPs are a robust Bayesian regression model, allowing for straightforward inferences about predictive uncertainty without expensive sampling techniques. However, GPs are limited in their scalability and flexibility. For standard regression problems, GPs can only learn functions of the form 𝑓 ∶ R 𝑝 → R. i.e., a scalar output, 𝑦 (𝑡+1) , is regressed onto a vector valued input, 𝐱 (𝑡) ,</p>
        <p>where, 𝑓 is a GP and 𝜖 (𝑡) is additive Gaussian noise. However, in GP regression, instead of producing a deterministic prediction, 𝑓 (𝐱 (𝑡) ), a full predictive posterior distribution over the output is constructed,</p>
        <p>where 𝜇 (𝑡) * and 𝜎 (𝑡) * are the predictive mean and standard deviation, respectively. Samples can be drawn from this posterior. However, the expected value and predictive uncertainty are determined analytically without sampling where, E[𝑦 (𝑡+1) ] = 𝜇 (𝑡+1) * and V[𝑦 (𝑡+1) ] = 𝜎 (𝑡+1) * . The regression problem outlined in Eq. ( 1) cannot be solved using standard GPs as these are three-dimensional tensors, not scalars. However, GPs can be extended to handle vector-to-vector regression problems. This can be achieved by modelling each output independently using a standard GP or through multi-task models using approaches such as the Linear Model of Coregionalization (Alvarez et al., 2012). Other methods (Liu et al., 2018) also exist, which generally model the problem as a standard GP but use structured kernel matrices to handle multiple tasks and specify inter-task dependencies.</p>
        <p>GPs suffer from high computational complexity costs, scaling cubically, 𝑂(𝑁 3 ), with the number of training instances, 𝑁, due to the inversion of a 𝑁 × 𝑁 matrix during training (Williams and Rasmussen, 2006;Chatrabgoun et al., 2022). Modelling 𝑑 tasks with an exact multitask GP similarly scale with 𝑂(𝑑 3 𝑁 3 ) complexity. While approximate GPs using a subset of inducing points, thereby reducing the size of the matrix being inverted, have been demonstrated effectively (Liu et al., 2020;Foreman-Mackey et al., 2017), a robust probabilistic model that demonstrates scalability like neural networks does not exist.</p>
        <p>To overcome these limitations, the underlying data, {𝐑 (𝑡) } 𝑇 𝑡=1 , can be modified to make it amenable for GP regression. By producing a sophisticated encoder, 𝑔, and corresponding decoder, ℎ, to transform the original tensors, 𝐑 (𝑡) , into latent vectors, 𝐳 (𝑡) , with minimal reconstruction error, 𝐿( R(𝑡) , 𝐑 (𝑡) ),</p>
        <p>the regression problem can be reinterpreted as, 𝐳 (𝑡+1) = 𝑓 (𝐳 (𝑡) ) + 𝝐 (𝑡) .</p>
        <p>(</p>
        <p>where 𝑓 is a multi-output GP model, and 𝑑, where 𝐳 (𝑡) ∈ R 𝑑 , is small enough to build such a model. Using a GP, a predictive posterior can be constructed over the latent features, 𝑝(𝐳 (𝑡+1) |𝐳 (𝑡) ), as shown in Eq. (3) By sampling in the latent space, producing a collection of realisations of the latent target, 𝐳 (𝑡+1) 𝑖 , uncertainty can be propagated back into the original tensor space,</p>
        <p>producing robust point-wise forecasts (Eq. ( 6)) and estimates of predictive uncertainty (Eq. ( 7)) for each element in the tensor simultaneously.</p>
        <p>A robust and accurate encoder-decoder scheme that generates nondiscontinuous features in the latent 𝑧-space with minimal reconstruction error could enable a GP regression model to accurately reproduce the system's behaviour in the encoded space. For an encoder-decoder to have sufficient capability to produce meaningful representations in the latent space from large tensors, 𝑔 and ℎ are likely to be complex, nonlinear functions. The overall methodology for the study is outlined in Fig. 1, by sampling in the latent space and propagating samples through the decoder, point-wise forecasts and estimates of uncertainty can be propagated through to the original feature space.</p>
        <p>Autoencoders are a class of neural network consisting of an encoder sub-network, 𝑔, and a decoder sub-network, ℎ, which take an arbitrary input 𝐗 and maps back to an output X with the same shape as the input. Autoencoders generally contain an under-parameterised bottleneck such that the encoder sub-network produces an intermediate output, 𝐳, smaller than the original input. Using self-supervised training, where the input and the output are the same, and a distance-based loss function, 𝐿, the network optimises its weights to minimise the reconstruction error between the input and the reconstructed output, 𝐿( X, 𝐗). Since the bottleneck of the network is under-parameterised, the network simultaneously learns an efficient encoding scheme for the original data, 𝐳 = ℎ(𝐗), and decoder for the latent features, X = 𝑔(𝐳), as previously expressed in Eq. ( 4).</p>
        <p>Neural networks can learn non-parametric, non-linear functions, which means that both the encoder and decoder sub-networks can be infinitely expressive and capable of learning complex patterns. By constructing highly complex networks and leveraging large datasets to optimise the weights, autoencoders can be a more powerful dimensionality reduction tool than parametric approaches such as PCA. Furthermore, neural network architectures, such as CNN are well-suited for to handling non-linear data with multiple channels in the input and output making them ideal for flexible tensor-based compression. An example of two-channel convolutional autoencoder is illustrated in Fig. 2.</p>
        <p>To construct highly flexible and powerful autoencoders, existing state-of-the-art CNN classification architectures were used as the basis of the network. The ResNet50 (illustrated in Fig. 3 2016) is a 150-layer CNN designed to classify RGB images into distinct categories. The architecture obtained SOTA performance on ImageNet (Krizhevsky et al., 2017) and CIFAR-10 ( Krizhevsky et al., 2009). In this architecture, multi-channel tensors are down-sampled through convolutions and pooling layers to finally produce classification probabilities in a vector-based output. The network can be modified such that the classification layer becomes the bottleneck of the autoencoder with a fixed size. In addition to the original ResNet50 architecture, smaller versions of ResNet, such as ResNet18 and ResNet34, which were inspired by the original architecture, were also tested to compare their performance.</p>
        <p>The proposed predictive model is developed for the case study of predicting temperature and pressure data recorded at 3-hour intervals on a global scale. The dataset consists of the full record of simulated hindcast data from the ERA5 reanalysis (Hersbach et al., 2020) for the year 2022. This meant the proposed regression model involved simultaneously forecasting global temperature and pressure values for 3-hours ahead based on the current values. The spatial resolution of the data is 1 • × 1 • , therefore for each time-step, the data is described by tensors 𝐑 (𝑡) ∈ R 2×180×360 and the full dataset consisted of approximately 2900 samples. This resulted in an aggregated dataset 𝐗 ∈ R 2900×2×180×360 . Examples of the original data are illustrated in Fig. 4, showing the transition of both temperature and pressure across 3 time steps. This data also highlights the vastly different scales of the two measurements, varying in units of measurement and the magnitude of the average values.</p>
        <p>In this study three autoencoders were trained and assessed, with the encoder's sub-network based on the existing ResNet18, ResNet34, and ResNet50 object-classification architectures with the output layers modified to produce continuous latent vectors rather than discrete classifications. In order to determine the dimensionality of the bottleneck and latent vectors, a trade-off between the expressiveness of the latent representations and the complexity of the GP model needs to be balanced. Balancing this trade-off along with preliminary testing determined a latent dimension size of 12 features such that 𝐳 𝑡 ∈ R 12 . The training cost and computational resources required to train the autoencoder are not sensitive to the size of 𝐳 𝑡 , the GP is highly sensitive to this choice. The computational complexity of the multi-task GP model is 𝑂(𝑑 3 𝑁 3 ) where 𝑑 is the number of features and 𝑁 is the number of instances, so as the latent dimension 𝑑 grows, the GP model can quickly become prohibitively expensive.</p>
        <p>In order to construct robust autoencoders that can effectively capture the intricate features of the underlying data and generalise well, it is necessary for the network to possess adequate flexibility. Additionally, the application of data augmentation and pre-processing strategies during training has been shown to be crucial for achieving good performance. The data were normalised to have zero mean and unit variance across each feature, then a series of augmentations were applied randomly during the training process. These augmentations consisted of crops and resize (i.e., randomly cropping then re-scaling back to the original shape), alternating the order of the channels (i.e., swapping the order of temperature and pressure data), applying a mask (i.e., randomly zeroing out cells in the data) and randomly applying Gaussian noise to cells. Examples of some of these augmentation strategies applied independently can be seen in Fig. 5.</p>
        <p>The training of the autoencoders was parallelised across 6 Nvidia Tesla K80 GPUs using a batch size of 32, Adam optimiser and initial learning rate of 1𝑒 -4 . Training was conducted for 100 epochs, or until convergence was reached. After producing the latent feature dataset from the trained autoencoder, a multi-task GP parameterised by a multi-task Matern 3∕2 kernel function and a constant mean function was trained. The GP training, i.e., tuning of the hyperparameters of the kernel function and additional noise parameters, was performed through minimising the negative marginal log likelihood (Williams and Rasmussen, 2006).</p>
        <p>Additionally, a second phase of GP training was initiated whereby the noise hyperparameter of the GP likelihood, i.e., the parameter determining how much the true target is corrupted by noise as illustrated in Eq. ( 1), is modified to fine-tune the level of predictive uncertainty. Purely data-driven optimisation of these parameters can overfit the data and produce values of V[𝐳 (𝑡+1) ] that are far too low in practice. However, the influence of the noise parameter on the value of E[𝐳 (𝑡+1) ] is minimal and therefore does not significantly change the performance of point-wise predictive performance.</p>
        <p>The performance of the ResNet-based autoencoders can be seen in Table 1. Increasing the network's capacity resulted in a consistent decrease in the RMSE for temperature predictions, but it did not necessarily lead to an improvement in predictive performance for pressure measurements. ResNet-34 is likely to be the right balance between good predictive performance, while not being too highly overparameterised. Training and inference costs also scale with size of the model and therefore again, ResNet-34 seemed the best architecture overall out of those tested in this study because of this trade-off between accuracy and computational resources. Furthermore, ResNet-34 obtained the best predictive score for the pressure measurements out of all the models. An RMSE of 2.79 for temperature predictions on the test set means that on average, the predicted value in each cell is incorrect by 2.79 • C, and likewise, for pressure, each cell is incorrect by, on average, 434 hPa.</p>
        <p>In Fig. 6, the reconstruction error on the test set can be seen for the ResNet-34 autoencoder. The plot shows the RMSE for each cell in the domain across all time-steps in the test set. In spite of the aggregate RMSE value being 2.79 • C, it is observed that much of the domain, especially those locations on land, have a higher average loss. Similarly, the values over the oceans tend to have a lower average loss. This outcome can be explained by Fig. 7, which shows the standard deviation of the test set values, highlighting that the areas of the domain with the most variability are the same areas where the prediction error is highest.</p>
        <p>In Fig. 8, the predictive performance for a collection of test samples is illustrated by showing the 𝐿 1 loss on the temperature and pressure data. As can be seen across both variables, the reconstruction error is low generally, with small areas where the reconstruction error is significantly higher. For instance, in sample 1, while the majority of the domain has a reconstruction error below 2, certain areas in North America exhibit a significantly higher reconstruction error, ranging from 8 and above. This is a common result of data-driven modelling in which predictive performance is lower when results deviate most strongly from their training set means.</p>
        <p>After having trained the autoencoder, the latent feature data was used to train the GP regression model. Regression performance of the GP model in the latent space is illustrated in Fig. 9 along with the 2 standard deviation predictive intervals. An accurate model would expect to see the true value within these intervals for 95% of the observations. The autoencoder resulted in non-discontinuous and smooth latent features facilitating very high latent space regression accuracy. Having trained both the autoencoder and GP regression model, the composite GP-autoencoder model can be used for inference by producing point-wise forecasts for the true tensor, E[ R(𝑡+1) ], and a tensor of equal shape, V[ R(𝑡+1) ], outlining predictive uncertainty associated with each element where</p>
        <p>𝑖𝑗𝑘 . The results of the GP-autoencoder performance on the test set can be seen in Table 2. The full GP-autoencoder model increases the average loss by 1.03 • C for the temperature predictions, and by 234 hPa for the pressure predictions compared to the baseline reconstruction error for the autoencoder alone. When compared to the RMSE values of 1.7 • C and 2.1 • C for temperature predictions in the studies by Zhang and Dong (2020) and Kreuzer et al. (2020), respectively, the RMSE of 3.82 • C achieved is lower in point-wise accuracy. However, it is important to note that these are not like-for-like comparisons. The proposed model performed in significantly higher dimensions, conducts simultaneous multi-task forecasting, and provides predictive uncertainty estimates. Considering these caveats, our model illustrates notable performance, achieving a high level of accuracy.</p>
        <p>Furthermore, in order to assess the robustness of the estimated predictive posterior distribution a custom probabilistic validation metric was proposed. This metric involved, for each sample in the test data, counting how many of the true individual tensor elements, 𝐑 (𝑡+1) 𝑖𝑗𝑘 , were within the plausible range of the posterior distribution. This was done by counting the number of tensor elements within 3 standard deviations of the mean of the predictive posterior,</p>
        <p>where,</p>
        <p>On average it was found that 95.6% of the true values were within this 3 s.d. range. For a fully accurate Gaussian probabilistic model it would be expected that approximately 99% of the values would fall within the 3 s.d. range. However, this importantly highlights that even when the point-wise estimate is not close to the target value, the true target value has a 95% chance of being within the range of the predictive posterior. This underscores the significance and practical value of incorporating uncertainty quantification in complex regression models.</p>
        <p>Examples of some of the best and worst performance with respect to this metric can be seen in Figs. 10-11, illustrating predictions where the true value falls within the predictive posterior distribution (3 s.d.). The cells marked in red represent cells where the respective forecast did not fall in the range for A) temperature and (B) pressure. Poor performance again results from true values that deviate mostly strong from their training set values. The average rate of 95.6% of cells falling within the reasonable range of the predictive posterior suggests that the model generally performs very well with respect to this metric. However, to handle more extreme cases additional probabilistic components could be considered in the model, such as a probabilistic encoding scheme through the use of variational autoencoders.</p>
        <p>In Fig. 12, the average RMSE for both temperature and pressure is illustrated alongside the average predictive uncertainty (one standard deviation) across the test data. This figure shows that the model generates mostly accurate point-wise estimates but also high-resolution dynamic estimates of predictive uncertainty. The figure only outlines the predictive uncertainty averaged across test samples. However, these estimates of predictive uncertainty vary from sample to sample are not constant. Fig. 7 showed that there is low variation in ocean conditions around the equator and as such the model produces very low estimates of predictive uncertainty, meaning point-wise forecasts can be interpreted with a high degree of confidence. Similarly, in areas where there is significant variation, a robust approach to forecasting would be to observe the full range of possible values and give less credibility to a single point-wise estimate. In the forecasting of complex systems, it is better to be imprecisely right than precisely wrong. Furthermore, by using a more diverse sample of training data the model could easily be improved with respect to predictive performance. However, for demonstrative purposes, only data from 2022 was used to train and validate the model.</p>
        <p>The proposed methodology has been demonstrated highly effective for the case study of forecasting temperature and pressure on a global scale. This is a highly versatile approach to forecasting problems involving high-dimensional tensor-based data with clear applications beyond the case study outlined here. This methodology relies on autoencoders for data compression and GPs for learning the latent-space dynamics. Both of these models are highly flexible; autoencoders can be made with arbitrary encoder/decoder sub-networks and GPs can be parameterised with a variety of kernel and mean functions.</p>
        <p>A limitation of the proposed method, the individual autoencoder, and GP models is that the data should exhibit some measure of smoothness, or facilitate a smooth representation through pre-processing. In the case study outlined here, this would mean that it should be expected that values within a pixel neighbourhood in the tensors should be similar and that the changes in the value of a pixel between time steps should not be very large and discontinuous. Dealing with highly nonlinear and discontinuous data has always been known as a limitation of GP models which later influenced the development of Deep Gaussian Process models (Damianou and Lawrence, 2013).</p>
        <p>Autoencoders are sensitive to the size of the latent dimension and as the size of the latent dimension converges to the size of the input, the reconstruction error will converge to zero. Conversely, the greater the compression the more information can be expected to be lost. Similarly, depending on the choice of kernel function, GPs will result in a smooth interpolation between points in the output space which can result in poor performance on outlying and extreme data points. Given these limitations, their composition into a single model could result in a prohibitively high amount of information loss from the original data if poorly constructed.</p>
        <p>An existing limitation of GP regression models is the high computational complexity during training, with multi-task GP models scaling with 𝑂(𝑑 3 𝑁 3 ) computational complexity for 𝑑 tasks/output features and 𝑁 instances. While autoencoders are highly scalable and can scale to arbitrarily large 𝑁, the model will be bottlenecked by the complexity of the latent-space GP model. Therefore, careful consideration of training data is required to obtain an optimal ratio of information-data through the selection of the 𝑁 training instances.</p>
        <p>Multi-task spatially-varying regression is a challenging task in datadriven modelling, with models needing to be sufficiently flexible to capture the underlying patterns while also not suffering from high computational costs and the curse of dimensionality. However, many natural and artificial dynamical systems generate this data in abundance through improvements in numerical simulation, sensor technology or the proliferation of satellite data. Forecasting the behaviour of these systems is important in many scientific and engineering disciplines, as well as crucial to modern life generally.</p>
        <p>To make robust forecasts and sophisticated inferences about the future behaviour of these systems, an appropriate consideration must be made for the uncertainty that can arise from the inherently chaotic nature of many of these systems. Therefore, to produce effective regression models for the behaviour of these systems, the models need to generalise well to high-dimensional spatiotemporal data and quantify the uncertainty associated with the forecasts. The existing literature and ML models fail to achieve this for representative problems in the real-world. Developing models with these capabilities is still an ongoing challenge in the existing literature due to clear challenges of tractability and computational complexity associated with expensive sampling procedures or Bayesian approaches.</p>
        <p>By utilising SOTA approaches to deep learning and probabilistic inference, a flexible and robust non-parametric approach to probabilistic tensor-based regression has been outlined. Using advanced CNN-based architectures from computer vision applications as the basis of autoencoder networks, it is shown possible to accurately reconstruct complex multi-channel tensor data from very small latent vectors. Further, the latent vectors produced were smooth, non-discontinuous, and amenable to exact multi-task GP regression.</p>
        <p>The proposed GP-autoencoder model has been applied to the case study of simultaneously forecasting one-step-ahead global temperature and pressure observations, significantly important high-level underlying drivers of global climate behaviour. Developing an ML-based model that facilitates near-instantaneous is important to reduce the reliance on expensive numerical simulation and consequently allows for realtime forecasting where simulation would not. This would be crucial to the development of early warning systems, nowcasting, and parameterise the inputs of smaller regional simulation models. Furthermore, by including dynamic estimates of predictive uncertainty the forecasts would allow for a more sophisticated and nuanced interpretation than a deterministic point-wise estimate. This can be particularly valuable in complex regression problems, where uncertainty quantification is crucial for informed decision making.</p>
        <p>The model was demonstrated to have good accuracy, obtaining an average error of 3.82 • C and 638 hPa for the temperature and pressure observations, respectively. Additionally, the model produced well-calibrated predictive posterior distributions, with the posterior distribution on average containing the true observation 95% of the time. This aspect of the model demonstrates that even when the pointwise error was above average there was a strong chance that the true observation falls within the predicted distribution. Contrasting these results with the existing SOTA outlined in Section 2 showed the proposed model to have a slightly higher error for temperature forecasting. However, the problem tackled in this study uses data order of magnitudes larger, forecasts multiple variables simultaneously, and provides probabilistic predictions. These contributions while still obtaining accurate and comparable forecasts, demonstrate the contributions outlined in this study.</p>
        <p>A commonality of all data-driven forecasting is that performance was poorest when the data being predicted was more anomalous and less similar to the examples within the training data. For demonstrative purposes in this study, the data used was limited only to 2022, which inevitably leads to lower performance in some of the test cases as only a fraction of the true range of outputs is observed. The proposed model's predictive power could be readily enhanced through the utilisation of a more extensive and sophisticated training dataset. However, these limitations are intrinsic to data-driven modelling, and are to some extent irreducible since finite training data is unlikely to ever fully capture the behaviour of a dynamic system. Probabilistic modelling can effectively mitigate many of these problems by offering dynamic and robust estimates of predictive uncertainty, thereby eliminating the issues associated with producing point-wise estimates on previously unseen data.</p>
        <p>The novel methodological approach outlined in this study can readily be repurposed to a variety of complex modelling problems involving</p>
        <p>The authors would like to thank The Scientific Computing Research Technology Platform at the University of Warwick for the computational resources allocated to this study.</p>
        <p>The data used is from publicly available sources cited in the manuscript.</p>
        <p>time-indexed tensor data. Based on the results outlined, the authors believe that using a highly-flexible autoencoder and a GP regression model is the best existing approach to forecasting high-dimensional multi-channel data in a robust and probabilistic manner.</p>
        <p>James Donnelly: Conceptualization, Methodology, Software, Validation, Writing -original draft, Writing -review &amp; editing. Alireza Daneshkhah: Supervision, Writing -review &amp; editing. Soroush Abolfathi: Supervision, Writing -review &amp; editing.</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </text>
</tei>

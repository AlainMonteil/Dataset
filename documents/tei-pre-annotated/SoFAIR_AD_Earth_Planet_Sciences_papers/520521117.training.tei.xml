<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T16:04+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>To create a reliable radiocarbon calibration curve, one needs not only high-quality data but also a robust statistical methodology. The unique aspects of much of the calibration data provide considerable modeling challenges and require a made-to-measure approach to curve construction that accurately represents and adapts to these individualities, bringing the data together into a single curve. For IntCal20, the statistical methodology has undergone a complete redesign, from the random walk used in IntCal04, IntCal09 and IntCal13, to an approach based upon Bayesian splines with errors-in-variables. The new spline approach is still fitted using Markov Chain Monte Carlo (MCMC) but offers considerable advantages over the previous random walk, including faster and more reliable curve construction together with greatly increased flexibility and detail in modeling choices. This paper describes the new methodology together with the tailored modifications required to integrate the various datasets. For an end-user, the key changes include the recognition and estimation of potential over-dispersion in 14 C determinations, and its consequences on calibration which we address through the provision of predictive intervals on the curve; improvements to the modeling of rapid 14 C excursions and reservoir ages/dead carbon fractions; and modifications made to, hopefully, ensure better mixing of the MCMC which consequently increase confidence in the estimated curve.To create a reliable radiocarbon calibration curve, one needs not only high-quality data but also a robust statistical methodology. The unique aspects of much of the calibration data provide considerable modeling challenges and require a made-to-measure approach to curve construction that accurately represents and adapts to these individualities, bringing the data together into a single curve. For IntCal20, the statistical methodology has undergone a complete redesign, from the random walk used in IntCal04, IntCal09 and IntCal13, to an approach based upon Bayesian splines with errors-in-variables. The new spline approach is still fitted using Markov Chain Monte Carlo (MCMC) but offers considerable advantages over the previous random walk, including faster and more reliable curve construction together with greatly increased flexibility and detail in modeling choices. This paper describes the new methodology together with the tailored modifications required to integrate the various datasets. For an end-user, the key changes include the recognition and estimation of potential over-dispersion in 14 C determinations, and its consequences on calibration which we address through the provision of predictive intervals on the curve; improvements to the modeling of rapid 14 C excursions and reservoir ages/dead carbon fractions; and modifications made to, hopefully, ensure better mixing of the MCMC which consequently increase confidence in the estimated curve.</p>
        <p>Over the years, the IntCal statistical methodology has developed and improved alongside our understanding of the constituent data and as we gain more knowledge of the underlying EarthOver the years, the IntCal statistical methodology has developed and improved alongside our understanding of the constituent data and as we gain more knowledge of the underlying Earth</p>
        <p>One of the core challenges in creating a reliable radiocarbon ( 14 C) calibration curve is ensuring that the statistical approach used for the curve'sp r o d u c t i o ni sa b l et o accurately synthesize the various datasets which make up the curve, specifically recognizing and incorporating their diverse and unique aspects in doing so. This is particularly critical in light of the recent increase in the availability of high-precision radiocarbon determinations, and the consequent demand by users for ever more precise and accurate calibration. For IntCal20, not only is the underlying data available for use in curve construction considerably more numerous and detailed than in previous curves but, due to the advances in our wider understanding of the Earth'ss y s t e m s ,o u ri n s i g h t into the specific and individual characteristics of much of that data has also improved. If we are able to harness and more accurately represent these specific features within our curve construction then this will hopefully improve the resultant calibration. To achieve this, for the new calibration curve we have completely revised the statistical methodology from a random walk to a Bayesian spline based approach. This new approach allows us to take better advantage of the underlying data and also to provide output which is more useful and relevant for calibration users.One of the core challenges in creating a reliable radiocarbon ( 14 C) calibration curve is ensuring that the statistical approach used for the curve'sp r o d u c t i o ni sa b l et o accurately synthesize the various datasets which make up the curve, specifically recognizing and incorporating their diverse and unique aspects in doing so. This is particularly critical in light of the recent increase in the availability of high-precision radiocarbon determinations, and the consequent demand by users for ever more precise and accurate calibration. For IntCal20, not only is the underlying data available for use in curve construction considerably more numerous and detailed than in previous curves but, due to the advances in our wider understanding of the Earth'ss y s t e m s ,o u ri n s i g h t into the specific and individual characteristics of much of that data has also improved. If we are able to harness and more accurately represent these specific features within our curve construction then this will hopefully improve the resultant calibration. To achieve this, for the new calibration curve we have completely revised the statistical methodology from a random walk to a Bayesian spline based approach. This new approach allows us to take better advantage of the underlying data and also to provide output which is more useful and relevant for calibration users.</p>
        <p>Systems processes. For IntCal98 (Stuiver et al. 1998), the curve was produced via a combination of linear interpolation of windowed tree-ring averages and frequentist splines. In 2004, the approach was updated to a random walk model (Buck and Blackwell 2004) which introduced an approximate Bayesian method, and began to incorporate some of the important additional aspects of the constituent data such as potential uncertainty in the calendar ages for the older determinations, and that many tree-ring observations related to multiple years of metabolization. This random walk was developed further for IntCal09 (Blackwell and Buck 2008;Heaton et al. 2009) and IntCal13 (Niu et al. 2013) into a fully Bayesian MCMC approach that enabled the inclusion of more of the unique structures in the calibration datasets such as floating sequences of tree rings and more complex covariances in the calendar age estimates provided by wiggle-matching or palaeoclimate tie-pointing.Systems processes. For IntCal98 (Stuiver et al. 1998), the curve was produced via a combination of linear interpolation of windowed tree-ring averages and frequentist splines. In 2004, the approach was updated to a random walk model (Buck and Blackwell 2004) which introduced an approximate Bayesian method, and began to incorporate some of the important additional aspects of the constituent data such as potential uncertainty in the calendar ages for the older determinations, and that many tree-ring observations related to multiple years of metabolization. This random walk was developed further for IntCal09 (Blackwell and Buck 2008;Heaton et al. 2009) and IntCal13 (Niu et al. 2013) into a fully Bayesian MCMC approach that enabled the inclusion of more of the unique structures in the calibration datasets such as floating sequences of tree rings and more complex covariances in the calendar age estimates provided by wiggle-matching or palaeoclimate tie-pointing.</p>
        <p>The random walk approach however had several disadvantages. While in principle, it allowed for modeling flexibility, the details of the implementation required a very large number of parameters that, despite being highly dependent, were predominantly updated individually. This made it extremely slow to run; difficult to ensure it explored the full space of possible curves; and also hard to assess whether the MCMC had reached its equilibrium distribution. Furthermore, this restricted the ability to explore the impact of specific modeling assumptions or individual data on the final curve. As the volume of the data used to generate IntCal has increased, alongside the need for further bespoke modeling of key data structures, this random walk approach to curve creation has consequently become computationally impractical for further use.The random walk approach however had several disadvantages. While in principle, it allowed for modeling flexibility, the details of the implementation required a very large number of parameters that, despite being highly dependent, were predominantly updated individually. This made it extremely slow to run; difficult to ensure it explored the full space of possible curves; and also hard to assess whether the MCMC had reached its equilibrium distribution. Furthermore, this restricted the ability to explore the impact of specific modeling assumptions or individual data on the final curve. As the volume of the data used to generate IntCal has increased, alongside the need for further bespoke modeling of key data structures, this random walk approach to curve creation has consequently become computationally impractical for further use.</p>
        <p>For the updated IntCal20 curve, the IntCal working group therefore requested a new approach to curve creation be developed. This new approach needed to be equally rigorous, from a statistical perspective, as the previous methodology; to incorporate the advances in both geoscientific understanding and radiocarbon precision that have occurred since 2013; and yet overcome the implementational difficulties of the previous random walk approach. Specifically, to increase confidence in the curve's robustness and reliability the new approach needed to run at a speed which allowed investigation of the effect of key modeling choices not possible with the previous methodology. The new approach is based upon Bayesian splines with errors-in-variables as introduced by Berry et al. (2002) but requires multiple bespoke innovations to accurately adapt to the specific data complexities. We believe it offers a significant improvement over previous approaches not just in the modeling but also the additional output it can provide for both calibration users and geoscientific users more widely.For the updated IntCal20 curve, the IntCal working group therefore requested a new approach to curve creation be developed. This new approach needed to be equally rigorous, from a statistical perspective, as the previous methodology; to incorporate the advances in both geoscientific understanding and radiocarbon precision that have occurred since 2013; and yet overcome the implementational difficulties of the previous random walk approach. Specifically, to increase confidence in the curve's robustness and reliability the new approach needed to run at a speed which allowed investigation of the effect of key modeling choices not possible with the previous methodology. The new approach is based upon Bayesian splines with errors-in-variables as introduced by Berry et al. (2002) but requires multiple bespoke innovations to accurately adapt to the specific data complexities. We believe it offers a significant improvement over previous approaches not just in the modeling but also the additional output it can provide for both calibration users and geoscientific users more widely.</p>
        <p>The paper is set out as follows. In Section 2 we list the main advances made in the statistical methodology, data understanding and modeling since the last set of IntCal curves (Reimer et al. 2013;Hogg et al. 2013). We then provide, in Section 3, a short non-technical introduction to three key practical ideas for calibration users: an explanation of Bayesian splines; the importance of recognizing potential calendar age uncertainty in curve construction; and the modeling of potential over-dispersion in the data. Section 4 then describes the main technical details of curve construction. The IntCal20 curve is created in two sections with somewhat different statistical concerns. The more recent part of the curve, back to approximately 14 cal kBP, is based entirely upon tree-ring determinations, that are predominantly dendrodated with exact, known calendar ages. Here the main challenges are to incorporate the large amount of data, provide high resolution in the curve with appropriate uncertainties, and accurately incorporate blocked determinations that represent the measurement of multiple years. This is described in Section 4.3.F u r t h e r back in time, the curve is based upon a wider range of 14 C material where, as well as tree rings, we use direct and indirect records of atmospheric 14 C in the form of corals, macrofossils, forams and speleothems. We describe the modifications required to incorporate these kinds of data in Section 4.4. This being a Bayesian approach we are able to incorporate prior information into our model and also provide posterior information on particular outputs of independent interest. In Section 5 we provide an indication of the kind of additional output the new methodology is able to provide. In particular the new approach generates not only pointwise means and variances for the calibration curve but, for the first time, sets of complete posterior realizations from 0-55 cal kBP which also allow access to covariance information. This covariance has the potential to improve future calibration of multiple radiocarbon determinations, for example in wiggle matches or more complex modeling. We also gain information on the level of additional variation (beyond laboratory reported uncertainty) present in tree-ring 14 C determinations arising from the same calendar year. Finally, we discuss potential future work in Section 6 along with areas we have identified for further improvement for the next IntCal iteration. Note that the statistical approaches to the creation of SHCal20 (Hogg et al. 2020 in this issue) and Marine20 (Heaton et al. 2020 in this issue) are presented within those papers and not discussed in detail here.The paper is set out as follows. In Section 2 we list the main advances made in the statistical methodology, data understanding and modeling since the last set of IntCal curves (Reimer et al. 2013;Hogg et al. 2013). We then provide, in Section 3, a short non-technical introduction to three key practical ideas for calibration users: an explanation of Bayesian splines; the importance of recognizing potential calendar age uncertainty in curve construction; and the modeling of potential over-dispersion in the data. Section 4 then describes the main technical details of curve construction. The IntCal20 curve is created in two sections with somewhat different statistical concerns. The more recent part of the curve, back to approximately 14 cal kBP, is based entirely upon tree-ring determinations, that are predominantly dendrodated with exact, known calendar ages. Here the main challenges are to incorporate the large amount of data, provide high resolution in the curve with appropriate uncertainties, and accurately incorporate blocked determinations that represent the measurement of multiple years. This is described in Section 4.3.F u r t h e r back in time, the curve is based upon a wider range of 14 C material where, as well as tree rings, we use direct and indirect records of atmospheric 14 C in the form of corals, macrofossils, forams and speleothems. We describe the modifications required to incorporate these kinds of data in Section 4.4. This being a Bayesian approach we are able to incorporate prior information into our model and also provide posterior information on particular outputs of independent interest. In Section 5 we provide an indication of the kind of additional output the new methodology is able to provide. In particular the new approach generates not only pointwise means and variances for the calibration curve but, for the first time, sets of complete posterior realizations from 0-55 cal kBP which also allow access to covariance information. This covariance has the potential to improve future calibration of multiple radiocarbon determinations, for example in wiggle matches or more complex modeling. We also gain information on the level of additional variation (beyond laboratory reported uncertainty) present in tree-ring 14 C determinations arising from the same calendar year. Finally, we discuss potential future work in Section 6 along with areas we have identified for further improvement for the next IntCal iteration. Note that the statistical approaches to the creation of SHCal20 (Hogg et al. 2020 in this issue) and Marine20 (Heaton et al. 2020 in this issue) are presented within those papers and not discussed in detail here.</p>
        <p>All ages in this paper and the database are reported relative to mid-1950 AD (= 0 BP, before present). Conventional, pre-calibration, 14 C ages are given in units " 14 C yrs BP." Calendar, or calibrated, ages are denoted as "cal BP" or "cal kBP" (thousands of calibrated years before present).All ages in this paper and the database are reported relative to mid-1950 AD (= 0 BP, before present). Conventional, pre-calibration, 14 C ages are given in units " 14 C yrs BP." Calendar, or calibrated, ages are denoted as "cal BP" or "cal kBP" (thousands of calibrated years before present).</p>
        <p>As in previous updates to the curve, the constituent data is available from the IntCal database 
            <rs type="url">http://www.intcal.org/</rs>. Other inputs are available on request, for example covariance matrices for the calendar ages of the tie-pointed records of the Cariaco Basin (Hughen and Heaton 2020 in this issue), Pakistan and Iberian margin (Bard et al. 2013); and Lake Suigetsu (Bronk Ramsey et al. 2020 in this issue). Coding was performed in 
            <rs type="software">R</rs> (
            <rs type="creator">R Core Team</rs> 2019), using the 
            <rs type="software">fda</rs> (Ramsay et al. 2018), 
            <rs type="software">mvtnorm</rs> (Genz et al. 2019), 
            <rs type="software">mvnfast</rs> (Fasiolo 2016) and Matrix (Bates and Maechler 2019) packages to efficiently create and fit the spline bases; and doParallel (Corporation and Weston 2019) to implement parallelization for the MCMC tempering. This code is available on request from the first author.
        </p>
        <p>The main differences/improvements in the updated IntCal20 methodology over the previous random walk can be split into three broad categories: those relating to improvements in the statistical implementation itself; progress in the detailed modeling of unique data aspects that are enabled by the updated methodology; and finally, advances in the curve output that are both relevant to users of the calibration curve and of potential interest in their own right.The main differences/improvements in the updated IntCal20 methodology over the previous random walk can be split into three broad categories: those relating to improvements in the statistical implementation itself; progress in the detailed modeling of unique data aspects that are enabled by the updated methodology; and finally, advances in the curve output that are both relevant to users of the calibration curve and of potential interest in their own right.</p>
        <p>1. Bayesian Splines-the change from the random walk of Niu et al. (2013) to splines allows a much more computationally feasible fitting algorithm. We retain the Bayesian aspect since it allows us to incorporate the various unique aspects of the calibration data; provides useful posterior information, e.g. the posterior calendar ages of the constituent data; and maintains consistency with calibration itself which is now universally implemented under that paradigm. This Bayesian spline approach is equally conceptually rigorous as the random walk but considerably more flexible and can be run much more quickly. 2. Change in modeling and fitting domain-function estimation via splines is based on a trade-off between creating a curve that passes close to the observed data yet is not too rough. Previously all aspects of the curve's construction occurred in the radiocarbon age domain. However, this is not the natural space in which to either model the curve roughness or decide on fidelity of the data to the curve. In the older section of the 14 C record, the measurement uncertainties on the radiocarbon age scale become nonsymmetric. This causes difficulties in fairly assessing the fit of a model to observed data if we judge it in this radiocarbon age domain. Instead, it is more natural to assess model fit in F 14 C where measurement uncertainty remains symmetric. Similarly, a more natural domain in which to model the curve roughness is in ∆ 14 C space. We therefore change our modeling domain to Δ 14 C and our fitting domain to F 14 Cw h e n creating the curve. See Section 3.1.2 for definitions of the radiocarbon age, F 14 Ca n d Δ 14 Cd o m a i n s . 3. Over-dispersion in dendrodated trees-as the volume of data entering IntCal increases, it is key to make sure we do not produce a curve which is over-precise as this would give inaccurate calibration for a user. While laboratories attempt to quantify all 14 C uncertainty in their measurements, including through intercomparison exercises such as Scott et al. (2017), there remain some sources of additional 14 C variation which are difficult for any laboratory to capture. For tree rings, potential examples include variation between local region, species, or growing season. Consequently, when we bring together 14 C measurements from the same calendar year, they may potentially be overdispersed, i.e. more widely spread than would be expected given their laboratory quoted uncertainty. The new approach incorporates a term allowing for potential overdispersion within 14 C determinations of the tree rings so that, if additional variability is seen in the underlying IntCal data, the method will account for it and prevent excessive confidence in the resultant curve. 4. Heavy tailed errors-in the older portion of the curve, where data come from a range of different 14 C reservoirs, we aim to reduce the influence of potential outliers by permitting each dataset to have heavier tailed errors. These tails are adaptively estimated during curve construction.1. Bayesian Splines-the change from the random walk of Niu et al. (2013) to splines allows a much more computationally feasible fitting algorithm. We retain the Bayesian aspect since it allows us to incorporate the various unique aspects of the calibration data; provides useful posterior information, e.g. the posterior calendar ages of the constituent data; and maintains consistency with calibration itself which is now universally implemented under that paradigm. This Bayesian spline approach is equally conceptually rigorous as the random walk but considerably more flexible and can be run much more quickly. 2. Change in modeling and fitting domain-function estimation via splines is based on a trade-off between creating a curve that passes close to the observed data yet is not too rough. Previously all aspects of the curve's construction occurred in the radiocarbon age domain. However, this is not the natural space in which to either model the curve roughness or decide on fidelity of the data to the curve. In the older section of the 14 C record, the measurement uncertainties on the radiocarbon age scale become nonsymmetric. This causes difficulties in fairly assessing the fit of a model to observed data if we judge it in this radiocarbon age domain. Instead, it is more natural to assess model fit in F 14 C where measurement uncertainty remains symmetric. Similarly, a more natural domain in which to model the curve roughness is in ∆ 14 C space. We therefore change our modeling domain to Δ 14 C and our fitting domain to F 14 Cw h e n creating the curve. See Section 3.1.2 for definitions of the radiocarbon age, F 14 Ca n d Δ 14 Cd o m a i n s . 3. Over-dispersion in dendrodated trees-as the volume of data entering IntCal increases, it is key to make sure we do not produce a curve which is over-precise as this would give inaccurate calibration for a user. While laboratories attempt to quantify all 14 C uncertainty in their measurements, including through intercomparison exercises such as Scott et al. (2017), there remain some sources of additional 14 C variation which are difficult for any laboratory to capture. For tree rings, potential examples include variation between local region, species, or growing season. Consequently, when we bring together 14 C measurements from the same calendar year, they may potentially be overdispersed, i.e. more widely spread than would be expected given their laboratory quoted uncertainty. The new approach incorporates a term allowing for potential overdispersion within 14 C determinations of the tree rings so that, if additional variability is seen in the underlying IntCal data, the method will account for it and prevent excessive confidence in the resultant curve. 4. Heavy tailed errors-in the older portion of the curve, where data come from a range of different 14 C reservoirs, we aim to reduce the influence of potential outliers by permitting each dataset to have heavier tailed errors. These tails are adaptively estimated during curve construction.</p>
        <p>5. Improved model mixing and parallel tempering-when using any MCMC method, it is crucial to ensure that the chain has reached its equilibrium distribution and that it is not stuck in one part of the model space. This was a significant concern with the previous random walk approach since the curve was updated one calendar year at a time.5. Improved model mixing and parallel tempering-when using any MCMC method, it is crucial to ensure that the chain has reached its equilibrium distribution and that it is not stuck in one part of the model space. This was a significant concern with the previous random walk approach since the curve was updated one calendar year at a time.</p>
        <p>Conversely, Bayesian splines enable updates of the complete curve simultaneously via Gibbs sampling. To address any additional concerns about mixing of the new MCMC and to ensure we explore the space of possible curves more freely, we also implement parallel tempering whereby we run multiple modified/tempered chains simultaneously in such a way that some can move around the space more easily. By appropriate switching between these tempered chains we can further improve model mixing.Conversely, Bayesian splines enable updates of the complete curve simultaneously via Gibbs sampling. To address any additional concerns about mixing of the new MCMC and to ensure we explore the space of possible curves more freely, we also implement parallel tempering whereby we run multiple modified/tempered chains simultaneously in such a way that some can move around the space more easily. By appropriate switching between these tempered chains we can further improve model mixing.</p>
        <p>These steps forward in the statistical implementation, while still maintaining computational feasibility, also enable us both to incorporate new data structures and to advance our modeling of the processes from which the data arise:These steps forward in the statistical implementation, while still maintaining computational feasibility, also enable us both to incorporate new data structures and to advance our modeling of the processes from which the data arise:</p>
        <p>1. Large increase in volume of data throughout the curve-the new IntCal20 is based upon a much greater number of 14 C determinations. These include many annual tree-ring determinations such as remeasurement within various laboratories of data from the period of Thera (e.g. Pearson et al. 2018); new wiggle-matched and floating sequences of late-glacial tree rings (e.g. Capano et al. 2020 in this issue); and new measurements of Hulu Cave extending further back in time (Cheng et al. 2018). In total, the new IntCal20 curve is based upon 12,904 raw 14 C measurements compared to the 7019 used for IntCal13 (Reimer et al. 2013). This rapid increase means a faster curve construction method is essential.1. Large increase in volume of data throughout the curve-the new IntCal20 is based upon a much greater number of 14 C determinations. These include many annual tree-ring determinations such as remeasurement within various laboratories of data from the period of Thera (e.g. Pearson et al. 2018); new wiggle-matched and floating sequences of late-glacial tree rings (e.g. Capano et al. 2020 in this issue); and new measurements of Hulu Cave extending further back in time (Cheng et al. 2018). In total, the new IntCal20 curve is based upon 12,904 raw 14 C measurements compared to the 7019 used for IntCal13 (Reimer et al. 2013). This rapid increase means a faster curve construction method is essential.</p>
        <p>2. Blocking in dendrodated trees-with the new radiocarbon revolution providing an increased ability to measure annual tree rings, there has been a concern that the inclusion within IntCal of determinations relating to decadal or bi-decadal averages alongside these new annual measurements could mean we lose critical information on short-term variation in atmospheric radiocarbon levels. Our approach fully recognizes the number of years each determination represents, meaning there is no loss of information as a result of such blocked determinations. 3. Variable marine reservoir ages-IntCal09 (Reimer et al. 2009) and IntCal13 (Reimer et al. 2013) modeled marine reservoir ages beyond the Holocene as constant over time. For IntCal20, we incorporate time-varying marine reservoir ages using the LSG model of Butzin et al. (2020 in this issue) and a separate adaptive spline for the Cariaco Basin.2. Blocking in dendrodated trees-with the new radiocarbon revolution providing an increased ability to measure annual tree rings, there has been a concern that the inclusion within IntCal of determinations relating to decadal or bi-decadal averages alongside these new annual measurements could mean we lose critical information on short-term variation in atmospheric radiocarbon levels. Our approach fully recognizes the number of years each determination represents, meaning there is no loss of information as a result of such blocked determinations. 3. Variable marine reservoir ages-IntCal09 (Reimer et al. 2009) and IntCal13 (Reimer et al. 2013) modeled marine reservoir ages beyond the Holocene as constant over time. For IntCal20, we incorporate time-varying marine reservoir ages using the LSG model of Butzin et al. (2020 in this issue) and a separate adaptive spline for the Cariaco Basin.</p>
        <p>4. Inclusion of new floating tree-ring sequences-the new curve includes several new lateglacial trees which have calendar age estimates obtained via wiggle matching (Reinig et al. 2018;Capano et al. 2020 in this issue), as well as three entirely floating tree chronologies around the time of the Bølling-Allerød 1 (Adolphi et al. 2017) and two older Southern Hemisphere floating kauri tree-ring sequences (Turney et al. 2010(Turney et al. , 2017) ) on which we have no absolute dating information and which need to be placed accurately amongst the other data.4. Inclusion of new floating tree-ring sequences-the new curve includes several new lateglacial trees which have calendar age estimates obtained via wiggle matching (Reinig et al. 2018;Capano et al. 2020 in this issue), as well as three entirely floating tree chronologies around the time of the Bølling-Allerød 1 (Adolphi et al. 2017) and two older Southern Hemisphere floating kauri tree-ring sequences (Turney et al. 2010(Turney et al. , 2017) ) on which we have no absolute dating information and which need to be placed accurately amongst the other data.</p>
        <p>5. Rapid 14 C excursions-there are several specific times when the level of atmospheric 14 Cis known to vary extremely rapidly. For IntCal20 we have identified three such events around 774-5 AD, 993-4 AD and 660 BC (Miyake et al. 2012(Miyake et al. , 2013;;O 'Hare et al. 2019). Such rapid 14 C changes will typically not be modeled well by standard regression which will tend to smooth them out. However, by increasing the density of knots forming our spline basis in the vicinity of these rapid excursions we can better represent these significant features.5. Rapid 14 C excursions-there are several specific times when the level of atmospheric 14 Cis known to vary extremely rapidly. For IntCal20 we have identified three such events around 774-5 AD, 993-4 AD and 660 BC (Miyake et al. 2012(Miyake et al. , 2013;;O 'Hare et al. 2019). Such rapid 14 C changes will typically not be modeled well by standard regression which will tend to smooth them out. However, by increasing the density of knots forming our spline basis in the vicinity of these rapid excursions we can better represent these significant features.</p>
        <p>11</p>
        <p>While Adolphi et al. (2017) do provide potential age estimates for their Bølling-Allerød trees obtained by comparison to ice-core 10 Be, to maintain independence between the ice-core and radiocarbon timescales these calendar age estimates are not used for IntCal20.While Adolphi et al. (2017) do provide potential age estimates for their Bølling-Allerød trees obtained by comparison to ice-core 10 Be, to maintain independence between the ice-core and radiocarbon timescales these calendar age estimates are not used for IntCal20.</p>
        <p>The statistical innovations also mean that we can now provide several new facets to our output:The statistical innovations also mean that we can now provide several new facets to our output:</p>
        <p>1. Annualized output-we are able to provide curve estimates on an annual grid enabling more detailed calibration. This will be needed in light of the increased demand to calibrate annual radiocarbon determinations. While we do not discuss the implications for calibration itself in this paper, such a detailed annual calibration curve will likely increase the potential for multimodal calendar age estimates which require significant care in interpretation, especially during periods of plateaus in the calibration curve. See the IntCal20 companion paper (van der Plicht et al. 2020 in this issue) for more details and an illustrative example with the calibration of the Minoan Santorini/Thera eruption.1. Annualized output-we are able to provide curve estimates on an annual grid enabling more detailed calibration. This will be needed in light of the increased demand to calibrate annual radiocarbon determinations. While we do not discuss the implications for calibration itself in this paper, such a detailed annual calibration curve will likely increase the potential for multimodal calendar age estimates which require significant care in interpretation, especially during periods of plateaus in the calibration curve. See the IntCal20 companion paper (van der Plicht et al. 2020 in this issue) for more details and an illustrative example with the calibration of the Minoan Santorini/Thera eruption.</p>
        <p>2. Predictive intervals-if the IntCal20 14 C data contain additional sources of variation beyond their laboratory quantified uncertainties (due to potential regional, species or growing season differences), i.e. they appear more widely spread around the calibration curve than can be explained by their quoted uncertainties, we will need to take this into account for calibration of new determinations too. Specifically, any 14 C determination a user calibrates against the curve is likely to contain similar levels of unseen additional variation. If we can assess the level of additional variation seen within the IntCal20 data, then we can incorporate this into calibration for a user by providing a predictive interval on the curve. Intuitively this predictive interval aims to incorporate both uncertainty in the value of the curve itself and potential additional uncertainty sources such as regional, species or growing season effects beyond the laboratory quoted uncertainty. These predictive intervals are therefore more relevant for calibration than curve intervals which do not incorporate or adapt to potential additional sources of variability.2. Predictive intervals-if the IntCal20 14 C data contain additional sources of variation beyond their laboratory quantified uncertainties (due to potential regional, species or growing season differences), i.e. they appear more widely spread around the calibration curve than can be explained by their quoted uncertainties, we will need to take this into account for calibration of new determinations too. Specifically, any 14 C determination a user calibrates against the curve is likely to contain similar levels of unseen additional variation. If we can assess the level of additional variation seen within the IntCal20 data, then we can incorporate this into calibration for a user by providing a predictive interval on the curve. Intuitively this predictive interval aims to incorporate both uncertainty in the value of the curve itself and potential additional uncertainty sources such as regional, species or growing season effects beyond the laboratory quoted uncertainty. These predictive intervals are therefore more relevant for calibration than curve intervals which do not incorporate or adapt to potential additional sources of variability.</p>
        <p>3. Posterior information arising from curve construction-the Bayesian implementation allows us to provide posterior estimates for many aspects of interest. For example, all of the records with uncertainties in their calendar timescales (the various floating tree-ring sequences, marine sediments, Lake Suigetsu and the speleothems) will be calibrated during the curve's construction. We can provide these posterior calibrated age estimates along with other information such as the level of over-dispersion seen in the data, and posterior estimates of marine reservoir ages and dead carbon fractions. 4. Complete realizations of the curve from 0-55 cal kBP-historically IntCal output has consisted of pointwise posterior means and variances. However, the Bayesian approach provides a set of underlying curve realizations which provide covariance information on the value of the curve at any two calendar ages. When calibrating single determinations, this covariance does not affect the calendar age estimates obtained, i.e. calibrating against the pointwise posterior means and variances provides the same inference as calibrating against the set of individual curve realizations. However, when calibrating multiple determinations simultaneously, for example if we are seeking to determine the length of time elapsed between two determinations or fit a more complex model, use of these complete realizations in consequent calibration offers potential to improve insight. Work is planned by the group to explore how this may be best incorporated into existing calibration software.3. Posterior information arising from curve construction-the Bayesian implementation allows us to provide posterior estimates for many aspects of interest. For example, all of the records with uncertainties in their calendar timescales (the various floating tree-ring sequences, marine sediments, Lake Suigetsu and the speleothems) will be calibrated during the curve's construction. We can provide these posterior calibrated age estimates along with other information such as the level of over-dispersion seen in the data, and posterior estimates of marine reservoir ages and dead carbon fractions. 4. Complete realizations of the curve from 0-55 cal kBP-historically IntCal output has consisted of pointwise posterior means and variances. However, the Bayesian approach provides a set of underlying curve realizations which provide covariance information on the value of the curve at any two calendar ages. When calibrating single determinations, this covariance does not affect the calendar age estimates obtained, i.e. calibrating against the pointwise posterior means and variances provides the same inference as calibrating against the set of individual curve realizations. However, when calibrating multiple determinations simultaneously, for example if we are seeking to determine the length of time elapsed between two determinations or fit a more complex model, use of these complete realizations in consequent calibration offers potential to improve insight. Work is planned by the group to explore how this may be best incorporated into existing calibration software.</p>
        <p>We discuss some of these aspects in more detail, and present some of the output available, in Section 5 and the Supplementary Information.We discuss some of these aspects in more detail, and present some of the output available, in Section 5 and the Supplementary Information.</p>
        <p>For a typical calibration user the key elements of the new methodology consist of the change to Bayesian splines with the accompanying shift in the modeling and fitting domains; the continued recognition that many of the data have uncertainties in their calendar ages and the significant effect that has on curve estimation; and the use of predictive intervals on the published curve for calibration. We therefore commence with an intuitive explanation of these three elements. The technical details can be found later in Section 4. We note that, in our intuitive descriptions, the observational model may be further complicated by 14 C determinations representing multiple, as opposed to single, calendar years and the inclusion of a reservoir age or dead carbon fraction for those observations which are not directly atmospheric. However, for clarity of exposition, we do not consider either of these factors here, and refer to Section 4 for details on how these additions can be incorporated.For a typical calibration user the key elements of the new methodology consist of the change to Bayesian splines with the accompanying shift in the modeling and fitting domains; the continued recognition that many of the data have uncertainties in their calendar ages and the significant effect that has on curve estimation; and the use of predictive intervals on the published curve for calibration. We therefore commence with an intuitive explanation of these three elements. The technical details can be found later in Section 4. We note that, in our intuitive descriptions, the observational model may be further complicated by 14 C determinations representing multiple, as opposed to single, calendar years and the inclusion of a reservoir age or dead carbon fraction for those observations which are not directly atmospheric. However, for clarity of exposition, we do not consider either of these factors here, and refer to Section 4 for details on how these additions can be incorporated.</p>
        <p>3.1 Bayesian Splines and Choice of Modeling Domain3.1 Bayesian Splines and Choice of Modeling Domain</p>
        <p>Suppose that we observe a function f , subject to noise, at a series of times i , i 1; ...; n, Y i f i " i i 1; ...; n;Suppose that we observe a function f , subject to noise, at a series of times i , i 1; ...; n, Y i f i " i i 1; ...; n;</p>
        <p>where, for the time being, we assume that the times i are known absolutely. To obtain a spline estimate for the unknown function we seek to find a function that provides a satisfactory compromise between going close to the data but yet does not overfit. This can be done by choosing the estimate f that minimizes, over a suitable set of functions,where, for the time being, we assume that the times i are known absolutely. To obtain a spline estimate for the unknown function we seek to find a function that provides a satisfactory compromise between going close to the data but yet does not overfit. This can be done by choosing the estimate f that minimizes, over a suitable set of functions,</p>
        <p>where FIT f ; Y measures the lack of agreement between a potential f and the observed Y; and PEN f represents a penalty for functions that might be overfitting. Typically, FIT f ; Y consists of the sum-of-squares difference between f i and Y i , while PEN f assesses the roughness of a proposed f ensuring that the more variable f the larger the penalty given to it with the aim of preventing the spline estimate from overfitting the data. The parameter determines the relative trade-off between how one values fidelity to the observed data, i.e. FIT f ; Y, compared with function roughness, PEN f . A large will heavily penalize roughness and typically results in a smooth curve that is less close to the data; while a small will mean the spline goes closer to the data but at the expense of being more variable.where FIT f ; Y measures the lack of agreement between a potential f and the observed Y; and PEN f represents a penalty for functions that might be overfitting. Typically, FIT f ; Y consists of the sum-of-squares difference between f i and Y i , while PEN f assesses the roughness of a proposed f ensuring that the more variable f the larger the penalty given to it with the aim of preventing the spline estimate from overfitting the data. The parameter determines the relative trade-off between how one values fidelity to the observed data, i.e. FIT f ; Y, compared with function roughness, PEN f . A large will heavily penalize roughness and typically results in a smooth curve that is less close to the data; while a small will mean the spline goes closer to the data but at the expense of being more variable.</p>
        <p>Within the radiocarbon community there are three commonly used domains 2 : Δ 14 C, F 14 C and the radiocarbon age. Given g, the historical level of Δ 14 C in year cal BP, we can freely convert between the domains as follows:Within the radiocarbon community there are three commonly used domains 2 : Δ 14 C, F 14 C and the radiocarbon age. Given g, the historical level of Δ 14 C in year cal BP, we can freely convert between the domains as follows:</p>
        <p>• Radiocarbon age domain: h 8033 ln f : The radiocarbon age, in 14 C yrs BP, is obtained from the fraction modern using Libby's original half-life and without any calibration. Since this is a non-linear mapping of F 14 C, the uncertainties are no longer even approximately normally distributed as we approach the limit of the technique.• Radiocarbon age domain: h 8033 ln f : The radiocarbon age, in 14 C yrs BP, is obtained from the fraction modern using Libby's original half-life and without any calibration. Since this is a non-linear mapping of F 14 C, the uncertainties are no longer even approximately normally distributed as we approach the limit of the technique.</p>
        <p>When fitting a smoothing spline, we have some flexibility in the precise choice of FIT f ; Y, our assessment of fit, and our roughness penalty PEN f . For radiocarbon purposes, the natural domain to assess the quality of fit of a proposed calibration curve to observations is the F 14 C domain, the raw scale on which the determinations are obtained. In this F 14 C domain, our measurement uncertainties are symmetric. Conversely, in the radiocarbon age domain these uncertainties become asymmetric as we progress back in time. We therefore chooseWhen fitting a smoothing spline, we have some flexibility in the precise choice of FIT f ; Y, our assessment of fit, and our roughness penalty PEN f . For radiocarbon purposes, the natural domain to assess the quality of fit of a proposed calibration curve to observations is the F 14 C domain, the raw scale on which the determinations are obtained. In this F 14 C domain, our measurement uncertainties are symmetric. Conversely, in the radiocarbon age domain these uncertainties become asymmetric as we progress back in time. We therefore choose</p>
        <p>where F i are the observed F 14 C values of the determinations and i their associated uncertainties in the F 14 C domain.where F i are the observed F 14 C values of the determinations and i their associated uncertainties in the F 14 C domain.</p>
        <p>F 14 C is not however the most appropriate domain in which to assess roughness since it exhibits exponential decay over time making equitable penalization of potential calibration curves more difficult. Instead a more natural choice is the Δ 14 C domain where, a priori, one might expect a calibration curve to display approximately equal roughness over its length. For our penalty function we therefore chooseF 14 C is not however the most appropriate domain in which to assess roughness since it exhibits exponential decay over time making equitable penalization of potential calibration curves more difficult. Instead a more natural choice is the Δ 14 C domain where, a priori, one might expect a calibration curve to display approximately equal roughness over its length. For our penalty function we therefore choose</p>
        <p>where g 00 is the second derivative of the proposed level of Δ 14 C, a standard penalty for function roughness.where g 00 is the second derivative of the proposed level of Δ 14 C, a standard penalty for function roughness.</p>
        <p>We summarize this by saying that we perform modeling in the ∆ 14 C domain as it is here we penalize roughness; while we perform fitting in the F 14 C domain since it is here we assess fidelity of the curve to the raw observations. Since, given , the transformation from F 14 C to Δ 14 C is affine we can utilize these different modeling and fitting domains while still maintaining a practical spline estimation approach.We summarize this by saying that we perform modeling in the ∆ 14 C domain as it is here we penalize roughness; while we perform fitting in the F 14 C domain since it is here we assess fidelity of the curve to the raw observations. Since, given , the transformation from F 14 C to Δ 14 C is affine we can utilize these different modeling and fitting domains while still maintaining a practical spline estimation approach.</p>
        <p>To reinterpret the above idea in a Bayesian framework we can split our functional S fTo reinterpret the above idea in a Bayesian framework we can split our functional S f</p>
        <p>into two distinct parts. The penalty component R g 00 2 d can be considered as specifying the prior distribution on the space of calibration curves; more precisely, it gives the negative log density of the prior. Intuitively this summarizes our prior beliefs about the form of the calibration curve before we observe any actual data. We then wish to update this prior belief, in light of the observed data, to obtain our posterior. This updating is achieved via the fitting component P n i1into two distinct parts. The penalty component R g 00 2 d can be considered as specifying the prior distribution on the space of calibration curves; more precisely, it gives the negative log density of the prior. Intuitively this summarizes our prior beliefs about the form of the calibration curve before we observe any actual data. We then wish to update this prior belief, in light of the observed data, to obtain our posterior. This updating is achieved via the fitting component P n i1</p>
        <p>2 which represents the negative log-likelihood of the observed data under an assumption that each F i N f i ; 2 i . The value of S f then represents the negative posterior log-density for a potential calibration curve f . 828 T J Heaton et al.2 which represents the negative log-likelihood of the observed data under an assumption that each F i N f i ; 2 i . The value of S f then represents the negative posterior log-density for a potential calibration curve f . 828 T J Heaton et al.</p>
        <p>This idea is illustrated in Figure 1. In panel (a) we present three potential calibration curves in the Δ 14 C domain. Using just our prior, we give each curve an initial weight corresponding to their roughness. The black curve is the least rough and so would be given highest prior weight as a plausible potential calibration curve. The green curve is the roughest and so has least weight according to our prior. Each of these Δ 14 C curves is then converted into the symmetric F 14 C space and compared with our observations as shown in panel (b). The plausibility of each curve is then updated to incorporate the fit to this data. Based upon this, the red curve would have a higher posterior density as it fits relatively closely while not being overly rough. Both the green and black curves would have a low posterior density and so be considered highly implausible as they do not pass near the data. Use of MCMC allows us to generate any number of plausible calibration curves drawn from the posterior that provide a satisfactory trade-off between the roughness prior and fidelity to the observations. Some such realizations are shown in panels (c) and (d) in both the Δ 14 C and F 14 C domains respectively. These realizations are then summarized to produce the final IntCal20 curve.This idea is illustrated in Figure 1. In panel (a) we present three potential calibration curves in the Δ 14 C domain. Using just our prior, we give each curve an initial weight corresponding to their roughness. The black curve is the least rough and so would be given highest prior weight as a plausible potential calibration curve. The green curve is the roughest and so has least weight according to our prior. Each of these Δ 14 C curves is then converted into the symmetric F 14 C space and compared with our observations as shown in panel (b). The plausibility of each curve is then updated to incorporate the fit to this data. Based upon this, the red curve would have a higher posterior density as it fits relatively closely while not being overly rough. Both the green and black curves would have a low posterior density and so be considered highly implausible as they do not pass near the data. Use of MCMC allows us to generate any number of plausible calibration curves drawn from the posterior that provide a satisfactory trade-off between the roughness prior and fidelity to the observations. Some such realizations are shown in panels (c) and (d) in both the Δ 14 C and F 14 C domains respectively. These realizations are then summarized to produce the final IntCal20 curve.</p>
        <p>A further choice to be made when spline smoothing is the set of functions (or potential calibration curves) to search over for our functional S f . This set is called a basis. We use cubic splines where this basis is determined by specifying what are known as knots at distinct calendar times. The more knots one has in a particular time period, the more the IntCal20 Approach to 14 C Calibration Curve Construction 829 spline can vary. One common option, known as a smoothing spline, is to place a knot at every calendar age i for i 1; ...; n. However, since we have a very large number of observations n this is computationally impractical. Instead we use P-splines (see Berry et al. 2002, for details) where we select a smaller number of knots; this is equivalent to restricting the potential calibration curves to a somewhat smaller subspace of functions.A further choice to be made when spline smoothing is the set of functions (or potential calibration curves) to search over for our functional S f . This set is called a basis. We use cubic splines where this basis is determined by specifying what are known as knots at distinct calendar times. The more knots one has in a particular time period, the more the IntCal20 Approach to 14 C Calibration Curve Construction 829 spline can vary. One common option, known as a smoothing spline, is to place a knot at every calendar age i for i 1; ...; n. However, since we have a very large number of observations n this is computationally impractical. Instead we use P-splines (see Berry et al. 2002, for details) where we select a smaller number of knots; this is equivalent to restricting the potential calibration curves to a somewhat smaller subspace of functions.</p>
        <p>In implementing P-splines we need to make sure that the curves we consider are still able to provide sufficient detail for a calibration user and identify fine scale features such as solar cycles where we have the ability to do so. The data on which we base our curve have highly variable density. In some periods, such as recent dendrodated trees, we have a great density of determinations while in others the underlying data is more sparse. To adapt to this and keep required detail, we choose a large number of knots and place them at calendar age quantiles of the data to provide variable smoothing. This approach of locating the knots at observed quantiles is standard in the regression literature (e.g. Harrell 2001). Where our data is dense, we can pick out the required fine detail but where the data is more sparse we smooth more significantly. An example can be seen in Figure 2. This figure also highlights how we pack additional knots around known Miyake-type events, narrow spikes Figure 2 Variable smoothing and knot selection. Shown as a rug of tick marks along the bottom are the locations of the knots for the cubic spline. These are placed at quantiles of the observed calendar ages to provide variable smoothing. In dense regions, we can identify more detail in the calibration curve; while where the underlying data is less dense we perform more smoothing. In particular, note the additional knots placed around the two Miyake-type events (i.e. 957 and 1176 cal BP) which allow the curve to vary much more rapidly at these times. The points from different datasets within the IntCal database are shown in different colors to distinguish them.In implementing P-splines we need to make sure that the curves we consider are still able to provide sufficient detail for a calibration user and identify fine scale features such as solar cycles where we have the ability to do so. The data on which we base our curve have highly variable density. In some periods, such as recent dendrodated trees, we have a great density of determinations while in others the underlying data is more sparse. To adapt to this and keep required detail, we choose a large number of knots and place them at calendar age quantiles of the data to provide variable smoothing. This approach of locating the knots at observed quantiles is standard in the regression literature (e.g. Harrell 2001). Where our data is dense, we can pick out the required fine detail but where the data is more sparse we smooth more significantly. An example can be seen in Figure 2. This figure also highlights how we pack additional knots around known Miyake-type events, narrow spikes Figure 2 Variable smoothing and knot selection. Shown as a rug of tick marks along the bottom are the locations of the knots for the cubic spline. These are placed at quantiles of the observed calendar ages to provide variable smoothing. In dense regions, we can identify more detail in the calibration curve; while where the underlying data is less dense we perform more smoothing. In particular, note the additional knots placed around the two Miyake-type events (i.e. 957 and 1176 cal BP) which allow the curve to vary much more rapidly at these times. The points from different datasets within the IntCal database are shown in different colors to distinguish them.</p>
        <p>(sub-annual) of increased 14 C production, to enable us to better retain these events in the final curve. For more details on choice of knots and placement see Sections 4.3.4 and 4.4.3.(sub-annual) of increased 14 C production, to enable us to better retain these events in the final curve. For more details on choice of knots and placement see Sections 4.3.4 and 4.4.3.</p>
        <p>Within the IntCal database (http://www.intcal.org/), many of the determinations have calendar ages which are not known absolutely. This is particularly the case as we progress further back in time and calendar age estimates are constructed from uranium thorium (U-Th) dating, e.g. speleothems (Southon et al. 2012;Cheng et al. 2018;B e c ke ta l .2001;H o f f m a n ne ta l .2010) and corals (Bard et al. 1990(Bard et al. , 1998(Bard et al. , 2004 1998,2004); varve counting, e.g. parts of Cariaco Basin (Hughen et al. 2004) and Lake Suigetsu (Bronk Ramsey et al. 2020 in this issue); and palaeoclimate tuning/tie-pointing, e.g. other parts of Cariaco Basin (Hughen and Heaton 2020 in this issue), and the Pakistan and Iberian Margins (Bard et al. 2013). Furthermore, in the case of several of the floating tree-ring sequences, we have only a relative set of calendar ages and no absolute age estimate. Regression in this situation is called errors-in-variables since we have errors/uncertainties in both the calendar age and radiocarbon determination variables. For these observations we therefore observe pairs F i ; T i fg i2I where:Within the IntCal database (http://www.intcal.org/), many of the determinations have calendar ages which are not known absolutely. This is particularly the case as we progress further back in time and calendar age estimates are constructed from uranium thorium (U-Th) dating, e.g. speleothems (Southon et al. 2012;Cheng et al. 2018;B e c ke ta l .2001;H o f f m a n ne ta l .2010) and corals (Bard et al. 1990(Bard et al. , 1998(Bard et al. , 2004 1998,2004); varve counting, e.g. parts of Cariaco Basin (Hughen et al. 2004) and Lake Suigetsu (Bronk Ramsey et al. 2020 in this issue); and palaeoclimate tuning/tie-pointing, e.g. other parts of Cariaco Basin (Hughen and Heaton 2020 in this issue), and the Pakistan and Iberian Margins (Bard et al. 2013). Furthermore, in the case of several of the floating tree-ring sequences, we have only a relative set of calendar ages and no absolute age estimate. Regression in this situation is called errors-in-variables since we have errors/uncertainties in both the calendar age and radiocarbon determination variables. For these observations we therefore observe pairs F i ; T i fg i2I where:</p>
        <p>Here f is our calibration curve of interest in the F 14 C domain; " i N0; 2 i independently; and i describes the uncertainty in our calendar age estimate. The form of this calendar age uncertainty varies between the datasets. While there is no restriction on the distribution of i for our Bayesian spline approach, we model all these calendar age uncertainties as normally distributed but with appropriate covariances. For some sets they are considered independent, e.g. corals dated by U-Th; while for others, e.g. floating tree-ring sequences and those records dated by varve counting or palaeoclimate tie-pointing (Heaton et al. 2013), there is considerable dependence between the observations. For more detail on the various covariance structures in the calendar age uncertainties see Niu et al. (2013).Here f is our calibration curve of interest in the F 14 C domain; " i N0; 2 i independently; and i describes the uncertainty in our calendar age estimate. The form of this calendar age uncertainty varies between the datasets. While there is no restriction on the distribution of i for our Bayesian spline approach, we model all these calendar age uncertainties as normally distributed but with appropriate covariances. For some sets they are considered independent, e.g. corals dated by U-Th; while for others, e.g. floating tree-ring sequences and those records dated by varve counting or palaeoclimate tie-pointing (Heaton et al. 2013), there is considerable dependence between the observations. For more detail on the various covariance structures in the calendar age uncertainties see Niu et al. (2013).</p>
        <p>Incorporation of calendar age uncertainty in the construction of the IntCal calibration curves has occurred since IntCal04 (Reimer et al. 2004a) and is key for reliable curve construction. It was therefore crucial to retain within the new methodology. A range of statistical methods have been developed to deal with errors-in-variables regression. From the frequentist, non-Bayesian, perspective Fan and Truong (1993) introduced a kernel-based approach which achieves global consistency; Cook and Stefanski (1994) proposed a more general approach known as SIMEX. These approaches were not however considered suitable for our application due to the genuine prior information we have on certain aspects of the data; the wider, almost universal, use of Bayesian methodology within radiocarbon calibration; and the independent interest in several aspects of the calibration data which could be provided more simply through a Bayesian method. We therefore began the development of our method using the Bayesian splines of Berry et al. (2002).Incorporation of calendar age uncertainty in the construction of the IntCal calibration curves has occurred since IntCal04 (Reimer et al. 2004a) and is key for reliable curve construction. It was therefore crucial to retain within the new methodology. A range of statistical methods have been developed to deal with errors-in-variables regression. From the frequentist, non-Bayesian, perspective Fan and Truong (1993) introduced a kernel-based approach which achieves global consistency; Cook and Stefanski (1994) proposed a more general approach known as SIMEX. These approaches were not however considered suitable for our application due to the genuine prior information we have on certain aspects of the data; the wider, almost universal, use of Bayesian methodology within radiocarbon calibration; and the independent interest in several aspects of the calibration data which could be provided more simply through a Bayesian method. We therefore began the development of our method using the Bayesian splines of Berry et al. (2002).</p>
        <p>The effect of not recognizing calendar age uncertainty, where it is present, in regression is quite varied. In the case that the calendar age uncertainties are entirely independent of one another, IntCal20 Approach to 14 C Calibration Curve Construction 831 such as U-Th dating, not recognizing calendar age uncertainty will typically result in curve estimates that are overly smooth and do not attain the peaks and troughs of the true underlying function. See Samworth and Poore (2005) for an illustrative case study. However, in the case of IntCal, the situation is made more complicated by the shared dependence of the calendar age estimates within particular datasets. Specifically, the entire timescale for, e.g. Hulu Cave, is not the same as the Suigetsu or Cariaco timescale. As a consequence, all of these individual datasets may show the same overall features but at slightly different times. We require our method to recognize that these different timescales may need to be aligned, within their respective uncertainties, to keep these shared features. This requires us to shift multiple ages jointly by stretching/squashing the timescales accordingly. A failure to adapt to this joint calendar age uncertainty can give curve estimates which either introduce spurious wiggles as the curve flips between the different datasets or lose major features entirely.The effect of not recognizing calendar age uncertainty, where it is present, in regression is quite varied. In the case that the calendar age uncertainties are entirely independent of one another, IntCal20 Approach to 14 C Calibration Curve Construction 831 such as U-Th dating, not recognizing calendar age uncertainty will typically result in curve estimates that are overly smooth and do not attain the peaks and troughs of the true underlying function. See Samworth and Poore (2005) for an illustrative case study. However, in the case of IntCal, the situation is made more complicated by the shared dependence of the calendar age estimates within particular datasets. Specifically, the entire timescale for, e.g. Hulu Cave, is not the same as the Suigetsu or Cariaco timescale. As a consequence, all of these individual datasets may show the same overall features but at slightly different times. We require our method to recognize that these different timescales may need to be aligned, within their respective uncertainties, to keep these shared features. This requires us to shift multiple ages jointly by stretching/squashing the timescales accordingly. A failure to adapt to this joint calendar age uncertainty can give curve estimates which either introduce spurious wiggles as the curve flips between the different datasets or lose major features entirely.</p>
        <p>We provide in Figure 3 an illustrative example of the need to incorporate calendar age uncertainty. For simplicity, in this example, we fit and model our spline in the same Figure 3 The importance of recognizing calendar age uncertainty (and correctly representing covariance within that uncertainty) when constructing a curve based on data arising from records with different observed timescales. Panel (a) shows the true, underlying, calendar ages of the data in two different records; while panel (b) shows a joint shift in the observed calendar ages within record 2. In such a case that observed timescales in two records are offset from one another, if we ignore this calendar age uncertainty then our spline estimate will introduce spurious variation as it flips between the records as in panel (c). Conversely if we incorporate such calendar age uncertainty and accurately represent it then we can still recover the underlying function accurately, as shown in (d).We provide in Figure 3 an illustrative example of the need to incorporate calendar age uncertainty. For simplicity, in this example, we fit and model our spline in the same Figure 3 The importance of recognizing calendar age uncertainty (and correctly representing covariance within that uncertainty) when constructing a curve based on data arising from records with different observed timescales. Panel (a) shows the true, underlying, calendar ages of the data in two different records; while panel (b) shows a joint shift in the observed calendar ages within record 2. In such a case that observed timescales in two records are offset from one another, if we ignore this calendar age uncertainty then our spline estimate will introduce spurious variation as it flips between the records as in panel (c). Conversely if we incorporate such calendar age uncertainty and accurately represent it then we can still recover the underlying function accurately, as shown in (d).</p>
        <p>domain. We consider a straightforward underlying function which we wish to reconstruct from 100 noisy observations Y i ; T i fg 100 i1 f e sin4cos2;domain. We consider a straightforward underlying function which we wish to reconstruct from 100 noisy observations Y i ; T i fg 100 i1 f e sin4cos2;</p>
        <p>where our underlying i Beta1:1; 1:1 and observed Y i N f i ; 0:05 2 for i 1; ...; 100.where our underlying i Beta1:1; 1:1 and observed Y i N f i ; 0:05 2 for i 1; ...; 100.</p>
        <p>Further, let us assume that these observations arise from two different sediment cores, 50 from core 1 (shown as black triangles) and 50 from core 2 (shown as red dots) as seen in panel (a). Within core 1, the calendar ages are known absolutely. However, within core 2 the observed timescale is somewhat shifted/biased so that when we observe the calendar ages T in this core they all share the same joint shift from their true values, i.e.Further, let us assume that these observations arise from two different sediment cores, 50 from core 1 (shown as black triangles) and 50 from core 2 (shown as red dots) as seen in panel (a). Within core 1, the calendar ages are known absolutely. However, within core 2 the observed timescale is somewhat shifted/biased so that when we observe the calendar ages T in this core they all share the same joint shift from their true values, i.e.</p>
        <p>where N0; 0:1 2 . The observed pairs Y i ; T i fg 100 i1 are presented in panel (b) showing the shift in timescale within core 2. If we attempt to reconstruct the function using splines based upon our observed Y i ; T i fg 100 i1 without recognizing the calendar age uncertainty, i.e. assuming both cores are on the same timescale, then we obtain the estimate in the panel (c). The estimate is spuriously variable as the spline will try to pass near all of the data on their non-comparable timescales. Conversely, if we recognize that the timescale in core 2 may need to be shifted onto the timescale of core 1, we obtain the estimate shown in panel (d). If we inform our Bayesian method that the calendar age observations in core 2 are subject to uncertainty, it will estimate the size of the joint shift needed to register core 2 onto the true timescale of core 1 and simultaneously reconstruct the true underlying function.where N0; 0:1 2 . The observed pairs Y i ; T i fg 100 i1 are presented in panel (b) showing the shift in timescale within core 2. If we attempt to reconstruct the function using splines based upon our observed Y i ; T i fg 100 i1 without recognizing the calendar age uncertainty, i.e. assuming both cores are on the same timescale, then we obtain the estimate in the panel (c). The estimate is spuriously variable as the spline will try to pass near all of the data on their non-comparable timescales. Conversely, if we recognize that the timescale in core 2 may need to be shifted onto the timescale of core 1, we obtain the estimate shown in panel (d). If we inform our Bayesian method that the calendar age observations in core 2 are subject to uncertainty, it will estimate the size of the joint shift needed to register core 2 onto the true timescale of core 1 and simultaneously reconstruct the true underlying function.</p>
        <p>While it is important to incorporate errors-in-variables in curve construction, and accurately represent any dependence in the calendar age uncertainties, this does cause some difficulty in visually assessing the quality of fit between the raw constituent data and the final IntCal curve. Since the proposed method can shift the calendar ages of the observed data left and right within their uncertainties as described, one cannot estimate the final curve's goodness-of-fit by eye using only the radiocarbon age axis if the raw data are only plotted at their initial, observed calendar ages. The method will have tried to align shared features even if they occur at somewhat different observed times within the different sets. This should be taken into consideration when viewing the final curve against the raw data. We provide the posterior calendar age estimates for the true calendar ages i for all the data. The difficulty of assessing the fit of the curve by eye is further compounded by the offsets in 14 C described in Section 4.4, in particular the marine reservoir ages which vary significantly over time and so will change as the MCMC updates the calendar ages of the data.While it is important to incorporate errors-in-variables in curve construction, and accurately represent any dependence in the calendar age uncertainties, this does cause some difficulty in visually assessing the quality of fit between the raw constituent data and the final IntCal curve. Since the proposed method can shift the calendar ages of the observed data left and right within their uncertainties as described, one cannot estimate the final curve's goodness-of-fit by eye using only the radiocarbon age axis if the raw data are only plotted at their initial, observed calendar ages. The method will have tried to align shared features even if they occur at somewhat different observed times within the different sets. This should be taken into consideration when viewing the final curve against the raw data. We provide the posterior calendar age estimates for the true calendar ages i for all the data. The difficulty of assessing the fit of the curve by eye is further compounded by the offsets in 14 C described in Section 4.4, in particular the marine reservoir ages which vary significantly over time and so will change as the MCMC updates the calendar ages of the data.</p>
        <p>As well as creating a curve which has the correct posterior mean, it is key to make sure we provide appropriate intervals on the curve to ensure that, when used for calibration, the calendar age estimates produced are not over-or under-precise. This becomes particularly relevant for IntCal20 due to the large increase in data available to create the curve, and also the increased measurement precision provided by current laboratories. In addition to the laboratory reported uncertainty, there are a wide range of possible further sources of variation in the recorded 14 C within objects of the same age, for example determinations come from different IntCal20 Approach to 14 C Calibration Curve Construction 833 locations; have different local environments; tree rings may be of different species; and have different periods of growth due to local weather and so may have differing elements of wood from late/early growth. Even when the same sample is measured in different laboratories, we have evidence of a greater level of observation spread (i.e. over-dispersion) in the 14 C measurements within objects from the same calendar year than the laboratory reported uncertainties would support (see e.g. Scott et al. 2017). Recognizing this potential overdispersion, whatever its cause, is important both for curve estimation and resultant calibration for IntCal20. Specifically, if there is more variation in determinations from the same calendar year than the uncertainties reported by the laboratory, we need to make sure we incorporate that in our modeling, and also recognize its existence in the objects users will then calibrate against the IntCal20 curve.As well as creating a curve which has the correct posterior mean, it is key to make sure we provide appropriate intervals on the curve to ensure that, when used for calibration, the calendar age estimates produced are not over-or under-precise. This becomes particularly relevant for IntCal20 due to the large increase in data available to create the curve, and also the increased measurement precision provided by current laboratories. In addition to the laboratory reported uncertainty, there are a wide range of possible further sources of variation in the recorded 14 C within objects of the same age, for example determinations come from different IntCal20 Approach to 14 C Calibration Curve Construction 833 locations; have different local environments; tree rings may be of different species; and have different periods of growth due to local weather and so may have differing elements of wood from late/early growth. Even when the same sample is measured in different laboratories, we have evidence of a greater level of observation spread (i.e. over-dispersion) in the 14 C measurements within objects from the same calendar year than the laboratory reported uncertainties would support (see e.g. Scott et al. 2017). Recognizing this potential overdispersion, whatever its cause, is important both for curve estimation and resultant calibration for IntCal20. Specifically, if there is more variation in determinations from the same calendar year than the uncertainties reported by the laboratory, we need to make sure we incorporate that in our modeling, and also recognize its existence in the objects users will then calibrate against the IntCal20 curve.</p>
        <p>We achieve this through the inclusion of a term to quantify the level of over-dispersion which we adaptively estimate, based on the high-quality and dense IntCal data, within our curve construction. This ensures that our IntCal20 curve does not become over-precise as an estimate of the Northern Hemispheric average. However, this alone is not sufficient for calibration since any additional sources of variation seen within the IntCal data are also likely to be present in uncalibrated determinations. We must therefore propagate this overdispersion through to the calibration process itself. This is achieved using predictive curve intervals which incorporate not only uncertainty in the Northern Hemispheric average atmospheric curve but also the potential additional sources of variation seen in individual 14 C determinations. Importantly, if no such additional variability exists then our model will estimate this appropriately, however if such additional sources of variability do seem present in the calibration data, our method will also recognize this and adapt accordingly. Without such an over-dispersion term then, as we get more data, the calibration curve will have intervals that become narrower and narrower even if the underlying data suggests considerably higher variability. Eventually, this would mean that the data entering the curve itself would potentially not calibrate to their known ages.We achieve this through the inclusion of a term to quantify the level of over-dispersion which we adaptively estimate, based on the high-quality and dense IntCal data, within our curve construction. This ensures that our IntCal20 curve does not become over-precise as an estimate of the Northern Hemispheric average. However, this alone is not sufficient for calibration since any additional sources of variation seen within the IntCal data are also likely to be present in uncalibrated determinations. We must therefore propagate this overdispersion through to the calibration process itself. This is achieved using predictive curve intervals which incorporate not only uncertainty in the Northern Hemispheric average atmospheric curve but also the potential additional sources of variation seen in individual 14 C determinations. Importantly, if no such additional variability exists then our model will estimate this appropriately, however if such additional sources of variability do seem present in the calibration data, our method will also recognize this and adapt accordingly. Without such an over-dispersion term then, as we get more data, the calibration curve will have intervals that become narrower and narrower even if the underlying data suggests considerably higher variability. Eventually, this would mean that the data entering the curve itself would potentially not calibrate to their known ages.</p>
        <p>Such an idea of irreducible uncertainty was introduced in Niu et al. ( 2013) but here we extend the idea further and generate predictive curve intervals which are more relevant for calibration users. Importantly, this does not however mean that we produce a calibration tool that any measurement (no matter its quality) can be calibrated against-we wish to maintain a calibration curve which has basic minima for data quality for both what goes into its construction and also what can be reliably calibrated using it.Such an idea of irreducible uncertainty was introduced in Niu et al. ( 2013) but here we extend the idea further and generate predictive curve intervals which are more relevant for calibration users. Importantly, this does not however mean that we produce a calibration tool that any measurement (no matter its quality) can be calibrated against-we wish to maintain a calibration curve which has basic minima for data quality for both what goes into its construction and also what can be reliably calibrated using it.</p>
        <p>The basic tree-ring model for an annual measurement assumes that a determination of calendar age arises from a hemispherically uniform atmospheric level of 14 C shared by all determinations of the same calendar age. Under this model, the only uncertainty is that reported by the laboratory so that, in the F 14 C domain, the observed value isThe basic tree-ring model for an annual measurement assumes that a determination of calendar age arises from a hemispherically uniform atmospheric level of 14 C shared by all determinations of the same calendar age. Under this model, the only uncertainty is that reported by the laboratory so that, in the F 14 C domain, the observed value is</p>
        <p>where i is the year the determination represents; and " i N0; 2 i with i the uncertainty reported by the laboratory. The function f is our unique estimate of the hemispheric level of F 14 C present in the atmosphere at calendar age .where i is the year the determination represents; and " i N0; 2 i with i the uncertainty reported by the laboratory. The function f is our unique estimate of the hemispheric level of F 14 C present in the atmosphere at calendar age .</p>
        <p>Currently it is not feasible to identify the specific sources that may potentially contribute extra variability beyond that reported by the laboratory (e.g. regional, species, growing season 834 T J Heaton et al.Currently it is not feasible to identify the specific sources that may potentially contribute extra variability beyond that reported by the laboratory (e.g. regional, species, growing season 834 T J Heaton et al.</p>
        <p>https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press differences) and so we aim to cover them all in a unified approach. We therefore modify the above model to allow any determination to have an additional and independent source of potential variability i beyond that reported by the laboratory, i.e. F i f i " i i for i 1; ...; n where i N0; 2 i are independent random effects with unknown variance 2 i . To create the curve we simultaneously estimate both i , the level of over-dispersion, and the calibration curve f . After investigation of several options, see Section 5.1 and the Supplementary Information, we model the level of over-dispersion (i.e. the level of any additional variability) to scale proportionally with the underlying value of f p i.e. i f i p so that i N0; 2 f . We choose a prior for the constant of proportionality based upon data taken from the SIRI inter-comparison project (Scott et al. 2017), and update this prior within curve construction based upon the IntCal data. The information provided by the very large volume of IntCal data dominates the SIRI prior and so the posterior estimate for the level of over-dispersion is primarily based upon the high-quality IntCal data.https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press differences) and so we aim to cover them all in a unified approach. We therefore modify the above model to allow any determination to have an additional and independent source of potential variability i beyond that reported by the laboratory, i.e. F i f i " i i for i 1; ...; n where i N0; 2 i are independent random effects with unknown variance 2 i . To create the curve we simultaneously estimate both i , the level of over-dispersion, and the calibration curve f . After investigation of several options, see Section 5.1 and the Supplementary Information, we model the level of over-dispersion (i.e. the level of any additional variability) to scale proportionally with the underlying value of f p i.e. i f i p so that i N0; 2 f . We choose a prior for the constant of proportionality based upon data taken from the SIRI inter-comparison project (Scott et al. 2017), and update this prior within curve construction based upon the IntCal data. The information provided by the very large volume of IntCal data dominates the SIRI prior and so the posterior estimate for the level of over-dispersion is primarily based upon the high-quality IntCal data.</p>
        <p>A user seeking to calibrate a high-quality 14 C determination against the IntCal20 curve is likely to have a measurement that has been subject to similar additional sources of potential variation to whatever is seen in the IntCal data, and so have a similar level of over-dispersion. However, such a user typically has no way of assessing this over-dispersion themselves. Consequently, they should calibrate their determination using predictive curve estimates which additionally account for potential over-dispersion in their own determi n a t i o n .T h e s ep r e d i c t i v ee s t i m a t e sa r eb a s e d upon the posterior of f ;A user seeking to calibrate a high-quality 14 C determination against the IntCal20 curve is likely to have a measurement that has been subject to similar additional sources of potential variation to whatever is seen in the IntCal data, and so have a similar level of over-dispersion. However, such a user typically has no way of assessing this over-dispersion themselves. Consequently, they should calibrate their determination using predictive curve estimates which additionally account for potential over-dispersion in their own determi n a t i o n .T h e s ep r e d i c t i v ee s t i m a t e sa r eb a s e d upon the posterior of f ;</p>
        <p>where N0; 2 f and is the posterior estimate of over-dispersion based upon the IntCal data. For IntCal20 we therefore report this predictive interval since it is more relevant for calibration. It is slightly wider than the corresponding credible interval for f but more likely to give accurate calibrated dates.where N0; 2 f and is the posterior estimate of over-dispersion based upon the IntCal data. For IntCal20 we therefore report this predictive interval since it is more relevant for calibration. It is slightly wider than the corresponding credible interval for f but more likely to give accurate calibrated dates.</p>
        <p>Note that the level of over-dispersion, and hence the predictive intervals, we incorporate into the IntCal20 curve is based upon the high-quality, and screened, IntCal database. If a new uncalibrated 14 C determination has additional sources of variability beyond those present in the IntCal tree-ring database (e.g. tree-ring species or locations not represented in IntCal, or a non-tree-ring sample) then this may mean that the level of over-dispersion for that uncalibrated determination is higher than that incorporated within the IntCal prediction intervals. This would result in a potentially over-precise calibrated age estimate. Users are advised to be cautious in such circumstances.Note that the level of over-dispersion, and hence the predictive intervals, we incorporate into the IntCal20 curve is based upon the high-quality, and screened, IntCal database. If a new uncalibrated 14 C determination has additional sources of variability beyond those present in the IntCal tree-ring database (e.g. tree-ring species or locations not represented in IntCal, or a non-tree-ring sample) then this may mean that the level of over-dispersion for that uncalibrated determination is higher than that incorporated within the IntCal prediction intervals. This would result in a potentially over-precise calibrated age estimate. Users are advised to be cautious in such circumstances.</p>
        <p>As in previous versions of IntCal, the IntCal20 curve itself is created in two linked sections. Firstly we create the more recent part of the curve (extending from 0 cal kBP back to approximately 14 cal kBP) which is predominantly based upon dendrodated tree-ring determinations. Secondly we create the older part of the curve (from approximately 14 cal kBP back to 55 cal kBP) which is based upon a wider range of material, e.g. corals, macrofossils, forams, speleothems as well as five floating tree-ring chronologies, that are of IntCal20 Approach to 14 C Calibration Curve Construction 835 uncertain calendar ages. Furthermore, these older 14 C determinations are often not direct atmospheric measurements and hence have marine reservoir ages or dead carbon fractions. We therefore split our technical description of the approach accordingly. The two curve sections are stitched together to ensure smoothness and continuity through appropriate design of spline basis and conditioning the older part of the curve on the, already estimated, more recent section. This means that we can still produce sets of complete curve realizations from 0-55 cal kBP. Future work will aim to adapt the approach into a single step, updating the entire 0-55 cal kBP range as one.As in previous versions of IntCal, the IntCal20 curve itself is created in two linked sections. Firstly we create the more recent part of the curve (extending from 0 cal kBP back to approximately 14 cal kBP) which is predominantly based upon dendrodated tree-ring determinations. Secondly we create the older part of the curve (from approximately 14 cal kBP back to 55 cal kBP) which is based upon a wider range of material, e.g. corals, macrofossils, forams, speleothems as well as five floating tree-ring chronologies, that are of IntCal20 Approach to 14 C Calibration Curve Construction 835 uncertain calendar ages. Furthermore, these older 14 C determinations are often not direct atmospheric measurements and hence have marine reservoir ages or dead carbon fractions. We therefore split our technical description of the approach accordingly. The two curve sections are stitched together to ensure smoothness and continuity through appropriate design of spline basis and conditioning the older part of the curve on the, already estimated, more recent section. This means that we can still produce sets of complete curve realizations from 0-55 cal kBP. Future work will aim to adapt the approach into a single step, updating the entire 0-55 cal kBP range as one.</p>
        <p>We can represent the atmospheric history of 14 C, as a function of calendar age , in three domains equivalently: Δ 14 C, F 14 C, and radiocarbon age. For any proposed history, i.e. calibration curve, we switch between these domains as appropriate within our statistical methodology. Let us denote:We can represent the atmospheric history of 14 C, as a function of calendar age , in three domains equivalently: Δ 14 C, F 14 C, and radiocarbon age. For any proposed history, i.e. calibration curve, we switch between these domains as appropriate within our statistical methodology. Let us denote:</p>
        <p>• g-the 14 C calibration curve represented in ∆ 14 C space, i.e. the level of Δ 14 Cat cal BP.• g-the 14 C calibration curve represented in ∆ 14 C space, i.e. the level of Δ 14 Cat cal BP.</p>
        <p>We model g in our spline basis and penalize roughness of the curve in this domain.We model g in our spline basis and penalize roughness of the curve in this domain.</p>
        <p>• f -the 14 C calibration curve represented in F 14 C space. Given g then• f -the 14 C calibration curve represented in F 14 C space. Given g then</p>
        <p>It is this F 14 C domain that is used to assess goodness-of-fit to the observed data. • h-the 14 C calibration curve represented in radiocarbon age: h 8033 ln f : This is the domain in which the calibration curve is plotted in the main IntCal20 paper (Reimer et al. 2020 in this issue).It is this F 14 C domain that is used to assess goodness-of-fit to the observed data. • h-the 14 C calibration curve represented in radiocarbon age: h 8033 ln f : This is the domain in which the calibration curve is plotted in the main IntCal20 paper (Reimer et al. 2020 in this issue).</p>
        <p>We consider all of our observations in F 14 C since, as explained in Section 3.1, in this domain our 14 C measurement uncertainties are symmetric. We define:We consider all of our observations in F 14 C since, as explained in Section 3.1, in this domain our 14 C measurement uncertainties are symmetric. We define:</p>
        <p>• F i -the observed F 14 C value of the data used to create the IntCal20 curve.• F i -the observed F 14 C value of the data used to create the IntCal20 curve.</p>
        <p>• i -the laboratory-reported uncertainty on the observed F 14 C.• i -the laboratory-reported uncertainty on the observed F 14 C.</p>
        <p>• m i -the number of annual years of metabolization that a determination represents. The determination is considered to represent the mean of this block.• m i -the number of annual years of metabolization that a determination represents. The determination is considered to represent the mean of this block.</p>
        <p>• T i -the observed calendar age of a determination, which could either be the true calendar age (in the case of absolutely dendrodated determinations) or an estimate with uncertainty (in the case of e.g. corals, varves, speleothems).• T i -the observed calendar age of a determination, which could either be the true calendar age (in the case of absolutely dendrodated determinations) or an estimate with uncertainty (in the case of e.g. corals, varves, speleothems).</p>
        <p>Within the model we update:Within the model we update:</p>
        <p>• j -the spline coefficients which describe the calibration curve.• j -the spline coefficients which describe the calibration curve.</p>
        <p>• -the smoothing parameter for the spline estimate.• -the smoothing parameter for the spline estimate.</p>
        <p>• i -the true calendar age of a determination or, in the case of a block-average determination, the most recent year of metabolization included in the block.• i -the true calendar age of a determination or, in the case of a block-average determination, the most recent year of metabolization included in the block.</p>
        <p>• -the level of over-dispersion in the observed F 14 C under a model whereby the potential additional variability for determination F i is i N0; 2 f i .• -the level of over-dispersion in the observed F 14 C under a model whereby the potential additional variability for determination F i is i N0; 2 f i .</p>
        <p>• r K -the offset measured in terms of radiocarbon age, either due to dead carbon fraction (dcf) or marine reservoir age (MRA), between a determination from set K and the atmosphere at time cal BP. These will be specified either by K , the mean dcf offset/coastal MRA shift for a particular dataset; or further spline coefficients C i nt h ec a s eo ft h eC a r i a c ou n v a r v e d record. These are only needed when estimating the older part of the curve.• r K -the offset measured in terms of radiocarbon age, either due to dead carbon fraction (dcf) or marine reservoir age (MRA), between a determination from set K and the atmosphere at time cal BP. These will be specified either by K , the mean dcf offset/coastal MRA shift for a particular dataset; or further spline coefficients C i nt h ec a s eo ft h eC a r i a c ou n v a r v e d record. These are only needed when estimating the older part of the curve.</p>
        <p>• i and % K -adaptive error multipliers for data arising from the older time period to enable heavier tailed errors and the down-weighting of outliers. Also only included when estimating the older part of the curve.• i and % K -adaptive error multipliers for data arising from the older time period to enable heavier tailed errors and the down-weighting of outliers. Also only included when estimating the older part of the curve.</p>
        <p>As described in Section 3.1, we fit our curve to the data in the F 14 C domain while the curve modeling is performed in the Δ 14 C domain. Specifically, for a single-year determination of the direct atmosphere with no offset and no over-dispersion, we observe pairs F i ; T i n i1 where F i f i " i ; g i 1000 1 e =8267 " i ;As described in Section 3.1, we fit our curve to the data in the F 14 C domain while the curve modeling is performed in the Δ 14 C domain. Specifically, for a single-year determination of the direct atmosphere with no offset and no over-dispersion, we observe pairs F i ; T i n i1 where F i f i " i ; g i 1000 1 e =8267 " i ;</p>
        <p>Here g, the value of Δ 14 C over time, is modeled asHere g, the value of Δ 14 C over time, is modeled as</p>
        <p>where BB 1 ; ...; B K T and the B j are cubic B-splines (Green and Silverman 1993) at a chosen fixed set of knots. To maintain computational feasibility, we consider K n so that the number of splines in our basis is considerably smaller than the total number of observations in the IntCal database. Knot number and placement is discussed in Sections 4.3.4 and 4.4.3.where BB 1 ; ...; B K T and the B j are cubic B-splines (Green and Silverman 1993) at a chosen fixed set of knots. To maintain computational feasibility, we consider K n so that the number of splines in our basis is considerably smaller than the total number of observations in the IntCal database. Knot number and placement is discussed in Sections 4.3.4 and 4.4.3.</p>
        <p>The Bayesian spline approach, equivalent to penalizing roughness in Δ 14 Cby R b a g 00 2 d, is obtained by placing a "partially improper" Gaussian process on the spline coefficients β:The Bayesian spline approach, equivalent to penalizing roughness in Δ 14 Cby R b a g 00 2 d, is obtained by placing a "partially improper" Gaussian process on the spline coefficients β:</p>
        <p>where D is a penalty matrix which penalizes the integrated squared second derivative and jDj ? its generalized determinant (see Green and Silverman 1993, for details). For splines of degree m (cubic splines have degree 3) then rankDK m 1.where D is a penalty matrix which penalizes the integrated squared second derivative and jDj ? its generalized determinant (see Green and Silverman 1993, for details). For splines of degree m (cubic splines have degree 3) then rankDK m 1.</p>
        <p>As standard within Bayesian splines (e.g. Berry et al. 2002), we place a hyperprior on the smoothing parameter GaA; B:As standard within Bayesian splines (e.g. Berry et al. 2002), we place a hyperprior on the smoothing parameter GaA; B:</p>
        <p>We select an uninformative prior on with A 1 and B 50000. Back to approximately 14 cal kBP, we have a sufficient density of tree-ring determinations that we can estimate the curve based solely upon these direct atmospheric observations3 . The main challenges in creating this more recent section of the curve are:We select an uninformative prior on with A 1 and B 50000. Back to approximately 14 cal kBP, we have a sufficient density of tree-ring determinations that we can estimate the curve based solely upon these direct atmospheric observations3 . The main challenges in creating this more recent section of the curve are:</p>
        <p>• High density and volume of data-there are 10,713 14 C measurements in this, predominantly dendrodated, section. Such a large volume of data makes a computationally efficient algorithm essential.• High density and volume of data-there are 10,713 14 C measurements in this, predominantly dendrodated, section. Such a large volume of data makes a computationally efficient algorithm essential.</p>
        <p>• Consequent demand for high level of detail in calibration curve-this, more recent, period of the calibration curve is the most heavily interrogated by archaeologists who require high precision, not just for individual calibration but also for modeling of multiple dates. This increases the user demand for fine detail in the calibration curve, e.g. incorporating solar cycles.• Consequent demand for high level of detail in calibration curve-this, more recent, period of the calibration curve is the most heavily interrogated by archaeologists who require high precision, not just for individual calibration but also for modeling of multiple dates. This increases the user demand for fine detail in the calibration curve, e.g. incorporating solar cycles.</p>
        <p>• Blocking within the data-many of the 14 C determinations do not relate to the measurement of the atmospheric 14 C in a single year but rather averages of multiple tree rings and so represent multiple years.• Blocking within the data-many of the 14 C determinations do not relate to the measurement of the atmospheric 14 C in a single year but rather averages of multiple tree rings and so represent multiple years.</p>
        <p>• Floating tree-ring sequences around 12:7 cal kBP-several late glacial trees have their chronologies estimated by wiggle matching and so while their internal chronology is known absolutely, their absolute ages are not. Otherwise, the true calendar ages of all determinations are known absolutely for this section of curve.• Floating tree-ring sequences around 12:7 cal kBP-several late glacial trees have their chronologies estimated by wiggle matching and so while their internal chronology is known absolutely, their absolute ages are not. Otherwise, the true calendar ages of all determinations are known absolutely for this section of curve.</p>
        <p>Our model for the tree-ring determinations, taking account of blocking, considers the observedOur model for the tree-ring determinations, taking account of blocking, considers the observed</p>
        <p>where m i is the number of annual rings in the block for that determination; i is the most recent year (start ring) of the block; and " i N0; 2 i where i is the uncertainty reported by the laboratory. This implicitly assumes that the tree rings over the section of m i rings are approximately equal in width, so that approximately equal annual amounts of wood were deposited and have ended up in the final measured sample. The i N0; 2 i are independent random effects, with i unknown, that represent potential over-dispersion in the 14 C determinations.where m i is the number of annual rings in the block for that determination; i is the most recent year (start ring) of the block; and " i N0; 2 i where i is the uncertainty reported by the laboratory. This implicitly assumes that the tree rings over the section of m i rings are approximately equal in width, so that approximately equal annual amounts of wood were deposited and have ended up in the final measured sample. The i N0; 2 i are independent random effects, with i unknown, that represent potential over-dispersion in the 14 C determinations.</p>
        <p>As described in Section 5.1 we model i , the standard deviation of the additive over-dispersion on the ith determination, as proportional to the square-root of the underlying F 14 C, i.e.As described in Section 5.1 we model i , the standard deviation of the additive over-dispersion on the ith determination, as proportional to the square-root of the underlying F 14 C, i.e.</p>
        <p>The individual values of i are not of interest and so we integrate them out, updating only the overall constant of proportionality . Formally, a structure where i depends upon the underlying curve means this term should be incorporated in the update to β within our sampler-a change to the curve (via its spline coefficients β) would mean a change to the additive uncertainty i on all observations. However, this would make the β update no longer a Gibbs step. Since, given the overall multiplier , the change in i between two potential curves is minimal, when updating β we consider i as fixed; see Section 4.3.3 for more details.The individual values of i are not of interest and so we integrate them out, updating only the overall constant of proportionality . Formally, a structure where i depends upon the underlying curve means this term should be incorporated in the update to β within our sampler-a change to the curve (via its spline coefficients β) would mean a change to the additive uncertainty i on all observations. However, this would make the β update no longer a Gibbs step. Since, given the overall multiplier , the change in i between two potential curves is minimal, when updating β we consider i as fixed; see Section 4.3.3 for more details.</p>
        <p>Several late-glacial tree sequences measured by Aix, ETH and Heidelberg (Reinig et al. 2018;Capano et al. 2020 in this issue) were dated by wiggle-matching so, while their internal relative chronologies were known precisely, their absolute calendar ages were somewhat uncertain. For such a floating tree-ring sequence, the true calendar ages are: float j j for j in floating sequence:Several late-glacial tree sequences measured by Aix, ETH and Heidelberg (Reinig et al. 2018;Capano et al. 2020 in this issue) were dated by wiggle-matching so, while their internal relative chronologies were known precisely, their absolute calendar ages were somewhat uncertain. For such a floating tree-ring sequence, the true calendar ages are: float j j for j in floating sequence:</p>
        <p>where is the unknown start date (i.e. most recent year) of the floating sequence and j is the precisely-known internal ring-count chronology. We place a discretized (integer) normal prior on NM; N 2 . Here, the prior mean M and variance N 2 for the start date of each such treering sequence were provided. All of the other dendrodated determinations were assumed to have their calendar age i known absolutely.where is the unknown start date (i.e. most recent year) of the floating sequence and j is the precisely-known internal ring-count chronology. We place a discretized (integer) normal prior on NM; N 2 . Here, the prior mean M and variance N 2 for the start date of each such treering sequence were provided. All of the other dendrodated determinations were assumed to have their calendar age i known absolutely.</p>
        <p>To incorporate blocking exactly, at each iteration of our MCMC we need to evaluate the level of F 14 C at every year represented within a block, and then average over them appropriately for each of our n determinations. One might think this additional calculation will make the method much slower as it requires the evaluation of the curve at many more than n calendar years. However, in the predominantly dendrodated part of the curve, it is possible to exactly incorporate blocking without any compromise on the speed of the estimation procedure. Since the vast majority of the data in this period have exactly known calendar ages (only the floating tree-ring sequences have uncertain ages) we can calculate an initial matrix that relates, for each blocked determination, the necessary averaged F 14 C to the underlying spline coefficients. This matrix then remains fixed throughout estimation. Further it has the same dimensions as if blocking had been ignored and hence does not significantly alter the speed of the MCMC updates.To incorporate blocking exactly, at each iteration of our MCMC we need to evaluate the level of F 14 C at every year represented within a block, and then average over them appropriately for each of our n determinations. One might think this additional calculation will make the method much slower as it requires the evaluation of the curve at many more than n calendar years. However, in the predominantly dendrodated part of the curve, it is possible to exactly incorporate blocking without any compromise on the speed of the estimation procedure. Since the vast majority of the data in this period have exactly known calendar ages (only the floating tree-ring sequences have uncertain ages) we can calculate an initial matrix that relates, for each blocked determination, the necessary averaged F 14 C to the underlying spline coefficients. This matrix then remains fixed throughout estimation. Further it has the same dimensions as if blocking had been ignored and hence does not significantly alter the speed of the MCMC updates.</p>
        <p>We begin by describing how we can relate each blocked determination to the spline coefficients β. Let θ A 1 ; 1 1; ...; 1 m 1 1; 2 ; ...; 2 m 2 1; ...; n m n 1 T be a vector concatenating every calendar year, including any potential blocking, represented in all n determinations. Also, define vectors c A θ and e A θ so that IntCal20 Approach to 14 C Calibration Curve Construction 839We begin by describing how we can relate each blocked determination to the spline coefficients β. Let θ A 1 ; 1 1; ...; 1 m 1 1; 2 ; ...; 2 m 2 1; ...; n m n 1 T be a vector concatenating every calendar year, including any potential blocking, represented in all n determinations. Also, define vectors c A θ and e A θ so that IntCal20 Approach to 14 C Calibration Curve Construction 839</p>
        <p>and e A ;l e A l =8267 :and e A ;l e A l =8267 :</p>
        <p>Then, create the matrix B A θ containing the spline bases evaluated at each value in θ A :Then, create the matrix B A θ containing the spline bases evaluated at each value in θ A :</p>
        <p>so that, at the calendar years θ A , the modeled value of Δ 14 Cisg A B A θ β where β represents the spline coefficients. The modeled value of F 14 C at an individual calendar age A l then becomes f A l c A ;l g A l e A ;l and at the full θ A , f A B y θ β e θ where B y θ is formed by multiplying each row in B A θ by the corresponding element in c A θ .so that, at the calendar years θ A , the modeled value of Δ 14 Cisg A B A θ β where β represents the spline coefficients. The modeled value of F 14 C at an individual calendar age A l then becomes f A l c A ;l g A l e A ;l and at the full θ A , f A B y θ β e θ where B y θ is formed by multiplying each row in B A θ by the corresponding element in c A θ .</p>
        <p>For each blocked determination, we now wish to average the values of F 14 C according to the multiple years represented in that block. Consider an averaging matrix M corresponding to averaging the individual values in f A :For each blocked determination, we now wish to average the values of F 14 C according to the multiple years represented in that block. Consider an averaging matrix M corresponding to averaging the individual values in f A :</p>
        <p>Gibbs Updating λjβ; A; B Due to conjugacy of the prior:Gibbs Updating λjβ; A; B Due to conjugacy of the prior:</p>
        <p>i.e. jβ GaA 0 ; B 0 where A 0 A rankD=2 and B 0 β T Dβ 2 1 B 1 .i.e. jβ GaA 0 ; B 0 where A 0 A rankD=2 and B 0 β T Dβ 2 1 B 1 .</p>
        <p>To update the start of our floating tree-ring sequences, we use an MH step: • Sample 0 N ; 2 ;To update the start of our floating tree-ring sequences, we use an MH step: • Sample 0 N ; 2 ;</p>
        <p>• Accept according to Hastings Ratio:• Accept according to Hastings Ratio:</p>
        <p>. Note the asymmetric proposal adjustment.. Note the asymmetric proposal adjustment.</p>
        <p>For the portion of the curve from 0 cal kBP back to approximately 14 cal kBP, we placed 2000 knots at the unique calendar age quantiles of the constituent determinations (each blocked determination was represented by its midpoint) to permit variable smoothing dependent upon data density. In regions where there were more data this allowed us to pick out finer scale variation. Such a selection enabled close to annual resolution in the detail of the final curve while still maintaining computational feasibility in curve construction. We also placed additional knots in the vicinity of the known Miyake-type events at 774-5 AD, 993-4A D and 660 BC (Miyake et al. 2012(Miyake et al. , 2013;;O 'Hare et al. 2019) to enable us to capture more rapid variation at these times. Around the calendar age of each such event, we added an extra 4 jittered knots, i.e. after addition of small amounts of random noise-for the 660 BC event we used a slightly larger jitter due to its slightly less certain timing. If there is no event at these times then the curve will not introduce one but, if there is, then these additional knots will allow it to be picked up more clearly. All knot locations then remained fixed throughout the sampler.For the portion of the curve from 0 cal kBP back to approximately 14 cal kBP, we placed 2000 knots at the unique calendar age quantiles of the constituent determinations (each blocked determination was represented by its midpoint) to permit variable smoothing dependent upon data density. In regions where there were more data this allowed us to pick out finer scale variation. Such a selection enabled close to annual resolution in the detail of the final curve while still maintaining computational feasibility in curve construction. We also placed additional knots in the vicinity of the known Miyake-type events at 774-5 AD, 993-4A D and 660 BC (Miyake et al. 2012(Miyake et al. , 2013;;O 'Hare et al. 2019) to enable us to capture more rapid variation at these times. Around the calendar age of each such event, we added an extra 4 jittered knots, i.e. after addition of small amounts of random noise-for the 660 BC event we used a slightly larger jitter due to its slightly less certain timing. If there is no event at these times then the curve will not introduce one but, if there is, then these additional knots will allow it to be picked up more clearly. All knot locations then remained fixed throughout the sampler.</p>
        <p>In order to identify potential outliers, after fitting a preliminary curve, scaled residuals (incorporating potential blocking) were calculated for each datum, i.e. for an annual measurementIn order to identify potential outliers, after fitting a preliminary curve, scaled residuals (incorporating potential blocking) were calculated for each datum, i.e. for an annual measurement</p>
        <p>where i is the posterior standard deviation on the calibration curve at i cal BP, the calendar age of that datum. These residuals were then combined for each dataset K into a scaled deviance, Z K P i2K r 2 i , and compared with a 2 n K where n K are the number of determinations in that set. The mean offset of each dataset from the preliminary curve was also calculatedwhere i is the posterior standard deviation on the calibration curve at i cal BP, the calendar age of that datum. These residuals were then combined for each dataset K into a scaled deviance, Z K P i2K r 2 i , and compared with a 2 n K where n K are the number of determinations in that set. The mean offset of each dataset from the preliminary curve was also calculated</p>
        <p>.S e t sw h i c hh a dl o wp-values for their deviance and high mean offsets were then discussed with the data set providers as to whether they should be included in the final IntCal or not (see Bayliss et al. 2020 in this issue, for more details)..S e t sw h i c hh a dl o wp-values for their deviance and high mean offsets were then discussed with the data set providers as to whether they should be included in the final IntCal or not (see Bayliss et al. 2020 in this issue, for more details).</p>
        <p>The MCMC was run for 50,000 iterations. The first 25,000 of these iterations were discarded as burn-in. The remaining 25,000 were used to create the final curve (thinned to every 10th) and passed to the older part of the curve to create a seamless merging between the curve sections as discussed in Section 4.4.1. Since the main update steps (i.e. β and ) are Gibbs this was felt to be sufficient although convergence was further assessed by initializing the sampler at different values and comparing the resultant curve estimates visually.The MCMC was run for 50,000 iterations. The first 25,000 of these iterations were discarded as burn-in. The remaining 25,000 were used to create the final curve (thinned to every 10th) and passed to the older part of the curve to create a seamless merging between the curve sections as discussed in Section 4.4.1. Since the main update steps (i.e. β and ) are Gibbs this was felt to be sufficient although convergence was further assessed by initializing the sampler at different values and comparing the resultant curve estimates visually.</p>
        <p>Beyond approximately 14 cal kBP, the number of tree-ring determinations decreases and they are not sufficient to create a precise calibration curve. We therefore merge the predominantly dendrodated curve with the older section of curve, which is estimated from a wider range of 14 C material, at 13,913 cal BP. In this older period, the underlying data used to construct the curve incorporates corals, foraminifera, macrofossils and speleothems, in addition to a small number of floating tree-ring sequences. These alternative sources of data are typically not direct measurements of the atmosphere but instead are offset due to marine reservoir effects and dead carbon fractions. Further, their true calendar ages are quite uncertain and can only be estimated. They therefore present several further challenges for construction of an atmospheric curve.Beyond approximately 14 cal kBP, the number of tree-ring determinations decreases and they are not sufficient to create a precise calibration curve. We therefore merge the predominantly dendrodated curve with the older section of curve, which is estimated from a wider range of 14 C material, at 13,913 cal BP. In this older period, the underlying data used to construct the curve incorporates corals, foraminifera, macrofossils and speleothems, in addition to a small number of floating tree-ring sequences. These alternative sources of data are typically not direct measurements of the atmosphere but instead are offset due to marine reservoir effects and dead carbon fractions. Further, their true calendar ages are quite uncertain and can only be estimated. They therefore present several further challenges for construction of an atmospheric curve.</p>
        <p>Uncertain Calendar AgesUncertain Calendar Ages</p>
        <p>All our data in the older part of the curve have estimates (either in the form of noisy observations or priors) for their calendar ages rather than absolutely known values. For the corals and speleothems these estimates are obtained via U-Th dating and are considered independent; for the Cariaco Basin they are either provided by varve counting or elastic palaeoclimate tie-pointing (Heaton et al. 2013); similar elastic palaeoclimate tie-pointing provides the ages for the Pakistan and Iberian margins; for Lake Suigetsu they are provided by wiggle matching; while the floating tree-ring sequences have internally known chronologies but no absolute age estimates. We assume all our calendar age estimates are (potentially multivariate) normal. For a dataset K on which we have noisy observations T K of its calendar ages e.g. U-Th dated corals and the varve counted section of Cariaco, we model:All our data in the older part of the curve have estimates (either in the form of noisy observations or priors) for their calendar ages rather than absolutely known values. For the corals and speleothems these estimates are obtained via U-Th dating and are considered independent; for the Cariaco Basin they are either provided by varve counting or elastic palaeoclimate tie-pointing (Heaton et al. 2013); similar elastic palaeoclimate tie-pointing provides the ages for the Pakistan and Iberian margins; for Lake Suigetsu they are provided by wiggle matching; while the floating tree-ring sequences have internally known chronologies but no absolute age estimates. We assume all our calendar age estimates are (potentially multivariate) normal. For a dataset K on which we have noisy observations T K of its calendar ages e.g. U-Th dated corals and the varve counted section of Cariaco, we model:</p>
        <p>Here Ψ K;T is the pre-specified and fixed covariance matrix that encodes the dependence within the calendar age observations for that dataset. For such datasets we place an uninformative prior on θ K . For Lake Suigetsu and the datasets where calendar age estimates are obtained via palaeoclimate tie-pointing, no actual calendar ages were observed but rather we have a prior on their true values:Here Ψ K;T is the pre-specified and fixed covariance matrix that encodes the dependence within the calendar age observations for that dataset. For such datasets we place an uninformative prior on θ K . For Lake Suigetsu and the datasets where calendar age estimates are obtained via palaeoclimate tie-pointing, no actual calendar ages were observed but rather we have a prior on their true values:</p>
        <p>and the T K and Ψ K;T now represent our prior mean and covariance for the calendar ages. As we have no datasets on which we have both priors and noisy calendar age observations, these two types of calendar age estimate become equivalent for the purposes of updating. Details of how to construct the various covariance matrices for the case of varve counting and wiggle matching can be found in Niu et al. (2013); for the datasets with priors obtained by tie-pointing in Heaton et al. (2013); and for Lake Suigetsu in Bronk Ramsey et al. (2020 in this issue).and the T K and Ψ K;T now represent our prior mean and covariance for the calendar ages. As we have no datasets on which we have both priors and noisy calendar age observations, these two types of calendar age estimate become equivalent for the purposes of updating. Details of how to construct the various covariance matrices for the case of varve counting and wiggle matching can be found in Niu et al. (2013); for the datasets with priors obtained by tie-pointing in Heaton et al. (2013); and for Lake Suigetsu in Bronk Ramsey et al. (2020 in this issue).</p>
        <p>Several of our datasets do not provide direct measurements of the atmosphere but are instead offset. The speleothem records contain carbon obtained from dripwater which has passed through old limestone and so is a mixture of atmospheric CO 2 and dissolved old (dead) carbon that has no 14 C activity. The offset in radiocarbon age this creates is called a dead IntCal20 Approach to 14 C Calibration Curve Construction 843 carbon fraction (dcf) and is specific to the speleothem. Similarly the marine records have an atmospheric offset called a marine reservoir age (MRA) that arises due to both the limited gas exchange with the atmosphere and ocean circulation drawing up deep old carbon. These MRAs are location specific and vary over time. For both of these types of data we can incorporate the offset (either dcf or MRA) in the radiocarbon age domain as:Several of our datasets do not provide direct measurements of the atmosphere but are instead offset. The speleothem records contain carbon obtained from dripwater which has passed through old limestone and so is a mixture of atmospheric CO 2 and dissolved old (dead) carbon that has no 14 C activity. The offset in radiocarbon age this creates is called a dead IntCal20 Approach to 14 C Calibration Curve Construction 843 carbon fraction (dcf) and is specific to the speleothem. Similarly the marine records have an atmospheric offset called a marine reservoir age (MRA) that arises due to both the limited gas exchange with the atmosphere and ocean circulation drawing up deep old carbon. These MRAs are location specific and vary over time. For both of these types of data we can incorporate the offset (either dcf or MRA) in the radiocarbon age domain as:</p>
        <p>where h offset is the radiocarbon age at time cal BP in the offset environment; h atmos our atmospheric calibration curve in the radiocarbon age domain; and r K our offset. This alters our observational model so that, in the F 14 C domain, for determinations from dataset K and with F 14 C domain calibration curve f , the offset becomes a multiplier,where h offset is the radiocarbon age at time cal BP in the offset environment; h atmos our atmospheric calibration curve in the radiocarbon age domain; and r K our offset. This alters our observational model so that, in the F 14 C domain, for determinations from dataset K and with F 14 C domain calibration curve f , the offset becomes a multiplier,</p>
        <p>These offsets must be estimated within curve construction to adaptively synchronize the different records. For IntCal20, speleothem dcfs were considered to be approximately constant over time but with an unknown level. MRAs were modeled as time-varying, providing a step forward from IntCal09 and IntCal13. Initial MRA estimates for each of our locations were obtained by creating a preliminary atmospheric calibration curve using the same Bayesian spline methodology but based only on the Hulu Cave record (Southon et al. 2012;Cheng et al. 2018). This Hulu-based curve was then used as a forcing for an enhanced Hamburg Large Scale Geostrophic (LSG) ocean general circulation model to provide estimates K of MRA at each given location (see Butzin et al. 2020 in this issue, for details). Due to coastal effects, these LSG estimates were considered to define the shape of the MRA, i.e. relative changes over time, but subject to a constant potential coastal shift. Ideally we might cycle the process of building the curve and re-estimating MRAs several times but the LSG model had a run time of several weeks and so this was not feasible. Notwithstanding the above attempts to accurately model dcf and MRA, we recognized that there was further variation in the offsets over time we were not able to fully capture. This was incorporated through the addition of further independent variability in the offsets from one calendar age to the next. Consequently our model for the offsets is:These offsets must be estimated within curve construction to adaptively synchronize the different records. For IntCal20, speleothem dcfs were considered to be approximately constant over time but with an unknown level. MRAs were modeled as time-varying, providing a step forward from IntCal09 and IntCal13. Initial MRA estimates for each of our locations were obtained by creating a preliminary atmospheric calibration curve using the same Bayesian spline methodology but based only on the Hulu Cave record (Southon et al. 2012;Cheng et al. 2018). This Hulu-based curve was then used as a forcing for an enhanced Hamburg Large Scale Geostrophic (LSG) ocean general circulation model to provide estimates K of MRA at each given location (see Butzin et al. 2020 in this issue, for details). Due to coastal effects, these LSG estimates were considered to define the shape of the MRA, i.e. relative changes over time, but subject to a constant potential coastal shift. Ideally we might cycle the process of building the curve and re-estimating MRAs several times but the LSG model had a run time of several weeks and so this was not feasible. Notwithstanding the above attempts to accurately model dcf and MRA, we recognized that there was further variation in the offsets over time we were not able to fully capture. This was incorporated through the addition of further independent variability in the offsets from one calendar age to the next. Consequently our model for the offsets is:</p>
        <p>where K is the further independent variability in offsets from year-to-year around our LSG/ constant dcf model and added to data before curve construction. We place priors on the constant dcf mean/coastal shifts K :where K is the further independent variability in offsets from year-to-year around our LSG/ constant dcf model and added to data before curve construction. We place priors on the constant dcf mean/coastal shifts K :</p>
        <p>We do not aim to estimate the precise values of r K i during curve construction. However, the parameter ν, containing the values of K which determine the mean dcf/MRA offset for each dataset, is updated within our MCMC sampler.We do not aim to estimate the precise values of r K i during curve construction. However, the parameter ν, containing the values of K which determine the mean dcf/MRA offset for each dataset, is updated within our MCMC sampler.</p>
        <p>To fully specify our offset model, values of the prior mean dcf/coastal shift, K , for each dataset; and ! K , our uncertainty on the level of this shift, were estimated from the overlap between additional data available for each speleothem/marine location and the dendrodated curve from 0-14 cal kBP. Similarly, these overlaps provided an estimate for each K . See Section 5.2 for more details. Finally, for the two floating Southern Hemisphere kauri trees we assumed an offset of 43 ± 23 14 C yrs (1) based upon the North-South hemispheric offset estimated in SHCal13 (Hogg et al. 2013). Being direct measurements of the NH atmosphere, both Suigetsu and the three Bølling-Allerød floating tree-ring sequences are not offset, i.e. for all , r K 0.To fully specify our offset model, values of the prior mean dcf/coastal shift, K , for each dataset; and ! K , our uncertainty on the level of this shift, were estimated from the overlap between additional data available for each speleothem/marine location and the dendrodated curve from 0-14 cal kBP. Similarly, these overlaps provided an estimate for each K . See Section 5.2 for more details. Finally, for the two floating Southern Hemisphere kauri trees we assumed an offset of 43 ± 23 14 C yrs (1) based upon the North-South hemispheric offset estimated in SHCal13 (Hogg et al. 2013). Being direct measurements of the NH atmosphere, both Suigetsu and the three Bølling-Allerød floating tree-ring sequences are not offset, i.e. for all , r K 0.</p>
        <p>The LSG model was not able to adequately resolve the MRA within the geographically unique Cariaco Basin which has a shallow sill that potentially limits exchange with the wider ocean. A further model for the MRAs in this location was therefore needed. As it covered a short time period, the MRA for the Cariaco varved record (Hughen et al. 2004) was modeled as for the speleothems, i.e. independently varying around a constant level. For the Cariaco unvarved record (Hughen and Heaton 2020 in this issue), we modeled the F 14 C domain multiplier corresponding to any offset asThe LSG model was not able to adequately resolve the MRA within the geographically unique Cariaco Basin which has a shallow sill that potentially limits exchange with the wider ocean. A further model for the MRAs in this location was therefore needed. As it covered a short time period, the MRA for the Cariaco varved record (Hughen et al. 2004) was modeled as for the speleothems, i.e. independently varying around a constant level. For the Cariaco unvarved record (Hughen and Heaton 2020 in this issue), we modeled the F 14 C domain multiplier corresponding to any offset as</p>
        <p>a further Bayesian spline with 30 knots placed at jittered quantiles of the Cariaco prior calendar ages. The value of β C C;1 ; ...; C;30 T determining this spline was also estimated during curve construction but with a fixed and large smoothing parameter C . This approach meant rapid changes in the 14 C determinations within Cariaco were considered as atmospheric signal while smoother, longer term drifts away from the other data that might otherwise introduce spurious features into the curve, would be resolved as time-varying reservoir effects.a further Bayesian spline with 30 knots placed at jittered quantiles of the Cariaco prior calendar ages. The value of β C C;1 ; ...; C;30 T determining this spline was also estimated during curve construction but with a fixed and large smoothing parameter C . This approach meant rapid changes in the 14 C determinations within Cariaco were considered as atmospheric signal while smoother, longer term drifts away from the other data that might otherwise introduce spurious features into the curve, would be resolved as time-varying reservoir effects.</p>
        <p>Due to the diversity of the datasets in this older time period, as well as the difficulty in incorporating all their potential complexities, we wished to create a curve that was not overly influenced by single determinations. We therefore permit the possibility of heavier tailed errors in the F 14 C determinations for the data in this older section of curve extending beyond 13,913 cal BP. This is incorporated by introducing an error multiplier where for each observation we maintainDue to the diversity of the datasets in this older time period, as well as the difficulty in incorporating all their potential complexities, we wished to create a curve that was not overly influenced by single determinations. We therefore permit the possibility of heavier tailed errors in the F 14 C determinations for the data in this older section of curve extending beyond 13,913 cal BP. This is incorporated by introducing an error multiplier where for each observation we maintain</p>
        <p>but with each determination's reported uncertainty i scaled according to a further parameter ibut with each determination's reported uncertainty i scaled according to a further parameter i</p>
        <p>where i Gashape % Kiwhere i Gashape % Ki</p>
        <p>; rate % Ki; rate % Ki</p>
        <p>and Ki is the dataset of observation i. All observations i belonging to the same dataset K therefore have the same shape and rate in the Gamma prior for their individual i . This model, integrating out i , equates to a Student's t-distribution for an individual observation i, " i tlocation 0; scale i ; df % Ki :and Ki is the dataset of observation i. All observations i belonging to the same dataset K therefore have the same shape and rate in the Gamma prior for their individual i . This model, integrating out i , equates to a Student's t-distribution for an individual observation i, " i tlocation 0; scale i ; df % Ki :</p>
        <p>Further, for each dataset K, we place an independent hyperprior on the value of its particular % K parameter so that the observations belonging to that dataset are grouped,Further, for each dataset K, we place an independent hyperprior on the value of its particular % K parameter so that the observations belonging to that dataset are grouped,</p>
        <p>to capture potential differences between the various records. Consequently, if a dataset is seen to contain several outliers, the other determinations in that same set will also be treated with IntCal20 Approach to 14 C Calibration Curve Construction 845 more caution. We choose a subjective hyperprior encapsulating an expectation of a low level of heavy-tailed behavior by selecting, for each dataset, A % 100 and B % 1 2 . We update both κ 1 ; ...; n T and ϱ, the values of the various % K , within our sampler.to capture potential differences between the various records. Consequently, if a dataset is seen to contain several outliers, the other determinations in that same set will also be treated with IntCal20 Approach to 14 C Calibration Curve Construction 845 more caution. We choose a subjective hyperprior encapsulating an expectation of a low level of heavy-tailed behavior by selecting, for each dataset, A % 100 and B % 1 2 . We update both κ 1 ; ...; n T and ϱ, the values of the various % K , within our sampler.</p>
        <p>A significant concern with the previous random walk approach was how well the MCMC sampler mixed. With this random walk approach, we only updated the calibration curve one calendar year at a time, conditional on the value at all other times, which restricted the ability of the curve estimate to move significantly. Furthermore, this update was performed by MH meaning we would frequently reject proposed updates. By switching to Bayesian splines, we are instead able to update, conditional on the current calendar ages, the entire curve through its spline coefficients simultaneously via Gibbs. These dual changes, to update the entire curve at once and to do so via Gibbs sampling as opposed to MH, hopefully begin to address mixing concerns. However, due to the uncertain calendar ages it is likely the posterior for the curve (and the true calendar ages) will remain multi-modal. In an attempt to overcome any remaining concerns over mixing, we also incorporate parallel tempering. In tempering, we run multiple chains concurrently. One of these MCMC chains has as its target our posterior of interest while the others have modified, higher temperature, targets. These higher temperature targets are typically flattened versions of the posterior of interest designed so that, as the temperature increases, the corresponding MCMC chain mixes more easily. We then propose swaps between the states of the multiple chains so that the chains that mix well (i.e. those that run at higher temperatures) improve the mixing of the chains which do not (i.e. our posterior of interest). We are free to choose the elements of the likelihood we wish to temper and by how much. We only temper the likelihood of the observed data, i.e. Fjθ; β; ν; β C ; κ; and Tjθ (or equivalent prior θ) since these were thought to be the main elements restricting mixing and doing so keeps the MCMC updates straightforward. Our coupled chains run with a pair of temperatures 1 ; 2 &gt; 1 with each chain sampling from a modified posterior, θ; β;;ν; β C ; κ; ϱjF; T 1 ; 2 ∝ L 1 ; 2 θ; β;;ν; β C ; κ; ϱjF; T Tjθ 1= 1 Fjθ; β; ν; β C ; κ 1= 2 βjνβ C κjϱϱθ 1= 1 ; so higher temperatures give flatter posteriors. We show in Section 4.4.3 how these modifications are simply equivalent to increasing our observational noise and so we can maintain straightforward and fast updates for each chain. We run the chain at four temperatures, including the unmodified posterior of actual interest where 1 2 1,i n parallel. We discuss the effect of tempering, and how it aids mixing, in more detail within the Supplementary Information.A significant concern with the previous random walk approach was how well the MCMC sampler mixed. With this random walk approach, we only updated the calibration curve one calendar year at a time, conditional on the value at all other times, which restricted the ability of the curve estimate to move significantly. Furthermore, this update was performed by MH meaning we would frequently reject proposed updates. By switching to Bayesian splines, we are instead able to update, conditional on the current calendar ages, the entire curve through its spline coefficients simultaneously via Gibbs. These dual changes, to update the entire curve at once and to do so via Gibbs sampling as opposed to MH, hopefully begin to address mixing concerns. However, due to the uncertain calendar ages it is likely the posterior for the curve (and the true calendar ages) will remain multi-modal. In an attempt to overcome any remaining concerns over mixing, we also incorporate parallel tempering. In tempering, we run multiple chains concurrently. One of these MCMC chains has as its target our posterior of interest while the others have modified, higher temperature, targets. These higher temperature targets are typically flattened versions of the posterior of interest designed so that, as the temperature increases, the corresponding MCMC chain mixes more easily. We then propose swaps between the states of the multiple chains so that the chains that mix well (i.e. those that run at higher temperatures) improve the mixing of the chains which do not (i.e. our posterior of interest). We are free to choose the elements of the likelihood we wish to temper and by how much. We only temper the likelihood of the observed data, i.e. Fjθ; β; ν; β C ; κ; and Tjθ (or equivalent prior θ) since these were thought to be the main elements restricting mixing and doing so keeps the MCMC updates straightforward. Our coupled chains run with a pair of temperatures 1 ; 2 &gt; 1 with each chain sampling from a modified posterior, θ; β;;ν; β C ; κ; ϱjF; T 1 ; 2 ∝ L 1 ; 2 θ; β;;ν; β C ; κ; ϱjF; T Tjθ 1= 1 Fjθ; β; ν; β C ; κ 1= 2 βjνβ C κjϱϱθ 1= 1 ; so higher temperatures give flatter posteriors. We show in Section 4.4.3 how these modifications are simply equivalent to increasing our observational noise and so we can maintain straightforward and fast updates for each chain. We run the chain at four temperatures, including the unmodified posterior of actual interest where 1 2 1,i n parallel. We discuss the effect of tempering, and how it aids mixing, in more detail within the Supplementary Information.</p>
        <p>We need to ensure we smoothly merge our older section of calibration curve with the independently created tree-ring-only curve described in Section 4.3. This is achieved by selection of an appropriate overlapping set of knots for the two sections. We first identify, in the tree-ring-only curve, the calendar age at which to create the join. This is determined to be the calendar age of the knot at which, due to the ending of the tree-ring determinations, the curve uncertainty on the tree-ring-only curve begins to increase significantly. For IntCal20, this was selected to be at 13,913 cal BP. This knot and the two knots either side that were used to create the tree-ring-only curve (five tree-ring-only knots in total) are then copied into the knot basis for the older portion of curve. On each update of the spline coefficients for the older section of curve, we pass the three spline coefficients that relate to the value of a particular realization of the tree-ring-only section of curve at the join. The Gibbs update for the older spline coefficients is then performed conditional on these three tree-ring-only spline coefficients. This ensures that when the posterior realizations of the two curves are connected together for the final curve, each realization is itself a cubic spline with not only continuity but also smoothness over the transition between the two sections.We need to ensure we smoothly merge our older section of calibration curve with the independently created tree-ring-only curve described in Section 4.3. This is achieved by selection of an appropriate overlapping set of knots for the two sections. We first identify, in the tree-ring-only curve, the calendar age at which to create the join. This is determined to be the calendar age of the knot at which, due to the ending of the tree-ring determinations, the curve uncertainty on the tree-ring-only curve begins to increase significantly. For IntCal20, this was selected to be at 13,913 cal BP. This knot and the two knots either side that were used to create the tree-ring-only curve (five tree-ring-only knots in total) are then copied into the knot basis for the older portion of curve. On each update of the spline coefficients for the older section of curve, we pass the three spline coefficients that relate to the value of a particular realization of the tree-ring-only section of curve at the join. The Gibbs update for the older spline coefficients is then performed conditional on these three tree-ring-only spline coefficients. This ensures that when the posterior realizations of the two curves are connected together for the final curve, each realization is itself a cubic spline with not only continuity but also smoothness over the transition between the two sections.</p>
        <p>To make the creation of the older section of the curve computationally practical, we do not incorporate exact modeling of blocking or over-dispersion. The large number of uncertain calendar ages mean that including blocking is not feasible. Each update to θ requires recalculation of the spline basis matrix B θ . Doing this at every year in θ A , i.e. every year represented in all the blocks, would be extremely slow. Instead, we consider each determination to represent the single year in its centre. This significantly reduces the number of calendar ages θ at which we have to recalculate our spline basis matrix. This is likely to have little effect on the overall curve since the density of data in this older section is not sufficient to identify very fine-scaled features. Over-dispersion was also not included in modeling the older curve as we lack the required data density to reliably estimate it. Further, our previous estimate of over-dispersion related to tree-ring determinations only, as opposed to the broader range of 14 C sources used within the older section of curve. Instead our modification to permit heavier tailed uncertainties aims to deal with potential over-dispersion in curve estimation. Note however, that when calibrating atmospheric 14 C determinations against the curve in this older time period, we would still expect them to have similar potential additional sources of variation (beyond the laboratory quoted uncertainty) as seen in the predominantly dendrodated section. We therefore maintain predictive intervals for our final, older curve estimate by adding back in the over-dispersion estimated within the 0-14 cal kBP tree rings to the posterior curve estimate.To make the creation of the older section of the curve computationally practical, we do not incorporate exact modeling of blocking or over-dispersion. The large number of uncertain calendar ages mean that including blocking is not feasible. Each update to θ requires recalculation of the spline basis matrix B θ . Doing this at every year in θ A , i.e. every year represented in all the blocks, would be extremely slow. Instead, we consider each determination to represent the single year in its centre. This significantly reduces the number of calendar ages θ at which we have to recalculate our spline basis matrix. This is likely to have little effect on the overall curve since the density of data in this older section is not sufficient to identify very fine-scaled features. Over-dispersion was also not included in modeling the older curve as we lack the required data density to reliably estimate it. Further, our previous estimate of over-dispersion related to tree-ring determinations only, as opposed to the broader range of 14 C sources used within the older section of curve. Instead our modification to permit heavier tailed uncertainties aims to deal with potential over-dispersion in curve estimation. Note however, that when calibrating atmospheric 14 C determinations against the curve in this older time period, we would still expect them to have similar potential additional sources of variation (beyond the laboratory quoted uncertainty) as seen in the predominantly dendrodated section. We therefore maintain predictive intervals for our final, older curve estimate by adding back in the over-dispersion estimated within the 0-14 cal kBP tree rings to the posterior curve estimate.</p>
        <p>We present the update scheme for the MCMC sampler using notation that refers to the full parameter space. In practice, several of the update steps are done dataset-by-dataset, for example calendar ages or offsets. The calculations required to perform such dataset-specific updates often only require consideration of the likelihood restricted to that particular dataset and so remain fast.We present the update scheme for the MCMC sampler using notation that refers to the full parameter space. In practice, several of the update steps are done dataset-by-dataset, for example calendar ages or offsets. The calculations required to perform such dataset-specific updates often only require consideration of the likelihood restricted to that particular dataset and so remain fast.</p>
        <p>Let us define, where subscripts denote the unknown (and updated) variables on which the elements depend:Let us define, where subscripts denote the unknown (and updated) variables on which the elements depend:</p>
        <p>Ee =8267 e θ E 1 ; ...; E n T :Ee =8267 e θ E 1 ; ...; E n T :</p>
        <p>Note that our atmospheric-offset multipliers K are determined by either K , the values of the corresponding mean dcf or coastal MRA shift, or β C in the case of the Cariaco unvarved record. Then, considering each datum as representing a single year and letting F F 1 ; ...; F n T , we can calculate for each dataset K the offset-adjusted calibration curve:Note that our atmospheric-offset multipliers K are determined by either K , the values of the corresponding mean dcf or coastal MRA shift, or β C in the case of the Cariaco unvarved record. Then, considering each datum as representing a single year and letting F F 1 ; ...; F n T , we can calculate for each dataset K the offset-adjusted calibration curve:</p>
        <p>where W κ diag 2 1 1 ; ...; 2 n n 1 includes the precision multiplier representing the heavy tails, and Ψ T is the combined covariance matrix for all the calendar ages. As for the predominantly dendrodated section of curve, we can sample from this using a Metropolis-within-Gibbs algorithm, and again much of the updating can be performed directly via Gibbs sampling.where W κ diag 2 1 1 ; ...; 2 n n 1 includes the precision multiplier representing the heavy tails, and Ψ T is the combined covariance matrix for all the calendar ages. As for the predominantly dendrodated section of curve, we can sample from this using a Metropolis-within-Gibbs algorithm, and again much of the updating can be performed directly via Gibbs sampling.</p>
        <p>Gibbs updating the calibration curve βjF; θ; λ; ν; β C ; κ. This update is very similar to the equivalent update in the tree-ring-based section of curve. Letting F ? F Γ ν;β C ;θ e θ then, were it not for our need to create a smooth transition from the trees, the posteriorGibbs updating the calibration curve βjF; θ; λ; ν; β C ; κ. This update is very similar to the equivalent update in the tree-ring-based section of curve. Letting F ? F Γ ν;β C ;θ e θ then, were it not for our need to create a smooth transition from the trees, the posterior</p>
        <p>and, due to our dropping of blocking, all the matrices in the square brackets are diagonal so can be multiplied rapidly. To create the smooth transition, we draw a realization from the, already estimated, tree-ring-based section of curve and condition on β R , the value of the three basis coefficients affecting the value of the spline at the joining knot. Thenand, due to our dropping of blocking, all the matrices in the square brackets are diagonal so can be multiplied rapidly. To create the smooth transition, we draw a realization from the, already estimated, tree-ring-based section of curve and condition on β R , the value of the three basis coefficients affecting the value of the spline at the joining knot. Then</p>
        <p>Here, for example Q R;R corresponds to the elements of the covariance that relate to the realized tree-ring based spline coefficients we condition on.Here, for example Q R;R corresponds to the elements of the covariance that relate to the realized tree-ring based spline coefficients we condition on.</p>
        <p>MH updating the calendar ages θjT; F; β; ν; β C ; κ This step is as for the update of the wiggle matched trees. Whether a dataset has a prior on their calendar ages or noisy observations does not affect the update. For each dataset K in turn we: • Propose θ K 0 MVNθ K ; Σ prop ;MH updating the calendar ages θjT; F; β; ν; β C ; κ This step is as for the update of the wiggle matched trees. Whether a dataset has a prior on their calendar ages or noisy observations does not affect the update. For each dataset K in turn we: • Propose θ K 0 MVNθ K ; Σ prop ;</p>
        <p>• Calculate B θ and B θ 0 (and similarly recalculate Γ ν;β C ;θ 0 , C θ 0 and E θ 0 at θ K 0 ). Accept according to Hastings Ratio:• Calculate B θ and B θ 0 (and similarly recalculate Γ ν;β C ;θ 0 , C θ 0 and E θ 0 at θ K 0 ). Accept according to Hastings Ratio:</p>
        <p>wherewhere</p>
        <p>Note that we only need to recalculate the elements in the likelihood of F (e.g. the elements of B θ ) that relate to θ K . In the case of independent error, i.e. T j N j ; 2 j , we use a proposal standard deviation prop 2 j and we can accept/reject each proposal j 1; ...; n K independently. In the case of dependent error with a covariance matrix, i.e. T K MVNθ K ; Ψ K;T , or an equivalent prior, we update the whole of θ K simultaneously by sampling a proposal θ 0 MVNθ; Σ prop , where Σ prop ∝ Ψ K;T , to obtain higher acceptance rates.Note that we only need to recalculate the elements in the likelihood of F (e.g. the elements of B θ ) that relate to θ K . In the case of independent error, i.e. T j N j ; 2 j , we use a proposal standard deviation prop 2 j and we can accept/reject each proposal j 1; ...; n K independently. In the case of dependent error with a covariance matrix, i.e. T K MVNθ K ; Ψ K;T , or an equivalent prior, we update the whole of θ K simultaneously by sampling a proposal θ 0 MVNθ; Σ prop , where Σ prop ∝ Ψ K;T , to obtain higher acceptance rates.</p>
        <p>MH updating the offsets ν K jF; θ; β; λ; ξ; τ Similarly, for each offset dataset:MH updating the offsets ν K jF; θ; β; λ; ξ; τ Similarly, for each offset dataset:</p>
        <p>• Calculate resultant F 14 C multiplier Γ ν 0 ;β C ;θ . Accept according to Hastings Ratio:• Calculate resultant F 14 C multiplier Γ ν 0 ;β C ;θ . Accept according to Hastings Ratio:</p>
        <p>wherewhere</p>
        <p>and '; K ;! 2 K is a normal density with mean K and variance ! 2 K .and '; K ;! 2 K is a normal density with mean K and variance ! 2 K .</p>
        <p>As when updating the calendar ages for each dataset in turn, when calculating this Hastings Ratio we can restrict our attention to the dataset in question, as elements relating to other datasets will cancel, making this step considerably faster.As when updating the calendar ages for each dataset in turn, when calculating this Hastings Ratio we can restrict our attention to the dataset in question, as elements relating to other datasets will cancel, making this step considerably faster.</p>
        <p>Gibbs updating the Cariaco Basin unvarved MRA β C jF; θ; β; λ; ν; κ Updating the spline MRA coefficients β C for the Cariaco Basin is analogous to the update of the calibration curve coefficients β. This can intuitively be seen by considering how these determinations are represented and swapping the offset multiplier K K with the calibration curve estimate f , i.e. for determinations F i belonging to the Cariaco Basin unvarved record,Gibbs updating the Cariaco Basin unvarved MRA β C jF; θ; β; λ; ν; κ Updating the spline MRA coefficients β C for the Cariaco Basin is analogous to the update of the calibration curve coefficients β. This can intuitively be seen by considering how these determinations are represented and swapping the offset multiplier K K with the calibration curve estimate f , i.e. for determinations F i belonging to the Cariaco Basin unvarved record,</p>
        <p>IntCal20 Approach to 14 C Calibration Curve Construction 849 and noting that, given β and θ, the curve estimates f i are known and K i P 30 k1 C;k B C;k i , a linear combination of the MRA spline coefficients β C . More rigorously, this can be seen if we restrict our likelihood to solely the Cariaco determinations, and change the ordering of Γ ν;β C ;θ and C θ B θ β e θ . Then, letting F K diagC θ K B θ K β e θ K be a diagonal matrix representing the current atmospheric calibration curve at the Cariaco calendar ages, our posterior for the MRA spline β C becomesIntCal20 Approach to 14 C Calibration Curve Construction 849 and noting that, given β and θ, the curve estimates f i are known and K i P 30 k1 C;k B C;k i , a linear combination of the MRA spline coefficients β C . More rigorously, this can be seen if we restrict our likelihood to solely the Cariaco determinations, and change the ordering of Γ ν;β C ;θ and C θ B θ β e θ . Then, letting F K diagC θ K B θ K β e θ K be a diagonal matrix representing the current atmospheric calibration curve at the Cariaco calendar ages, our posterior for the MRA spline β C becomes</p>
        <p>Gibbs updating the smoothing parameter λjβGibbs updating the smoothing parameter λjβ</p>
        <p>As before, jβ GaA 0 ; B 0 where A 0 A rankD=2 andAs before, jβ GaA 0 ; B 0 where A 0 A rankD=2 and</p>
        <p>Gibbs updating the precision multipliers κjF; β; θ; ν; β C ; ϱ For each datum,Gibbs updating the precision multipliers κjF; β; θ; ν; β C ; ϱ For each datum,</p>
        <p>wherewhere</p>
        <p>2 is the current shape/rate prior for the dataset K to which datum i belongs.2 is the current shape/rate prior for the dataset K to which datum i belongs.</p>
        <p>MH updating the expected tail behavior ϱ K jκ For each dataset K:MH updating the expected tail behavior ϱ K jκ For each dataset K:</p>
        <p>• Propose new parameter % K 0 N% K ; prop ;• Propose new parameter % K 0 N% K ; prop ;</p>
        <p>• Accept according to Hastings Ratio:• Accept according to Hastings Ratio:</p>
        <p>wherewhere</p>
        <p>The approach to creating the older part of the calibration curve is considerably more computationally intensive than for the predominantly dendrodated section of curve. In this older section, we have to update the calendar ages of the constituent determinations requiring recalculation of the value of the curve each time. This becomes less feasible as the number of knots increases. Consequently, for this older section, we are not able to choose such a large number of knots for our spline basis as used in the dendrodated section. To select our basis for the older section of curve, we therefore began by placing 400 knots at quantiles (with the addition of a small amount of random jitter) of the observed/prior calendar ages T i . This meant that a knot was placed approximately every 5th observed calendar age, similar to the dendrodated section of curve. However, in order to make the fitted placement of the three Bølling-Allerød floating tree-ring sequences more equitable, we overrode this knot choice in the period from 14.2-15.2 cal kBP. In this period, 50 knots were placed evenly (i.e. one every 20 calendar years) and all other knots were removed. Tree-ring sequences P305u and P317 in particular show considerable variability in 14 C over their course; see Adolphi et al. (2017) and Figure 7. If the knots used to model the calibration curve over the potential fitted range of these trees lie unevenly in calendar age, as would be the case if we maintained a knot placement based upon the observed quantiles of the other IntCal20 data, the highly variable 14 C nature of these tree-ring sequences may have meant that the method had a preference/bias to locate them where there were more knots as opposed to where they best fitted the other data. Conversely placing knots at even calendar age spacings aims to remove this potential bias. One knot every 20 calendar years is still sufficient to pick out required curve detail in this period. Finally, we added into the spline bases the 5 knots required for merging with the tree-ring section of curve and removed other knots in this period. In total, after these modifications, we were left with 444 knots meaning that the overall method still maintained computational feasibility. These knot locations then remained fixed throughout the sampler, i.e. did not change as the calendar ages i were updated.The approach to creating the older part of the calibration curve is considerably more computationally intensive than for the predominantly dendrodated section of curve. In this older section, we have to update the calendar ages of the constituent determinations requiring recalculation of the value of the curve each time. This becomes less feasible as the number of knots increases. Consequently, for this older section, we are not able to choose such a large number of knots for our spline basis as used in the dendrodated section. To select our basis for the older section of curve, we therefore began by placing 400 knots at quantiles (with the addition of a small amount of random jitter) of the observed/prior calendar ages T i . This meant that a knot was placed approximately every 5th observed calendar age, similar to the dendrodated section of curve. However, in order to make the fitted placement of the three Bølling-Allerød floating tree-ring sequences more equitable, we overrode this knot choice in the period from 14.2-15.2 cal kBP. In this period, 50 knots were placed evenly (i.e. one every 20 calendar years) and all other knots were removed. Tree-ring sequences P305u and P317 in particular show considerable variability in 14 C over their course; see Adolphi et al. (2017) and Figure 7. If the knots used to model the calibration curve over the potential fitted range of these trees lie unevenly in calendar age, as would be the case if we maintained a knot placement based upon the observed quantiles of the other IntCal20 data, the highly variable 14 C nature of these tree-ring sequences may have meant that the method had a preference/bias to locate them where there were more knots as opposed to where they best fitted the other data. Conversely placing knots at even calendar age spacings aims to remove this potential bias. One knot every 20 calendar years is still sufficient to pick out required curve detail in this period. Finally, we added into the spline bases the 5 knots required for merging with the tree-ring section of curve and removed other knots in this period. In total, after these modifications, we were left with 444 knots meaning that the overall method still maintained computational feasibility. These knot locations then remained fixed throughout the sampler, i.e. did not change as the calendar ages i were updated.</p>
        <p>Tempering our target likelihood for the calendar age terms Tjθ (or alternatively the prior θ) concerning a particular dataset K, we chooseTempering our target likelihood for the calendar age terms Tjθ (or alternatively the prior θ) concerning a particular dataset K, we choose</p>
        <p>This is equivalent to the untempered version but with an adjusted covariance Ψ K;T 1 1 Ψ K;T . To run the chain at this temperature we simply adjust the calendar age covariance (for observed T or prior) accordingly. We can then perform all updates as described in Section 4.4.2 but with this modified calendar age covariance.This is equivalent to the untempered version but with an adjusted covariance Ψ K;T 1 1 Ψ K;T . To run the chain at this temperature we simply adjust the calendar age covariance (for observed T or prior) accordingly. We can then perform all updates as described in Section 4.4.2 but with this modified calendar age covariance.</p>
        <p>In tempering the likelihood for the observed F K for a particular dataset K, we selectIn tempering the likelihood for the observed F K for a particular dataset K, we select</p>
        <p>where for example, in a slight abuse of notation, Γ K ;θ K denotes the diagonal offset multiplier matrix for the specific dataset K, and will depend upon only K . In the case of the Cariaco Basin unvarved record, we would instead have Γ β C ;θ K . This is also equivalent to the untempered version but modifying all the F 14 C variances within dataset K to be 2 j 2 2 2 j for all j 2 K. To perform MCMC on the chain with this temperature, we make this modification and then all update steps remain as in Section 4.4.2 but with the temperature-adjusted F 14 C variances.where for example, in a slight abuse of notation, Γ K ;θ K denotes the diagonal offset multiplier matrix for the specific dataset K, and will depend upon only K . In the case of the Cariaco Basin unvarved record, we would instead have Γ β C ;θ K . This is also equivalent to the untempered version but modifying all the F 14 C variances within dataset K to be 2 j 2 2 2 j for all j 2 K. To perform MCMC on the chain with this temperature, we make this modification and then all update steps remain as in Section 4.4.2 but with the temperature-adjusted F 14 C variances.</p>
        <p>The amount of tempering applied is dependent upon the datasets. We apply most tempering to the two floating Bølling-Allerød tree-ring sequences P305u and P317 since their internal 14 C determinations are highly variable and we wish to fully explore their potential fitting location; IntCal20 Approach to 14 C Calibration Curve Construction 851 and also the Cariaco Basin unvarved record which has a large number of observations with high calendar age covariances where again we might otherwise be concerned about mixing. For these three datasets, we chose values of 2 that were equivalent, at the highest temperature of the four parallel chains, to increasing the uncertainty i on the F 14 C determinations by 36% from the quoted laboratory values. For the other data, the highest 2 temperature was equivalent to increasing i by 5%. We performed the same, small amount of tempering for the calendar ages as otherwise the acceptance rate for swapping between chains became very low. For all the data, the highest 1 temperature of the four chains was equivalent to increasing the calendar age covariance by 10%.The amount of tempering applied is dependent upon the datasets. We apply most tempering to the two floating Bølling-Allerød tree-ring sequences P305u and P317 since their internal 14 C determinations are highly variable and we wish to fully explore their potential fitting location; IntCal20 Approach to 14 C Calibration Curve Construction 851 and also the Cariaco Basin unvarved record which has a large number of observations with high calendar age covariances where again we might otherwise be concerned about mixing. For these three datasets, we chose values of 2 that were equivalent, at the highest temperature of the four parallel chains, to increasing the uncertainty i on the F 14 C determinations by 36% from the quoted laboratory values. For the other data, the highest 2 temperature was equivalent to increasing i by 5%. We performed the same, small amount of tempering for the calendar ages as otherwise the acceptance rate for swapping between chains became very low. For all the data, the highest 1 temperature of the four chains was equivalent to increasing the calendar age covariance by 10%.</p>
        <p>We ran the four tempered chains in parallel, each for 250,000 iterations. Four independent swaps between the tempered chains were proposed every 5th iteration. Convergence was assessed by initializing the chains at several different starting points and then visually observing the robustness in the curve estimates; along with assessment of other outputs such as the posterior estimates for the calendar ages of the floating tree-ring sequences and the spline based MRA of the Cariaco Basin; and the overall model likelihood. For a more thorough discussion of convergence and the effect of tempering, see the Supplementary Information. The level of tempering chosen was based upon a trade-off between maintaining a reasonable acceptance rate of chain swaps, and a highest temperature that both allowed the Bølling-Allerød trees P305u and P317 to move and gave a "highesttemperature" curve estimate that had removed most fine detail; again see the Supplementary Information. In total, there were 4407 accepted chain swaps, with 559 between the two lowest temperatures, i.e. swaps that involved the untempered target of true interest. To create the final curve, we removed the first half of the target chain as burn-in and then thinned to every 50th realization leaving us with 2500 posterior realizations covering the older time-period from which to create the final complete curve.We ran the four tempered chains in parallel, each for 250,000 iterations. Four independent swaps between the tempered chains were proposed every 5th iteration. Convergence was assessed by initializing the chains at several different starting points and then visually observing the robustness in the curve estimates; along with assessment of other outputs such as the posterior estimates for the calendar ages of the floating tree-ring sequences and the spline based MRA of the Cariaco Basin; and the overall model likelihood. For a more thorough discussion of convergence and the effect of tempering, see the Supplementary Information. The level of tempering chosen was based upon a trade-off between maintaining a reasonable acceptance rate of chain swaps, and a highest temperature that both allowed the Bølling-Allerød trees P305u and P317 to move and gave a "highesttemperature" curve estimate that had removed most fine detail; again see the Supplementary Information. In total, there were 4407 accepted chain swaps, with 559 between the two lowest temperatures, i.e. swaps that involved the untempered target of true interest. To create the final curve, we removed the first half of the target chain as burn-in and then thinned to every 50th realization leaving us with 2500 posterior realizations covering the older time-period from which to create the final complete curve.</p>
        <p>Due to our merging process, the posterior realizations from the two sections of the curve occurred in pairs with one another. Each realization from the older section had a matching realization from the tree-ring-only section. When combined, such a pair gives a complete curve realization extending from 0-55 cal kBP that is still, in itself, a cubic spline and hence smooth over the join between the two sections. After pairing these realizations, our MCMC provided 2500 plausible posterior realizations of complete Δ 14 C calibration curves, i.e. g j for j 1; ...; 2500, from 0-55 cal kBP.Due to our merging process, the posterior realizations from the two sections of the curve occurred in pairs with one another. Each realization from the older section had a matching realization from the tree-ring-only section. When combined, such a pair gives a complete curve realization extending from 0-55 cal kBP that is still, in itself, a cubic spline and hence smooth over the join between the two sections. After pairing these realizations, our MCMC provided 2500 plausible posterior realizations of complete Δ 14 C calibration curves, i.e. g j for j 1; ...; 2500, from 0-55 cal kBP.</p>
        <p>Additionally, each of these complete curves has an accompanying posterior estimate for the constant of proportionality in our (square-root) model for the over-dispersion/additional uncertainty linked with the specific tree-ring realization. To create the posterior mean and predictive variance IntCal summaries, each of these Δ 14 C curves, g j for j 1; ...; 2500, were transformed into the F 14 C domain. For each corresponding f j , and each calendar year on which output was desired, 50 predictive values of F 14 C were created by adding back in the estimated over-dispersion corresponding to that particular curve realization. These predictive values therefore incorporate the potential additional variability (beyond the reported lab uncertainty) one might expect to see in a new tree-ring 14 C determination to be calibrated: f jk N f j ; 2 j f j for k 1; ...; 50:Additionally, each of these complete curves has an accompanying posterior estimate for the constant of proportionality in our (square-root) model for the over-dispersion/additional uncertainty linked with the specific tree-ring realization. To create the posterior mean and predictive variance IntCal summaries, each of these Δ 14 C curves, g j for j 1; ...; 2500, were transformed into the F 14 C domain. For each corresponding f j , and each calendar year on which output was desired, 50 predictive values of F 14 C were created by adding back in the estimated over-dispersion corresponding to that particular curve realization. These predictive values therefore incorporate the potential additional variability (beyond the reported lab uncertainty) one might expect to see in a new tree-ring 14 C determination to be calibrated: f jk N f j ; 2 j f j for k 1; ...; 50:</p>
        <p>Here j is the estimated constant of proportionality corresponding to curve realization j. These 125,000 predictive F 14 C values (50 for each of the 2500 curve realizations) were transformed into radiocarbon ages and finally summarized by pointwise means and variances to form the published IntCal20. The difference between using the thinned set of 2500 realizations rather than the entire post-burn-in sample was negligible. However, this thinning allows us to retain a practical set of complete curve realizations for later independent use which would give the same calibrated ages of individual 14 C determinations as the published IntCal20 summaries. Each of these 2500 realizations has been stored for potential future extraction of covariance information and to enable more precise age modeling should it be desired.Here j is the estimated constant of proportionality corresponding to curve realization j. These 125,000 predictive F 14 C values (50 for each of the 2500 curve realizations) were transformed into radiocarbon ages and finally summarized by pointwise means and variances to form the published IntCal20. The difference between using the thinned set of 2500 realizations rather than the entire post-burn-in sample was negligible. However, this thinning allows us to retain a practical set of complete curve realizations for later independent use which would give the same calibrated ages of individual 14 C determinations as the published IntCal20 summaries. Each of these 2500 realizations has been stored for potential future extraction of covariance information and to enable more precise age modeling should it be desired.</p>
        <p>5.1 Over-Dispersion5.1 Over-Dispersion</p>
        <p>To select both the appropriate scaling model for the level of over-dispersion present within treering 14 C determinations and a suitable prior on its size, we investigated the over-dispersion seen in comparisons of same-tree-ring determinations made across laboratories taken from SIRI (the sixth radiocarbon laboratory intercomparison exercise of Scott et al. 2017). This SIRI data consisted of repeat measurements of trees from two distinct time periods-3 trees from a period about 300 14 C yrs BP; and 2 trees from a period about 10,000 14 C yrs BP. Each of the trees had about 80 such repeat measurements. For each tree we fitted a Bayesian modelTo select both the appropriate scaling model for the level of over-dispersion present within treering 14 C determinations and a suitable prior on its size, we investigated the over-dispersion seen in comparisons of same-tree-ring determinations made across laboratories taken from SIRI (the sixth radiocarbon laboratory intercomparison exercise of Scott et al. 2017). This SIRI data consisted of repeat measurements of trees from two distinct time periods-3 trees from a period about 300 14 C yrs BP; and 2 trees from a period about 10,000 14 C yrs BP. Each of the trees had about 80 such repeat measurements. For each tree we fitted a Bayesian model</p>
        <p>where " i N0; 2 i is the laboratory reported uncertainty and i N0; 2 i is the additive error which models the potential over-dispersion present in our 14 C observations. We considered three potential scaling models for the form of i , the standard deviation of the additional uncertainty on F i : a constant model where i for all i; a model where it scales linearly with the underlying value of F 14 C, i.e. i f i ; and a model where it scales with the square-root of F 14 C, i.e. i f i p .where " i N0; 2 i is the laboratory reported uncertainty and i N0; 2 i is the additive error which models the potential over-dispersion present in our 14 C observations. We considered three potential scaling models for the form of i , the standard deviation of the additional uncertainty on F i : a constant model where i for all i; a model where it scales linearly with the underlying value of F 14 C, i.e. i f i ; and a model where it scales with the square-root of F 14 C, i.e. i f i p .</p>
        <p>To investigate the suitability of each potential scaling model, we fitted them to the recent and the older sets of SIRI trees separately and compared the posterior estimates for the constant of proportionality obtained in each. The most suitable scaling model was taken as the one that provides the same posterior for this parameter when the model is fitted to the older trees as for when it is fitted to the recent trees. The results of fitting all three scaling models can be seen in the Supplementary Information. The optimal model was clearly seen to be the square-root scaling. Figure 4 provides the posterior estimates under this square-root scaling model on each of the two time periods separately; as we can see the estimates of the proportionality parameter we get under this scaling model are fairly similar and show significant levels of over-dispersion in the SIRI data.To investigate the suitability of each potential scaling model, we fitted them to the recent and the older sets of SIRI trees separately and compared the posterior estimates for the constant of proportionality obtained in each. The most suitable scaling model was taken as the one that provides the same posterior for this parameter when the model is fitted to the older trees as for when it is fitted to the recent trees. The results of fitting all three scaling models can be seen in the Supplementary Information. The optimal model was clearly seen to be the square-root scaling. Figure 4 provides the posterior estimates under this square-root scaling model on each of the two time periods separately; as we can see the estimates of the proportionality parameter we get under this scaling model are fairly similar and show significant levels of over-dispersion in the SIRI data.</p>
        <p>This square-root scaling model for the additional uncertainty, i.e. i N0; 2 f i was therefore incorporated into curve construction. To provide a prior on , the constant of proportionality, to take forward into IntCal the SIRI data were combined before the overdispersion model was refitted to them. The posterior mean and variance of the resultant joint estimate for were then used to provide our prior: Note that this SIRI data does not itself enter into IntCal and so provides a genuinely independent prior on .This square-root scaling model for the additional uncertainty, i.e. i N0; 2 f i was therefore incorporated into curve construction. To provide a prior on , the constant of proportionality, to take forward into IntCal the SIRI data were combined before the overdispersion model was refitted to them. The posterior mean and variance of the resultant joint estimate for were then used to provide our prior: Note that this SIRI data does not itself enter into IntCal and so provides a genuinely independent prior on .</p>
        <p>In Figure 5 we plot the posterior estimate for the constant of proportionality after curve construction. Due to the large volume of IntCal data, this posterior is largely determined by the IntCal tree-ring determinations which dominates the SIRI-informed prior. This posterior estimate incorporates both additional variation between the IntCal laboratories and other potential sources of variation in the 14 C measurement of tree rings such as location and species. This IntCal-based posterior for the level of over-dispersion is considerably smaller than the SIRI prior, approximately one fifth of the size. This is to be expected since the IntCal data is screened and the SIRI data covered a large number of laboratories. The difference between the SIRI-informed prior and the IntCal-based Figure 5 The posterior for the level of over-dispersion in the tree-ring determinations within the IntCal data. We model the additional variation, in the F 14 C domain, as i N0; 2 f i .In Figure 5 we plot the posterior estimate for the constant of proportionality after curve construction. Due to the large volume of IntCal data, this posterior is largely determined by the IntCal tree-ring determinations which dominates the SIRI-informed prior. This posterior estimate incorporates both additional variation between the IntCal laboratories and other potential sources of variation in the 14 C measurement of tree rings such as location and species. This IntCal-based posterior for the level of over-dispersion is considerably smaller than the SIRI prior, approximately one fifth of the size. This is to be expected since the IntCal data is screened and the SIRI data covered a large number of laboratories. The difference between the SIRI-informed prior and the IntCal-based Figure 5 The posterior for the level of over-dispersion in the tree-ring determinations within the IntCal data. We model the additional variation, in the F 14 C domain, as i N0; 2 f i .</p>
        <p>854 T J Heaton et al.854 T J Heaton et al.</p>
        <p>posterior does however give some indication as to the much greater range of over-dispersion that might exist within uncalibrated determinations and emphasizes the need for laboratories to engage with intercomparison exercises and provide appropriate assessments of measurement uncertainty before determinations are calibrated against IntCal20 in order to avoid overconfident dating.posterior does however give some indication as to the much greater range of over-dispersion that might exist within uncalibrated determinations and emphasizes the need for laboratories to engage with intercomparison exercises and provide appropriate assessments of measurement uncertainty before determinations are calibrated against IntCal20 in order to avoid overconfident dating.</p>
        <p>5.2 MRAs and Dead Carbon Fractions5.2 MRAs and Dead Carbon Fractions</p>
        <p>For each marine and speleothem dataset, the offset to the Northern Hemisphere atmosphere in terms of radiocarbon age was modeled as Speleothems dcfs :For each marine and speleothem dataset, the offset to the Northern Hemisphere atmosphere in terms of radiocarbon age was modeled as Speleothems dcfs :</p>
        <p>Here K are our location-specific MRA estimates obtained from the LSG model in its "GS" scenario (Butzin et al. 2020 in this issue) forced a preliminary estimate of atmospheric Δ 14 C using the same Bayesian spline and errors-in-variables approach as IntCal20 but based only upon data from Hulu Cave. To complete our offset model, we are required to specify K , the variability in MRA/dcf offset from one calendar year to the next beyond that already present in the LSG/constant dcf model. We must also provide suitable values for K and ! K which determine our prior on K , the mean dcf/coastal shift. Note we only place a prior on this shift K , its precise value will be updated during curve construction.Here K are our location-specific MRA estimates obtained from the LSG model in its "GS" scenario (Butzin et al. 2020 in this issue) forced a preliminary estimate of atmospheric Δ 14 C using the same Bayesian spline and errors-in-variables approach as IntCal20 but based only upon data from Hulu Cave. To complete our offset model, we are required to specify K , the variability in MRA/dcf offset from one calendar year to the next beyond that already present in the LSG/constant dcf model. We must also provide suitable values for K and ! K which determine our prior on K , the mean dcf/coastal shift. Note we only place a prior on this shift K , its precise value will be updated during curve construction.</p>
        <p>Most of the offset datasets have additional younger observations in the IntCal database which overlap with the tree-ring-only part of the calibration curve from 0-14 cal kBP that has already been created. These, more recent, offset observations are not themselves used to create the IntCal curve but, by comparison with these overlapping atmospheric tree-rings, can independently inform us on suitable values for these three parameters. We therefore fitted statistical models for the offset of the form above and estimated the various components for each set. In Figure 6 we show the results for both the Hulu Cave H82 speleothem (Southon et al. 2012) and the Kiritimati corals (Fairbanks et al. 2005). For H82, we see that the estimated mean dcf value in this period of overlap is approximately 480 ± 8 14 C yrs 1 and that we need to add a further independent uncertainty of 50 14 C yrs to the determinations to make them consistent with the dendrodated trees. For the H82 speleothem data used for the older part of the curve, we therefore select K 480;! K 8 and K 50 14 C yrs. For the Kiritimati corals, where we have data which overlap with atmospheric trees, we need to shift the LSG estimate down by approximately 280 ± 11 14 C yrs (1) and add a similar independent uncertainty of 50 14 C yrs to the determinations, i.e. we would select K 280, ! K 11 and K 50 14 C yrs. Comparing the plotted red curves, representing the constant dcf model for Hulu and the shifted LSG output for Kiritimati, we see reasonable agreement with the raw data suggesting our approach to modeling the offsets is appropriate. These comparisons of overlap also give some insight into how different types of indirect records might perform as recorders of the atmospheric 14 C, and where more modeling might be needed to understand the processes they undergo. K , determine our prior on K . The bottom plots show an estimate of the additional variability needed for the determinations to be consistent with the tree rings. We set K to be the mean of these values.Most of the offset datasets have additional younger observations in the IntCal database which overlap with the tree-ring-only part of the calibration curve from 0-14 cal kBP that has already been created. These, more recent, offset observations are not themselves used to create the IntCal curve but, by comparison with these overlapping atmospheric tree-rings, can independently inform us on suitable values for these three parameters. We therefore fitted statistical models for the offset of the form above and estimated the various components for each set. In Figure 6 we show the results for both the Hulu Cave H82 speleothem (Southon et al. 2012) and the Kiritimati corals (Fairbanks et al. 2005). For H82, we see that the estimated mean dcf value in this period of overlap is approximately 480 ± 8 14 C yrs 1 and that we need to add a further independent uncertainty of 50 14 C yrs to the determinations to make them consistent with the dendrodated trees. For the H82 speleothem data used for the older part of the curve, we therefore select K 480;! K 8 and K 50 14 C yrs. For the Kiritimati corals, where we have data which overlap with atmospheric trees, we need to shift the LSG estimate down by approximately 280 ± 11 14 C yrs (1) and add a similar independent uncertainty of 50 14 C yrs to the determinations, i.e. we would select K 280, ! K 11 and K 50 14 C yrs. Comparing the plotted red curves, representing the constant dcf model for Hulu and the shifted LSG output for Kiritimati, we see reasonable agreement with the raw data suggesting our approach to modeling the offsets is appropriate. These comparisons of overlap also give some insight into how different types of indirect records might perform as recorders of the atmospheric 14 C, and where more modeling might be needed to understand the processes they undergo. K , determine our prior on K . The bottom plots show an estimate of the additional variability needed for the determinations to be consistent with the tree rings. We set K to be the mean of these values.</p>
        <p>Some datasets did not have any 14 C determinations that were more recent than 14 cal kBP and so no information was available on overlap with the tree rings. For the Mururoa corals (Bard et al. 1998) and the Iberian and Pakistan Margins (Bard et al. 2013), we considered that the additional uncertainty K , to account for MRA variability beyond that seen in LSG, was 150 14 C yrs to match coral data on which we did have overlap. We also placed an uninformative prior on the LSG coastal shift K . This keeps the internal shape of the dataset but allows it to jointly move up/down together to fit the other records. The other two Hulu Cave speleothems (MSD and MSL in Cheng et al. 2018) were modeled as having separate and independent dcfs from the H82 speleothem but each having the same prior on its mean and the same additional variability K .Some datasets did not have any 14 C determinations that were more recent than 14 cal kBP and so no information was available on overlap with the tree rings. For the Mururoa corals (Bard et al. 1998) and the Iberian and Pakistan Margins (Bard et al. 2013), we considered that the additional uncertainty K , to account for MRA variability beyond that seen in LSG, was 150 14 C yrs to match coral data on which we did have overlap. We also placed an uninformative prior on the LSG coastal shift K . This keeps the internal shape of the dataset but allows it to jointly move up/down together to fit the other records. The other two Hulu Cave speleothems (MSD and MSL in Cheng et al. 2018) were modeled as having separate and independent dcfs from the H82 speleothem but each having the same prior on its mean and the same additional variability K .</p>
        <p>The Hoffman speleothem (Hoffmann et al. 2010) consists of two separated sections-a set of relatively young data for which there was some overlap with the trees, and then a set of much older measurements. These were split in curve creation and a separate dcf was applied to the older measurements. For the more recent section, the prior for the mean dcf K was based on the overlapping determinations, while an uninformative prior was placed on the mean dcf for the older measurements. Again this keeps the internal shape/structure but allows the dataset to jointly move up/down to fit best with the other material.The Hoffman speleothem (Hoffmann et al. 2010) consists of two separated sections-a set of relatively young data for which there was some overlap with the trees, and then a set of much older measurements. These were split in curve creation and a separate dcf was applied to the older measurements. For the more recent section, the prior for the mean dcf K was based on the overlapping determinations, while an uninformative prior was placed on the mean dcf for the older measurements. Again this keeps the internal shape/structure but allows the dataset to jointly move up/down to fit best with the other material.</p>
        <p>Each iteration of our MCMC sampler generates a curve realization, i.e. what it believes is a plausible record of atmospheric 14 C history. To create the published IntCal20 posterior mean and variance for the calibration curve a large set of these individual realizations are summarized/averaged on a calendar year by calendar year (pointwise) basis. In creating these pointwise summaries we lose potentially valuable curve information, in particular on the dependence in the value of the curve from one calendar age to the next. When we calibrate against IntCal20 using its pointwise means and variances we assume that the value of the curve is independent from one calendar year to the next, i.e. that the curve could potentially switch from being towards the top of its probability interval in one calendar year, to being towards the bottom in the next. For the calibration of single 14 C determinations, losing such covariance information has no effect upon the calibrated age estimate one obtains. However, covariance may matter when one is jointly calibrating multiple determinations, for example when wiggle matching or incorporating them into a more complex model. In such instances, one may therefore wish to consider the individual realizations since they do contain covariance information. Furthermore, and importantly, the pointwise IntCal20 posterior mean should not be viewed in itself as a realization, i.e. it is not itself a potential 14 C history, but rather an average of many.Each iteration of our MCMC sampler generates a curve realization, i.e. what it believes is a plausible record of atmospheric 14 C history. To create the published IntCal20 posterior mean and variance for the calibration curve a large set of these individual realizations are summarized/averaged on a calendar year by calendar year (pointwise) basis. In creating these pointwise summaries we lose potentially valuable curve information, in particular on the dependence in the value of the curve from one calendar age to the next. When we calibrate against IntCal20 using its pointwise means and variances we assume that the value of the curve is independent from one calendar year to the next, i.e. that the curve could potentially switch from being towards the top of its probability interval in one calendar year, to being towards the bottom in the next. For the calibration of single 14 C determinations, losing such covariance information has no effect upon the calibrated age estimate one obtains. However, covariance may matter when one is jointly calibrating multiple determinations, for example when wiggle matching or incorporating them into a more complex model. In such instances, one may therefore wish to consider the individual realizations since they do contain covariance information. Furthermore, and importantly, the pointwise IntCal20 posterior mean should not be viewed in itself as a realization, i.e. it is not itself a potential 14 C history, but rather an average of many.</p>
        <p>We illustrate the difference between an individual posterior realization and the pointwise summarized values in Figure 7. Here we show, in color, four individual curve realizations compared to the summarized pointwise mean values (typically what is reported as the IntCal curve) with their 95% predictive intervals plotted in black. As can be seen the individual curve realizations are considerably more variable than the summarized IntCal mean. Individual realizations are always likely to exhibit significantly more short-term variability than the pointwise mean IntCal20 curve due to the averaging process by which the latter is obtained. This is most evident in periods where the underlying data is sparse IntCal20 Approach to 14 C Calibration Curve Construction 857 https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press and consequently we have little information on the true value of the curve. In these periods each individual posterior realization will retain similar levels of internal variation/wiggliness as seen in other periods of the curve, as it has been informed that this is the kind of variation expected, but will place these wiggles at slightly different times due to the lack of data. However, once these individual realizations have been averaged over, the pointwise mean summary will tend towards interpolation and hence appear smooth.We illustrate the difference between an individual posterior realization and the pointwise summarized values in Figure 7. Here we show, in color, four individual curve realizations compared to the summarized pointwise mean values (typically what is reported as the IntCal curve) with their 95% predictive intervals plotted in black. As can be seen the individual curve realizations are considerably more variable than the summarized IntCal mean. Individual realizations are always likely to exhibit significantly more short-term variability than the pointwise mean IntCal20 curve due to the averaging process by which the latter is obtained. This is most evident in periods where the underlying data is sparse IntCal20 Approach to 14 C Calibration Curve Construction 857 https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press and consequently we have little information on the true value of the curve. In these periods each individual posterior realization will retain similar levels of internal variation/wiggliness as seen in other periods of the curve, as it has been informed that this is the kind of variation expected, but will place these wiggles at slightly different times due to the lack of data. However, once these individual realizations have been averaged over, the pointwise mean summary will tend towards interpolation and hence appear smooth.</p>
        <p>In addition to this reduction in short-term variability, where the constituent IntCal data have uncertain calendar ages the posterior IntCal pointwise mean may also appear to smooth out more significant features. Such an effect can be seen in Figure 7 where the inversion in the mean curve around 14.75 cal kBP (corresponding to floating tree-ring sequence P317) is reduced compared with both the observed data and the individual realizations. In such instances where underlying data have uncertain calendar ages, each iteration of the sampler will have posited a particular set of potential true calendar ages for these determinations, and the resultant realization will be a plausible 14 C history on that basis. Each individual realization may therefore retain the significant feature but, once the differing calendar ages at which that feature may occur are averaged over, it may appear reduced in the pointwise mean summary. This can also be seen in Figure 7 where all the individual realizations show a more significant inversion than the pointwise IntCal20 mean, but located at slightly different times to account for uncertainty in the true calendar age of tree P317.In addition to this reduction in short-term variability, where the constituent IntCal data have uncertain calendar ages the posterior IntCal pointwise mean may also appear to smooth out more significant features. Such an effect can be seen in Figure 7 where the inversion in the mean curve around 14.75 cal kBP (corresponding to floating tree-ring sequence P317) is reduced compared with both the observed data and the individual realizations. In such instances where underlying data have uncertain calendar ages, each iteration of the sampler will have posited a particular set of potential true calendar ages for these determinations, and the resultant realization will be a plausible 14 C history on that basis. Each individual realization may therefore retain the significant feature but, once the differing calendar ages at which that feature may occur are averaged over, it may appear reduced in the pointwise mean summary. This can also be seen in Figure 7 where all the individual realizations show a more significant inversion than the pointwise IntCal20 mean, but located at slightly different times to account for uncertainty in the true calendar age of tree P317.</p>
        <p>It is important to stress that, for calibration of single determinations, users do not need to consider more than the pointwise mean and variance summaries for accurate calibration. These pointwise values are sufficient and provide the simplest way for a calibration user with a single 14 C determination to obtain the correct calendar dates within current calibration software. We therefore have retained these summary values as our published output for 
            <rs type="software">IntCal</rs>20. However, for those wishing to obtain ultimate accuracy in jointly modeling multiple 14 C determinations, or if interest is in atmospheric 14 C variability, individual curve realizations may provide more insight. The IntCal group is currently discussing how best to provide this additional information.
        </p>
        <p>Our Bayesian approach, in addition to creating a calibration curve, also provides posterior estimates for all of the other parameters entered into the model, many of which are of potential interest in their own right. These parameters include the level of over-dispersion seen within tree-ring 14 C determinations of the same calendar age as already discussed; the posterior MRA for the Cariaco Basin, which is shown in Hughen and Heaton (2020 in this issue); and the calendar ages i of all our uncertain age data. Figure 7 provides an illustration of the type of posterior estimates for the calendar ages i we obtain. The points overlaid in panel (a), shown with 1 14 C uncertainty bars, represent the three floating Bølling-Allerød tree-ring sequences. While these trees enter the curve with no information as to their true calendar age , they are themselves calibrated simultaneously to creating the curve according to how they best fit with the rest of the data. After curve construction we therefore obtain a posterior estimate for their true calendar ages. The plotted locations of the trees in panel (a) represent their posterior mean calendar ages. However, we obtain a complete posterior distribution for their calibrated ages. Panel (b) presents a histogram of the posterior calibrated age estimate for specific tree P305u. As we can see, this posterior estimate is bimodal suggesting two potential fitting locations-this potential for multimodality motivates our use of MCMC tempering. We generate posterior calibrated age estimates i for all our data in a similar way. See the Supplementary Information for posterior calendar age estimates for all our floating tree-ring sequences.Our Bayesian approach, in addition to creating a calibration curve, also provides posterior estimates for all of the other parameters entered into the model, many of which are of potential interest in their own right. These parameters include the level of over-dispersion seen within tree-ring 14 C determinations of the same calendar age as already discussed; the posterior MRA for the Cariaco Basin, which is shown in Hughen and Heaton (2020 in this issue); and the calendar ages i of all our uncertain age data. Figure 7 provides an illustration of the type of posterior estimates for the calendar ages i we obtain. The points overlaid in panel (a), shown with 1 14 C uncertainty bars, represent the three floating Bølling-Allerød tree-ring sequences. While these trees enter the curve with no information as to their true calendar age , they are themselves calibrated simultaneously to creating the curve according to how they best fit with the rest of the data. After curve construction we therefore obtain a posterior estimate for their true calendar ages. The plotted locations of the trees in panel (a) represent their posterior mean calendar ages. However, we obtain a complete posterior distribution for their calibrated ages. Panel (b) presents a histogram of the posterior calibrated age estimate for specific tree P305u. As we can see, this posterior estimate is bimodal suggesting two potential fitting locations-this potential for multimodality motivates our use of MCMC tempering. We generate posterior calibrated age estimates i for all our data in a similar way. See the Supplementary Information for posterior calendar age estimates for all our floating tree-ring sequences.</p>
        <p>Our revised approach to the construction of the IntCal calibration curve offers several distinct advantages over the previous random walk approach used for IntCal09 and IntCal13 (Heaton et al. 2009;Niu et al. 2013). The use of Bayesian splines maintains a method of equal theoretical rigour and still fits within the desired Bayesian framework that is now universal for 14 C calibration. However the increased speed with which the Bayesian spline curve can be constructed enables much greater practical flexibility in our modeling choices, and more detailed investigation of the effect of key elements on the final curve. The main reason for this increase in speed is a change to the MCMC. While, as in the random walk approach, the Bayesian spline fits using Metropolis-within-Gibbs, the critical step of updating the curve itself is performed via Gibbs and updates the entire curve simultaneously. Conversely, the random walk approach updated the curve one calendar year at a time and via Metropolis-Hastings which led to mixing issues and extremely slow convergence.Our revised approach to the construction of the IntCal calibration curve offers several distinct advantages over the previous random walk approach used for IntCal09 and IntCal13 (Heaton et al. 2009;Niu et al. 2013). The use of Bayesian splines maintains a method of equal theoretical rigour and still fits within the desired Bayesian framework that is now universal for 14 C calibration. However the increased speed with which the Bayesian spline curve can be constructed enables much greater practical flexibility in our modeling choices, and more detailed investigation of the effect of key elements on the final curve. The main reason for this increase in speed is a change to the MCMC. While, as in the random walk approach, the Bayesian spline fits using Metropolis-within-Gibbs, the critical step of updating the curve itself is performed via Gibbs and updates the entire curve simultaneously. Conversely, the random walk approach updated the curve one calendar year at a time and via Metropolis-Hastings which led to mixing issues and extremely slow convergence.</p>
        <p>For users, the main differences in using the IntCal20 curves will be found through an increase in the level of annual detail in the curve; increased wiggliness in the curve as we extend back in time; and the introduction of prediction intervals which recognize potential additional sources of variation in observed 14 C determinations beyond that occurring in laboratory measurement.For users, the main differences in using the IntCal20 curves will be found through an increase in the level of annual detail in the curve; increased wiggliness in the curve as we extend back in time; and the introduction of prediction intervals which recognize potential additional sources of variation in observed 14 C determinations beyond that occurring in laboratory measurement.</p>
        <p>For the first time, our new approach is also able to generate complete sets of plausible calibration curves that span from 0-55 cal kBP allowing access to covariance information. For joint calibration of multiple determinations, such as when analysing the time elapsed between events, this covariance information has the potential to offer more detailed insight. The IntCal working group is currently in discussion about how to best disseminate this covariance information and is planning to provide a guide as to how it could be used within calibration. Including such covariance is likely to require modifications to current calibration software. These realizations, with their covariance information, have however already been used in the creation of Marine20 (Heaton et al. 2020 in this issue) to rigorously propagate our atmospheric uncertainties through to the marine calibration curve.For the first time, our new approach is also able to generate complete sets of plausible calibration curves that span from 0-55 cal kBP allowing access to covariance information. For joint calibration of multiple determinations, such as when analysing the time elapsed between events, this covariance information has the potential to offer more detailed insight. The IntCal working group is currently in discussion about how to best disseminate this covariance information and is planning to provide a guide as to how it could be used within calibration. Including such covariance is likely to require modifications to current calibration software. These realizations, with their covariance information, have however already been used in the creation of Marine20 (Heaton et al. 2020 in this issue) to rigorously propagate our atmospheric uncertainties through to the marine calibration curve.</p>
        <p>Further work is needed in several areas. Currently the methodology splits curve construction into two sections. Firstly, we create the curve back to approximately 14 cal kBP based upon only tree-ring determinations. The older section, based upon a wider range of 14 C material, is then created conditional on this more recent section. Future work aims to combine these two steps into a single integrated approach. Our current approach to screening potential data outliers, based on comparison to a curve created using all determinations, is also somewhat influenced by data volume. A large, or dense, dataset will dominate the curve and so is less likely to be identified as an outlier than a shorter, or sparser, dataset. An alternative approach might consider the observed residuals for each set when compared against a curve estimate that excludes the particular set under consideration. This would however be considerably more computationally intensive. We also suggest valuable work would be to investigate potential causes of the over-dispersion seen in the data, e.g. whether this variability is shared by data coming from the same location, tree species, or laboratory. Some preliminary work in this area has already been done by Hogg et al. (2019). Further insight would improve our ability to calibrate multiple determinations such as when wiggle matching a series of radiocarbon determinations from the same tree and laboratory. Finally further work is also needed to resolve the MRA within the Cariaco Basin.Further work is needed in several areas. Currently the methodology splits curve construction into two sections. Firstly, we create the curve back to approximately 14 cal kBP based upon only tree-ring determinations. The older section, based upon a wider range of 14 C material, is then created conditional on this more recent section. Future work aims to combine these two steps into a single integrated approach. Our current approach to screening potential data outliers, based on comparison to a curve created using all determinations, is also somewhat influenced by data volume. A large, or dense, dataset will dominate the curve and so is less likely to be identified as an outlier than a shorter, or sparser, dataset. An alternative approach might consider the observed residuals for each set when compared against a curve estimate that excludes the particular set under consideration. This would however be considerably more computationally intensive. We also suggest valuable work would be to investigate potential causes of the over-dispersion seen in the data, e.g. whether this variability is shared by data coming from the same location, tree species, or laboratory. Some preliminary work in this area has already been done by Hogg et al. (2019). Further insight would improve our ability to calibrate multiple determinations such as when wiggle matching a series of radiocarbon determinations from the same tree and laboratory. Finally further work is also needed to resolve the MRA within the Cariaco Basin.</p>
        <p>https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Presshttps://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press</p>
        <p>https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press f K K CB T β E ; and so θ; β;;ν; β C ; κ; ϱjF; T ∝ TjθFjθ; β; ν; β Chttps://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press f K K CB T β E ; and so θ; β;;ν; β C ; κ; ϱjF; T ∝ TjθFjθ; β; ν; β C</p>
        <p>https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Presshttps://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press</p>
        <p>https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Presshttps://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press</p>
        <p>https://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Presshttps://doi.org/10.1017/RDC.2020.46 Published online by Cambridge University Press</p>
        <p>The IntCal database contains tree-ring determinations extending back to 14,189 cal BP all of which are used to create a predominantly dendrodated tree-ring-only curve from 0-14,189 cal BP. However, beyond 13,913 cal BP the density of this data was not sufficient to estimate the curve precisely. Hence, we merge the predominantly dendrodated curve section with the older section at 13,913 cal BP.The IntCal database contains tree-ring determinations extending back to 14,189 cal BP all of which are used to create a predominantly dendrodated tree-ring-only curve from 0-14,189 cal BP. However, beyond 13,913 cal BP the density of this data was not sufficient to estimate the curve precisely. Hence, we merge the predominantly dendrodated curve section with the older section at 13,913 cal BP.</p>
        <p>We would like to thank two reviewers for helpful comments which have improved the quality of this paper. T.J. Heaton is supported by Leverhulme Trust Fellowship RF-2019-140\9, "Improving the Measurement of Time Using Radiocarbon."We would like to thank two reviewers for helpful comments which have improved the quality of this paper. T.J. Heaton is supported by Leverhulme Trust Fellowship RF-2019-140\9, "Improving the Measurement of Time Using Radiocarbon."</p>
        <p>To view supplementary material for this article, please visit https://doi.org/10.1017/RDC. 2020.46To view supplementary material for this article, please visit https://doi.org/10.1017/RDC. 2020.46</p>
    </text>
</tei>

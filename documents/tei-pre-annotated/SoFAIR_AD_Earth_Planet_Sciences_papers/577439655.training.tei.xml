<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T16:05+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>The emerging concept of digital twins outlines the pathway towards intelligent buildings. Although abundant building data carries an overwhelming amount of information, if not well exploited, the redundant and irrelevant data dimensions result in the overfitting problem and heavy computational load. Taking the fault detection and diagnosis process for building HVAC systems as the case, this paper adopts a symbolic artificial intelligence technique to identify informative sensory dimensions for building-specific faults by exploring the symbolic representation of labelled time-series. To preserve this ad-hoc temporal knowledge in the digital twin ecosystem, machine-readable fault tags are defined to label corresponding sensor entities. A digital twin data platform is developed to annotate the real-time data with fault tags and produce filtered low-latency data streams associated with a specified tag to automate this process. This paper describes a digital twin-based approach to automatically identify and pick up informative data to support dynamic asset management.The emerging concept of digital twins outlines the pathway towards intelligent buildings. Although abundant building data carries an overwhelming amount of information, if not well exploited, the redundant and irrelevant data dimensions result in the overfitting problem and heavy computational load. Taking the fault detection and diagnosis process for building HVAC systems as the case, this paper adopts a symbolic artificial intelligence technique to identify informative sensory dimensions for building-specific faults by exploring the symbolic representation of labelled time-series. To preserve this ad-hoc temporal knowledge in the digital twin ecosystem, machine-readable fault tags are defined to label corresponding sensor entities. A digital twin data platform is developed to annotate the real-time data with fault tags and produce filtered low-latency data streams associated with a specified tag to automate this process. This paper describes a digital twin-based approach to automatically identify and pick up informative data to support dynamic asset management.</p>
        <p>People living in modern society spend a considerable amount of time indoors. For example, the US Environmental Protection Agency reported that Americans spend, on average, 87% of their time inside buildings [1]. Comparably, UK adults spend an average of 22 h a day, amounting to over 90% of their time, in enclosed spaces [2]. The orchestration of diverse building systems is imperative to create an optimal indoor environment in modern facilities. To support this, the recent digital transformation of buildings has pushed building operations from conventional and programmatic to responsive and intelligent [3]. Different building data from disparate sources allow complex building systems to be intelligent, leveraging ubiquitous sensing capability and computing power [4].People living in modern society spend a considerable amount of time indoors. For example, the US Environmental Protection Agency reported that Americans spend, on average, 87% of their time inside buildings [1]. Comparably, UK adults spend an average of 22 h a day, amounting to over 90% of their time, in enclosed spaces [2]. The orchestration of diverse building systems is imperative to create an optimal indoor environment in modern facilities. To support this, the recent digital transformation of buildings has pushed building operations from conventional and programmatic to responsive and intelligent [3]. Different building data from disparate sources allow complex building systems to be intelligent, leveraging ubiquitous sensing capability and computing power [4].</p>
        <p>As one of the enablers of building intelligence, a digital twin is suitable for replicating, predicting, and optimising the conditions and behaviours of assets (i.e., buildings, systems and components) during their lifecycle. According to [5], the digital twin is a set of virtual information constructs that fully describes a potential or actual physical manufactured product from the micro atomic level to the macro geometric level. In the Architecture, Engineering, Construction, and Operation (AECO) industry, Boje et al. [6] defined the ability of construction Exchange (COBie) spreadsheets] and dynamic (e.g., Internet of Things devices, building management systems), that can be used to enable building intelligence. It is a common practice to decompose the building systems into 'subsystems' (i.e., 'divide-and-conquer' strategy) and gain localised insights from disaggregated datasets. It ensures efficient decision-making for asset management in a coordinated way under the digital twin analytical framework [12]. The disaggregation, as the adhoc knowledge aiming at picking up informative and representative data to drive the decision-making, needs to be incorporated into the digital twin ecosystem in a machine-readable way.As one of the enablers of building intelligence, a digital twin is suitable for replicating, predicting, and optimising the conditions and behaviours of assets (i.e., buildings, systems and components) during their lifecycle. According to [5], the digital twin is a set of virtual information constructs that fully describes a potential or actual physical manufactured product from the micro atomic level to the macro geometric level. In the Architecture, Engineering, Construction, and Operation (AECO) industry, Boje et al. [6] defined the ability of construction Exchange (COBie) spreadsheets] and dynamic (e.g., Internet of Things devices, building management systems), that can be used to enable building intelligence. It is a common practice to decompose the building systems into 'subsystems' (i.e., 'divide-and-conquer' strategy) and gain localised insights from disaggregated datasets. It ensures efficient decision-making for asset management in a coordinated way under the digital twin analytical framework [12]. The disaggregation, as the adhoc knowledge aiming at picking up informative and representative data to drive the decision-making, needs to be incorporated into the digital twin ecosystem in a machine-readable way.</p>
        <p>This study aims to demonstrate a digital twin enabled Fault Detection and Diagnosis (FDD) process, which aims at identifying abnormalities in building Heating, Ventilation and Air Conditioning (HVAC) systems to prevent poor indoor air quality, thermal discomfort, and low productivity [13]. With semantic web technologies [14], the integration of heterogeneous data from different sources and characteristics can be easily achieved. More importantly, to support more controlled decision-making for fault detection, 'knowledge tags' can be defined semantically to form fault-targeted subsystems, which encapsulate the most representative subset of data dimensions to the corresponding faults respectively. This is a trial to better utilise massive real-time data. Although data carries a bewildering amount of information, excess HVAC sensory data actually degrades FDD performance, masking and flushing the more informative dimensions and causing overfitting between targeted faults and irrelevant dimensions [15]. To reduce the risk of overfitting and avoid the 'curse of dimensionality' [16], a Bag-of-Words (BoW) based feature extraction and selection method is introduced to identify the most informative sensory data dimensions from the labelled data. The knowledge tags are defined to incorporate this contextual knowledge learned from real-time data and automatically update the semantic model. It is key to enabling the needed self-evolving character of the digital twin model and adaptively feeding appropriate data to realise FDD of the random faults concisely [17]. The proposed fault detection and diagnosis of building HVAC systems inform the development of digital twins and the awakening of building intelligence for effective asset management.This study aims to demonstrate a digital twin enabled Fault Detection and Diagnosis (FDD) process, which aims at identifying abnormalities in building Heating, Ventilation and Air Conditioning (HVAC) systems to prevent poor indoor air quality, thermal discomfort, and low productivity [13]. With semantic web technologies [14], the integration of heterogeneous data from different sources and characteristics can be easily achieved. More importantly, to support more controlled decision-making for fault detection, 'knowledge tags' can be defined semantically to form fault-targeted subsystems, which encapsulate the most representative subset of data dimensions to the corresponding faults respectively. This is a trial to better utilise massive real-time data. Although data carries a bewildering amount of information, excess HVAC sensory data actually degrades FDD performance, masking and flushing the more informative dimensions and causing overfitting between targeted faults and irrelevant dimensions [15]. To reduce the risk of overfitting and avoid the 'curse of dimensionality' [16], a Bag-of-Words (BoW) based feature extraction and selection method is introduced to identify the most informative sensory data dimensions from the labelled data. The knowledge tags are defined to incorporate this contextual knowledge learned from real-time data and automatically update the semantic model. It is key to enabling the needed self-evolving character of the digital twin model and adaptively feeding appropriate data to realise FDD of the random faults concisely [17]. The proposed fault detection and diagnosis of building HVAC systems inform the development of digital twins and the awakening of building intelligence for effective asset management.</p>
        <p>The rest of this paper is organised as follows. Section 2 includes the literature review of semantic web technologies used in building systems and the fault detection and diagnosis of building HVAC systems. Section 3 discusses the proposed methodology. Section 4 presents the case study, demonstrating the novel digital twin-enabled fault detection and diagnosis process for building HVAC systems. Finally, Section 5 presents a discussion followed by Section 6 concluding this study.The rest of this paper is organised as follows. Section 2 includes the literature review of semantic web technologies used in building systems and the fault detection and diagnosis of building HVAC systems. Section 3 discusses the proposed methodology. Section 4 presents the case study, demonstrating the novel digital twin-enabled fault detection and diagnosis process for building HVAC systems. Finally, Section 5 presents a discussion followed by Section 6 concluding this study.</p>
        <p>By definition, the digital twin is a digital representation of physical assets, processes or systems in the built environment [18]. The built assets, inextricably linked into a complex and highly interconnected 'system of systems', deliver continuous services to various stakeholders [19]. The development of digital twins for built assets, where physical meets digital and sets out its core value proposition, aims to inform the decision makers of the system and improve their decisions driven by data, and ultimately deliver better outcomes for people.By definition, the digital twin is a digital representation of physical assets, processes or systems in the built environment [18]. The built assets, inextricably linked into a complex and highly interconnected 'system of systems', deliver continuous services to various stakeholders [19]. The development of digital twins for built assets, where physical meets digital and sets out its core value proposition, aims to inform the decision makers of the system and improve their decisions driven by data, and ultimately deliver better outcomes for people.</p>
        <p>Semantic web technologies play a key role in expressing, integrating and managing data and information for the development of building digital twin. As summarised in [20], the usage of semantics conventionally focused on: overcoming the interoperability issues amongst software tools and improving information exchange processes; connecting data across different domains, including but not limited to Building Information Model (BIM), Geographic Information System (GIS) and sensor data; and enabling logic-based declarative inference that extracts extra information from the original representations. These semanticsbased models are the natural successor to the current ambition in BIM that has become pervasive in this industry since the 1980s [21]. In short, some of the values enriched by the semantic web based approach are: (1) the addition of a logical basis that allows declarative inference as opposed to procedural coding, and (2) the much more open and web-based approach to data integration.Semantic web technologies play a key role in expressing, integrating and managing data and information for the development of building digital twin. As summarised in [20], the usage of semantics conventionally focused on: overcoming the interoperability issues amongst software tools and improving information exchange processes; connecting data across different domains, including but not limited to Building Information Model (BIM), Geographic Information System (GIS) and sensor data; and enabling logic-based declarative inference that extracts extra information from the original representations. These semanticsbased models are the natural successor to the current ambition in BIM that has become pervasive in this industry since the 1980s [21]. In short, some of the values enriched by the semantic web based approach are: (1) the addition of a logical basis that allows declarative inference as opposed to procedural coding, and (2) the much more open and web-based approach to data integration.</p>
        <p>What we highlight in this study is that, armed with semantics that is readily available in BIM models and more easily expressive in a graph-based semantic web world, digital twins can be created more flexibly with novel connections established using data associations continuously learned from real-time data [6]. Real-time data streams, for example those from sensors, carry a massive amount of information. To extract insights from the heterogeneous data streams, it is necessary to integrate them with solid knowledge in the form of taxonomies and class hierarchies defined by diverse domain ontologies. In this context, RDF Stream Processing (RSP) provides reasoning capabilities to infer implicit facts about Resource Description Framework (RDF) streams by incessantly answering SPARQL Protocol and RDF Query Language (SPARQL) queries (e.g., C-SPARQL, 
            <rs type="software">CQELS</rs>) [22,23]. Supporting algebraic operators such as queries, joints, filters, and aggregations, the RSP is demonstrated to be a useful engine in mediating the data pipelines from data sources to data storage, ultimately to applications and services on demand. However, the digital twinning process is an evolving process, as explained in Fig. 1. Deeper insights into the physical 'system of systems' are gradually and constantly upgraded through continuously ingesting the latest data streams. The up-to-date contextual knowledge acquired by various artificial intelligence algorithms [24] has got to be incorporated by introducing machine-readable connections and feeds back to the graph-based semantic models [25]. This is critical in the development of the digital twin analytical framework, because knowledge, initially held in domain ontologies and more importantly learned from latest data, must be expressed and used automatically to minimise the human intervention.
        </p>
        <p>The focus of this paper is to experiment with the integration of ad-hoc knowledge learned through symbolic artificial intelligence techniques into the digital twin ecosystem. Aiming at reducing the data numerosity while preserving the most relevant information, this study enables decomposing complex systems into separated subsystems by defining knowledge tags in the corresponding semantic models. And this will be explained in the next subsection. In this way, the data analysis becomes more computationally efficient leveraging the divide-andconquer strategy, which instinctively supports more localised decisionmaking.The focus of this paper is to experiment with the integration of ad-hoc knowledge learned through symbolic artificial intelligence techniques into the digital twin ecosystem. Aiming at reducing the data numerosity while preserving the most relevant information, this study enables decomposing complex systems into separated subsystems by defining knowledge tags in the corresponding semantic models. And this will be explained in the next subsection. In this way, the data analysis becomes more computationally efficient leveraging the divide-andconquer strategy, which instinctively supports more localised decisionmaking.</p>
        <p>To enable the integration of ad-hoc knowledge for supporting asset management processes, a stable semantic basis is needed to enable a solid backbone for the digital twins. In this regard, a series of domain-specific ontologies have been developed to solve problems in different application domains. Reviews of diverse domain ontologies are presented in Zhong et al. and Pauwels et al. [20,26,27] and out of the scope for this paper. Typically, two large clusters of research can be found, one situated in the conventional AEC domain, and the other in the building HVAC systems domain.To enable the integration of ad-hoc knowledge for supporting asset management processes, a stable semantic basis is needed to enable a solid backbone for the digital twins. In this regard, a series of domain-specific ontologies have been developed to solve problems in different application domains. Reviews of diverse domain ontologies are presented in Zhong et al. and Pauwels et al. [20,26,27] and out of the scope for this paper. Typically, two large clusters of research can be found, one situated in the conventional AEC domain, and the other in the building HVAC systems domain.</p>
        <p>In the AEC domain, most of the research on semantic modelling of buildings is situated around the Industry Foundation Classes (IFC) and Linked Building Data (LBD). The 
            <rs type="software">Web Ontology Language (OWL)</rs> implementation of IFC is summarised in [28,29]. And under the umbrella of LBD, several lightweight domain ontologies have been created using OWL, such as the Building Topology Ontology (BOT) [30,31]. Starting from the BOT ontology, several extended domain ontologies are developed, aiming to describe building products/elements (e.g., walls, windows, devices) and properties [32][33][34][35]. In the building HVAC systems domain, much less focus is put on the actual building topology, nor on the specific building products or their properties. Instead, focus is concentrated on the representation of the systems and sensing points as well as generated time-series data. The Brick ontology1 and Haystack tagging ontology2 have been developed to collect specific object types and properties for building system components [36][37][38]. Of particular difficulty in this domain are flows, devices states, control logics, and sensor data streams. Kukkonen et al. [39] designed a Flow Systems Ontology3 (FSO) for describing the composition of flow systems, such as the chilled beam system, and their mass and energy flow relationships. Xie et al. [40] proposed a federated ontology for representing building spatial and metering system hierarchies, which connects building submetering data with spatial characteristics for fine-grained energy analysis.
        </p>
        <p>By definition, a tag is a keyword or term deliberately assigned to a piece of information. Based on the structured semantics in the ontologies, the semantic tagging binds sets of 'knowledge tags' to entities or classes, helping to reserve, browse and search tacit knowledge of domain experts or particularly ad-hoc temporal knowledge learned from data [41]. The captured knowledge shows in the forms of descriptions, categorisations, classifications, comments, notes, hyperdata, hyperlinks, or references that are collected in tag profiles. Mishra et al. [25] proposed to automate Haystack tagging by leveraging rule-based knowledge (e.g., inferred semantic facts based on raw point names) and data-driven techniques (e.g., supervised labelling using time-series data). In this study, the Brick ontology with relatively basic and lightweight conceptual structures is adopted, allowing for the flexible description of points within a building and the formalisation of mapping between tags and entities/classes. The extra information resulting from introduced tags, adds additional value, context, and meaning to the explicitly represented information.By definition, a tag is a keyword or term deliberately assigned to a piece of information. Based on the structured semantics in the ontologies, the semantic tagging binds sets of 'knowledge tags' to entities or classes, helping to reserve, browse and search tacit knowledge of domain experts or particularly ad-hoc temporal knowledge learned from data [41]. The captured knowledge shows in the forms of descriptions, categorisations, classifications, comments, notes, hyperdata, hyperlinks, or references that are collected in tag profiles. Mishra et al. [25] proposed to automate Haystack tagging by leveraging rule-based knowledge (e.g., inferred semantic facts based on raw point names) and data-driven techniques (e.g., supervised labelling using time-series data). In this study, the Brick ontology with relatively basic and lightweight conceptual structures is adopted, allowing for the flexible description of points within a building and the formalisation of mapping between tags and entities/classes. The extra information resulting from introduced tags, adds additional value, context, and meaning to the explicitly represented information.</p>
        <p>Fault detection and diagnosis, like many other intelligent functions, allows to make better-informed decisions based on the multifaceted insights gained from massive data from building automation systems (BAS). The insights can either be acquired from calibrated physical models or be mined using data-driven approaches [13]. Accurate modelling of the HVAC system is vital for the physical model-based FDD. However, considering the interacting subsystems with complicated and non-linear dynamics, it is challenging and expensive to approximate the system's characteristics using the first principle model [42] or grey box model [43]. Furthermore, the uncertainties from inaccurate model formulations or modifications to the original system would greatly compromise the performance of model-based FDD.Fault detection and diagnosis, like many other intelligent functions, allows to make better-informed decisions based on the multifaceted insights gained from massive data from building automation systems (BAS). The insights can either be acquired from calibrated physical models or be mined using data-driven approaches [13]. Accurate modelling of the HVAC system is vital for the physical model-based FDD. However, considering the interacting subsystems with complicated and non-linear dynamics, it is challenging and expensive to approximate the system's characteristics using the first principle model [42] or grey box model [43]. Furthermore, the uncertainties from inaccurate model formulations or modifications to the original system would greatly compromise the performance of model-based FDD.</p>
        <p>Alternatively, data-driven approaches are widely explored to realise the FDD of HVAC systems. Relying on historical and online data, datadriven FDD reveals the intra-attribute patterns, which help identify the normal and faulty system behaviours in a supervised or unsupervised way. For instance, Zhao et al. [44,45] developed a method based on diagnostic Bayesian networks (DBNs) for diagnosing 28 types of faults in air handling units (AHUs) in buildings; Yu et al. [46] and Petrova et al. [47] adopted association rule mining (ARM) to discover faults in air conditioning systems from building operational data, and in the second case returning the found association rules back into the core semantic model.Alternatively, data-driven approaches are widely explored to realise the FDD of HVAC systems. Relying on historical and online data, datadriven FDD reveals the intra-attribute patterns, which help identify the normal and faulty system behaviours in a supervised or unsupervised way. For instance, Zhao et al. [44,45] developed a method based on diagnostic Bayesian networks (DBNs) for diagnosing 28 types of faults in air handling units (AHUs) in buildings; Yu et al. [46] and Petrova et al. [47] adopted association rule mining (ARM) to discover faults in air conditioning systems from building operational data, and in the second case returning the found association rules back into the core semantic model.</p>
        <p>Data-driven FDD is relatively simple to implement in an automated manner. However, excessive trials on adopting data-driven FDD in actual buildings hardly give satisfactory results. The unsatisfactory performance usually results from the fact that the performance of FDD largely depends on the quality of the data input. The selection of the most relevant data dimensions from the complete dataset, as the key knowledge to support data-driven FDD, is vital to decrease the risks of overfitting and reduce computational complexity. Yan et al. [48] and Mulumba et al. [49] used a filter-based algorithm called ReliefF to select the optimal feature subset for the FDD application to chillers. Li et al. [50] adopted the information greedy feature filter (IGFF) to eliminate noisy and noninformative features that compromise the FDD, maximising mutual information between selected features and the fault labels. Zhang et al. [51] proposed a statistical feature extraction technique (standard deviation, mean, minimum, maximum) with varying window sizes and use the filter and wrapper methods to select the optimal dataset based on cross-validation. To sum up the above literature, some focused on the typical faults of specific equipment (e.g., condenser fouling of chiller), preserving the physical significance of the selected features [52] but not necessarily guaranteeing equivalent performance when utilised in alternative scenarios. Similar to the idea of IGFF and statistical measures, in this study, a lightweight Bag of Words (BoW) based feature extraction and selection method [53] is adopted to recognise the most distinguishing sensory dimensions based on the time-series features. The reality is, in addition to typical faults, buildings are prone to many 'baffling' faults. The BoW-based method is appropriate in determining a selection of the nominated features for intractable faults in complicated systems. Besides, compared with IGFF and statistic measures, the BoW-based method reduces the size of a time series without losing key information, while also enjoying higher tolerance of sensor noise. The identified sensory dimensions, forming separated subsystems for detecting corresponding faults, are preserved through fault tagging and annotation. This piece of new knowledge helps to deliver FDD functionality using the most relevant dimensions and adapt the data pipelines accordingly.Data-driven FDD is relatively simple to implement in an automated manner. However, excessive trials on adopting data-driven FDD in actual buildings hardly give satisfactory results. The unsatisfactory performance usually results from the fact that the performance of FDD largely depends on the quality of the data input. The selection of the most relevant data dimensions from the complete dataset, as the key knowledge to support data-driven FDD, is vital to decrease the risks of overfitting and reduce computational complexity. Yan et al. [48] and Mulumba et al. [49] used a filter-based algorithm called ReliefF to select the optimal feature subset for the FDD application to chillers. Li et al. [50] adopted the information greedy feature filter (IGFF) to eliminate noisy and noninformative features that compromise the FDD, maximising mutual information between selected features and the fault labels. Zhang et al. [51] proposed a statistical feature extraction technique (standard deviation, mean, minimum, maximum) with varying window sizes and use the filter and wrapper methods to select the optimal dataset based on cross-validation. To sum up the above literature, some focused on the typical faults of specific equipment (e.g., condenser fouling of chiller), preserving the physical significance of the selected features [52] but not necessarily guaranteeing equivalent performance when utilised in alternative scenarios. Similar to the idea of IGFF and statistical measures, in this study, a lightweight Bag of Words (BoW) based feature extraction and selection method [53] is adopted to recognise the most distinguishing sensory dimensions based on the time-series features. The reality is, in addition to typical faults, buildings are prone to many 'baffling' faults. The BoW-based method is appropriate in determining a selection of the nominated features for intractable faults in complicated systems. Besides, compared with IGFF and statistic measures, the BoW-based method reduces the size of a time series without losing key information, while also enjoying higher tolerance of sensor noise. The identified sensory dimensions, forming separated subsystems for detecting corresponding faults, are preserved through fault tagging and annotation. This piece of new knowledge helps to deliver FDD functionality using the most relevant dimensions and adapt the data pipelines accordingly.</p>
        <p>Different from the general data-driven fault detection and diagnosis studies, this paper focuses on the realisation of the FDD process in a systematic way. By recognising the subsystems informative to arbitrary faults and labelling the relevant sensors within each subsystem using the knowledge tags, the ad-hoc temporal knowledge is preserved to customise corresponding data pipelines to drive FDD. It is key to automating the digital twin enabled HVAC FDD process, illustrated in Fig. 2. More specifically, facility management professionals are encouraged to name the most concerned or frequent faults in a particular building HVAC system, and the corresponding work orders can be retrieved from computer-aided facility management (
            <rs type="software">CAFM)</rs> software, containing the historical duration of these faults. A case-specific fault watch list is established to record all concerned faults raised by facility managers, and normal and faulty data can be acquired according to the work orders for faults in the watch list. Importantly, for any particular fault, it is unnecessary to utilise the whole HVAC dataset for enabling the FDD. With the help of the supervised BoW-based feature extraction and selection method, sensor collections restricted to detect every nominated fault can be uniquely selected based on the labelled normal and faulty data, automatically forming the subsystem dedicated to the particular fault without the need for human intervention. Fault tags are created in the semantic model accordingly, integrating these pieces of knowledge into the digital twin ecosystem as ad-hoc knowledge to associate the most relevant sensors with the corresponding fault tags. The continuously produced data from sensors is annotated with the associated fault tags, making sure that the processed data is appended to be independent and self-contained. The data streams in the form of a sequence of JavaScript Object Notation (JSON) objects are filtered to find out those containing a particular fault tag, and the occurrence of the fault can be alarmed based on the Goodness-of-Fit test.
        </p>
        <p>In summary, the digital twin analytical framework automatically identifies the most valuable real-time data dimensions for specific faults, according to the knowledge learned from data through the BoWbased feature selection method (Section 3.2-part A). By semantically defining fault tags to label these picked dimensions (Section 3.2-part B), this knowledge is preserved to allow the automation of the FDD functionality, processing the continuous real-time data streams without human intervention and fetching the right data for the nominated fault types in the customised fault watch list (Section 3.3). The subset of realtime data streams, corresponding to those fault-relevant dimensions, is used to detect the occurrence of specific faults. The fault alarm is triggered once the frequency of the symbols transformed from the latest data deviates from the normal condition based on the criterion of Kullback-Leibler divergence (Section 3.4). Adopting the 'divideand-conquer' strategy, this framework promotes asset management efficiency by making more controlled decisions using only the most informative and representative subset of data.In summary, the digital twin analytical framework automatically identifies the most valuable real-time data dimensions for specific faults, according to the knowledge learned from data through the BoWbased feature selection method (Section 3.2-part A). By semantically defining fault tags to label these picked dimensions (Section 3.2-part B), this knowledge is preserved to allow the automation of the FDD functionality, processing the continuous real-time data streams without human intervention and fetching the right data for the nominated fault types in the customised fault watch list (Section 3.3). The subset of realtime data streams, corresponding to those fault-relevant dimensions, is used to detect the occurrence of specific faults. The fault alarm is triggered once the frequency of the symbols transformed from the latest data deviates from the normal condition based on the criterion of Kullback-Leibler divergence (Section 3.4). Adopting the 'divideand-conquer' strategy, this framework promotes asset management efficiency by making more controlled decisions using only the most informative and representative subset of data.</p>
        <p>The Brick schema (version 1.2) is adopted as the ontology for establishing the semantic model for building systems, because of its comprehensive expressiveness and wide adoption in describing the HVAC system components and their connection with monitored sensory data. It provides standardised vocabularies for representing the physical, logical, and virtual assets in buildings and the relationships between them [36]. Specifically, point, location, and equipment are defined as the scaffolding of Brick's class hierarchy [27]. It has been verified to be capable of semantically twinning HVAC systems in buildings, with standardised metadata of HVAC entities and their relationships [37].The Brick schema (version 1.2) is adopted as the ontology for establishing the semantic model for building systems, because of its comprehensive expressiveness and wide adoption in describing the HVAC system components and their connection with monitored sensory data. It provides standardised vocabularies for representing the physical, logical, and virtual assets in buildings and the relationships between them [36]. Specifically, point, location, and equipment are defined as the scaffolding of Brick's class hierarchy [27]. It has been verified to be capable of semantically twinning HVAC systems in buildings, with standardised metadata of HVAC entities and their relationships [37].</p>
        <p>To further extend the expressiveness of the Brick model in contextual knowledge, knowledge tags for faults are defined as atomic facts under the Brick Tag namespace to reserve the most relevant sensory dimensions effective in detecting specific faults. Standard semantic definitions for the HVAC system faults can be integrated to orderly classify the possible HVAC faults according to their characteristics and causal relations [54]. The incorporated fault taxonomy can not only unify the naming convention of various faults, but more importantly, define a consistent physical hierarchy, which can be used to classify faults occurring at different levels of operation (e.g., component level, sub-system level, whole system level). However, in practice, there exist faults with undetermined causes. In this paper, the fault taxonomy is not used for better flexibility in accommodating any customised fault type. Each proposed fault tag is associated with a set of selected sensors (i.e., points) accordingly. And the connections in the form of hasTag/isTagOf (defined in Brick schema) are introduced to map between each type of fault and the associated sensors. This can reduce the computational complexity of the FDD functionality and facilitates the identification of each fault independently. In the case of the FDD for building HVAC systems, the Brick ontology is adopted instinctively. But the knowledge tagging should apply to any other suitable ontologies, as a 'carrier' of knowledge.To further extend the expressiveness of the Brick model in contextual knowledge, knowledge tags for faults are defined as atomic facts under the Brick Tag namespace to reserve the most relevant sensory dimensions effective in detecting specific faults. Standard semantic definitions for the HVAC system faults can be integrated to orderly classify the possible HVAC faults according to their characteristics and causal relations [54]. The incorporated fault taxonomy can not only unify the naming convention of various faults, but more importantly, define a consistent physical hierarchy, which can be used to classify faults occurring at different levels of operation (e.g., component level, sub-system level, whole system level). However, in practice, there exist faults with undetermined causes. In this paper, the fault taxonomy is not used for better flexibility in accommodating any customised fault type. Each proposed fault tag is associated with a set of selected sensors (i.e., points) accordingly. And the connections in the form of hasTag/isTagOf (defined in Brick schema) are introduced to map between each type of fault and the associated sensors. This can reduce the computational complexity of the FDD functionality and facilitates the identification of each fault independently. In the case of the FDD for building HVAC systems, the Brick ontology is adopted instinctively. But the knowledge tagging should apply to any other suitable ontologies, as a 'carrier' of knowledge.</p>
        <p>The relevant sensory dimensions are selected through analysing the symbolic aggregate approximation (SAX) of labelled data. This assumes that a considerable number of sensors is embedded in the HVAC systems, generating a massive amount of data. The FDD process is challenging because the intricate dependencies between fault-irrelevant sensory data dimensions unnecessarily increase the problem complexity and mask the true associations between the multisensor data and the targeted HVAC faults. In this study, the 
            <rs type="software">Bag-of-Word</rs> (BoW) based feature extraction and selection method is adopted to identify the fault-relevant dimensions using labelled normal and faulty data, and thus support subsystem disaggregation for detecting targeted HVAC faults [17]. The SAX in the BoW family is used, for extracting features (codewords) from the HVAC sensory data. Because sensor data in buildings is typically sampled with the interval of 1 min, 5 min, 15 min or even an hour, it is believed that building systems always stay in a pseudo-steady state. As a generalised version of standard deviation, mean and other statistical measures, the temporal characteristics extracted using SAX implicitly carry important information indicating, for example, the occurrence of specific events.
        </p>
        <p>Leveraging symbolic artificial intelligence, the SAX has discretisation functions that transform data segments into symbols and further transform each time-series to codewords (i.e., symbolic aggregates). A codeword is a combination of unordered alphabetic letters, like 'babbc'. As shown in Fig. 3, for the normalised time-series data from the sensor 𝑖 (𝒙 𝑖 = [𝑥 1𝑖 , 𝑥 2𝑖 , 𝑥 𝑡𝑖 ] 𝑇 ), a sliding window with length 𝑤 (𝑤 ≪ 𝑡) is used to segment the 𝑖th sensory data into fixed-length observations in an overlapping manner. Assuming the 𝑘th observation as [𝑥 (𝑡 𝑘 -𝑤+1)𝑖 , 𝑥 (𝑡 𝑘 -𝑤+2)𝑖 , … , 𝑥 𝑡 𝑘 𝑖 ] 𝑇 , the observation is further partitioned into 𝑝 equal sized pieces, with 𝑝 denoting the codeword length. These 𝑝 pieces are discretised into 𝑝 letters, from one of the 𝑎 alphabets. To achieve maximum entropy partitioning, the breakpoints of the discretisation (corresponds to the horizontal dashed lines in Fig. 3) are defined to produce 𝑎 alphabets with equiprobability under a Gaussian curve [55]. If the average value of 𝑤∕𝑝 observations is below the smallest breakpoint, these pieces are mapped to the symbol 'a'. If greater than or equal to the smallest breakpoint and less than the second smallest breakpoint, they are mapped to the symbol 'b', and the procedure continues up to the highest average value that is mapped to the last symbol of the chosen alphabet.Leveraging symbolic artificial intelligence, the SAX has discretisation functions that transform data segments into symbols and further transform each time-series to codewords (i.e., symbolic aggregates). A codeword is a combination of unordered alphabetic letters, like 'babbc'. As shown in Fig. 3, for the normalised time-series data from the sensor 𝑖 (𝒙 𝑖 = [𝑥 1𝑖 , 𝑥 2𝑖 , 𝑥 𝑡𝑖 ] 𝑇 ), a sliding window with length 𝑤 (𝑤 ≪ 𝑡) is used to segment the 𝑖th sensory data into fixed-length observations in an overlapping manner. Assuming the 𝑘th observation as [𝑥 (𝑡 𝑘 -𝑤+1)𝑖 , 𝑥 (𝑡 𝑘 -𝑤+2)𝑖 , … , 𝑥 𝑡 𝑘 𝑖 ] 𝑇 , the observation is further partitioned into 𝑝 equal sized pieces, with 𝑝 denoting the codeword length. These 𝑝 pieces are discretised into 𝑝 letters, from one of the 𝑎 alphabets. To achieve maximum entropy partitioning, the breakpoints of the discretisation (corresponds to the horizontal dashed lines in Fig. 3) are defined to produce 𝑎 alphabets with equiprobability under a Gaussian curve [55]. If the average value of 𝑤∕𝑝 observations is below the smallest breakpoint, these pieces are mapped to the symbol 'a'. If greater than or equal to the smallest breakpoint and less than the second smallest breakpoint, they are mapped to the symbol 'b', and the procedure continues up to the highest average value that is mapped to the last symbol of the chosen alphabet.</p>
        <p>The frequency of the symbols in codewords can evidence repetitive patterns in the data. Therefore, the histograms of codewords extracted for normal and faulty data respectively, are used as the criteria for feature selection. The 𝜒 2 test is a typical statistical hypothesis test, determining whether there exists a statistically significant difference between two categorical variables. Here, the 𝜒 2 test is used to test the consistency/distinguishability of codeword histograms extracted for normal and faulty operational data, and identify codewords that are most relevant to faults. Let 𝑇 and 𝑃 denote the counts of a specific codeword and all codewords found in faulty data, and 𝑀 and 𝑍 denote the counts of a specific codeword and all codewords found in normal and faulty data together. The 𝜒 2 score of the corresponding codeword is calculated as [17]:The frequency of the symbols in codewords can evidence repetitive patterns in the data. Therefore, the histograms of codewords extracted for normal and faulty data respectively, are used as the criteria for feature selection. The 𝜒 2 test is a typical statistical hypothesis test, determining whether there exists a statistically significant difference between two categorical variables. Here, the 𝜒 2 test is used to test the consistency/distinguishability of codeword histograms extracted for normal and faulty operational data, and identify codewords that are most relevant to faults. Let 𝑇 and 𝑃 denote the counts of a specific codeword and all codewords found in faulty data, and 𝑀 and 𝑍 denote the counts of a specific codeword and all codewords found in normal and faulty data together. The 𝜒 2 score of the corresponding codeword is calculated as [17]:</p>
        <p>The 𝜒 2 score follows a standard chi-square distribution (𝜒 2 ∼ 𝜒 2 1 (0)). Only those codewords with 𝜒 2 score passing a predefined threshold are kept as the distinguishing features that can differentiate normal and faulty HVAC system conditions. Those sensory data dimensions that generate these distinguishing codewords (normal versus faulty) are obviously more relevant to the specific HVAC fault. Accordingly, the defined fault tag is then connected with these sensors through hasTag/isTagOf, forming unique subsystems for detecting customised faults.The 𝜒 2 score follows a standard chi-square distribution (𝜒 2 ∼ 𝜒 2 1 (0)). Only those codewords with 𝜒 2 score passing a predefined threshold are kept as the distinguishing features that can differentiate normal and faulty HVAC system conditions. Those sensory data dimensions that generate these distinguishing codewords (normal versus faulty) are obviously more relevant to the specific HVAC fault. Accordingly, the defined fault tag is then connected with these sensors through hasTag/isTagOf, forming unique subsystems for detecting customised faults.</p>
        <p>This section addresses the integration of knowledge tags defined in the Brick model and real-time data typically from building management systems (BMS) or IoT sensors. Metadata, including the knowledge tags here, means 'data about data' and can be defined as pieces of information describing entities in buildings. Real-time data, referred to as stream data as well, are pieces of information about events or results of measurements at a specific time instant, for instance, humidity measured by a sensor near a condenser. Based on the digital twin data platform developed by the authors, we demonstrate the feasibility of adding semantics to the real-time data produced by sensors. Of course, the integration of customised fault tags with real-time data is applicable in other data platforms with alike data architectures.This section addresses the integration of knowledge tags defined in the Brick model and real-time data typically from building management systems (BMS) or IoT sensors. Metadata, including the knowledge tags here, means 'data about data' and can be defined as pieces of information describing entities in buildings. Real-time data, referred to as stream data as well, are pieces of information about events or results of measurements at a specific time instant, for instance, humidity measured by a sensor near a condenser. Based on the digital twin data platform developed by the authors, we demonstrate the feasibility of adding semantics to the real-time data produced by sensors. Of course, the integration of customised fault tags with real-time data is applicable in other data platforms with alike data architectures.</p>
        <p>The Adaptive City Platform (ACP) 4 [56] is adopted as the digital twin data platform in this study, responsible for integrating real-time data with corresponding fault tags. As highlighted in the landmark paper about the semantic web [14], developments will usher in significant new functionality as machines become much better able to process and 'understand' the data that they merely display at present. In the case of real-time data mostly coming from IoT sensors, the data messages usually contain very limited semantics (e.g., sensor ID) in addition to the raw data. Therefore, adding extra semantics (e.g., fault tags) to stream data without breaking the constraints on resource usage becomes increasingly important, as appended data become independent and self-contained. The ACP is designed as a data lake where metadata and stream data are integrated in this platform following an Extract, Transform, Load (ETL) process, and stored for later use. Subsequently, 'data pipelines' are defined to filter (according to the annotated fault tag) and expose extant data as required by functionalities, such as FDD.The Adaptive City Platform (ACP) 4 [56] is adopted as the digital twin data platform in this study, responsible for integrating real-time data with corresponding fault tags. As highlighted in the landmark paper about the semantic web [14], developments will usher in significant new functionality as machines become much better able to process and 'understand' the data that they merely display at present. In the case of real-time data mostly coming from IoT sensors, the data messages usually contain very limited semantics (e.g., sensor ID) in addition to the raw data. Therefore, adding extra semantics (e.g., fault tags) to stream data without breaking the constraints on resource usage becomes increasingly important, as appended data become independent and self-contained. The ACP is designed as a data lake where metadata and stream data are integrated in this platform following an Extract, Transform, Load (ETL) process, and stored for later use. Subsequently, 'data pipelines' are defined to filter (according to the annotated fault tag) and expose extant data as required by functionalities, such as FDD.</p>
        <p>According to the comparison of different formats in [57], in the IoT domain, the JavaScript Object Notation (JSON) format with entitycentric structure show advantages in terms of expressivity, query execution time and resource consumption when compared with a tripletscentric structure like RDF (e.g., Brick model in turtle format). The ACP is engineered towards minimising the end-to-end latency for real-time data in the JSON format. The latency between a data entry (i.e., when it is ingested) and exit (i.e., when it is available for use) averages a few milliseconds. To ensure better compatibility and fast querying, the ACP data modelling strategy opts for transforming both the brick model, including metadata and defined fault tags, as well as real-time data, to a more flexible 'crate model' in the JSON format. The transformation process of Brick model (Brick2ACP) is elaborated in Merino et al. [58]. A crate is an entity with its own attributes plus zero or more parents. All crates together with their connections to parents form a hierarchical structure, although it seems that every crate sits at the same level. Every crate is uniquely identified through an indexed key for quick access and query. Listing 1 shows an example of the transformed crate data in the JSON format. Metadata, particularly the fault tags defined in Section 3.2, can be transformed to be in line with the ACP data modelling strategy. Sharing the same format as real-time data, timestamps can be added easily to track the changes of metadata and fault tags. More importantly, the crate model stays in memory, which accelerate the annotation of real-time data messages with metadata and fault tags.According to the comparison of different formats in [57], in the IoT domain, the JavaScript Object Notation (JSON) format with entitycentric structure show advantages in terms of expressivity, query execution time and resource consumption when compared with a tripletscentric structure like RDF (e.g., Brick model in turtle format). The ACP is engineered towards minimising the end-to-end latency for real-time data in the JSON format. The latency between a data entry (i.e., when it is ingested) and exit (i.e., when it is available for use) averages a few milliseconds. To ensure better compatibility and fast querying, the ACP data modelling strategy opts for transforming both the brick model, including metadata and defined fault tags, as well as real-time data, to a more flexible 'crate model' in the JSON format. The transformation process of Brick model (Brick2ACP) is elaborated in Merino et al. [58]. A crate is an entity with its own attributes plus zero or more parents. All crates together with their connections to parents form a hierarchical structure, although it seems that every crate sits at the same level. Every crate is uniquely identified through an indexed key for quick access and query. Listing 1 shows an example of the transformed crate data in the JSON format. Metadata, particularly the fault tags defined in Section 3.2, can be transformed to be in line with the ACP data modelling strategy. Sharing the same format as real-time data, timestamps can be added easily to track the changes of metadata and fault tags. More importantly, the crate model stays in memory, which accelerate the annotation of real-time data messages with metadata and fault tags.</p>
        <p>Point in Brick schema represents devices attached to a location or equipment that generate periodic data (e.g., every minute) or event data (e.g., alerts). Real-time data from points (sensors in this context) is always timestamped to record the time instant when that measurement or event happened. Conceptually, real-time data belongs to the equipment or location that the sensor monitors, and likewise the fault tags that the sensor is affiliated with. Once the new sensor reading arrives through the MQTT protocol (Message Queuing Telemetry Transport, a standard messaging protocol for the IoT), it comes with point/sensor identifier, timestamp and reading (value). According to the sensor identifier, the metadata of corresponding equipment or location and fault tags should be integrated into the sensor reading in the crate format. Listings 2 and 3 show the format of original temperature sensor readings transmitted through MQTT and the appended reading in the ACP. As shown in Fig. 4, data pipelines can be customised in the digital twin data platform to search or subscribe to the real-time data containing a specific fault tag. The Brick model is transformed into the crate data model and the real-time data stream is annotated with the metadata of corresponding equipment, location and fault tags. Finally, the real-time data stored as a sequence of JSON objects can be filtered, and the data containing the targeted fault tag is then exposed and published to the FDD functionality, which drives the FDD functionality using the Kullback-Leibler divergence.Point in Brick schema represents devices attached to a location or equipment that generate periodic data (e.g., every minute) or event data (e.g., alerts). Real-time data from points (sensors in this context) is always timestamped to record the time instant when that measurement or event happened. Conceptually, real-time data belongs to the equipment or location that the sensor monitors, and likewise the fault tags that the sensor is affiliated with. Once the new sensor reading arrives through the MQTT protocol (Message Queuing Telemetry Transport, a standard messaging protocol for the IoT), it comes with point/sensor identifier, timestamp and reading (value). According to the sensor identifier, the metadata of corresponding equipment or location and fault tags should be integrated into the sensor reading in the crate format. Listings 2 and 3 show the format of original temperature sensor readings transmitted through MQTT and the appended reading in the ACP. As shown in Fig. 4, data pipelines can be customised in the digital twin data platform to search or subscribe to the real-time data containing a specific fault tag. The Brick model is transformed into the crate data model and the real-time data stream is annotated with the metadata of corresponding equipment, location and fault tags. Finally, the real-time data stored as a sequence of JSON objects can be filtered, and the data containing the targeted fault tag is then exposed and published to the FDD functionality, which drives the FDD functionality using the Kullback-Leibler divergence.</p>
        <p>SAX is also a promising technology available to reduce the dimensionality of the time-series, while keeping the essential information. Note that 𝑎 and 𝑤∕𝑝 determine the information loss of the transformation and furthermore the computational and memory savings of the overall FDD process. The window size 𝑤 is usually configured around one to several hours, acknowledging that hour-long data is sufficient to expose most common faults. The optimal values for 𝑎 and 𝑝 are determined by trial and error. Generally, 𝑎 and 𝑝 values resulting in higher 𝜒 2 scores should be adopted, considering that more distinguishing symbolic sequences between normal and faulty conditions can be acquired under these values. By transforming time-series into a symbolic representation, the lower-dimensional symbolic sequence can be effectively coupled with the Kullback-Leibler divergence based Goodness-of-Fit test to reveal the occurrence of specific faults [59]. The Kullback-Leibler divergence is a type of statistical distance measuring the difference between two discrete random variables. Let 𝑄 and Q denote two discrete random variables, and the Kullback-Leibler divergence of 𝑄 from Q is defined as follows:SAX is also a promising technology available to reduce the dimensionality of the time-series, while keeping the essential information. Note that 𝑎 and 𝑤∕𝑝 determine the information loss of the transformation and furthermore the computational and memory savings of the overall FDD process. The window size 𝑤 is usually configured around one to several hours, acknowledging that hour-long data is sufficient to expose most common faults. The optimal values for 𝑎 and 𝑝 are determined by trial and error. Generally, 𝑎 and 𝑝 values resulting in higher 𝜒 2 scores should be adopted, considering that more distinguishing symbolic sequences between normal and faulty conditions can be acquired under these values. By transforming time-series into a symbolic representation, the lower-dimensional symbolic sequence can be effectively coupled with the Kullback-Leibler divergence based Goodness-of-Fit test to reveal the occurrence of specific faults [59]. The Kullback-Leibler divergence is a type of statistical distance measuring the difference between two discrete random variables. Let 𝑄 and Q denote two discrete random variables, and the Kullback-Leibler divergence of 𝑄 from Q is defined as follows:</p>
        <p>Here, the Kullback-Leibler divergence based goodness-of-fit test is adopted to classify the sliding time-series as faulty or normal by tracking the time-evolving distribution of the generated codewords. Assuming the sliding time-series from the tagged sensor dimensions is transformed into symbolic sequences of codewords, and in this case, 𝑄 and Q represent the probability histograms of these codewords generated by normal and observed data respectively. As shown in Eq. ( 3), the goodness-of-test test is performed by comparing 2𝑁 times the Kullback-Leibler divergence (𝑁 is the total number of observed codewords) against the threshold determined by the compositional inverse of the Cumulative Distribution Function (CDF) of the chisquared distribution, denoted by 𝐹 -1 𝜒 2 (𝛾). The fault alarm is triggered if the condition in Eq. ( 3) holds, where 𝛾 is typically set equal to 0.05 or 0.01. The condition indicates that the probability distribution Q of the alphabet symbols generated from observed data is considered to be asymptotically dissimilar from that of the symbols from normal data, labelled as 𝑄.Here, the Kullback-Leibler divergence based goodness-of-fit test is adopted to classify the sliding time-series as faulty or normal by tracking the time-evolving distribution of the generated codewords. Assuming the sliding time-series from the tagged sensor dimensions is transformed into symbolic sequences of codewords, and in this case, 𝑄 and Q represent the probability histograms of these codewords generated by normal and observed data respectively. As shown in Eq. ( 3), the goodness-of-test test is performed by comparing 2𝑁 times the Kullback-Leibler divergence (𝑁 is the total number of observed codewords) against the threshold determined by the compositional inverse of the Cumulative Distribution Function (CDF) of the chisquared distribution, denoted by 𝐹 -1 𝜒 2 (𝛾). The fault alarm is triggered if the condition in Eq. ( 3) holds, where 𝛾 is typically set equal to 0.05 or 0.01. The condition indicates that the probability distribution Q of the alphabet symbols generated from observed data is considered to be asymptotically dissimilar from that of the symbols from normal data, labelled as 𝑄.</p>
        <p>The proposed digital twin enabled fault detection and diagnosis functionality for building HVAC systems is tested using the experimental data from a 300𝑚 2 research facility in the Oak Ridge National Laboratory (ORNL) [60]. The facility is reserved for experiments, and the internal loads are designed to emulate the working condition of an office building. During the experiment, this facility is conditioned with a single packaged rooftop unit (RTU), connecting to multi-zone variable air volume (VAV) terminal systems with electric resistance reheat. The outdoor air intake of the RTU is blocked throughout theThe proposed digital twin enabled fault detection and diagnosis functionality for building HVAC systems is tested using the experimental data from a 300𝑚 2 research facility in the Oak Ridge National Laboratory (ORNL) [60]. The facility is reserved for experiments, and the internal loads are designed to emulate the working condition of an office building. During the experiment, this facility is conditioned with a single packaged rooftop unit (RTU), connecting to multi-zone variable air volume (VAV) terminal systems with electric resistance reheat. The outdoor air intake of the RTU is blocked throughout the</p>
        <p>Deployed sensors used for the FDD of HVAC system within the facility.Deployed sensors used for the FDD of HVAC system within the facility.</p>
        <p>Sensor type Description experiments, indicating that only indoor air recirculates. Functionally, the RTU and VAVs provide heating and cooling to this experimental building, serving 2 core zones and 8 perimeter zones respectively. Fig. 5 presents the schematic diagram for the air conditioning system of the facility.Sensor type Description experiments, indicating that only indoor air recirculates. Functionally, the RTU and VAVs provide heating and cooling to this experimental building, serving 2 core zones and 8 perimeter zones respectively. Fig. 5 presents the schematic diagram for the air conditioning system of the facility.</p>
        <p>The experimental facility is scheduled to operate automatically following the designed occupied and unoccupied modes. The occupied mode starts at 7:00 am and ends at 10:00 pm, while the unoccupied mode lasts for the rest of the day. The cooling coil valve and heating coil valve within the RTU are modulated to maintain the supply air temperature at 55 • F year-round. During the occupied time period, the zone air temperature heating setpoint is 69.8 • F, while the setpoint is reduced to 60 • F during the unoccupied hours.The experimental facility is scheduled to operate automatically following the designed occupied and unoccupied modes. The occupied mode starts at 7:00 am and ends at 10:00 pm, while the unoccupied mode lasts for the rest of the day. The cooling coil valve and heating coil valve within the RTU are modulated to maintain the supply air temperature at 55 • F year-round. During the occupied time period, the zone air temperature heating setpoint is 69.8 • F, while the setpoint is reduced to 60 • F during the unoccupied hours.</p>
        <p>Various sensors are deployed within the facility to monitor the working condition of the RTU and VAVs with a sampling interval of one minute, including temperature, humidity, pressure, airflow and energy consumption (i.e., electricity, gas) sensors. Table 1 lists the sensors deployed within the facility to realise the fault detection and diagnosis, and the critical sensors have been marked on Fig. 5.Various sensors are deployed within the facility to monitor the working condition of the RTU and VAVs with a sampling interval of one minute, including temperature, humidity, pressure, airflow and energy consumption (i.e., electricity, gas) sensors. Table 1 lists the sensors deployed within the facility to realise the fault detection and diagnosis, and the critical sensors have been marked on Fig. 5.</p>
        <p>Instead of focusing on the HVAC and lighting schedule setback faults caused by the disordered control sequences, this study targets five types of functional faults that substantially change the behaviour of the HVAC system, including excessive infiltration, thermostat measurement positive and negative biases of core and perimeter zones (+4 • F and -4 • F for core zone 103 and perimeter zone 205). These faults were artificially created and imposed onto the facility: the excessive infiltration is realised by opening windows to achieve the target infiltration rate, and thermostat measurement positive or negative bias is realised by adjusting the temperature setpoint in the opposite direction.Instead of focusing on the HVAC and lighting schedule setback faults caused by the disordered control sequences, this study targets five types of functional faults that substantially change the behaviour of the HVAC system, including excessive infiltration, thermostat measurement positive and negative biases of core and perimeter zones (+4 • F and -4 • F for core zone 103 and perimeter zone 205). These faults were artificially created and imposed onto the facility: the excessive infiltration is realised by opening windows to achieve the target infiltration rate, and thermostat measurement positive or negative bias is realised by adjusting the temperature setpoint in the opposite direction.</p>
        <p>The BoW-based feature extraction and selection method is used to pick up the most distinguishing sensor sets corresponding to each of these five faults. To address missing sensor data values, the labelled data under normal and faulty conditions is first down-sampled to five minutes. Following the data processing procedure shown in Fig. 3, the down-sampled data is sliced in an overlapping manner using a sliding window with size of 12 (𝑤 = 12, a window of one hour). Data from 4 labelled normal days and 1 faulty day is used to identify the sensitive sensory dimensions for each type of fault. For data in each labelled day, it is converted to 277 codewords of 4 letters from a dictionary of 4 alphabets (𝑝 = 4, 𝑎 = 4). A 𝜒 2 test is conducted to find the most distinguishing codewords that differentiate normal and five faulty conditions. The sensory dimensions that generate corresponding codewords with 𝜒 2 score greater than the threshold would be selected. The threshold of 7.879 is selected with the significant level 𝛼 = 0.005. Table 2 presents the selected sensor sets for the FDD of these five fault types.The BoW-based feature extraction and selection method is used to pick up the most distinguishing sensor sets corresponding to each of these five faults. To address missing sensor data values, the labelled data under normal and faulty conditions is first down-sampled to five minutes. Following the data processing procedure shown in Fig. 3, the down-sampled data is sliced in an overlapping manner using a sliding window with size of 12 (𝑤 = 12, a window of one hour). Data from 4 labelled normal days and 1 faulty day is used to identify the sensitive sensory dimensions for each type of fault. For data in each labelled day, it is converted to 277 codewords of 4 letters from a dictionary of 4 alphabets (𝑝 = 4, 𝑎 = 4). A 𝜒 2 test is conducted to find the most distinguishing codewords that differentiate normal and five faulty conditions. The sensory dimensions that generate corresponding codewords with 𝜒 2 score greater than the threshold would be selected. The threshold of 7.879 is selected with the significant level 𝛼 = 0.005. Table 2 presents the selected sensor sets for the FDD of these five fault types.</p>
        <p>As shown in Table 2, only 2 to 5 sensors out of the total 51 sensors are seen as informative for the detection and diagnosis of each typical fault respectively. The selection of sensors does follow certain physical knowledge and common sense. For example, the excessive infiltration caused by inappropriately opening windows would directly influence the return air temperature (no return air temperature sensor deployed) and subsequently the discharge temperature of the cooling coil before the air is heated in winter. Besides, the bias of the zone thermostat measurement would be easily noticed from the ambient temperature of the zone. Of course, the correlation between the fault and sensor data and the strength of that correlation can only be seen from data. To preserve this ad-hoc knowledge in the digital twin ecosystem, fault tags need to be supplemented in the corresponding semantic model. Fig. 6 presents the Brick model of the HVAC system in the facility defined using the Brick ontology. The physical and logical entities in the facility, such as the zones, RTU, VAVs, their components and deployed sensors are explicitly defined, and the relationships (e.g., hasPart, feeds, hasPoint ) are modelled to capture the connections between these entities. Five tags (i.e., fault_a, fault_b, fault_c, fault_d, fault_e) are added in the Brick model for the five fault types listed in Table 2 and label the sensors that are informative to each fault (i.e., points hasTag fault_x).As shown in Table 2, only 2 to 5 sensors out of the total 51 sensors are seen as informative for the detection and diagnosis of each typical fault respectively. The selection of sensors does follow certain physical knowledge and common sense. For example, the excessive infiltration caused by inappropriately opening windows would directly influence the return air temperature (no return air temperature sensor deployed) and subsequently the discharge temperature of the cooling coil before the air is heated in winter. Besides, the bias of the zone thermostat measurement would be easily noticed from the ambient temperature of the zone. Of course, the correlation between the fault and sensor data and the strength of that correlation can only be seen from data. To preserve this ad-hoc knowledge in the digital twin ecosystem, fault tags need to be supplemented in the corresponding semantic model. Fig. 6 presents the Brick model of the HVAC system in the facility defined using the Brick ontology. The physical and logical entities in the facility, such as the zones, RTU, VAVs, their components and deployed sensors are explicitly defined, and the relationships (e.g., hasPart, feeds, hasPoint ) are modelled to capture the connections between these entities. Five tags (i.e., fault_a, fault_b, fault_c, fault_d, fault_e) are added in the Brick model for the five fault types listed in Table 2 and label the sensors that are informative to each fault (i.e., points hasTag fault_x).</p>
        <p>Leveraging the fault tags, sensitive sensory dimensions to each fault are uniquely labelled. Furthermore, the digital twin data platform annotates the minute-level data stream with metadata including the defined fault tags. As an example, Fig. 7 illustrates a part of the HVAC system of this facility, which contains the five sensors informative to fault_d and as well as the equipment and locations associated with these sensors. The detection of fault_d mainly relies on the following five temperature sensors: To guarantee the real-time character of the data integration, the real-time data stream is converted into the JSON format. The sliced Brick model (see Fig. 7) can be transformed into crates, within which the hasPoint and hasTag connections in the Brick model are highlighted. These connections contain all metadata needed to annotate real-time data streams. Using the fault_d as an example, real-time readings from the five tagged sensors are annotated with the associated equipment, location and particularly the fault tag (fault_d) to create a self-contained message feeding into the FDD functionality (see Fig. 8). Specifically, for the sensor readings with the point identifier of ZoneT_205, the location of the sensor (Zone_205) and the associated fault tag (fault_d) are appended and annotated to the JSON object. As a result, the identified sensory dimensions of massive real-time data are filtered to efficiently feed the FDD functionality through the semantically defined fault tags. That is to say, the data stream annotated with the specific fault tag is filtered and exposed through the data pipelines to feed into the FDD functionality. For the implementation, the real-time sensor data preprocessed in the digital twin data platform are fetched using 
            <rs type="software">WebSocket</rs> connections [56]. Uniquely, the FDD in this paper is conducted in a real-time manner, by building a probability model of normal data [61]. In this case, the probability histogram 𝑄 of codewords transformed from the 4 day normal operational data is computed offline and remains constant during the entire FDD process. The faulty data stream is artificially generated from the experimental facility for each fault type. To showcase the use of the one-sided goodness-of-fit test on revealing faulty scenarios, the fraction of times the codewords transformed from the reduced-dimensional faulty data stream appear in the past few windows (i.e., Q) is compared against
        </p>
        <p>As shown in Fig. 9, Boi-Ukeme et al. [62] summarised the typical FDD workflow, including data collection from the building systems of interest, data understanding, data preparation and analysis, data modelling, and deployment of the models for FDD. Most of the FDD research focuses on data modelling and its application in fault classification using supervised or unsupervised FDD algorithms. However, the data understanding, preparation and analysis (e.g., exploratory analysis and determining appropriate input for the algorithm), which often require human intervention, present a significant obstacle to the automation of FDD. To reduce human intervention and automate the FDD process, it is necessary to ensure that ad-hoc knowledge learned through statistical and symbolic artificial intelligence techniques from real-time data becomes comprehensible by computers, on top of the existing ontological knowledge. The contextual/temporal knowledge in the FDD process refers to the recognised sensor dimensions dedicated to the detection of specific faults. To preserve the new knowledge for better understanding, this study extends the Brick ontology with fault tags, which support labelling the selected sensors with machine-readable fault tags. By annotating the real-time sensor data with fault tags in the Brick model, the data pipelines prepare the real-time data targeting specific faults to serve the FDD functionality. This is a preliminary trial in realising building intelligence, considering that a massive amount of data from heterogeneous sources must be filtered and prepared before feeding into the corresponding functionalities. It contributes to the dimensionality reduction of the original problem and preparing a reasonable dataset following the 'divide-and-conquer' strategy.As shown in Fig. 9, Boi-Ukeme et al. [62] summarised the typical FDD workflow, including data collection from the building systems of interest, data understanding, data preparation and analysis, data modelling, and deployment of the models for FDD. Most of the FDD research focuses on data modelling and its application in fault classification using supervised or unsupervised FDD algorithms. However, the data understanding, preparation and analysis (e.g., exploratory analysis and determining appropriate input for the algorithm), which often require human intervention, present a significant obstacle to the automation of FDD. To reduce human intervention and automate the FDD process, it is necessary to ensure that ad-hoc knowledge learned through statistical and symbolic artificial intelligence techniques from real-time data becomes comprehensible by computers, on top of the existing ontological knowledge. The contextual/temporal knowledge in the FDD process refers to the recognised sensor dimensions dedicated to the detection of specific faults. To preserve the new knowledge for better understanding, this study extends the Brick ontology with fault tags, which support labelling the selected sensors with machine-readable fault tags. By annotating the real-time sensor data with fault tags in the Brick model, the data pipelines prepare the real-time data targeting specific faults to serve the FDD functionality. This is a preliminary trial in realising building intelligence, considering that a massive amount of data from heterogeneous sources must be filtered and prepared before feeding into the corresponding functionalities. It contributes to the dimensionality reduction of the original problem and preparing a reasonable dataset following the 'divide-and-conquer' strategy.</p>
        <p>The proposed digital twin enabled FDD process contributes to the automation in monitoring building critical assets. Instead of performing manual inspections according to a predefined schedule, the FDD automatically informs the facility managers of the emergence of faults. Meanwhile, the fault watch list is defined by the facility management professionals that comes with historical work order lists for different faults. The historical work orders can be used to suggest corrective actions based on the past maintenance procedures, implemented to maintain or repair corresponding assets that lead to the emerging fault. The proposed FDD process can be integrated with existing asset management databases, for instance the Computer Aided Facility Management (CAFM) software [63], where the work order data can be managed. This is the first step towards the automation of Operations Maintenance and Repair (OM &amp; R). It would be of interest here to expand the static building system representation with the plethora of building information available in a Linked Building Data (LBD) cloud to enable even more holistic analyses for asset management. Automating the FDD process also empowers other intelligent building functionalities, for example the detected fault history can be used to train machine learning models to predict the occurrence of future faults of the same type. This would support the further automation of the asset management processes.The proposed digital twin enabled FDD process contributes to the automation in monitoring building critical assets. Instead of performing manual inspections according to a predefined schedule, the FDD automatically informs the facility managers of the emergence of faults. Meanwhile, the fault watch list is defined by the facility management professionals that comes with historical work order lists for different faults. The historical work orders can be used to suggest corrective actions based on the past maintenance procedures, implemented to maintain or repair corresponding assets that lead to the emerging fault. The proposed FDD process can be integrated with existing asset management databases, for instance the Computer Aided Facility Management (CAFM) software [63], where the work order data can be managed. This is the first step towards the automation of Operations Maintenance and Repair (OM &amp; R). It would be of interest here to expand the static building system representation with the plethora of building information available in a Linked Building Data (LBD) cloud to enable even more holistic analyses for asset management. Automating the FDD process also empowers other intelligent building functionalities, for example the detected fault history can be used to train machine learning models to predict the occurrence of future faults of the same type. This would support the further automation of the asset management processes.</p>
        <p>Abundant data, particularly real-time data, is generated in dynamic building systems. These pieces of data, carrying a bewildering amount of information, need to be selected and filtered to drive corresponding intelligent building functionalities, otherwise, the irrelevant data dimensions would mask and flush the most informative data. In the case of FDD, the Bag-of-Words based (i.e., SAX) feature extraction and selection method, as a typical symbolic artificial intelligence technique, is introduced to boost computational efficiency. It tries to extract the temporal characteristics of historical time-series and identify the distinguishing sensory dimensions from the labelled normal and faulty data, which deepens the understanding of all the concerned faults. The 'fault tags' are defined accordingly in the Brick model to label these relevant sensor entities, preserving the learned contextual knowledge/understanding in a machine-readable way. The digital twin data platform is adopted to integrate the defined knowledge tags with real-time data. By annotating the data stream with auxiliary fault tags, low-latency highbandwidth real-time data streams appended with the specified fault tag can be automatically extracted to feed the FDD functionality. This informs the way to enable dynamic asset management functionalities through digital twinning. The divide-and-conquer strategy used here contributes to the real-time character and the computational burden reduction for delivering building intelligent functionalities.Abundant data, particularly real-time data, is generated in dynamic building systems. These pieces of data, carrying a bewildering amount of information, need to be selected and filtered to drive corresponding intelligent building functionalities, otherwise, the irrelevant data dimensions would mask and flush the most informative data. In the case of FDD, the Bag-of-Words based (i.e., SAX) feature extraction and selection method, as a typical symbolic artificial intelligence technique, is introduced to boost computational efficiency. It tries to extract the temporal characteristics of historical time-series and identify the distinguishing sensory dimensions from the labelled normal and faulty data, which deepens the understanding of all the concerned faults. The 'fault tags' are defined accordingly in the Brick model to label these relevant sensor entities, preserving the learned contextual knowledge/understanding in a machine-readable way. The digital twin data platform is adopted to integrate the defined knowledge tags with real-time data. By annotating the data stream with auxiliary fault tags, low-latency highbandwidth real-time data streams appended with the specified fault tag can be automatically extracted to feed the FDD functionality. This informs the way to enable dynamic asset management functionalities through digital twinning. The divide-and-conquer strategy used here contributes to the real-time character and the computational burden reduction for delivering building intelligent functionalities.</p>
        <p>To the future of digital twins, the proposed framework allows to overcome the problem of 'big' data (high volume, high variety, but not high velocity) and filter the large amount of data coming from the digital twin ecosystem. Thanks to the metadata tagging and annotating approaches, the developed methodologies can be replicated and allow to take good advantage of ontological knowledge and ad-hoc knowledge from artificial intelligence techniques to support intelligent building functionalities. The expressiveness and richness of semantics are leveraged for developing complex functionalities, allowing to better represent comprehensive understandings about the building systems and drive the real-time management of the physical assets. However, additional research efforts are needed to achieve further support and automation in the decision-making process (e.g., through the integration with CAFM and other organisational systems). The data integration approach is based on the digital twin data platform, prioritising the real-time character of data and treating the pseudo-static information as metadata appended to real-time data streams. It makes this approach flexible and very effective for the development of dynamic intelligent building functionalities (e.g., asset management applications).To the future of digital twins, the proposed framework allows to overcome the problem of 'big' data (high volume, high variety, but not high velocity) and filter the large amount of data coming from the digital twin ecosystem. Thanks to the metadata tagging and annotating approaches, the developed methodologies can be replicated and allow to take good advantage of ontological knowledge and ad-hoc knowledge from artificial intelligence techniques to support intelligent building functionalities. The expressiveness and richness of semantics are leveraged for developing complex functionalities, allowing to better represent comprehensive understandings about the building systems and drive the real-time management of the physical assets. However, additional research efforts are needed to achieve further support and automation in the decision-making process (e.g., through the integration with CAFM and other organisational systems). The data integration approach is based on the digital twin data platform, prioritising the real-time character of data and treating the pseudo-static information as metadata appended to real-time data streams. It makes this approach flexible and very effective for the development of dynamic intelligent building functionalities (e.g., asset management applications).</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
        <p>the five fault types respectively. The results are given in Tablethe five fault types respectively. The results are given in Table</p>
        <p>33</p>
        <p>, indicating that the Goodness-of-Fit test can largely detect the target five faults based on the data provided. It is also worth mentioning that the proposed digital twin analytical framework is compatible with other FDD algorithms as well, which identify the faults based on diverse machine learning techniques (e.g., Artificial Neural Networks) using the filtered data streams as input through the same data pipeline., indicating that the Goodness-of-Fit test can largely detect the target five faults based on the data provided. It is also worth mentioning that the proposed digital twin analytical framework is compatible with other FDD algorithms as well, which identify the faults based on diverse machine learning techniques (e.g., Artificial Neural Networks) using the filtered data streams as input through the same data pipeline.</p>
        <p>https://brickschema.org/schema/Brickhttps://brickschema.org/schema/Brick</p>
        <p>https://project-haystack.org/doc/docHaystack/Ontologyhttps://project-haystack.org/doc/docHaystack/Ontology</p>
        <p>https://alikucukavci.github.io/FSO/https://alikucukavci.github.io/FSO/</p>
        <p>https://pages.cdbb.uk/projects/https://pages.cdbb.uk/projects/</p>
        <p>This research forms part of the Centre for Digital Built Britain's (CDBB) work at the University of Cambridge within the Construction Innovation Hub (CIH). The Construction Innovation Hub is funded by UK Research and Innovation, UK through the Industrial Strategy Fund. Furthermore, the funding support by the Dutch Netherlands Enterprise Agency for the Brains4Buildings project is acknowledged as well to make part of this work possible.This research forms part of the Centre for Digital Built Britain's (CDBB) work at the University of Cambridge within the Construction Innovation Hub (CIH). The Construction Innovation Hub is funded by UK Research and Innovation, UK through the Industrial Strategy Fund. Furthermore, the funding support by the Dutch Netherlands Enterprise Agency for the Brains4Buildings project is acknowledged as well to make part of this work possible.</p>
        <p>Data will be made available on request.Data will be made available on request.</p>
    </text>
</tei>

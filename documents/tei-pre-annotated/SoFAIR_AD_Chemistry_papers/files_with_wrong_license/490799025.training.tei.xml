<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:05+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Forensic investigations are often reliant on physical evidence to reconstruct events surrounding a crime. However, there remains a need for more objective approaches to evidential interpretation, along with rigorously validated procedures for handling, storage and analysis. Chemometrics has been recognised as a powerful tool within forensic science for interpretation and optimisation of analytical procedures. However, careful consideration must be given to factors such as sampling, validation and underpinning study design. This tutorial review aims to provide an accessible overview of chemometric methods within the context of forensic science. The review begins with an overview of selected chemometric techniques, followed by a broad review of studies demonstrating the utility of chemometrics across various forensic disciplines. The tutorial review ends with the discussion of the challenges and emerging trends in this rapidly growing field. Table 1: Common distance metrics and linkage criteria used for agglomerative HCA. Mode Description Metric Euclidean 'Straight line' distance between objects in n-dimensional spaceForensic investigations are often reliant on physical evidence to reconstruct events surrounding a crime. However, there remains a need for more objective approaches to evidential interpretation, along with rigorously validated procedures for handling, storage and analysis. Chemometrics has been recognised as a powerful tool within forensic science for interpretation and optimisation of analytical procedures. However, careful consideration must be given to factors such as sampling, validation and underpinning study design. This tutorial review aims to provide an accessible overview of chemometric methods within the context of forensic science. The review begins with an overview of selected chemometric techniques, followed by a broad review of studies demonstrating the utility of chemometrics across various forensic disciplines. The tutorial review ends with the discussion of the challenges and emerging trends in this rapidly growing field. Table 1: Common distance metrics and linkage criteria used for agglomerative HCA. Mode Description Metric Euclidean 'Straight line' distance between objects in n-dimensional space</p>
        <p>Collective distance between objects along each dimension in n-dimensional spaceCollective distance between objects along each dimension in n-dimensional space</p>
        <p>Distance between an object and a distribution determined from the covariance between dimensions Linkage criterionDistance between an object and a distribution determined from the covariance between dimensions Linkage criterion</p>
        <p>The shortest distance between objects of two clusters, resulting in long clusters.The shortest distance between objects of two clusters, resulting in long clusters.</p>
        <p>The longest distance between objects of two clusters, resulting in compact clusters.The longest distance between objects of two clusters, resulting in compact clusters.</p>
        <p>Ward's linkage Groups clusters so as to minimise the increase in variance, resulting in globular and fairly compact clusters.Ward's linkage Groups clusters so as to minimise the increase in variance, resulting in globular and fairly compact clusters.</p>
        <p>A primary aim in many forensic investigations is to establish links between people, places, or objects in order to reconstruct events surrounding a crime. This is typically done through the recovery, analysis and interpretation of physical evidence. Many items of physical evidence are macroscopic items such as clothing or firearms. However, this category also includes 'trace evidence' (such as soil, glass, paint, hair, fibres, or explosive particulates) that can be cross-transferred between surfaces through physical contact or proximity. [1][2][3] Assuming that an item of physical evidence is successfully recovered and analysed, significant challenges arise in its interpretation. Many forensic disciplines rely on visual comparisons of complex images or multivariate chemical data in the form of spectra, chromatograms or other analytical output. 4,5 These comparisons require substantial time and expertise on the part of the examiner, and the visual complexity of the data may veil potentially useful information. 6,7 It has also become increasingly recognised that examiners are prone to cognitive or social biases that may affect their conclusions. [8][9][10] Finally, evaluative interpretation of evidence must be made in a way that is understood by the courts. 11 Several reports have identified the need for more objective and reliable approaches for evidence interpretation. These include reports issued by the 2009 National Academy of Sciences (United States), 2015-2018 Forensic Science Regulator (United Kingdom), and 2016 President's Council of Advisors on Science and Technology (United States). [12][13][14][15][16][17] These have encouraged strengthened efforts in the past decade to establish statistically validated bases for assessing evidential value. A widely accepted way of approaching this is through statistical inference, which allows the strength of evidence to be described in probabilistic terms. This can take the form of the frequentist approach commonly used in the United States, 18 or the Bayesian approach recommended by the European Network of Forensic Science Institutes 11 .A primary aim in many forensic investigations is to establish links between people, places, or objects in order to reconstruct events surrounding a crime. This is typically done through the recovery, analysis and interpretation of physical evidence. Many items of physical evidence are macroscopic items such as clothing or firearms. However, this category also includes 'trace evidence' (such as soil, glass, paint, hair, fibres, or explosive particulates) that can be cross-transferred between surfaces through physical contact or proximity. [1][2][3] Assuming that an item of physical evidence is successfully recovered and analysed, significant challenges arise in its interpretation. Many forensic disciplines rely on visual comparisons of complex images or multivariate chemical data in the form of spectra, chromatograms or other analytical output. 4,5 These comparisons require substantial time and expertise on the part of the examiner, and the visual complexity of the data may veil potentially useful information. 6,7 It has also become increasingly recognised that examiners are prone to cognitive or social biases that may affect their conclusions. [8][9][10] Finally, evaluative interpretation of evidence must be made in a way that is understood by the courts. 11 Several reports have identified the need for more objective and reliable approaches for evidence interpretation. These include reports issued by the 2009 National Academy of Sciences (United States), 2015-2018 Forensic Science Regulator (United Kingdom), and 2016 President's Council of Advisors on Science and Technology (United States). [12][13][14][15][16][17] These have encouraged strengthened efforts in the past decade to establish statistically validated bases for assessing evidential value. A widely accepted way of approaching this is through statistical inference, which allows the strength of evidence to be described in probabilistic terms. This can take the form of the frequentist approach commonly used in the United States, 18 or the Bayesian approach recommended by the European Network of Forensic Science Institutes 11 .</p>
        <p>The key difference between the two approaches is the way that they treat probability. In the frequentist view, probability can be applied to repeatable random events, but not to fixed parameters. Consider a case in which DNA from a crime scene has been associated with a suspect, and the significance of this result is in question. The frequentist approach would be to determine a random match probability, considering: "What is the probability that a randomly selected individual other than the suspect would exhibit an indistinguishable DNA profile?". Whether or not the DNA did in fact originate from the suspect is considered a fixed (albeit unknown) parameter and hence does not have a probability.The key difference between the two approaches is the way that they treat probability. In the frequentist view, probability can be applied to repeatable random events, but not to fixed parameters. Consider a case in which DNA from a crime scene has been associated with a suspect, and the significance of this result is in question. The frequentist approach would be to determine a random match probability, considering: "What is the probability that a randomly selected individual other than the suspect would exhibit an indistinguishable DNA profile?". Whether or not the DNA did in fact originate from the suspect is considered a fixed (albeit unknown) parameter and hence does not have a probability.</p>
        <p>Using a Bayesian approach on the other hand, the question becomes: "What is the probability of the profile being observed if the DNA originated from the suspect, compared to if it originated from another random individual?" Probability in this instance is used to describe the uncertainty around the fixed parameter of whether the DNA originated from the suspect. Moreover, this probability is considered in light of two opposing propositions (the suspect's guilt or innocence), with the relative probability of each expressed through a likelihood ratio (LR).Using a Bayesian approach on the other hand, the question becomes: "What is the probability of the profile being observed if the DNA originated from the suspect, compared to if it originated from another random individual?" Probability in this instance is used to describe the uncertainty around the fixed parameter of whether the DNA originated from the suspect. Moreover, this probability is considered in light of two opposing propositions (the suspect's guilt or innocence), with the relative probability of each expressed through a likelihood ratio (LR).</p>
        <p>Both frequentist and Bayesian approaches make use of calculated probabilities with defined levels of uncertainty, allowing evaluative interpretations to be expressed in more objective terms compared to "the evidence strongly supports" or "the evidence is consistent with". 19,20 However, this does not address potential error or bias in the interpretation of the evidence itself. For this reason, an increasing volume of literature has emerged investigating chemometric techniques for the analysis and interpretation of physical evidence.Both frequentist and Bayesian approaches make use of calculated probabilities with defined levels of uncertainty, allowing evaluative interpretations to be expressed in more objective terms compared to "the evidence strongly supports" or "the evidence is consistent with". 19,20 However, this does not address potential error or bias in the interpretation of the evidence itself. For this reason, an increasing volume of literature has emerged investigating chemometric techniques for the analysis and interpretation of physical evidence.</p>
        <p>Emerging in the 1970s for process monitoring and control, chemometrics involves the use of statistical approaches to analyse and model chemical information. 21 Many of these methods can simplify or reduce the dimensionality of complex data, which may reveal or explain underlying trends. As well as enhancing discrimination in questioned versus known comparisons, this may be valuable in generating investigative leads or forensic intelligence. Chemometric tools may also be used to investigate factors affecting these analyses through experimental design, which as of yet remains largely underutilised within a forensic context. Such approaches may lead to the development of statistically validated protocols for evidence collection, storage and analysis.Emerging in the 1970s for process monitoring and control, chemometrics involves the use of statistical approaches to analyse and model chemical information. 21 Many of these methods can simplify or reduce the dimensionality of complex data, which may reveal or explain underlying trends. As well as enhancing discrimination in questioned versus known comparisons, this may be valuable in generating investigative leads or forensic intelligence. Chemometric tools may also be used to investigate factors affecting these analyses through experimental design, which as of yet remains largely underutilised within a forensic context. Such approaches may lead to the development of statistically validated protocols for evidence collection, storage and analysis.</p>
        <p>The use of statistical techniques allows more objective and quantitative measures of data to be made compared to visual inspections. Although this offers improves objectivity in the actual data analysis, it is important to recognise that elements of subjectivity remain concerning parameter selection (e.g. data pre-processing methods, distance measures, or pre-defined numbers of groups) and interpretation of the output, as will be demonstrated in the sections below. These methods can thus reduce -but not eliminate -potential error or bias in forensic examinations. This review is intended to provide an accessible overview of common chemometric methods and their potential applications within forensic science. The first part of this review describes a selection of chemometric techniques, their outputs, and how this information may be interpreted using examples relevant to forensic casework. The second part outlines the applicability of chemometric techniques across various forensic disciplines, illustrated through a broad analysis of existing research. Finally, the review concludes with emerging trends and challenges in this rapidly growing field. It is not the intention to provide a comprehensive review of all chemometric methods or potential applications, for which the reader is directed to other literature as appropriate.The use of statistical techniques allows more objective and quantitative measures of data to be made compared to visual inspections. Although this offers improves objectivity in the actual data analysis, it is important to recognise that elements of subjectivity remain concerning parameter selection (e.g. data pre-processing methods, distance measures, or pre-defined numbers of groups) and interpretation of the output, as will be demonstrated in the sections below. These methods can thus reduce -but not eliminate -potential error or bias in forensic examinations. This review is intended to provide an accessible overview of common chemometric methods and their potential applications within forensic science. The first part of this review describes a selection of chemometric techniques, their outputs, and how this information may be interpreted using examples relevant to forensic casework. The second part outlines the applicability of chemometric techniques across various forensic disciplines, illustrated through a broad analysis of existing research. Finally, the review concludes with emerging trends and challenges in this rapidly growing field. It is not the intention to provide a comprehensive review of all chemometric methods or potential applications, for which the reader is directed to other literature as appropriate.</p>
        <p>Although they cover a range of potential applications, chemometric techniques can be divided into three general categories. Pattern recognition techniques are oriented towards the automated recognition of relationships within a dataset. These techniques can be further categorised as unsupervised methods (exploratory data analysis) that aim to identify trends, or supervised methods intended to model these trends for classification and prediction purposes. Regression methods are designed for the quantitative prediction of sample properties, and in some cases may be used as binary classifiers. Finally, experimental design techniques are used to uncover information about chemical processes, facilitating the design and optimisation of analytical procedures.Although they cover a range of potential applications, chemometric techniques can be divided into three general categories. Pattern recognition techniques are oriented towards the automated recognition of relationships within a dataset. These techniques can be further categorised as unsupervised methods (exploratory data analysis) that aim to identify trends, or supervised methods intended to model these trends for classification and prediction purposes. Regression methods are designed for the quantitative prediction of sample properties, and in some cases may be used as binary classifiers. Finally, experimental design techniques are used to uncover information about chemical processes, facilitating the design and optimisation of analytical procedures.</p>
        <p>The use and interpretation of selected chemometric techniques reported in the literature for forensic science are discussed below, making use of modelled open-access data in several instances. These models were designed using The 
            <rs type="software">Unscrambler</rs>
            <rs type="version">X 10.5</rs> (
            <rs type="creator">Camo Analytics</rs>, Norway) bundled with 
            <rs type="software">Design-Expert</rs>
            <rs type="version">10</rs> (
            <rs type="creator">Stat-Ease</rs>, USA), which collectively offer a broad range of data pre-processing, pattern recognition, regression and experimental design methods. Other commercial and open-source software packages for chemometric analysis are available, a selection of which are listed in Table S1.
        </p>
        <p>The purpose of unsupervised learning is to detect patterns in datasets without setting any prior labels or outcomes. The algorithm is thus left to infer patterns with minimal human intervention. Whilst these methods cannot be directly applied to classification or regression problems, they are ideal for probing the underlying structure of data. Additionally, some unsupervised methods allow new samples to be projected onto a pre-existing dataset. This is useful for comparative purposes but should not strictly be considered classification, as there is no assumption of specific classes existing.The purpose of unsupervised learning is to detect patterns in datasets without setting any prior labels or outcomes. The algorithm is thus left to infer patterns with minimal human intervention. Whilst these methods cannot be directly applied to classification or regression problems, they are ideal for probing the underlying structure of data. Additionally, some unsupervised methods allow new samples to be projected onto a pre-existing dataset. This is useful for comparative purposes but should not strictly be considered classification, as there is no assumption of specific classes existing.</p>
        <p>Cluster analysis refers to a group of algorithms in which objects (samples) are grouped according to their relative similarity. The choice of algorithm and its parameters depends on the properties of the dataset and the purpose of the analysis.Cluster analysis refers to a group of algorithms in which objects (samples) are grouped according to their relative similarity. The choice of algorithm and its parameters depends on the properties of the dataset and the purpose of the analysis.</p>
        <p>Hierarchical cluster analysis (HCA) is a common approach in which objects are connected to form clusters based on separation distance. This process is most often agglomerative; starting with single objects and progressively grouping them into larger clusters. In this 'bottom-up' approach, each object is initially treated as a cluster of size one, and the distances between each possible pair compared. Those that are the closest together are merged to form a new cluster, and the process repeated until a single cluster is obtained. The clusters formed are dependent on the user's selection of an appropriate metric (the measure of distance between objects) and linkage criteria (the manner in which relationships between clusters are established). 22,23 The choice of metric will influence the grouping of objects, as some objects may be closely related according to one metric but not using another. The linkage criteria on the other hand influences the shape of the clusters formed. Common metrics and linkage criteria are described in Table 1.Hierarchical cluster analysis (HCA) is a common approach in which objects are connected to form clusters based on separation distance. This process is most often agglomerative; starting with single objects and progressively grouping them into larger clusters. In this 'bottom-up' approach, each object is initially treated as a cluster of size one, and the distances between each possible pair compared. Those that are the closest together are merged to form a new cluster, and the process repeated until a single cluster is obtained. The clusters formed are dependent on the user's selection of an appropriate metric (the measure of distance between objects) and linkage criteria (the manner in which relationships between clusters are established). 22,23 The choice of metric will influence the grouping of objects, as some objects may be closely related according to one metric but not using another. The linkage criteria on the other hand influences the shape of the clusters formed. Common metrics and linkage criteria are described in Table 1.</p>
        <p>The hierarchical relationship between clusters is typically illustrated using a dendrogram, where objects are plotted according to their determined clustering pattern. An example is shown in Figure 1, where Maric et al. used HCA to investigate over 700 automotive clear coats using attenuated total reflectance Fourier transform infrared (ATR-FTIR) spectroscopy. 24 Using the squared Euclidean metric, Ward's linkage method and Calinski-Harabasz pseudo-F index as a stopping rule to determine the optimal number of clusters, eight classes were identified and associated with the country of vehicle manufacture. In this case, the horizontal axis of the dendrogram represents the relative distance between samples. The vertical bars of the dendrogram thus indicate the distance at which a pair of samples or clusters are merged. However, it is also common to plot samples according to a calculated similarity measure, such as by comparing the distance between any given pair of samples or clusters to the maximum distance between any pair in the dataset.The hierarchical relationship between clusters is typically illustrated using a dendrogram, where objects are plotted according to their determined clustering pattern. An example is shown in Figure 1, where Maric et al. used HCA to investigate over 700 automotive clear coats using attenuated total reflectance Fourier transform infrared (ATR-FTIR) spectroscopy. 24 Using the squared Euclidean metric, Ward's linkage method and Calinski-Harabasz pseudo-F index as a stopping rule to determine the optimal number of clusters, eight classes were identified and associated with the country of vehicle manufacture. In this case, the horizontal axis of the dendrogram represents the relative distance between samples. The vertical bars of the dendrogram thus indicate the distance at which a pair of samples or clusters are merged. However, it is also common to plot samples according to a calculated similarity measure, such as by comparing the distance between any given pair of samples or clusters to the maximum distance between any pair in the dataset.</p>
        <p>The dendrogram provides a simple means of visualising relative similarity in quantitative terms. In Figure 1 for example, Class 1 is the last to be merged with the remaining clusters, at a relative distance of almost 10. Classes 5 and 7 by contrast are within a relative distance of approximately 0.4, and so are merged at an early stage of clustering. These hierarchical relationships reflect the relative similarities of the input data, which can be seen through examination of the original spectra. A typical spectrum for Class 1 lacks several of the absorbance bands seen in the spectra of other classes, which can be inferred to be the reason for its greater separation distance. Identifying relationships within a dataset can be challenging when examining complex multivariate data such as spectra or chromatograms. Additionally, it may be of interest to identify the features responsible for similarity or dissimilarity between objects. This information can only be extracted from cluster analysis by inferring from visual comparisons of the original data (seen above for HCA), which may not be practical for large datasets.The dendrogram provides a simple means of visualising relative similarity in quantitative terms. In Figure 1 for example, Class 1 is the last to be merged with the remaining clusters, at a relative distance of almost 10. Classes 5 and 7 by contrast are within a relative distance of approximately 0.4, and so are merged at an early stage of clustering. These hierarchical relationships reflect the relative similarities of the input data, which can be seen through examination of the original spectra. A typical spectrum for Class 1 lacks several of the absorbance bands seen in the spectra of other classes, which can be inferred to be the reason for its greater separation distance. Identifying relationships within a dataset can be challenging when examining complex multivariate data such as spectra or chromatograms. Additionally, it may be of interest to identify the features responsible for similarity or dissimilarity between objects. This information can only be extracted from cluster analysis by inferring from visual comparisons of the original data (seen above for HCA), which may not be practical for large datasets.</p>
        <p>PCA reduces data dimensionality by extracting the orthogonal sources of variation; known as 'principal components' or PCs. These PCs are a linear combination of the original variables, each multiplied by a loading. Each successive PC is calculated to describe the maximum proportion of remaining variation in the data. This process is represented in Figure 2a for a simplified dataset. Here, samples are initially represented as data points in a feature space described by the original variables; X, Y and Z. Each PC is determined by finding the direction along which the remaining dispersion of the data points is the greatest. Projection of the samples onto the components then allows them to be alternatively described using the PC coordinates, or scores. A scree plot, which shows the cumulative variance accounted for with each successive PC, can be used to indicate those containing 'useful' information. From a mathematical perspective, PCA transforms the original data matrix (X) into separate scores (T), loadings (P) and residuals(E) matrices, according to the general equation:PCA reduces data dimensionality by extracting the orthogonal sources of variation; known as 'principal components' or PCs. These PCs are a linear combination of the original variables, each multiplied by a loading. Each successive PC is calculated to describe the maximum proportion of remaining variation in the data. This process is represented in Figure 2a for a simplified dataset. Here, samples are initially represented as data points in a feature space described by the original variables; X, Y and Z. Each PC is determined by finding the direction along which the remaining dispersion of the data points is the greatest. Projection of the samples onto the components then allows them to be alternatively described using the PC coordinates, or scores. A scree plot, which shows the cumulative variance accounted for with each successive PC, can be used to indicate those containing 'useful' information. From a mathematical perspective, PCA transforms the original data matrix (X) into separate scores (T), loadings (P) and residuals(E) matrices, according to the general equation:</p>
        <p>This may be done through analysis of either the correlation or the covariance matrix. 22 The correlation method mean centres and scales the data to place equal weight on each variable, accounting for different units or levels of variance. The covariance method applies mean centring without data scaling, which is preferred where the units and magnitude of variance are comparable across all variables.This may be done through analysis of either the correlation or the covariance matrix. 22 The correlation method mean centres and scales the data to place equal weight on each variable, accounting for different units or levels of variance. The covariance method applies mean centring without data scaling, which is preferred where the units and magnitude of variance are comparable across all variables.</p>
        <p>As the majority of variation from the original dataset is retained in the first few PCs, multiplying the scores and loadings for these PCs gives an approximation of the original dataset. The discrepancy between the actual original data and this approximation is described by the residuals matrix (E), which ideally encompasses random variation or noise. The residuals can be plotted to assess the magnitude and pattern of residual discrepancy across each sample, with small and randomly dispersed residuals being ideal. Residual plots can hence be useful in evaluating and optimising the performance of the model.As the majority of variation from the original dataset is retained in the first few PCs, multiplying the scores and loadings for these PCs gives an approximation of the original dataset. The discrepancy between the actual original data and this approximation is described by the residuals matrix (E), which ideally encompasses random variation or noise. The residuals can be plotted to assess the magnitude and pattern of residual discrepancy across each sample, with small and randomly dispersed residuals being ideal. Residual plots can hence be useful in evaluating and optimising the performance of the model.</p>
        <p>The scores of a sample attained against any two or three PCs may be used as a new coordinate system, generating a scores plot where similar samples are clustered together. Additionally, the PCA loadings may be examined to identify how variables of the original dataset are weighted against a particular PC. Variables with highly positive or negative loadings are significant in determining a sample's score along that component. PCA hence not only allows the separation of samples, but can reveal the basis for this separation according to the underlying features of the sample.The scores of a sample attained against any two or three PCs may be used as a new coordinate system, generating a scores plot where similar samples are clustered together. Additionally, the PCA loadings may be examined to identify how variables of the original dataset are weighted against a particular PC. Variables with highly positive or negative loadings are significant in determining a sample's score along that component. PCA hence not only allows the separation of samples, but can reveal the basis for this separation according to the underlying features of the sample.</p>
        <p>PCA plots offer an alternative means of visualising data, which can assist in identifying patterns that are not obvious from the raw information. However, as with HCA, a full understanding of these trends still requires examination of the initial data. This is illustrated in Figure 3, in which PCA was performed on ATR-FTIR spectra acquired from 22 red-shaded lipsticks. 26 In this example, the Labiotte 'Chardonnay Orange' lipstick was found to be highly dissimilar (clustered further away) from the remaining samples, particularly along PC1. The factor loadings for PC1 showed a strong positive correlation at ca. 3300 cm -1 , consistent with an OH stretching band. When examining the original infrared spectra, the Labiotte lipstick showed strong absorbance in this region, attributed to water and butylene glycol in its composition. This high absorbance, combined with a strong positive loading, resulted in a more positive PC score. Combined examination of both the PCA plots and the original spectra thus led to the conclusion that the Labiotte lipstick had a distinct chemical formulation from other lipsticks; specifically as it contained water and butylene glycol as co-solvents.PCA plots offer an alternative means of visualising data, which can assist in identifying patterns that are not obvious from the raw information. However, as with HCA, a full understanding of these trends still requires examination of the initial data. This is illustrated in Figure 3, in which PCA was performed on ATR-FTIR spectra acquired from 22 red-shaded lipsticks. 26 In this example, the Labiotte 'Chardonnay Orange' lipstick was found to be highly dissimilar (clustered further away) from the remaining samples, particularly along PC1. The factor loadings for PC1 showed a strong positive correlation at ca. 3300 cm -1 , consistent with an OH stretching band. When examining the original infrared spectra, the Labiotte lipstick showed strong absorbance in this region, attributed to water and butylene glycol in its composition. This high absorbance, combined with a strong positive loading, resulted in a more positive PC score. Combined examination of both the PCA plots and the original spectra thus led to the conclusion that the Labiotte lipstick had a distinct chemical formulation from other lipsticks; specifically as it contained water and butylene glycol as co-solvents.</p>
        <p>The purpose of supervised learning is to map labelled inputs to an expected output. Input variables are paired with the desired output or classification, with the algorithm being tasked to develop a function that correlates the two. In other words, supervised algorithms are designed to create functions based on known data that can then draw inferences about new samples. These methods are ideal for classification and regression problems, as the model can be 'trained' to detect and accurately model specific patterns. The model must then be validated to assess how it will generalise these patterns to independent datasets. 23 The importance of rigorous validation cannot be overstated, as failure to establish accurate error rates has significant implications for criminal justice. For a more detailed discussion of validation approaches, the reader is directed to focussed reviews on this topic. 28,29 Initial model selection often makes use of resampling, where 'test' samples representing new inputs are drawn from the original data. This is often done multiple times, with the overall predictive ability calculated as an average across all iterations. The most popular of these approaches is cross-validation (CV), in which a subset of the data is withheld as the testing set. The withheld portion may consist of individual samples (leave-one-out CV) or a block (leave-p-out CV and related variants). Another mode of resampling is the bootstrap, where test samples are drawn with replacement and hence the same sample can be represented multiple times. Resampling methods are economical, but the overlap between training and testing data can over-estimate the model's performance. This is particularly the case for leave-one-out CV, which is discouraged except where samples are extremely limited.The purpose of supervised learning is to map labelled inputs to an expected output. Input variables are paired with the desired output or classification, with the algorithm being tasked to develop a function that correlates the two. In other words, supervised algorithms are designed to create functions based on known data that can then draw inferences about new samples. These methods are ideal for classification and regression problems, as the model can be 'trained' to detect and accurately model specific patterns. The model must then be validated to assess how it will generalise these patterns to independent datasets. 23 The importance of rigorous validation cannot be overstated, as failure to establish accurate error rates has significant implications for criminal justice. For a more detailed discussion of validation approaches, the reader is directed to focussed reviews on this topic. 28,29 Initial model selection often makes use of resampling, where 'test' samples representing new inputs are drawn from the original data. This is often done multiple times, with the overall predictive ability calculated as an average across all iterations. The most popular of these approaches is cross-validation (CV), in which a subset of the data is withheld as the testing set. The withheld portion may consist of individual samples (leave-one-out CV) or a block (leave-p-out CV and related variants). Another mode of resampling is the bootstrap, where test samples are drawn with replacement and hence the same sample can be represented multiple times. Resampling methods are economical, but the overlap between training and testing data can over-estimate the model's performance. This is particularly the case for leave-one-out CV, which is discouraged except where samples are extremely limited.</p>
        <p>A more robust method is test set validation, where the model is evaluated using an independent test set that commonly makes up 30 % of the total data. To provide the most rigorous evaluation of the model, this data should be selected in such a way as to adequately represent the full sample population, including samples likely to be challenging from a predictive standpoint. 30A more robust method is test set validation, where the model is evaluated using an independent test set that commonly makes up 30 % of the total data. To provide the most rigorous evaluation of the model, this data should be selected in such a way as to adequately represent the full sample population, including samples likely to be challenging from a predictive standpoint. 30</p>
        <p>The k-nearest neighbour (kNN) method is based on the distance between known and unknown samples. The training set is divided into known classes, and the distance metric between the unknown and each training sample determined. The unknown is subsequently assigned to the pre-determined class that is most common among its 'k' nearest neighbours (Figure 4). 'k' is generally a small integer, with the optimal value often determined through cross-validation. This selection may alter the predictive result, demonstrating how some degree of subjectivity can remain in chemometric analysis. In Figure 4, setting 'k' at a value of 3 leads to the unknown sample (red star) being classified into Class A, but a marginally higher 'k' of 7 leads to an assignation of Class B. kNN has the advantage of being simple to implement, as the training phase is minimal and there is no requirement for linear separation of classes. Additionally, the only assumption placed on the data is that it exists in a feature space, allowing for distance measures to be calculated. However, limitations of the kNN method are its dependence on the 'k' value selected and its bias towards larger training classes, as these represent a greater number of potential neighbours. Additionally, as there is no reduction or screening of the data prior to classification, the potential exists for overly influential variables to distort the results. For this reason, kNN is primarily used in forensic research to establish the relative performance of other classification methods. [31][32][33][34] 2.2.2 Linear discriminant analysis (LDA) Discriminant analysis (DA) aims to classify objects into pre-defined, mutually exclusive classes based on scores derived from a discriminant function. This function is a combination of input variables, calculated to maximise the ratio of between-class to within-class variance (the Fisher ratio). If it is assumed that the separating function is linear, this method is referred to as linear discriminant analysis (LDA). This approach is similar to PCA, in that they each look for linear combinations of variables to best describe trends in complex data. LDA is hence a valuable tool for data visualisation and classification. However, LDA requires the number of samples to exceed the number of descriptor variables. For this reason, LDA is often carried out following a preliminary data reduction method such as PCA. This is particularly the case when dealing with highly multivariate data and correlated data such as spectra.The k-nearest neighbour (kNN) method is based on the distance between known and unknown samples. The training set is divided into known classes, and the distance metric between the unknown and each training sample determined. The unknown is subsequently assigned to the pre-determined class that is most common among its 'k' nearest neighbours (Figure 4). 'k' is generally a small integer, with the optimal value often determined through cross-validation. This selection may alter the predictive result, demonstrating how some degree of subjectivity can remain in chemometric analysis. In Figure 4, setting 'k' at a value of 3 leads to the unknown sample (red star) being classified into Class A, but a marginally higher 'k' of 7 leads to an assignation of Class B. kNN has the advantage of being simple to implement, as the training phase is minimal and there is no requirement for linear separation of classes. Additionally, the only assumption placed on the data is that it exists in a feature space, allowing for distance measures to be calculated. However, limitations of the kNN method are its dependence on the 'k' value selected and its bias towards larger training classes, as these represent a greater number of potential neighbours. Additionally, as there is no reduction or screening of the data prior to classification, the potential exists for overly influential variables to distort the results. For this reason, kNN is primarily used in forensic research to establish the relative performance of other classification methods. [31][32][33][34] 2.2.2 Linear discriminant analysis (LDA) Discriminant analysis (DA) aims to classify objects into pre-defined, mutually exclusive classes based on scores derived from a discriminant function. This function is a combination of input variables, calculated to maximise the ratio of between-class to within-class variance (the Fisher ratio). If it is assumed that the separating function is linear, this method is referred to as linear discriminant analysis (LDA). This approach is similar to PCA, in that they each look for linear combinations of variables to best describe trends in complex data. LDA is hence a valuable tool for data visualisation and classification. However, LDA requires the number of samples to exceed the number of descriptor variables. For this reason, LDA is often carried out following a preliminary data reduction method such as PCA. This is particularly the case when dealing with highly multivariate data and correlated data such as spectra.</p>
        <p>Table 2 shows the primary output of LDA; a confusion matrix; performed on the ATR-FTIR spectra of three classes of polyethylene cling film identified through PCA. 35 As the sample size of each group must exceed the number of variables, the first two PCs from PCA were substituted in place of the original variable set. Several other approaches to variable selection have also been described, such as the Wilks' lambda test statistic, Bayesian information criterion, or simply observing features common to (but variable amongst) each group. [36][37][38] The confusion matrix summarises the number of objects in each known class of the training set and their predicted classes using the LDA model. Numbers in the diagonal of the matrix (bolded above) thus indicate objects that have been correctly assigned, and this can be used to determine a discrimination accuracy for the model. This matrix can also be used to identify classes that are not effectively separated, as these will appear as misclassified samples.Table 2 shows the primary output of LDA; a confusion matrix; performed on the ATR-FTIR spectra of three classes of polyethylene cling film identified through PCA. 35 As the sample size of each group must exceed the number of variables, the first two PCs from PCA were substituted in place of the original variable set. Several other approaches to variable selection have also been described, such as the Wilks' lambda test statistic, Bayesian information criterion, or simply observing features common to (but variable amongst) each group. [36][37][38] The confusion matrix summarises the number of objects in each known class of the training set and their predicted classes using the LDA model. Numbers in the diagonal of the matrix (bolded above) thus indicate objects that have been correctly assigned, and this can be used to determine a discrimination accuracy for the model. This matrix can also be used to identify classes that are not effectively separated, as these will appear as misclassified samples.</p>
        <p>One should be wary, however, of accepting these results without further critique. As LDA is a 'hard' classification method, objects will be assigned to the single class to which it demonstrates the closest similarity. This does not recognise the possibility of objects not yet described by any data within the model, which would hence be poorly classified. However, these atypical objects may be identified through their discriminant values (DVs), which act as distance measures between the object and the centroid of each known class. Samples may also be plotted according to their DVs against any two classes, producing a discriminant plot. This may be interpreted in a similar fashion to the PCA scores plot. However, the fundamental difference is that PCA is designed to explore the largest variation present regardless of whether this corresponds to particular sample features. LDA by contrast looks for variation that is useful in distinguishing pre-established groups. For this reason, the clustering pattern may differ when examining the discriminant plot (Figure 5). The discriminant plot may be used as a visual way of assessing the quality of separation. In some scenarios, classification needs to be more flexible than kNN or LDA would permit. Soft independent modelling of class analogy (SIMCA) is a disjoint technique, meaning that it constructs separate models (in this case derived using PCA) describing a boundary around each class. The number of PCs retained for each model is highly influential, as too few will result in a loss of information whilst too many will introduce noise. Cross-validation is typically used for determining the optimal number of PCs describing each class, although other measures such as the Malinowski indicator function can also be used. 40,41 Classification of a new object is based on its sample-to-model distance for each class as determined by two limits; residual variance and leverage. 42 The residual variance of an object, calculated as a residual sum of squares, is that which remains unexplained after projection onto a known class. The critical limit for residuals distances is commonly determined using a chi-square distribution or Jackson-Mudholkar approximation. 43 Leverage is the Mahalanobis distance between an object and the centroid of a given class, with a high leverage indicating outlying objects in the model space. An object falls within the above limits if the residual variance and leverage do not exceed the selected cut-off values.One should be wary, however, of accepting these results without further critique. As LDA is a 'hard' classification method, objects will be assigned to the single class to which it demonstrates the closest similarity. This does not recognise the possibility of objects not yet described by any data within the model, which would hence be poorly classified. However, these atypical objects may be identified through their discriminant values (DVs), which act as distance measures between the object and the centroid of each known class. Samples may also be plotted according to their DVs against any two classes, producing a discriminant plot. This may be interpreted in a similar fashion to the PCA scores plot. However, the fundamental difference is that PCA is designed to explore the largest variation present regardless of whether this corresponds to particular sample features. LDA by contrast looks for variation that is useful in distinguishing pre-established groups. For this reason, the clustering pattern may differ when examining the discriminant plot (Figure 5). The discriminant plot may be used as a visual way of assessing the quality of separation. In some scenarios, classification needs to be more flexible than kNN or LDA would permit. Soft independent modelling of class analogy (SIMCA) is a disjoint technique, meaning that it constructs separate models (in this case derived using PCA) describing a boundary around each class. The number of PCs retained for each model is highly influential, as too few will result in a loss of information whilst too many will introduce noise. Cross-validation is typically used for determining the optimal number of PCs describing each class, although other measures such as the Malinowski indicator function can also be used. 40,41 Classification of a new object is based on its sample-to-model distance for each class as determined by two limits; residual variance and leverage. 42 The residual variance of an object, calculated as a residual sum of squares, is that which remains unexplained after projection onto a known class. The critical limit for residuals distances is commonly determined using a chi-square distribution or Jackson-Mudholkar approximation. 43 Leverage is the Mahalanobis distance between an object and the centroid of a given class, with a high leverage indicating outlying objects in the model space. An object falls within the above limits if the residual variance and leverage do not exceed the selected cut-off values.</p>
        <p>Objects will be assigned to any class for which they meet both of the above limits. Unlike other supervised techniques, SIMCA thus permits classification into one, multiple, or none of the known classes. 'Soft' classifications allow easier identification of atypical samples compared to kNN or LDA. On the other hand, the disjoint modelling of each class means that there is limited measure of between-class to within-class variation. Consequently, SIMCA is highly sensitive to sample leverage and variance, which may result in incorrect rejections (false negatives). This may be compensated for by reducing the significance level (α), or the percentage of training samples deemed acceptable as outliers. 44 By reducing α and thus expanding the model boundaries, more samples may be accepted for classification -but at the risk of increased false positive classifications.Objects will be assigned to any class for which they meet both of the above limits. Unlike other supervised techniques, SIMCA thus permits classification into one, multiple, or none of the known classes. 'Soft' classifications allow easier identification of atypical samples compared to kNN or LDA. On the other hand, the disjoint modelling of each class means that there is limited measure of between-class to within-class variation. Consequently, SIMCA is highly sensitive to sample leverage and variance, which may result in incorrect rejections (false negatives). This may be compensated for by reducing the significance level (α), or the percentage of training samples deemed acceptable as outliers. 44 By reducing α and thus expanding the model boundaries, more samples may be accepted for classification -but at the risk of increased false positive classifications.</p>
        <p>An example is shown in Table 4, where PCA models were constructed to describe three issued series of Western Australian driver's licence cards based on their cross-sectional ATR-FTIR spectra. The SIMCA approach was then used to predict spectra originating from a Western Australian driver's licence of each series, as well as a Swiss driver's licence not yet represented in the model. The 2001 test sample was assigned to both the 2001 and 2014 series, as these had been established to be chemically similar, 45 whereas the Swiss licence was identified as an outlier and not assigned to any of the three series. It can also be seen that two of the four replicate spectra from the 2014 test sample were incorrectly rejected, reflecting a drawback to SIMCA prediction. These classifications are often visualised using a Coomans plot, which shows the sample-tomodel distance of a given sample against any two groups. 46 This may be considered analogous to the discriminant plot within LDA. As with PCA, residuals plots can also be used as an additional tool for evaluating and optimising the SIMCA model.An example is shown in Table 4, where PCA models were constructed to describe three issued series of Western Australian driver's licence cards based on their cross-sectional ATR-FTIR spectra. The SIMCA approach was then used to predict spectra originating from a Western Australian driver's licence of each series, as well as a Swiss driver's licence not yet represented in the model. The 2001 test sample was assigned to both the 2001 and 2014 series, as these had been established to be chemically similar, 45 whereas the Swiss licence was identified as an outlier and not assigned to any of the three series. It can also be seen that two of the four replicate spectra from the 2014 test sample were incorrectly rejected, reflecting a drawback to SIMCA prediction. These classifications are often visualised using a Coomans plot, which shows the sample-tomodel distance of a given sample against any two groups. 46 This may be considered analogous to the discriminant plot within LDA. As with PCA, residuals plots can also be used as an additional tool for evaluating and optimising the SIMCA model.</p>
        <p>Metrics of note in the SIMCA output are the modelling and discriminatory power. 42,47 Modelling power is analogous to a factor loading with PCA; showing the contribution of an initial variable to describing a particular class. A high modelling power (approaching 1) indicates the variable to be more relevant. By contrast, discriminatory power describes a variable's importance to discriminating between two separate models. Variables that possess both low modelling and low discriminatory power are likely to be encompassing noise, and can potentially be omitted. Another important indicator is the model distance, which reflects how well two classes are separated based on residual variance. Generally, models with an inter-class distance above three may be considered well-separated.Metrics of note in the SIMCA output are the modelling and discriminatory power. 42,47 Modelling power is analogous to a factor loading with PCA; showing the contribution of an initial variable to describing a particular class. A high modelling power (approaching 1) indicates the variable to be more relevant. By contrast, discriminatory power describes a variable's importance to discriminating between two separate models. Variables that possess both low modelling and low discriminatory power are likely to be encompassing noise, and can potentially be omitted. Another important indicator is the model distance, which reflects how well two classes are separated based on residual variance. Generally, models with an inter-class distance above three may be considered well-separated.</p>
        <p>In situations where known classes cannot be linearly separated, support vector machines (SVMs) are considered by many as an ideal classifier. In SVM analysis, samples are considered as points in a multi-dimensional space. Boundaries between different classes are established in the form of a separating gap or hyperplane, which is mapped to be as wide as possible (Figure 6a). The hyperplane (classifier) can be expressed in terms of the samples lying closest to its boundaries, which are known as support vectors. Classification then occurs by mapping new samples into the same space as the training set, and assigning them to a known class based upon which side of the boundary they fall on.In situations where known classes cannot be linearly separated, support vector machines (SVMs) are considered by many as an ideal classifier. In SVM analysis, samples are considered as points in a multi-dimensional space. Boundaries between different classes are established in the form of a separating gap or hyperplane, which is mapped to be as wide as possible (Figure 6a). The hyperplane (classifier) can be expressed in terms of the samples lying closest to its boundaries, which are known as support vectors. Classification then occurs by mapping new samples into the same space as the training set, and assigning them to a known class based upon which side of the boundary they fall on.</p>
        <p>In order to establish the classifier between non-linearly separable classes, the data is transformed through a mathematical function (referred to as a kernel). This transforms the data into a higher-dimensional space, in which linear separation is then achievable (Figure 6b). The mathematical basis behind such kernel transformations (the most common being linear, polynomial or Gaussian radial basis function) or computation of the classifier function are beyond the scope of this review, but have been covered in other literature. Table 5 shows the results of SVM applied to the driver's licence series previously shown for SIMCA. Because SVM is designed for binary classification, its application to multi-class problems requires the construction of separate classifiers to distinguish each possible class pair. On this occasion, all three Western Australian licences were correctly classified, and unlike SIMCA, all replicate spectra were assigned to a class. However, this has also resulted in the anomalous Swiss licence being assigned to the most similar series (2011), despite possessing a different polymer composition. SVM models can prove advantageous for analysing high dimensionality data, as they allow non-linear separations without the tedious process of building and training an artificial neural network (discussed further below). They also work with fewer training samples, and are able to handle datasets where the number of variables exceeds the number of samples. A disadvantage however is that there are no immediate metrics provided to indicate the reliability of discrimination. Additionally, results may be highly dependent on the selection of the kernel function and associated parameters, which can make optimisation more arduous.In order to establish the classifier between non-linearly separable classes, the data is transformed through a mathematical function (referred to as a kernel). This transforms the data into a higher-dimensional space, in which linear separation is then achievable (Figure 6b). The mathematical basis behind such kernel transformations (the most common being linear, polynomial or Gaussian radial basis function) or computation of the classifier function are beyond the scope of this review, but have been covered in other literature. Table 5 shows the results of SVM applied to the driver's licence series previously shown for SIMCA. Because SVM is designed for binary classification, its application to multi-class problems requires the construction of separate classifiers to distinguish each possible class pair. On this occasion, all three Western Australian licences were correctly classified, and unlike SIMCA, all replicate spectra were assigned to a class. However, this has also resulted in the anomalous Swiss licence being assigned to the most similar series (2011), despite possessing a different polymer composition. SVM models can prove advantageous for analysing high dimensionality data, as they allow non-linear separations without the tedious process of building and training an artificial neural network (discussed further below). They also work with fewer training samples, and are able to handle datasets where the number of variables exceeds the number of samples. A disadvantage however is that there are no immediate metrics provided to indicate the reliability of discrimination. Additionally, results may be highly dependent on the selection of the kernel function and associated parameters, which can make optimisation more arduous.</p>
        <p>Decision trees are a popular tool in machine learning for constructing and visualising classification rules. The tree is made up of several nodes, each representing a test on a particular sample attribute. Depending on the outcome of the test, the sample is directed along the corresponding branch to the next node, where the process repeats until a class label (leaf) is reached.Decision trees are a popular tool in machine learning for constructing and visualising classification rules. The tree is made up of several nodes, each representing a test on a particular sample attribute. Depending on the outcome of the test, the sample is directed along the corresponding branch to the next node, where the process repeats until a class label (leaf) is reached.</p>
        <p>Whilst decision trees have the advantages of both simplicity and transparency, they are inherently unstable. Small changes in the dataset can lead to significant changes in the tree structure, which in turn can lead to errors. This can be remedied through use of a random forest (RF); an ensemble of independent decision trees (Figure 7). These trees are built via a 'bagging' process, wherein a random subset of samples is selected for each tree, and a random subset of features used to build each tree's structure. 49,50 The trees thus have low correlation to each other, compensating for individual errors. Input from an unknown sample is simultaneously processed by each tree, and the mode result given as the RF classification. Although usually considered a classification method, RFs can also be applied to unsupervised learning as their architecture naturally investigates dissimilarity amongst samples. Alternatively, they can be used for regression, with the mean prediction value given as the overall output. 50 Random forests are hence a versatile approach relatively robust to outliers or noise. Unlike many other classifiers, they are also able to handle missing data values. On the other hand, the complexity of the network can be more difficult to interpret and requires more computational resources to build and train.Whilst decision trees have the advantages of both simplicity and transparency, they are inherently unstable. Small changes in the dataset can lead to significant changes in the tree structure, which in turn can lead to errors. This can be remedied through use of a random forest (RF); an ensemble of independent decision trees (Figure 7). These trees are built via a 'bagging' process, wherein a random subset of samples is selected for each tree, and a random subset of features used to build each tree's structure. 49,50 The trees thus have low correlation to each other, compensating for individual errors. Input from an unknown sample is simultaneously processed by each tree, and the mode result given as the RF classification. Although usually considered a classification method, RFs can also be applied to unsupervised learning as their architecture naturally investigates dissimilarity amongst samples. Alternatively, they can be used for regression, with the mean prediction value given as the overall output. 50 Random forests are hence a versatile approach relatively robust to outliers or noise. Unlike many other classifiers, they are also able to handle missing data values. On the other hand, the complexity of the network can be more difficult to interpret and requires more computational resources to build and train.</p>
        <p>RFs remain an emerging area of forensic interest, with studies to date largely focussed on body fluid identification [51][52][53] and age predictions. 54,55 As this approach is still being trialled within forensic contexts, a detailed example of RF classification is considered beyond the scope of this review. The reader is directed towards cited the studies for additional reading.RFs remain an emerging area of forensic interest, with studies to date largely focussed on body fluid identification [51][52][53] and age predictions. 54,55 As this approach is still being trialled within forensic contexts, a detailed example of RF classification is considered beyond the scope of this review. The reader is directed towards cited the studies for additional reading.</p>
        <p>Artificial neural networks (ANNs) are computational models based on the assembly and functions of biological neural structures. ANNs consist of interconnected nodes or 'neurons' that are typically aggregated into an input layer, one or more hidden layers, and an output layer, as shown in Figure 8a. This figure shows a fully connected network, where each neuron in a given layer connects to every neuron in the next layer. Other architectures include pooled (feed-forward) networks, in which neurons of one layer connect to a single neuron in the subsequent layer; or recurrent networks that allow links to be formed to the same or previous layers. 56 The more complex the network, the more powerful it becomes. However, they will also require a greater degree of training in order to establish relationships.Artificial neural networks (ANNs) are computational models based on the assembly and functions of biological neural structures. ANNs consist of interconnected nodes or 'neurons' that are typically aggregated into an input layer, one or more hidden layers, and an output layer, as shown in Figure 8a. This figure shows a fully connected network, where each neuron in a given layer connects to every neuron in the next layer. Other architectures include pooled (feed-forward) networks, in which neurons of one layer connect to a single neuron in the subsequent layer; or recurrent networks that allow links to be formed to the same or previous layers. 56 The more complex the network, the more powerful it becomes. However, they will also require a greater degree of training in order to establish relationships.</p>
        <p>Inputs received by a neuron are multiplied by a weighting through various mathematical operations (Figure 8b) that can be considered to reflect the strength of the neural connection.Inputs received by a neuron are multiplied by a weighting through various mathematical operations (Figure 8b) that can be considered to reflect the strength of the neural connection.</p>
        <p>The resulting products of the inputs are summed, then processed through a non-linear activation function to generate an output. 22 This output must meet a minimum threshold in order to be passed on to subsequent neurons. When the information reaches the output layer, an appropriate response is generated. The key advantage of ANNs is their ability to learn from example. The ANN is trained by conducting an initial prediction of a training set, then using any errors to adjust the hidden layers until the required accuracy is achieved (back-propagation). 57 ANNs also do not assume any initial mathematical relationship between inputs and outputs, and have the ability to implement non-linear functions. 23 This makes them highly flexible, which is ideal for analysing complex data. On the other hand, the lack of an assumed model may require more training data in order to derive relationships.The resulting products of the inputs are summed, then processed through a non-linear activation function to generate an output. 22 This output must meet a minimum threshold in order to be passed on to subsequent neurons. When the information reaches the output layer, an appropriate response is generated. The key advantage of ANNs is their ability to learn from example. The ANN is trained by conducting an initial prediction of a training set, then using any errors to adjust the hidden layers until the required accuracy is achieved (back-propagation). 57 ANNs also do not assume any initial mathematical relationship between inputs and outputs, and have the ability to implement non-linear functions. 23 This makes them highly flexible, which is ideal for analysing complex data. On the other hand, the lack of an assumed model may require more training data in order to derive relationships.</p>
        <p>A significant drawback to ANNs is the lack of transparency in the hidden layers, meaning there is no direct means of extracting information on the model's processes. This has to date limited the use of ANNs in forensic analysis, where the ability to explain and justify results is a fundamental requirement. Despite this, a small volume of literature has applied ANNs within forensic entomology for insect age determination, [58][59][60] anthropology for sex or vertebral height estimation, [61][62][63] crash velocity prediction, 64 or age estimation on the basis of blood DNA methylation. 65 The reader is again directed towards these research studies to further explore this approach.A significant drawback to ANNs is the lack of transparency in the hidden layers, meaning there is no direct means of extracting information on the model's processes. This has to date limited the use of ANNs in forensic analysis, where the ability to explain and justify results is a fundamental requirement. Despite this, a small volume of literature has applied ANNs within forensic entomology for insect age determination, [58][59][60] anthropology for sex or vertebral height estimation, [61][62][63] crash velocity prediction, 64 or age estimation on the basis of blood DNA methylation. 65 The reader is again directed towards these research studies to further explore this approach.</p>
        <p>Multivariate regression methods are used to establish quantitative relationships between multiple predictor variables and a dependent response. The resulting model can then predict the response of an unknown sample based on measured predictor data. Such approaches can also be used for binary classification by assigning arbitrary responses to each class.Multivariate regression methods are used to establish quantitative relationships between multiple predictor variables and a dependent response. The resulting model can then predict the response of an unknown sample based on measured predictor data. Such approaches can also be used for binary classification by assigning arbitrary responses to each class.</p>
        <p>The simplest multivariate regression technique is multiple linear regression (MLR), which constructs a linear combination of variables to describe the dataset in the form:The simplest multivariate regression technique is multiple linear regression (MLR), which constructs a linear combination of variables to describe the dataset in the form:</p>
        <p>Where yi is the response variable, β0 is an intercept, βp is the coefficient loading applied to the corresponding predictor variable xip, and ϵ is the residual error. This regression equation is constructed to best approximate the individual data points (i.e. to minimise residuals).Where yi is the response variable, β0 is an intercept, βp is the coefficient loading applied to the corresponding predictor variable xip, and ϵ is the residual error. This regression equation is constructed to best approximate the individual data points (i.e. to minimise residuals).</p>
        <p>p-values can be generated for the coefficients to identify variables that have negligible influence on the response. 23 p-values tests the null hypothesis that a variable's coefficient is not significantly different from zero (i.e. the variable has negligible influence). Low p-values, generally considered to be less than 0.05, provide evidence against this hypothesis and indicate variables that are in important to measuring the response, whereas those with high p-values can be omitted. This is important as MLR requires the sample size to exceed the number of predictor variables, which can limit its use in complex datasets. A further limitation of MLR is the assumption that there is no major correlation or multicollinearity between predictors, which does not hold for most spectral data and chromatographic data.p-values can be generated for the coefficients to identify variables that have negligible influence on the response. 23 p-values tests the null hypothesis that a variable's coefficient is not significantly different from zero (i.e. the variable has negligible influence). Low p-values, generally considered to be less than 0.05, provide evidence against this hypothesis and indicate variables that are in important to measuring the response, whereas those with high p-values can be omitted. This is important as MLR requires the sample size to exceed the number of predictor variables, which can limit its use in complex datasets. A further limitation of MLR is the assumption that there is no major correlation or multicollinearity between predictors, which does not hold for most spectral data and chromatographic data.</p>
        <p>One way to overcome the limitations of MLR is to first use a dimensionality reduction technique to remove multicollinearity and reduce the number of predictors for analysis. Principal component regression (PCR) does this by applying PCA to the original variables, with selected PCs retained as orthogonal predictors. 66 PCs are generally retained on the basis of their eigenvalues, or the percentage of variance accounted according to the scree plot. PCs with low variances often encompass noise that can increase the uncertainty of the model. On the other hand, these PCs may also contain important information for distinguishing specific sample groups.One way to overcome the limitations of MLR is to first use a dimensionality reduction technique to remove multicollinearity and reduce the number of predictors for analysis. Principal component regression (PCR) does this by applying PCA to the original variables, with selected PCs retained as orthogonal predictors. 66 PCs are generally retained on the basis of their eigenvalues, or the percentage of variance accounted according to the scree plot. PCs with low variances often encompass noise that can increase the uncertainty of the model. On the other hand, these PCs may also contain important information for distinguishing specific sample groups.</p>
        <p>Figure 9 shows the output of PCR used to model the changes in diffuse reflectance visible spectra occurring in a blue ballpoint pen ink exposed to light for 32 months. 67 PCA is first carried out, producing a scores plot. These can be used to identify systematic changes between the samples that may be linked to the response. In this instance, more positive scores along PC 1 were observed with increased ageing. The regression plot (in this case, using the first four PCs accounting for 99.3 % of variance) then shows the predicted vs. actual responses for the dataset based on re-substitution of the calibration data and a crossvalidation. A lack of agreement or overlap between the calibration and cross-validation points is an indication of outliers in the data, where removal of a single data point substantially alters the structure of the model.Figure 9 shows the output of PCR used to model the changes in diffuse reflectance visible spectra occurring in a blue ballpoint pen ink exposed to light for 32 months. 67 PCA is first carried out, producing a scores plot. These can be used to identify systematic changes between the samples that may be linked to the response. In this instance, more positive scores along PC 1 were observed with increased ageing. The regression plot (in this case, using the first four PCs accounting for 99.3 % of variance) then shows the predicted vs. actual responses for the dataset based on re-substitution of the calibration data and a crossvalidation. A lack of agreement or overlap between the calibration and cross-validation points is an indication of outliers in the data, where removal of a single data point substantially alters the structure of the model.</p>
        <p>The regression model gave a high linear correlation (R 2 = 0.993) and a root mean square error of approximately 24 days between predicted and reference values. This appeared to indicate strong predictive performance in estimating the age of an unknown ink deposit to within one month. However, prediction of a separate validation set was far less accurate, particularly for samples in the first six months (170 days) of ageing (Table 6). Several of the age estimations were negative values, whilst results across replicate spectra displayed a high relative standard deviation (RSD). It was concluded that the rapid chemical changes occurring during this initial ageing period were too variable between separate ink deposits to be precisely modelled. This example highlights the importance of appropriate test set validation for chemometric models.The regression model gave a high linear correlation (R 2 = 0.993) and a root mean square error of approximately 24 days between predicted and reference values. This appeared to indicate strong predictive performance in estimating the age of an unknown ink deposit to within one month. However, prediction of a separate validation set was far less accurate, particularly for samples in the first six months (170 days) of ageing (Table 6). Several of the age estimations were negative values, whilst results across replicate spectra displayed a high relative standard deviation (RSD). It was concluded that the rapid chemical changes occurring during this initial ageing period were too variable between separate ink deposits to be precisely modelled. This example highlights the importance of appropriate test set validation for chemometric models.</p>
        <p>A potential limitation to PCR is that it focusses on variation amongst the predictors, but not in the response. That is, it applies heavier weighting to X variables that show the greatest variability, even if this is unrelated to changes in Y. 23 Partial least squares regression (PLSR) calculates factors that maximise the covariance between X and Y. Greater weight is hence applied to predictor variables that are highly correlated with the response, under the assumption that these will be most accurate for predictive purposes. 66 It must be noted that this does not guarantee an increase in predictive performance. As seen in Table 6, age estimation of blue ballpoint inks was no better using a PLSR model than PCR. This indicates that in this instance, the predictors showing the greatest overall variation were also those most strongly correlated with the response. Another common application of the PLS algorithm is binary classification. Partial least squares-discriminant analysis (PLS-DA) uses dummy variables to represent the two classes to be distinguished. Regression of new samples onto the model results in a classification based on whether the estimated value falls close to zero (Class A) or one (Class B). This process is illustrated in Figure 10, where PLS-DA was used to differentiate authentic and counterfeit perfumes analysed using paper spray mass spectrometry (PS-MS). 69 Like SVM, a drawback to this approach is the need for several models to solve multi-class problems. PLS-DA also does not account for the within-class variability, and predicted values between zero and one can be challenging to assign. On the other hand, PLS-DA can provide valuable insights into the basis of discrimination based on generated weights and loadings.A potential limitation to PCR is that it focusses on variation amongst the predictors, but not in the response. That is, it applies heavier weighting to X variables that show the greatest variability, even if this is unrelated to changes in Y. 23 Partial least squares regression (PLSR) calculates factors that maximise the covariance between X and Y. Greater weight is hence applied to predictor variables that are highly correlated with the response, under the assumption that these will be most accurate for predictive purposes. 66 It must be noted that this does not guarantee an increase in predictive performance. As seen in Table 6, age estimation of blue ballpoint inks was no better using a PLSR model than PCR. This indicates that in this instance, the predictors showing the greatest overall variation were also those most strongly correlated with the response. Another common application of the PLS algorithm is binary classification. Partial least squares-discriminant analysis (PLS-DA) uses dummy variables to represent the two classes to be distinguished. Regression of new samples onto the model results in a classification based on whether the estimated value falls close to zero (Class A) or one (Class B). This process is illustrated in Figure 10, where PLS-DA was used to differentiate authentic and counterfeit perfumes analysed using paper spray mass spectrometry (PS-MS). 69 Like SVM, a drawback to this approach is the need for several models to solve multi-class problems. PLS-DA also does not account for the within-class variability, and predicted values between zero and one can be challenging to assign. On the other hand, PLS-DA can provide valuable insights into the basis of discrimination based on generated weights and loadings.</p>
        <p>The quality of outputs from any chemometric model depends on the quality of the initial input data. It is therefore critical that the analytical parameters chosen to acquire analytical data from forensic evidence are fit for purpose. For this reason, experimental design methods are valuable tools to identify, model and optimise factors that may affect analytical results. These may include sample preparation procedures, storage methods, or instrumental analysis parameters. Here we focus on multivariate designs that vary several factors simultaneously, rather than a one-factor-at-a-time (OFAT) approach. The key disadvantage to OFAT is that it does not consider potential interactions between factors. For example, the optimal extraction time for a solid phase microextraction might be expected to vary depending on the temperature. An OFAT approach can be tedious in such scenarios, as the optimisation of one factor will require re-optimisation of others. The multivariate approach is thus more efficient when investigating several factors, as it directly assesses interactions between them.The quality of outputs from any chemometric model depends on the quality of the initial input data. It is therefore critical that the analytical parameters chosen to acquire analytical data from forensic evidence are fit for purpose. For this reason, experimental design methods are valuable tools to identify, model and optimise factors that may affect analytical results. These may include sample preparation procedures, storage methods, or instrumental analysis parameters. Here we focus on multivariate designs that vary several factors simultaneously, rather than a one-factor-at-a-time (OFAT) approach. The key disadvantage to OFAT is that it does not consider potential interactions between factors. For example, the optimal extraction time for a solid phase microextraction might be expected to vary depending on the temperature. An OFAT approach can be tedious in such scenarios, as the optimisation of one factor will require re-optimisation of others. The multivariate approach is thus more efficient when investigating several factors, as it directly assesses interactions between them.</p>
        <p>An initial goal of experimental design is often to identify main factors with a significant effect on the target response. This can be done using a factorial design, the simplest of which is a two-level (2 k ) factorial design. In this case, 'k' refers to the number of continuous variables being investigated, each with a 'low' and 'high' level. Figure 11a shows a graphical representation of a 2 3 design where the three factors form x, y, and z dimensions of a cubic design region (Figures 11b-d depict other experimental designs that will be discussed further below). For a full two-level factorial design, experiments are run at all possible combinations of low and high levels across all factors (represented as vertices of the cube for a 2 3 design). These are used to derive a linear plane representing the change in response with each factor. Additional replicates of these design points may be carried out to increase the model's power in detecting statistically significant effects. Analysis of variance (ANOVA) is used to evaluate the fit of the model, along with the potential significance of each factor or factor interaction on the target response. 70 Table 7 shows the ANOVA for a 2 3 factorial design intended to maximise the extraction of cocaine from hair using enzymatic hydrolysis, with two replicates of each point (16 runs in total). 71 The factors investigated were buffer solution volume, Pronase E concentration ([P-E]) and dispersing mass to hair mass ratio (D-S); coded as variables A, B and C respectively. A high R 2 value was obtained, indicating a good model fit; whilst a predicted R 2 of 0.689 suggested reasonable predictive ability for new experimental runs. The predicted R 2 is akin to a cross-validation accuracy, where each observation is iteratively removed and predicted using a regression equation estimated from the remaining data. A predicted R 2 substantially less than the R 2 of the model may indicate that the model is 'over-fitted' to the initial data, and subsequently unable to adapt to new information. ANOVA was carried out on each of the model terms to identify those with a statistically significant effect on cocaine extraction. This process is based on a null hypothesis that the effect of a given parameter on the response (in this instance, recovered amount of cocaine) is not statistically significant. The calculated p-value for each parameter reflects the probability of observing a response as large as that obtained in the experiment if this assumption is true. Low p-values (generally &lt; 0.05, representing a 5 % probability) are thus strong evidence against the null hypothesis. That is, they indicate parameters that do in fact significantly affect the response. In this instance, Pronase E concentration and buffer volume were found to significantly affect the recovery of cocaine using enzymatic hydrolysis extraction.An initial goal of experimental design is often to identify main factors with a significant effect on the target response. This can be done using a factorial design, the simplest of which is a two-level (2 k ) factorial design. In this case, 'k' refers to the number of continuous variables being investigated, each with a 'low' and 'high' level. Figure 11a shows a graphical representation of a 2 3 design where the three factors form x, y, and z dimensions of a cubic design region (Figures 11b-d depict other experimental designs that will be discussed further below). For a full two-level factorial design, experiments are run at all possible combinations of low and high levels across all factors (represented as vertices of the cube for a 2 3 design). These are used to derive a linear plane representing the change in response with each factor. Additional replicates of these design points may be carried out to increase the model's power in detecting statistically significant effects. Analysis of variance (ANOVA) is used to evaluate the fit of the model, along with the potential significance of each factor or factor interaction on the target response. 70 Table 7 shows the ANOVA for a 2 3 factorial design intended to maximise the extraction of cocaine from hair using enzymatic hydrolysis, with two replicates of each point (16 runs in total). 71 The factors investigated were buffer solution volume, Pronase E concentration ([P-E]) and dispersing mass to hair mass ratio (D-S); coded as variables A, B and C respectively. A high R 2 value was obtained, indicating a good model fit; whilst a predicted R 2 of 0.689 suggested reasonable predictive ability for new experimental runs. The predicted R 2 is akin to a cross-validation accuracy, where each observation is iteratively removed and predicted using a regression equation estimated from the remaining data. A predicted R 2 substantially less than the R 2 of the model may indicate that the model is 'over-fitted' to the initial data, and subsequently unable to adapt to new information. ANOVA was carried out on each of the model terms to identify those with a statistically significant effect on cocaine extraction. This process is based on a null hypothesis that the effect of a given parameter on the response (in this instance, recovered amount of cocaine) is not statistically significant. The calculated p-value for each parameter reflects the probability of observing a response as large as that obtained in the experiment if this assumption is true. Low p-values (generally &lt; 0.05, representing a 5 % probability) are thus strong evidence against the null hypothesis. That is, they indicate parameters that do in fact significantly affect the response. In this instance, Pronase E concentration and buffer volume were found to significantly affect the recovery of cocaine using enzymatic hydrolysis extraction.</p>
        <p>This was reinforced by examination of the Pareto chart (Figure 12a), which shows the ranked standardised effects of each factor based on t-statistics. Both factors fell above the t-value limit, indicating a statistically significant effect on the cocaine concentration. The Pareto chart and one-factor plots (Figure 12b) showed these effects to be negative, meaning that lower buffer volumes and Pronase E concentrations led to a greater extraction efficiency of cocaine from hair. A normal plot of residuals (Figure 12c) was used to check the distribution of residuals for each of the 16 experimental runs (two replicates of each factorial point). Nonlinearity of the residuals plot could indicate the presence of outlying samples, or an undetected factor not accounted for within the design. Figure 12d shows the response surface for the concentration of extracted cocaine as a function of buffer volume and dispersing mass to hair mass ratio, assuming a minimal Pronase E concentration. This response surface is modelled as a linear plane anchored by the average response at each design point. The highest concentration of extracted cocaine was obtained using a minimal Pronase E concentration and buffer volume (as expected) and a maximal dispersing mass to hair mass ratio of 50. However, these may not in fact be the absolute optimal extraction parameters.This was reinforced by examination of the Pareto chart (Figure 12a), which shows the ranked standardised effects of each factor based on t-statistics. Both factors fell above the t-value limit, indicating a statistically significant effect on the cocaine concentration. The Pareto chart and one-factor plots (Figure 12b) showed these effects to be negative, meaning that lower buffer volumes and Pronase E concentrations led to a greater extraction efficiency of cocaine from hair. A normal plot of residuals (Figure 12c) was used to check the distribution of residuals for each of the 16 experimental runs (two replicates of each factorial point). Nonlinearity of the residuals plot could indicate the presence of outlying samples, or an undetected factor not accounted for within the design. Figure 12d shows the response surface for the concentration of extracted cocaine as a function of buffer volume and dispersing mass to hair mass ratio, assuming a minimal Pronase E concentration. This response surface is modelled as a linear plane anchored by the average response at each design point. The highest concentration of extracted cocaine was obtained using a minimal Pronase E concentration and buffer volume (as expected) and a maximal dispersing mass to hair mass ratio of 50. However, these may not in fact be the absolute optimal extraction parameters.</p>
        <p>A disadvantage of 2 k factorials is that they assume the response changes linearly with each factor and hence only indicates if the response is positive or negative. Detecting non-linearity (curvature and a maximum or minimum) in the response requires investigation of additional values between the low and high levels. One approach to this would be a three-level factorial (3 k ) design, but this can again lead to an unfeasible number of experiments. As seen in Figure 11b, investigation of three factors at three levels (a 3 3 full factorial design) already requires 27 experiments! There is hence a need for more efficient designs that are able to estimate curvature in response.A disadvantage of 2 k factorials is that they assume the response changes linearly with each factor and hence only indicates if the response is positive or negative. Detecting non-linearity (curvature and a maximum or minimum) in the response requires investigation of additional values between the low and high levels. One approach to this would be a three-level factorial (3 k ) design, but this can again lead to an unfeasible number of experiments. As seen in Figure 11b, investigation of three factors at three levels (a 3 3 full factorial design) already requires 27 experiments! There is hence a need for more efficient designs that are able to estimate curvature in response.</p>
        <p>Each additional factor doubles the number of possible combinations of levels, and hence the number of experiments. As this may not be practical for a high number of factors (five factors for example would require 64 experiments), a fractional factorial design may also be used, in which some possible interactions are omitted. 72 Another alternative is to use a Plackett Burman design, which allows designs to be created with a number of runs with a number of runs between those of a 2 k factorial, at the cost of main factor effects being confounded with factor interactions. 70,72Each additional factor doubles the number of possible combinations of levels, and hence the number of experiments. As this may not be practical for a high number of factors (five factors for example would require 64 experiments), a fractional factorial design may also be used, in which some possible interactions are omitted. 72 Another alternative is to use a Plackett Burman design, which allows designs to be created with a number of runs with a number of runs between those of a 2 k factorial, at the cost of main factor effects being confounded with factor interactions. 70,72</p>
        <p>A central composite design (CCD) augments a two-level factorial design with replicated centre points in which all factors are set to a median level, and a set of axial (star) points where a single factor takes on levels above and below the median (Figure 11c). These additional design points allow the detection and modelling of curvature in fewer runs than a three-level factorial design. 73 The axial and centre points can also be run without the embedded factorial, producing a Box-Behnken design (Figure 11d) that requires even fewer experiments. 72 Although highly cost-efficient, this can make modelling of the response less precise.A central composite design (CCD) augments a two-level factorial design with replicated centre points in which all factors are set to a median level, and a set of axial (star) points where a single factor takes on levels above and below the median (Figure 11c). These additional design points allow the detection and modelling of curvature in fewer runs than a three-level factorial design. 73 The axial and centre points can also be run without the embedded factorial, producing a Box-Behnken design (Figure 11d) that requires even fewer experiments. 72 Although highly cost-efficient, this can make modelling of the response less precise.</p>
        <p>CCD models may be described as circumscribed, inscribed, or face-centred depending on whether the axial points fall beyond, within, or on the factorial space respectively. 70 Circumscribed designs are useful for covering a larger investigative range, whereas an inscribed design may be needed if settings outside of the factorial levels are not readily or safely achievable. Both of these designs are also rotatable, meaning they give a uniform prediction error across the design area. Face-centred CCDs are non-rotatable, but can be simpler to run (as there will only be three possible levels per factor) and may be needed if levels between or beyond the factorial levels are not practicable.CCD models may be described as circumscribed, inscribed, or face-centred depending on whether the axial points fall beyond, within, or on the factorial space respectively. 70 Circumscribed designs are useful for covering a larger investigative range, whereas an inscribed design may be needed if settings outside of the factorial levels are not readily or safely achievable. Both of these designs are also rotatable, meaning they give a uniform prediction error across the design area. Face-centred CCDs are non-rotatable, but can be simpler to run (as there will only be three possible levels per factor) and may be needed if levels between or beyond the factorial levels are not practicable.</p>
        <p>Figure 13 shows a non-linear 'saddle' response surface for the concentration of extracted cocaine using a circumscribed CCD created by adding axial and centre points (in duplicate) to the original factorial design. ANOVA again found the Pronase E concentration to have a significant negative effect on response, and buffer volume was similarly found to again be negatively associated with cocaine concentration. However, the optimal dispersing mass to hair mass ratio was found to be 35, due to curvature in the response surface. This optimum was not detectable in the 2 3 full factorial design due to the lack of experiments carried out at median values of each factor. CCDs are thus a powerful method for optimising the handling and analytical protocols of evidence samples.Figure 13 shows a non-linear 'saddle' response surface for the concentration of extracted cocaine using a circumscribed CCD created by adding axial and centre points (in duplicate) to the original factorial design. ANOVA again found the Pronase E concentration to have a significant negative effect on response, and buffer volume was similarly found to again be negatively associated with cocaine concentration. However, the optimal dispersing mass to hair mass ratio was found to be 35, due to curvature in the response surface. This optimum was not detectable in the 2 3 full factorial design due to the lack of experiments carried out at median values of each factor. CCDs are thus a powerful method for optimising the handling and analytical protocols of evidence samples.</p>
        <p>Given the wealth of information obtainable from physical evidence using chemometrics, it is no surprise that an increasing volume of research has emerged in this field. This section provides a broad overview of these studies, both recent and not-so-recent, to showcase the applicability of chemometrics across various disciplines. Although previous reviews exist on this topic, [74][75][76][77][78][79][80] they have generally focussed on specific disciplines and/or chemometric methods. The present review is intended to provide a broader overview of how experimental design, pattern recognition and regression can be applied across the forensic sciences.Given the wealth of information obtainable from physical evidence using chemometrics, it is no surprise that an increasing volume of research has emerged in this field. This section provides a broad overview of these studies, both recent and not-so-recent, to showcase the applicability of chemometrics across various disciplines. Although previous reviews exist on this topic, [74][75][76][77][78][79][80] they have generally focussed on specific disciplines and/or chemometric methods. The present review is intended to provide a broader overview of how experimental design, pattern recognition and regression can be applied across the forensic sciences.</p>
        <p>Substantial research on the forensic application of chemometrics has focussed on drugs of abuse, particularly for the purposes of batch comparisons or impurity profiling. The most substantial portion of this work has involved the qualitative and quantitative analysis of amphetamines, [81][82][83][84][85][86][87][88][89][90] opiates, 91,92,101,102,93-100 cocaine, 103,104,113,114,[105][106][107][108][109][110][111][112] pharmaceuticals, 33,115- 119 or a combination of these. 91,[120][121][122][123][124][125] Specific studies of interest in these areas are discussed in more detail below, though more comprehensive reviews of the literature have been covered elsewhere. 77,80 Recent years have also seen a greater interest in the characterisation of emerging or novel psychoactive substances, [126][127][128][129][130][131][132][133][134] precursor chemicals 135 and psychoactive plants. [136][137][138][139] 3.1.1 Amphetamine-type stimulants Andersson et al. used PLS-DA and statistical distance metrics to identify linked samples of amphetamine with GC-MS. 81 The resulting models were able to identify linked and non-linked samples, in addition to distinguishing those synthesised by different routes. However, no quantitative information concerning sample composition or similarity was derived. Research by Goh et al. instead used field-portable ATR-FTIR spectroscopy with PLSR to quantify solid mixtures containing methylamphetamine, glucose and caffeine, with predicted concentrations of these components typically within 6 % w/w of known values. 89 Hughes et al. applied the same methods on a larger sample set, resulting in models able to quantify samples containing as little as 0.3 % w/w methamphetamine. 90 The ability of this model to distinguish methamphetamine from structurally similar drugs is yet to be determined.Substantial research on the forensic application of chemometrics has focussed on drugs of abuse, particularly for the purposes of batch comparisons or impurity profiling. The most substantial portion of this work has involved the qualitative and quantitative analysis of amphetamines, [81][82][83][84][85][86][87][88][89][90] opiates, 91,92,101,102,93-100 cocaine, 103,104,113,114,[105][106][107][108][109][110][111][112] pharmaceuticals, 33,115- 119 or a combination of these. 91,[120][121][122][123][124][125] Specific studies of interest in these areas are discussed in more detail below, though more comprehensive reviews of the literature have been covered elsewhere. 77,80 Recent years have also seen a greater interest in the characterisation of emerging or novel psychoactive substances, [126][127][128][129][130][131][132][133][134] precursor chemicals 135 and psychoactive plants. [136][137][138][139] 3.1.1 Amphetamine-type stimulants Andersson et al. used PLS-DA and statistical distance metrics to identify linked samples of amphetamine with GC-MS. 81 The resulting models were able to identify linked and non-linked samples, in addition to distinguishing those synthesised by different routes. However, no quantitative information concerning sample composition or similarity was derived. Research by Goh et al. instead used field-portable ATR-FTIR spectroscopy with PLSR to quantify solid mixtures containing methylamphetamine, glucose and caffeine, with predicted concentrations of these components typically within 6 % w/w of known values. 89 Hughes et al. applied the same methods on a larger sample set, resulting in models able to quantify samples containing as little as 0.3 % w/w methamphetamine. 90 The ability of this model to distinguish methamphetamine from structurally similar drugs is yet to be determined.</p>
        <p>Research by Moros et al. used diffuse reflectance near-infrared (DR-NIR) spectroscopy and PLSR to quantify heroin in seized illicit street drugs, resulting in the accurate quantitation of validation samples ranging from 6 -34 % w/w purity. 100 However, it should be noted that the validation set consisted of only 10 samples, and no replicate spectra were acquired to gauge the predictive reproducibility. Turner et al., using FTIR spectroscopy with modified PLSR and PLS-DA, achieved successful and reproducible separation of heroin samples originating from three different poppy cultivars. 101 Five component opiates of the poppy heads were also identified and distinguished, though only morphine could be reliably quantified.Research by Moros et al. used diffuse reflectance near-infrared (DR-NIR) spectroscopy and PLSR to quantify heroin in seized illicit street drugs, resulting in the accurate quantitation of validation samples ranging from 6 -34 % w/w purity. 100 However, it should be noted that the validation set consisted of only 10 samples, and no replicate spectra were acquired to gauge the predictive reproducibility. Turner et al., using FTIR spectroscopy with modified PLSR and PLS-DA, achieved successful and reproducible separation of heroin samples originating from three different poppy cultivars. 101 Five component opiates of the poppy heads were also identified and distinguished, though only morphine could be reliably quantified.</p>
        <p>In addition to spectroscopic methods, more recent studies have utilised inductively coupled plasma (ICP) as a more sensitive method of analysis based upon elemental composition. Chan et al. applied ICP-MS to street heroin samples seized in Malaysia, identifying two separate classes of samples using PCA. 93 It was noted, though, that class similarities did not necessarily indicate batch linkages, as the profiled elements could have been introduced from contaminant sources in separate distribution chains. Later work by Liu et al. employed the same approaches to distinguish opiate samples originating from the Golden Crescent and Golden Triangle; Asia's two principal areas of illicit opium production. 102 Ten elements and seven elemental ratios were found to markedly differ between samples originating from the two regions, and a subsequent PLS-DA model gave a 97 % prediction accuracy of 175 validation samples.In addition to spectroscopic methods, more recent studies have utilised inductively coupled plasma (ICP) as a more sensitive method of analysis based upon elemental composition. Chan et al. applied ICP-MS to street heroin samples seized in Malaysia, identifying two separate classes of samples using PCA. 93 It was noted, though, that class similarities did not necessarily indicate batch linkages, as the profiled elements could have been introduced from contaminant sources in separate distribution chains. Later work by Liu et al. employed the same approaches to distinguish opiate samples originating from the Golden Crescent and Golden Triangle; Asia's two principal areas of illicit opium production. 102 Ten elements and seven elemental ratios were found to markedly differ between samples originating from the two regions, and a subsequent PLS-DA model gave a 97 % prediction accuracy of 175 validation samples.</p>
        <p>Dujourdy et al. studied the source determination of hydrochloride cocaine samples based upon the headspace profiling of residual solvents. 111 Cluster analysis was largely able to separate samples seized in Bolivia, Peru or Columbia, with a limited capacity to also distinguish hydrochloride (salt) and base forms of cocaine. Separate studies by Rodrigues et al. 112 and Groberio et al. 113 on Brazilian seizures were able to reliably distinguish between salt and base samples based on their IR spectra, with the latter establishing PLSR models for the quantification of cocaine and selected common adulterants. More recent work by Eliaerts et al. used ATR-FTIR with SVM for the detection and quantification of cocaine in seized powders. 114 The resulting models gave 99 % sensitivity and specificity, and a quantitative root mean square error of prediction (RMSEP) of only 6 % over a wide working range.Dujourdy et al. studied the source determination of hydrochloride cocaine samples based upon the headspace profiling of residual solvents. 111 Cluster analysis was largely able to separate samples seized in Bolivia, Peru or Columbia, with a limited capacity to also distinguish hydrochloride (salt) and base forms of cocaine. Separate studies by Rodrigues et al. 112 and Groberio et al. 113 on Brazilian seizures were able to reliably distinguish between salt and base samples based on their IR spectra, with the latter establishing PLSR models for the quantification of cocaine and selected common adulterants. More recent work by Eliaerts et al. used ATR-FTIR with SVM for the detection and quantification of cocaine in seized powders. 114 The resulting models gave 99 % sensitivity and specificity, and a quantitative root mean square error of prediction (RMSEP) of only 6 % over a wide working range.</p>
        <p>Within forensic toxicology, there is strong interest in simple, rapid and non-destructive screening methods for drugs of abuse, especially in situations involving public health or security. Risoluti et al. used a miniaturised MicroNIR spectrometer with PCA and PLS-DA to detect cocaine in non-treated oral fluids. 140 PCA was able to differentiate between blank and spiked samples, as well as detecting variations in the spectral profile over time due to metabolic breakdown of cocaine to benzoylecgonine. Six real samples were used to validate the approach, which performed comparably to GC-MS.Within forensic toxicology, there is strong interest in simple, rapid and non-destructive screening methods for drugs of abuse, especially in situations involving public health or security. Risoluti et al. used a miniaturised MicroNIR spectrometer with PCA and PLS-DA to detect cocaine in non-treated oral fluids. 140 PCA was able to differentiate between blank and spiked samples, as well as detecting variations in the spectral profile over time due to metabolic breakdown of cocaine to benzoylecgonine. Six real samples were used to validate the approach, which performed comparably to GC-MS.</p>
        <p>In addition to illicit drugs, recent studies have also investigated pharmaceutical products, particularly in regard to counterfeit medications. Researchers at the University of Lausanne have explored several approaches toward counterfeit identification and profiling. Roggo et al. employed Raman spectroscopy with SVM models to distinguish between 25 therapeutic product families, and postulated that this methodology could potentially be applied to detect counterfeit substitutes. 117 Been et al. then utilised NIR and Raman spectroscopy with pattern recognition (including PCA, HCA, kNN, PLS-DA and ANN) to successfully distinguish six genuine batches of a pharmaceutical product from 27 counterfeit seizures. 118 Dégardin et al. combined the above approaches; using Raman spectroscopy with SVM models in a two-step method to detect counterfeit products and compare them against a known reference database. 119 The resulting methodology successfully discriminated counterfeit seizures from genuine products, and identified several seizures of similar chemical profiles. An external validation set of generic brand medications was also recognised as being distinct from both the brand-name medications and existing counterfeit products.In addition to illicit drugs, recent studies have also investigated pharmaceutical products, particularly in regard to counterfeit medications. Researchers at the University of Lausanne have explored several approaches toward counterfeit identification and profiling. Roggo et al. employed Raman spectroscopy with SVM models to distinguish between 25 therapeutic product families, and postulated that this methodology could potentially be applied to detect counterfeit substitutes. 117 Been et al. then utilised NIR and Raman spectroscopy with pattern recognition (including PCA, HCA, kNN, PLS-DA and ANN) to successfully distinguish six genuine batches of a pharmaceutical product from 27 counterfeit seizures. 118 Dégardin et al. combined the above approaches; using Raman spectroscopy with SVM models in a two-step method to detect counterfeit products and compare them against a known reference database. 119 The resulting methodology successfully discriminated counterfeit seizures from genuine products, and identified several seizures of similar chemical profiles. An external validation set of generic brand medications was also recognised as being distinct from both the brand-name medications and existing counterfeit products.</p>
        <p>More recent work by Custers et al. used ATR-FTIR spectroscopy to characterise brand-name, generic and counterfeit erectile dysfunction medications, with PCA enabling discrimination based on the active pharmaceutical ingredients. 33 Although kNN classification and regression tree analysis were unsuccessful in classifying the majority of spectra, SIMCA provided 100 % discrimination of counterfeit tablets from both genuine and generic brand products.More recent work by Custers et al. used ATR-FTIR spectroscopy to characterise brand-name, generic and counterfeit erectile dysfunction medications, with PCA enabling discrimination based on the active pharmaceutical ingredients. 33 Although kNN classification and regression tree analysis were unsuccessful in classifying the majority of spectra, SIMCA provided 100 % discrimination of counterfeit tablets from both genuine and generic brand products.</p>
        <p>Several studies have made use of experimental design methods in order to optimise screening procedures for drugs and their metabolites in biological samples such as urine, blood, hair, or vitreous humor. [141][142][143][144][145][146][147][148][149] For example, Ho et al. used a Plackett-Burman design and CCD to identify and optimise parameters influencing the separation of frequently consumed drugs analysed with capillary electrophoresis. 141 The optimised parameters according to the experimental design approach were compared with those established using an OFAT approach, and found to provide improved resolution within a shorter analysis time.Several studies have made use of experimental design methods in order to optimise screening procedures for drugs and their metabolites in biological samples such as urine, blood, hair, or vitreous humor. [141][142][143][144][145][146][147][148][149] For example, Ho et al. used a Plackett-Burman design and CCD to identify and optimise parameters influencing the separation of frequently consumed drugs analysed with capillary electrophoresis. 141 The optimised parameters according to the experimental design approach were compared with those established using an OFAT approach, and found to provide improved resolution within a shorter analysis time.</p>
        <p>Decaestecker carried out a Plackett-Burman screening procedure to identify factors affecting the solid phase extraction (SPE) of 18 substances (including benzoylecgonine, codeine, strychnine and diazepam) from human 142 Three factors were found to have a significant effect; the volume of the first washing step, percentage of methanol in the first washing step; and molarity of the used buffer. A CCD was subsequently used to optimise these parameters based on the overall extraction yield, number of compounds effectively retrieved, and total number of ions detected. It was reported that this procedure is currently being utilised for real toxicological samples and has been found fit-for-purpose, although no data was presented in this regard.Decaestecker carried out a Plackett-Burman screening procedure to identify factors affecting the solid phase extraction (SPE) of 18 substances (including benzoylecgonine, codeine, strychnine and diazepam) from human 142 Three factors were found to have a significant effect; the volume of the first washing step, percentage of methanol in the first washing step; and molarity of the used buffer. A CCD was subsequently used to optimise these parameters based on the overall extraction yield, number of compounds effectively retrieved, and total number of ions detected. It was reported that this procedure is currently being utilised for real toxicological samples and has been found fit-for-purpose, although no data was presented in this regard.</p>
        <p>Hložek, Bursová and Čabala also used an experimental design approach to investigate eight factors affecting GC determination of ethylene glycol, 1,2-propylene glycol and glycolic acid in blood serum and urine. 143 In this instance, a fractional factorial was selected as an initial screening design, followed by a face-centred CCD for optimisation. The optimised method performed well in terms of both accuracy and precision, with samples spiked with as little as 25 mg L -1 of the target analytes being accurately determined with calculated recoveries between 92.4 % and 108.7 %.Hložek, Bursová and Čabala also used an experimental design approach to investigate eight factors affecting GC determination of ethylene glycol, 1,2-propylene glycol and glycolic acid in blood serum and urine. 143 In this instance, a fractional factorial was selected as an initial screening design, followed by a face-centred CCD for optimisation. The optimised method performed well in terms of both accuracy and precision, with samples spiked with as little as 25 mg L -1 of the target analytes being accurately determined with calculated recoveries between 92.4 % and 108.7 %.</p>
        <p>A number of recent studies have specifically applied chemometric approaches specifically for drug isomer differentiation. Many novel psychoactive drugs include isomeric analogues of controlled substances (e.g. methcathinone, fentanyl or amphetamines) that can be challenging to distinguish due to their chemical similarity. This is particularly important where different isomers of a drug may be subject to different legal classifications or controls.A number of recent studies have specifically applied chemometric approaches specifically for drug isomer differentiation. Many novel psychoactive drugs include isomeric analogues of controlled substances (e.g. methcathinone, fentanyl or amphetamines) that can be challenging to distinguish due to their chemical similarity. This is particularly important where different isomers of a drug may be subject to different legal classifications or controls.</p>
        <p>Bonetti examined three structural isomers each of fluoromethcathinone and fluorofentanyl using GC/EI-MS with PCA and LDA, the latter from which posterior probabilities were also determined. 127 All isomers were successfully differentiated, including within blind trial and case study validation, though sample dilution could cause misclassifications. Kranenburg et al. used a similar approach to distinguish three fluoroamphetamines, three methylmethcathinones and two methylethcathinones at low and high ionisation voltages. 133 Low energy GC/EI-MS was found to improve the selectivity for ring-isomeric differentiation, particularly amongst cathinones. Stuhmer et al. then used PCA and unequal variance t-tests to examine EI-MS spectra from ortho-, meta-and para-isomers of ethylmethcathinone and fluoromethamphetamine. Statistical association of the corresponding isomers was demonstrated at the 99 % confidence level or above, with ortho-isomers more readily distinct from meta-or para-analogues. As with earlier studies, low concentration spectra were identified as a challenge, along with the impacts of instrumental variation.Bonetti examined three structural isomers each of fluoromethcathinone and fluorofentanyl using GC/EI-MS with PCA and LDA, the latter from which posterior probabilities were also determined. 127 All isomers were successfully differentiated, including within blind trial and case study validation, though sample dilution could cause misclassifications. Kranenburg et al. used a similar approach to distinguish three fluoroamphetamines, three methylmethcathinones and two methylethcathinones at low and high ionisation voltages. 133 Low energy GC/EI-MS was found to improve the selectivity for ring-isomeric differentiation, particularly amongst cathinones. Stuhmer et al. then used PCA and unequal variance t-tests to examine EI-MS spectra from ortho-, meta-and para-isomers of ethylmethcathinone and fluoromethamphetamine. Statistical association of the corresponding isomers was demonstrated at the 99 % confidence level or above, with ortho-isomers more readily distinct from meta-or para-analogues. As with earlier studies, low concentration spectra were identified as a challenge, along with the impacts of instrumental variation.</p>
        <p>Davidson and Jackson distinguished six synthetic phenethylamine derivatives by carrying out PCA and canonical DA on their ion abundances in EI-MS spectra. 131 Variance was assessed based on the 15 most abundant ions, resulting in a 99.5 % classification accuracy even when pooling data from multiple instruments and including low abundance spectra. Roberson and Goodpaster similarly applied PCA and DA to GC vacuum ultraviolet (VUV) spectra from eight phenethylamines. 132 Subtle differences were identified in the VUV spectra and were supported by chemometric analysis to be distinctive between different isomers, including the diastereometers ephedrine and pseudoephedrine. GC-VUV was thus suggested as a complementary method to GC-MS, though it was recognised that further work would be needed to expand on the available VUV spectra of reference compounds.Davidson and Jackson distinguished six synthetic phenethylamine derivatives by carrying out PCA and canonical DA on their ion abundances in EI-MS spectra. 131 Variance was assessed based on the 15 most abundant ions, resulting in a 99.5 % classification accuracy even when pooling data from multiple instruments and including low abundance spectra. Roberson and Goodpaster similarly applied PCA and DA to GC vacuum ultraviolet (VUV) spectra from eight phenethylamines. 132 Subtle differences were identified in the VUV spectra and were supported by chemometric analysis to be distinctive between different isomers, including the diastereometers ephedrine and pseudoephedrine. GC-VUV was thus suggested as a complementary method to GC-MS, though it was recognised that further work would be needed to expand on the available VUV spectra of reference compounds.</p>
        <p>Despite the increasing trend toward electronic communication and transactions, physical documents are still widely used in financial, legal and personal matters. An array of research has hence examined the use of chemometrics in questioned document examination, particularly in the characterisation of paper and inks from handwritten documents.Despite the increasing trend toward electronic communication and transactions, physical documents are still widely used in financial, legal and personal matters. An array of research has hence examined the use of chemometrics in questioned document examination, particularly in the characterisation of paper and inks from handwritten documents.</p>
        <p>Several studies have sought to gain improved understanding of the variability of paper substrates. [150][151][152][153][154] An early study by Kher et al. utilised infrared spectroscopy techniques with pattern recognition tools to distinguish between 14 white or yellow paper substrates. 153 PCA of spectra collected using ATR-FTIR spectroscopy was able to distinguish almost 68 % of possible pairings, while the analysis of diffuse reflectance FTIR spectra gave 100 % discrimination using cross-validation. An approach by Sarkar et al. employed statistical correlation measures and t-testing with laser-induced breakdown spectroscopy (LIBS) to match 10 unknown paper substrates with a known database. 154 100 % correct identification was achieved; although as these substrates were all acquired from a single source, the applicability of the method to a wider range of substrates was not firmly established.Several studies have sought to gain improved understanding of the variability of paper substrates. [150][151][152][153][154] An early study by Kher et al. utilised infrared spectroscopy techniques with pattern recognition tools to distinguish between 14 white or yellow paper substrates. 153 PCA of spectra collected using ATR-FTIR spectroscopy was able to distinguish almost 68 % of possible pairings, while the analysis of diffuse reflectance FTIR spectra gave 100 % discrimination using cross-validation. An approach by Sarkar et al. employed statistical correlation measures and t-testing with laser-induced breakdown spectroscopy (LIBS) to match 10 unknown paper substrates with a known database. 154 100 % correct identification was achieved; although as these substrates were all acquired from a single source, the applicability of the method to a wider range of substrates was not firmly established.</p>
        <p>Other research has instead focussed on the potential ageing of paper documents. [155][156][157][158] Silva suggested a non-destructive dating method based on FTIR spectroscopy and PLSR. 157 Due to the potential variability in paper composition, different weighting methods were trialled to enable construction of a single model. Modelling of documents representing 15 different years from 1985 -2012 gave an acceptable RMSEP of ca. 4 years; however, it was evident that inorganic components such as kaolinite and calcium carbonate were exerting high influence on the PLS models, potentially affecting their performance.Other research has instead focussed on the potential ageing of paper documents. [155][156][157][158] Silva suggested a non-destructive dating method based on FTIR spectroscopy and PLSR. 157 Due to the potential variability in paper composition, different weighting methods were trialled to enable construction of a single model. Modelling of documents representing 15 different years from 1985 -2012 gave an acceptable RMSEP of ca. 4 years; however, it was evident that inorganic components such as kaolinite and calcium carbonate were exerting high influence on the PLS models, potentially affecting their performance.</p>
        <p>More recently, Xia et al. explored the analysis of naturally aged and conserved journals dated at five-year intervals between 1940 to 1980 using FTIR. 158 Least squares SVM was able to distinguish documents of different ages with an accuracy of 99 %, outperforming both LDA and SIMCA. However, it is unclear whether this approach would be able to distinguish documents within five years of age. This classification-based approach is also limited to estimating document ages already represented in the training samples.More recently, Xia et al. explored the analysis of naturally aged and conserved journals dated at five-year intervals between 1940 to 1980 using FTIR. 158 Least squares SVM was able to distinguish documents of different ages with an accuracy of 99 %, outperforming both LDA and SIMCA. However, it is unclear whether this approach would be able to distinguish documents within five years of age. This classification-based approach is also limited to estimating document ages already represented in the training samples.</p>
        <p>In addition to paper, several studies have been conducted regarding the characterisation of writing inks. The majority of these articles have focussed on blue ballpoint pens; one of the most widespread types of writing instrument. The successful discrimination and classification of blue ballpoint inks using secondary ion or ICP mass spectrometry, 159,160 vibrational spectroscopy [161][162][163][164][165][166][167][168][169] , chromatographic methods, 163,165,170,171 ultraviolet-visible (UV-Vis) spectroscopy, [172][173][174][175][176][177] and image processing 178 have been described in the open literature.In addition to paper, several studies have been conducted regarding the characterisation of writing inks. The majority of these articles have focussed on blue ballpoint pens; one of the most widespread types of writing instrument. The successful discrimination and classification of blue ballpoint inks using secondary ion or ICP mass spectrometry, 159,160 vibrational spectroscopy [161][162][163][164][165][166][167][168][169] , chromatographic methods, 163,165,170,171 ultraviolet-visible (UV-Vis) spectroscopy, [172][173][174][175][176][177] and image processing 178 have been described in the open literature.</p>
        <p>Of particular interest, Denman et al. conducted surface analysis of ballpoint inks on paper using time-of-flight secondary ion MS using PCA, providing in situ analysis of organic and inorganic components with no interference from the underlying substrate. 160 This was able to discriminate 41 out of 45 pairs of inks (sourced from seven different pen models), and in one case was able to distinguish between separate batches of the same pen. Braz et al. instead applied PCA to Raman spectra for the non-destructive, in situ analysis of over 300 pens, representing 38 pen models sourced from 12 known brands. 169 This was able to distinguish inks of different brands, models or batches, as well as identifying the main colourants present.Of particular interest, Denman et al. conducted surface analysis of ballpoint inks on paper using time-of-flight secondary ion MS using PCA, providing in situ analysis of organic and inorganic components with no interference from the underlying substrate. 160 This was able to discriminate 41 out of 45 pairs of inks (sourced from seven different pen models), and in one case was able to distinguish between separate batches of the same pen. Braz et al. instead applied PCA to Raman spectra for the non-destructive, in situ analysis of over 300 pens, representing 38 pen models sourced from 12 known brands. 169 This was able to distinguish inks of different brands, models or batches, as well as identifying the main colourants present.</p>
        <p>A smaller volume of research has also been published on the characterisation of black writing inks. [179][180][181][182][183][184] For example, da Silva et al. employed PLS-DA to distinguish 55 inks of six different types based on their visible spectra, demonstrating the applicability of this method in casework scenarios to identify the pen type, brand and model used on several pages of a questioned document. 185 Work by Adam et al. used UV-Vis spectroscopy and PCA to distinguish 25 black ballpoint inks, 186 and later analysed a subset of these pens in pairwise comparisons by luminescence spectroscopy, resulting in 60 % of handwritten samples being successfully discriminated. 187 Silva et al. employed NIR hyperspectral imaging to identify alterations made to documents by the obliteration or adding of text. 188 A combination of PCA, multivariate curve resolution-alternating least squares (MCR-ALS) and PLS-DA was able to identify 82 % of additions and 85 % of alterations made using different inks.A smaller volume of research has also been published on the characterisation of black writing inks. [179][180][181][182][183][184] For example, da Silva et al. employed PLS-DA to distinguish 55 inks of six different types based on their visible spectra, demonstrating the applicability of this method in casework scenarios to identify the pen type, brand and model used on several pages of a questioned document. 185 Work by Adam et al. used UV-Vis spectroscopy and PCA to distinguish 25 black ballpoint inks, 186 and later analysed a subset of these pens in pairwise comparisons by luminescence spectroscopy, resulting in 60 % of handwritten samples being successfully discriminated. 187 Silva et al. employed NIR hyperspectral imaging to identify alterations made to documents by the obliteration or adding of text. 188 A combination of PCA, multivariate curve resolution-alternating least squares (MCR-ALS) and PLS-DA was able to identify 82 % of additions and 85 % of alterations made using different inks.</p>
        <p>The absolute or relative dating of an ink deposit is a frequent topic of interest in document examination, leading to attempts to construct reliable dating models using chemometric approaches. A number of studies have demonstrated the use of UV-Vis-NIR spectroscopy with multivariate regression to estimate the age of an ink deposit to within 14 days, based on analysis of inks aged between 2-18 months. 176,189,190 However, these approaches generally required solvent extraction of ink and/or investigated relatively short-term ageing. Sauzier et al. instead proposed diffuse reflectance visible spectroscopy with PLSR as a non-destructive approach. 68 Regression models for constructed for inks aged under a diurnal light cycle over 32 months, then evaluated using a test set aged up to 24 months. Predictive accuracy was poor for the first 2-6 months of ageing but found to improve for long-term ageing, with twoyear old samples yielding age estimates with a maximum error of 6 months.The absolute or relative dating of an ink deposit is a frequent topic of interest in document examination, leading to attempts to construct reliable dating models using chemometric approaches. A number of studies have demonstrated the use of UV-Vis-NIR spectroscopy with multivariate regression to estimate the age of an ink deposit to within 14 days, based on analysis of inks aged between 2-18 months. 176,189,190 However, these approaches generally required solvent extraction of ink and/or investigated relatively short-term ageing. Sauzier et al. instead proposed diffuse reflectance visible spectroscopy with PLSR as a non-destructive approach. 68 Regression models for constructed for inks aged under a diurnal light cycle over 32 months, then evaluated using a test set aged up to 24 months. Predictive accuracy was poor for the first 2-6 months of ageing but found to improve for long-term ageing, with twoyear old samples yielding age estimates with a maximum error of 6 months.</p>
        <p>Another potential point of interest in questioned document examinations is the order in which two or more inks were deposited. 192 This approach was able to successfully establish the chronological order in which inks were deposited and satisfactorily recover obliterated text. Later work by Brito et al. compared the use of k-means clustering, MCR-ALS and PLS-DA to study crossings involving combinations of blue and black pen inks. MCR-ALS was found to be the more efficient approach, correctly determining the order of over 70 % of crossings. 193 An alternative approach by Almeida et al. used laser desorption ionisation (LDI)-MS and LDI imaging with PCA to characterise 18 blue and black pen inks. 194 By monitoring the relative intensity of ions associated with the various dyes present, LDI-MSI was able to determine the sequence of different pens, pens with different printer inks, or lines produced by the same pen at least seven days apart. The use of MS offers the advantage of being able to identify the dyes present in each ink, though at the cost of potential destruction to the document.Another potential point of interest in questioned document examinations is the order in which two or more inks were deposited. 192 This approach was able to successfully establish the chronological order in which inks were deposited and satisfactorily recover obliterated text. Later work by Brito et al. compared the use of k-means clustering, MCR-ALS and PLS-DA to study crossings involving combinations of blue and black pen inks. MCR-ALS was found to be the more efficient approach, correctly determining the order of over 70 % of crossings. 193 An alternative approach by Almeida et al. used laser desorption ionisation (LDI)-MS and LDI imaging with PCA to characterise 18 blue and black pen inks. 194 By monitoring the relative intensity of ions associated with the various dyes present, LDI-MSI was able to determine the sequence of different pens, pens with different printer inks, or lines produced by the same pen at least seven days apart. The use of MS offers the advantage of being able to identify the dyes present in each ink, though at the cost of potential destruction to the document.</p>
        <p>Identifying the source of a printed document can be important in forensic investigations involving a range of fraudulent materials. This has naturally led to chemometric investigations into inkjet inks and printing toners. [195][196][197][198][199] Gál et al. studied 19 black inkjet inks of six different brands using Vis-NIR fibre optics reflection spectroscopy, enabling analysis directly from the printed paper surface. 195 PCA of the resulting spectra was able to separate inks containing carbon black as main colorant from the other inks. Spectra from a separately prepared set of validation samples was projected onto the PCA and were clustered with the expected reference samples, indicating a good level of reproducibility. However, this analysis made use of averaged spectra, making it difficult to evaluate the reliability of discrimination based on between-group to within-group variation. There was also no supervised analysis involved.Identifying the source of a printed document can be important in forensic investigations involving a range of fraudulent materials. This has naturally led to chemometric investigations into inkjet inks and printing toners. [195][196][197][198][199] Gál et al. studied 19 black inkjet inks of six different brands using Vis-NIR fibre optics reflection spectroscopy, enabling analysis directly from the printed paper surface. 195 PCA of the resulting spectra was able to separate inks containing carbon black as main colorant from the other inks. Spectra from a separately prepared set of validation samples was projected onto the PCA and were clustered with the expected reference samples, indicating a good level of reproducibility. However, this analysis made use of averaged spectra, making it difficult to evaluate the reliability of discrimination based on between-group to within-group variation. There was also no supervised analysis involved.</p>
        <p>Subsequent work led by Oravec et al. applied FTIR spectroscopy with DA to distinguish black ink from 22 inkjet printers. 196 In this instance, 45 replicate spectra were acquired across three separately printed squares of each ink, resulting in a total 990 spectra that were split into a calibration and test set. LDA using a Euclidean distance measure was found to give the best classification accuracy, with a 3-PC model correctly assigning all spectra from carbon blackbased inks, and a 5-PC model correctly assigning nearly 80 % of spectra from other black colourant inks. However, this analysis again made use of averaged spectra, and the discriminant values of the LDA models were not examined for any further insight.Subsequent work led by Oravec et al. applied FTIR spectroscopy with DA to distinguish black ink from 22 inkjet printers. 196 In this instance, 45 replicate spectra were acquired across three separately printed squares of each ink, resulting in a total 990 spectra that were split into a calibration and test set. LDA using a Euclidean distance measure was found to give the best classification accuracy, with a 3-PC model correctly assigning all spectra from carbon blackbased inks, and a 5-PC model correctly assigning nearly 80 % of spectra from other black colourant inks. However, this analysis again made use of averaged spectra, and the discriminant values of the LDA models were not examined for any further insight.</p>
        <p>Explosive events have become of increasing concern over recent decades, with a large number of high-profile incidents resulting in mass civilian casualties. Chemometrics can provide a rapid means of assessing the large volumes of data generated from an explosives investigation, as well as distinguishing key signatures of various explosive materials.Explosive events have become of increasing concern over recent decades, with a large number of high-profile incidents resulting in mass civilian casualties. Chemometrics can provide a rapid means of assessing the large volumes of data generated from an explosives investigation, as well as distinguishing key signatures of various explosive materials.</p>
        <p>Although they have lower reaction velocities, low explosives such as propellants or pyrotechnics can nevertheless cause devastating damage. Several studies have examined how chemometric techniques can be used to detect, discriminate or trace low explosive residues recovered following an explosive incident. 200,201,[210][211][212][213][214][215][216][217][218][219][202][203][204][205][206][207][208][209] In a series of three articles, Bueno et al. described the identification and discrimination of gunshot residues (GSR) using a combination of Raman and FTIR data. The first of these employed NIR Raman microspectroscopy to differentiate GSR particles originating from different calibre ammunition. 200 Differentiation algorithms based on SVM and PLS-DA resulted in 9 mm and 0.38 calibre residues being successfully distinguished with only one misclassified spectrum. In the second paper, Raman and FTIR data were combined into a single dataset to improve statistical discrimination, yielding increased sensitivity and specificity compared to the original method. 202 Finally, automated Raman microspectroscopic mapping was used as a novel approach for GSR detection on adhesive tape. 203 Validation tests resulted in true positive rates of 85 % for organic residues and 90.4 % of inorganic residues, and a detection limit of 3.4 µm was proposed.Although they have lower reaction velocities, low explosives such as propellants or pyrotechnics can nevertheless cause devastating damage. Several studies have examined how chemometric techniques can be used to detect, discriminate or trace low explosive residues recovered following an explosive incident. 200,201,[210][211][212][213][214][215][216][217][218][219][202][203][204][205][206][207][208][209] In a series of three articles, Bueno et al. described the identification and discrimination of gunshot residues (GSR) using a combination of Raman and FTIR data. The first of these employed NIR Raman microspectroscopy to differentiate GSR particles originating from different calibre ammunition. 200 Differentiation algorithms based on SVM and PLS-DA resulted in 9 mm and 0.38 calibre residues being successfully distinguished with only one misclassified spectrum. In the second paper, Raman and FTIR data were combined into a single dataset to improve statistical discrimination, yielding increased sensitivity and specificity compared to the original method. 202 Finally, automated Raman microspectroscopic mapping was used as a novel approach for GSR detection on adhesive tape. 203 Validation tests resulted in true positive rates of 85 % for organic residues and 90.4 % of inorganic residues, and a detection limit of 3.4 µm was proposed.</p>
        <p>Ceto et al. demonstrated a system for the identification of subjects involved in firearm-related incidents based on electroanalysis with chemometric data treatment (PCA and ANN). 217 This approach was successfully able to distinguish subjects with no GSR exposure, secondary exposure, and exposure related to the loading and firing of ammunition. Steffen et al., using energy dispersive X-ray (EDX) and ICP-MS with DA, was able to separate 15 primers from different ammunition brands based on their elemental and isotopic composition. 212 Fernández de la Ossa et al. took a broader approach, using NIR hyperspectral imaging with PLS-DA to successfully distinguish black powder, smokeless powders, nitrocellulose and ammonium nitrate residues in handprints. 215 This holds potential as a non-invasive technique for explosives security screening, although limits of detection and the potential impact of skin contaminants are still under investigation.Ceto et al. demonstrated a system for the identification of subjects involved in firearm-related incidents based on electroanalysis with chemometric data treatment (PCA and ANN). 217 This approach was successfully able to distinguish subjects with no GSR exposure, secondary exposure, and exposure related to the loading and firing of ammunition. Steffen et al., using energy dispersive X-ray (EDX) and ICP-MS with DA, was able to separate 15 primers from different ammunition brands based on their elemental and isotopic composition. 212 Fernández de la Ossa et al. took a broader approach, using NIR hyperspectral imaging with PLS-DA to successfully distinguish black powder, smokeless powders, nitrocellulose and ammonium nitrate residues in handprints. 215 This holds potential as a non-invasive technique for explosives security screening, although limits of detection and the potential impact of skin contaminants are still under investigation.</p>
        <p>An optimisation study by Sauzier et al. used a face-centred CCD to investigate the effect of swabbing solvent, storage time, storage temperature and extraction time on the recovery of double-base smokeless powder residues analysed with GC-MS. 216 The best recoveries were obtained from isopropanol-wetted swabs stored under refrigerated conditions, then extracted for 15 minutes on the same day as collection. These parameters were applied to the recovery of post-blast residues deposited on steel plates following a pipe bomb detonation, and gave over 95 % detection rates for nitroglycerin and diphenylamine, though ethyl centralite proved challenging due to its low volatility.An optimisation study by Sauzier et al. used a face-centred CCD to investigate the effect of swabbing solvent, storage time, storage temperature and extraction time on the recovery of double-base smokeless powder residues analysed with GC-MS. 216 The best recoveries were obtained from isopropanol-wetted swabs stored under refrigerated conditions, then extracted for 15 minutes on the same day as collection. These parameters were applied to the recovery of post-blast residues deposited on steel plates following a pipe bomb detonation, and gave over 95 % detection rates for nitroglycerin and diphenylamine, though ethyl centralite proved challenging due to its low volatility.</p>
        <p>Several authors have examined the analysis of explosives using chemometrics in combination with laser-induced spectroscopy or emission techniques, [220][221][222][223][224][225][226] electrochemical analysis, 227- 229 vibrational spectroscopy, [230][231][232][233][234] ion mobility or isotope ratio mass spectrometry, [235][236][237] capillary electrophoresis, 238,239 gas chromatography, 240 or atmospheric flow tube-mass spectrometry. 241 Gottfried et al. demonstrated the detection of cyclotrimethylene trinitramine (RDX) and nonexplosive residues on various surfaces using LIBS and PLS-DA. 223 Models were constructed for each substrate based on nine peak intensities and 20 peak ratios, acquired from a 25 m distance. A detection rate of at least 90 % was achieved on all surfaces, though non-explosive residues on wood or travertine gave false positive rates exceeding 10 %. A combined model incorporating all substrates gave a true positive rate of 88.6 % and false positive rate of 12.7 %, with almost all false positives originating from wood, travertine or cardboard. Despite the issues encountered with these surfaces, the ability of LIBS to acquire spectra from a distance of several metres makes this a highly promising method for the remote detection of explosives.Several authors have examined the analysis of explosives using chemometrics in combination with laser-induced spectroscopy or emission techniques, [220][221][222][223][224][225][226] electrochemical analysis, 227- 229 vibrational spectroscopy, [230][231][232][233][234] ion mobility or isotope ratio mass spectrometry, [235][236][237] capillary electrophoresis, 238,239 gas chromatography, 240 or atmospheric flow tube-mass spectrometry. 241 Gottfried et al. demonstrated the detection of cyclotrimethylene trinitramine (RDX) and nonexplosive residues on various surfaces using LIBS and PLS-DA. 223 Models were constructed for each substrate based on nine peak intensities and 20 peak ratios, acquired from a 25 m distance. A detection rate of at least 90 % was achieved on all surfaces, though non-explosive residues on wood or travertine gave false positive rates exceeding 10 %. A combined model incorporating all substrates gave a true positive rate of 88.6 % and false positive rate of 12.7 %, with almost all false positives originating from wood, travertine or cardboard. Despite the issues encountered with these surfaces, the ability of LIBS to acquire spectra from a distance of several metres makes this a highly promising method for the remote detection of explosives.</p>
        <p>Ceto et al. described the simultaneous determination of five nitro-containing explosive compounds using cyclic voltammetry. 227 Voltammetric responses were pre-processed using discrete wavelet transform and the resulting coefficients analysed by PCA, distinguishing the pure components from five commercial mixtures. An artificial neural network was constructed to individually quantify RDX, trinitrotoluene (TNT) and pentaerythritol tetranitrate (PETN) in each mixture, yielding highly accurate estimates for both the training and validation sets. It was suggested that this methodology could be expanded to peroxide explosives such as triacetone triperoxide (TATP) that may prove challenging to analyse using traditional detection techniques, such as GC-MS.Ceto et al. described the simultaneous determination of five nitro-containing explosive compounds using cyclic voltammetry. 227 Voltammetric responses were pre-processed using discrete wavelet transform and the resulting coefficients analysed by PCA, distinguishing the pure components from five commercial mixtures. An artificial neural network was constructed to individually quantify RDX, trinitrotoluene (TNT) and pentaerythritol tetranitrate (PETN) in each mixture, yielding highly accurate estimates for both the training and validation sets. It was suggested that this methodology could be expanded to peroxide explosives such as triacetone triperoxide (TATP) that may prove challenging to analyse using traditional detection techniques, such as GC-MS.</p>
        <p>Buxton and Harrington examined the use of ion mobility spectrometry with multivariate mixture analysis to distinguish PETN and cyclotetramethylene tetranitrate (HMX) residues from potential interferents encountered during airport baggage screening. 235 Modification of the instrument to allow temperature programming yielded greater sensitivity and selectivity in comparison to standard thermal desorption; however, the analysis time was increased from 5 to 20 seconds. This could potentially be problematic during peak periods when the throughput demanded for luggage screening is extremely high.Buxton and Harrington examined the use of ion mobility spectrometry with multivariate mixture analysis to distinguish PETN and cyclotetramethylene tetranitrate (HMX) residues from potential interferents encountered during airport baggage screening. 235 Modification of the instrument to allow temperature programming yielded greater sensitivity and selectivity in comparison to standard thermal desorption; however, the analysis time was increased from 5 to 20 seconds. This could potentially be problematic during peak periods when the throughput demanded for luggage screening is extremely high.</p>
        <p>Each year, deliberately lit fires cause significant damage to people, property and the environment. Various studies have used chemometrics to extract key information from accelerants (including diesel fuels, [242][243][244][245] gasolines 31,[246][247][248][249][250][251][252][253][254] and other ignitable liquids [255][256][257][258][259][260][261][262] ) or fire debris. [263][264][265][266][267][268] The reader is also directed to existing in-depth reviews on this subject. 76,269 Of particular interest, Sinkov et al. applied PLS-DA and SIMCA classification to the GC-MS data acquired from 220 casework arson samples. 247 Chromatograms were first aligned based on a spiked ladder of perdeuterated n-alkanes, with variable selection and model optimisation performed using an in-house program. This resulted in all 55 validation samples being correctly assigned as either gasoline-containing or gasoline-free, although no determination could be made regarding the type of gasoline used or any other classes of accelerant present.Each year, deliberately lit fires cause significant damage to people, property and the environment. Various studies have used chemometrics to extract key information from accelerants (including diesel fuels, [242][243][244][245] gasolines 31,[246][247][248][249][250][251][252][253][254] and other ignitable liquids [255][256][257][258][259][260][261][262] ) or fire debris. [263][264][265][266][267][268] The reader is also directed to existing in-depth reviews on this subject. 76,269 Of particular interest, Sinkov et al. applied PLS-DA and SIMCA classification to the GC-MS data acquired from 220 casework arson samples. 247 Chromatograms were first aligned based on a spiked ladder of perdeuterated n-alkanes, with variable selection and model optimisation performed using an in-house program. This resulted in all 55 validation samples being correctly assigned as either gasoline-containing or gasoline-free, although no determination could be made regarding the type of gasoline used or any other classes of accelerant present.</p>
        <p>Research by Bodle and Hardy employed solid phase microextraction (SPME)-GC and SIMCA to distinguish five classes of accelerants, with a cross-validation accuracy of 97.2 %. 257 Waddell similarly utilised several multivariate methods to distinguish ignitable liquid classes based upon their GC-MS total ion spectra. 265 An initial approach using quadratic DA resulted in a 70.9 % validation accuracy based on samples produced in laboratory and field-test burns. A subsequent article evaluated SIMCA as an alternative classification scheme compared to inspection by an examiner. 264 It was found that whilst the examiner achieved a 90.5 % classification accuracy compared to 79.1 % using SIMCA, the false positive rate also increased from 8.9 % to 15.0 %.Research by Bodle and Hardy employed solid phase microextraction (SPME)-GC and SIMCA to distinguish five classes of accelerants, with a cross-validation accuracy of 97.2 %. 257 Waddell similarly utilised several multivariate methods to distinguish ignitable liquid classes based upon their GC-MS total ion spectra. 265 An initial approach using quadratic DA resulted in a 70.9 % validation accuracy based on samples produced in laboratory and field-test burns. A subsequent article evaluated SIMCA as an alternative classification scheme compared to inspection by an examiner. 264 It was found that whilst the examiner achieved a 90.5 % classification accuracy compared to 79.1 % using SIMCA, the false positive rate also increased from 8.9 % to 15.0 %.</p>
        <p>Chemometric methods have also been used to discriminate accelerants within a class according to their refining process, brand or other distinguishing features. Monfreda and Gregori for example were able to separate 50 unevaporated gasoline samples of five brands based on GC-MS determination of their aromatic compound content modelled using PCA and DA. 250 For two brands, it was additionally possible to link the chemical characteristics of samples to the crude oil employed. Balabin et al. were also successful in separating classes of liquid gasoline according to their refining site, refinery stream or octane rating using NIR spectroscopy and nine multivariate classifiers including LDA, SIMCA, kNN and SVM. 31 A limitation when establishing chemometric classifiers for arson investigations is the potential impact of sample weathering or degradation. Additionally, the analysis of 'neat' samples does not take into account the various matrix interferences found in casework samples. Baerncopf et al. thus developed a methodology to associate post-burn ignitable liquid residues to the corresponding neat liquid, with discrimination from matrix interferences. 255 Six ignitable liquids were burned on carpet, extracted using passive headspace extraction and analysed by GC-MS. PCA resulted in the six liquids being discriminated based upon their alkane and aromatic content, while Pearson product moment correlation coefficients were able to correctly associate all residues to their neat liquid equivalent.Chemometric methods have also been used to discriminate accelerants within a class according to their refining process, brand or other distinguishing features. Monfreda and Gregori for example were able to separate 50 unevaporated gasoline samples of five brands based on GC-MS determination of their aromatic compound content modelled using PCA and DA. 250 For two brands, it was additionally possible to link the chemical characteristics of samples to the crude oil employed. Balabin et al. were also successful in separating classes of liquid gasoline according to their refining site, refinery stream or octane rating using NIR spectroscopy and nine multivariate classifiers including LDA, SIMCA, kNN and SVM. 31 A limitation when establishing chemometric classifiers for arson investigations is the potential impact of sample weathering or degradation. Additionally, the analysis of 'neat' samples does not take into account the various matrix interferences found in casework samples. Baerncopf et al. thus developed a methodology to associate post-burn ignitable liquid residues to the corresponding neat liquid, with discrimination from matrix interferences. 255 Six ignitable liquids were burned on carpet, extracted using passive headspace extraction and analysed by GC-MS. PCA resulted in the six liquids being discriminated based upon their alkane and aromatic content, while Pearson product moment correlation coefficients were able to correctly associate all residues to their neat liquid equivalent.</p>
        <p>Turner and Goodpaster subjected simulated samples containing gasoline to weathering and microbial degradation in soils prior to analysis by GC-MS and PCA. 253 Volatile components were found to be susceptible to weathering, while mono-substituted aromatics or long-chain alkanes were most affected by microbial action. Highly substituted aromatics were found to be most resistant to weathering or degradation, and hence these compounds can be suggested as ideal targets for analysis.Turner and Goodpaster subjected simulated samples containing gasoline to weathering and microbial degradation in soils prior to analysis by GC-MS and PCA. 253 Volatile components were found to be susceptible to weathering, while mono-substituted aromatics or long-chain alkanes were most affected by microbial action. Highly substituted aromatics were found to be most resistant to weathering or degradation, and hence these compounds can be suggested as ideal targets for analysis.</p>
        <p>Polymeric products are prevalent throughout daily life and so could reasonably be expected to appear in the course of criminal investigations. Several studies have used chemometrics to assess the chemical diversity amongst polymers such as paints, 24,270,[279][280][281][282][283][284][285][286][287][288]271,[289][290][291][292][293][294][295][296][297][272][273][274][275][276][277][278] banknotes, [298][299][300] adhesives tapes, 301,302 or cling films 35,303 in order to better exploit them as forms of evidence.Polymeric products are prevalent throughout daily life and so could reasonably be expected to appear in the course of criminal investigations. Several studies have used chemometrics to assess the chemical diversity amongst polymers such as paints, 24,270,[279][280][281][282][283][284][285][286][287][288]271,[289][290][291][292][293][294][295][296][297][272][273][274][275][276][277][278] banknotes, [298][299][300] adhesives tapes, 301,302 or cling films 35,303 in order to better exploit them as forms of evidence.</p>
        <p>Paints and other coatings are applied to many manufactured items to protect or aesthetically improve their surface. Paint chips and smears are thus commonly encountered as forensic transfer evidence, and their analysis may prove essential in obtaining investigative leads.Paints and other coatings are applied to many manufactured items to protect or aesthetically improve their surface. Paint chips and smears are thus commonly encountered as forensic transfer evidence, and their analysis may prove essential in obtaining investigative leads.</p>
        <p>Muehlethaler et al. analysed 34 red household paint samples using both FTIR and Raman spectroscopy combined with unsupervised chemometric analysis. 287 PCA of the FTIR data was able to distinguish samples based on their binder type (alkyd or acrylic resin) and presence or absence of calcium carbonate, whilst Raman spectroscopy differentiated samples according to their pigment composition. The same authors later applied FTIR with supervised and unsupervised methods to analyse 74 red, green and blue spray paints. 288 Iterative PCA was able to discriminate over 90 % of samples in each category, and SIMCA models gave an approximately 95 % classification accuracy of a separate validation set.Muehlethaler et al. analysed 34 red household paint samples using both FTIR and Raman spectroscopy combined with unsupervised chemometric analysis. 287 PCA of the FTIR data was able to distinguish samples based on their binder type (alkyd or acrylic resin) and presence or absence of calcium carbonate, whilst Raman spectroscopy differentiated samples according to their pigment composition. The same authors later applied FTIR with supervised and unsupervised methods to analyse 74 red, green and blue spray paints. 288 Iterative PCA was able to discriminate over 90 % of samples in each category, and SIMCA models gave an approximately 95 % classification accuracy of a separate validation set.</p>
        <p>Chemometric methods have also proved a valuable tool in the analysis of paints used in artworks. Of particular interest, Rosi et al. combined PCA with reflection micro IR spectroscopy to map cross-sectioned paint from simulated ancient easel paintings. 292 This method allowed characterisation of the layer sequence according to the inorganic pigments and organic binders present in each layer. Although these studies were originally conducted from an art provenance and conservation standpoint, the established approach could potentially be applied to investigations of art forgery. Liszewski et al. 281 and Mendlein et al. 285 examined a series of automotive clear coats using UV or micro Raman spectroscopy. Pattern recognition revealed broad classes, but these could not be correlated with any specific features of the source vehicles. Later studies by Maric et al. used vibrational spectroscopy with PCA and LDA to discriminate the clear coat or primer surfacer layers from Australian and international vehicles. 24,[282][283][284] This ultimately revealed 19 distinct classes related to the vehicle manufacturer, model and in some instances specific year ranges or manufacturing plants. Further work established that post-manufacture coatings or long-term environmental exposure could cause erroneous classifications when relying solely on analysis of the upper clear coat, though these samples could be identified as atypical. 293,296 This result has implications for the sampling of automotive paint from known vehicles, and for the interpretation of questioned versus known comparisons.Chemometric methods have also proved a valuable tool in the analysis of paints used in artworks. Of particular interest, Rosi et al. combined PCA with reflection micro IR spectroscopy to map cross-sectioned paint from simulated ancient easel paintings. 292 This method allowed characterisation of the layer sequence according to the inorganic pigments and organic binders present in each layer. Although these studies were originally conducted from an art provenance and conservation standpoint, the established approach could potentially be applied to investigations of art forgery. Liszewski et al. 281 and Mendlein et al. 285 examined a series of automotive clear coats using UV or micro Raman spectroscopy. Pattern recognition revealed broad classes, but these could not be correlated with any specific features of the source vehicles. Later studies by Maric et al. used vibrational spectroscopy with PCA and LDA to discriminate the clear coat or primer surfacer layers from Australian and international vehicles. 24,[282][283][284] This ultimately revealed 19 distinct classes related to the vehicle manufacturer, model and in some instances specific year ranges or manufacturing plants. Further work established that post-manufacture coatings or long-term environmental exposure could cause erroneous classifications when relying solely on analysis of the upper clear coat, though these samples could be identified as atypical. 293,296 This result has implications for the sampling of automotive paint from known vehicles, and for the interpretation of questioned versus known comparisons.</p>
        <p>Several studies by Lavine et al. investigated the use of genetic algorithms to match automotive clear coats against the IR spectral library of the Paint Data Query database. [278][279][280] Successful discrimination was made between 2000 -2006 model Chrysler and General Motor vehicles according to their assembly plant, allowing identification of the model, line or manufacturing year of the source vehicles. However, as the samples utilised in this study originated solely from North American manufacturing plants, the methodology currently has limited applicability within an international context.Several studies by Lavine et al. investigated the use of genetic algorithms to match automotive clear coats against the IR spectral library of the Paint Data Query database. [278][279][280] Successful discrimination was made between 2000 -2006 model Chrysler and General Motor vehicles according to their assembly plant, allowing identification of the model, line or manufacturing year of the source vehicles. However, as the samples utilised in this study originated solely from North American manufacturing plants, the methodology currently has limited applicability within an international context.</p>
        <p>Less than a decade after the first modern polymer banknotes were issued, the first polymer counterfeits were detected. Chemometrics has recently been proposed as a rapid and reproducible means of authentication. Early work by de Almeida et al. used Raman spectroscopy with PLS-DA to distinguish authentic and counterfeit Brazilian banknotes. 298 Although successful, the counterfeits were paper based, so the applicability of this approach to more sophisticated polymer counterfeits is uncertain. Correia et al. instead proposed using NIR spectroscopy, with PCA identifying three areas most likely to vary between authentic and counterfeit banknotes. 299 PLS-DA successfully distinguished three authentic and nine counterfeit banknotes, though a separate test set was not examined. Additionally, da Silva Oliveira et al. used portable NIR spectroscopy with SIMCA and LDA to distinguish 300 authentic and 227 counterfeit banknotes. 300 Both approaches were reported to be successful, although no metrics such as discriminant values were used to assess the quality of the classifications.Less than a decade after the first modern polymer banknotes were issued, the first polymer counterfeits were detected. Chemometrics has recently been proposed as a rapid and reproducible means of authentication. Early work by de Almeida et al. used Raman spectroscopy with PLS-DA to distinguish authentic and counterfeit Brazilian banknotes. 298 Although successful, the counterfeits were paper based, so the applicability of this approach to more sophisticated polymer counterfeits is uncertain. Correia et al. instead proposed using NIR spectroscopy, with PCA identifying three areas most likely to vary between authentic and counterfeit banknotes. 299 PLS-DA successfully distinguished three authentic and nine counterfeit banknotes, though a separate test set was not examined. Additionally, da Silva Oliveira et al. used portable NIR spectroscopy with SIMCA and LDA to distinguish 300 authentic and 227 counterfeit banknotes. 300 Both approaches were reported to be successful, although no metrics such as discriminant values were used to assess the quality of the classifications.</p>
        <p>Electrical tapes are a common commercial and household product often encountered as physical evidence, particularly in explosives investigations. Goodpaster et al. employed PCA and DA to characterise 67 electrical tape rolls from the reference collection of the Bureau of Alcohol, Tobacco, Firearms and Explosives (ATF), according to their surface texture and elemental composition. 301 36 classes were identified within the dataset, to which samples could be assigned with up to 94 % accuracy. A subsequent study employed FTIR spectroscopy and DA to classify 72 tape rolls to their nominal brand based on spectra acquired from the tape backing and adhesive, with accuracy rates of up to 99 %. 302 This model was additionally utilised to correctly associate two fragments of blast-damaged tape a detonated pipe bomb to their respective brands of origin.Electrical tapes are a common commercial and household product often encountered as physical evidence, particularly in explosives investigations. Goodpaster et al. employed PCA and DA to characterise 67 electrical tape rolls from the reference collection of the Bureau of Alcohol, Tobacco, Firearms and Explosives (ATF), according to their surface texture and elemental composition. 301 36 classes were identified within the dataset, to which samples could be assigned with up to 94 % accuracy. A subsequent study employed FTIR spectroscopy and DA to classify 72 tape rolls to their nominal brand based on spectra acquired from the tape backing and adhesive, with accuracy rates of up to 99 %. 302 This model was additionally utilised to correctly associate two fragments of blast-damaged tape a detonated pipe bomb to their respective brands of origin.</p>
        <p>Several other polymeric products are also of interest for forensic purposes. Cling films for example are commonly used in the packaging of illicit drugs, hence their analysis could potentially establish linkages between seizures. Telford et al. studied nine rolls of low-density polyethylene cling films using ATR-FTIR spectroscopy, with PCA identifying three main clusters associated with the manufacturing brand. 35 Whilst several studies have been carried out on automotive paint, other automotive polymers may provide useful information to an investigation. Grant et al. investigated the chemical diversity amongst 40 automotive window tints using ATR-FTIR spectroscopy with PCA and LDA. 304 Substantial variability was observed between tints of different brands, and in some instances individual tint products. Subsequent predictive models were able to associate unknown tint samples to their brand, and found to be robust to both adhesive curing and short-term environmental exposure over a five-month period. May and Watling successfully discriminated polycarbonate headlamp lenses from three vehicle types based on trace elemental analysis with LA-ICP-MS and PCA-LDA. 305 Blind validation trials for each headlamp type gave correct assignment of simulated unknowns, with the exception of some headlamp lenses produced on the same day.Several other polymeric products are also of interest for forensic purposes. Cling films for example are commonly used in the packaging of illicit drugs, hence their analysis could potentially establish linkages between seizures. Telford et al. studied nine rolls of low-density polyethylene cling films using ATR-FTIR spectroscopy, with PCA identifying three main clusters associated with the manufacturing brand. 35 Whilst several studies have been carried out on automotive paint, other automotive polymers may provide useful information to an investigation. Grant et al. investigated the chemical diversity amongst 40 automotive window tints using ATR-FTIR spectroscopy with PCA and LDA. 304 Substantial variability was observed between tints of different brands, and in some instances individual tint products. Subsequent predictive models were able to associate unknown tint samples to their brand, and found to be robust to both adhesive curing and short-term environmental exposure over a five-month period. May and Watling successfully discriminated polycarbonate headlamp lenses from three vehicle types based on trace elemental analysis with LA-ICP-MS and PCA-LDA. 305 Blind validation trials for each headlamp type gave correct assignment of simulated unknowns, with the exception of some headlamp lenses produced on the same day.</p>
        <p>Counterfeiting of polymer identity cards poses significant security and economic concerns.Counterfeiting of polymer identity cards poses significant security and economic concerns.</p>
        <p>McGann and Willans et al. highlighted SVM as a possible tool to assist expert examination of suspected counterfeits. 45 Three issuing series of Western Australian driver's licences were distinguished based on their cross-sectional ATR-FTIR spectra, and test licences of each series correctly assigned. Whilst this approach has not yet been validated on known counterfeits, it established that variations in the polymer composition could indeed be detected using simple, non-destructive analysis.McGann and Willans et al. highlighted SVM as a possible tool to assist expert examination of suspected counterfeits. 45 Three issuing series of Western Australian driver's licences were distinguished based on their cross-sectional ATR-FTIR spectra, and test licences of each series correctly assigned. Whilst this approach has not yet been validated on known counterfeits, it established that variations in the polymer composition could indeed be detected using simple, non-destructive analysis.</p>
        <p>Two of the most frequently encountered forms of evidence in forensic investigations are hairs and fibres. The wide range of potential colours or morphologies, particularly for fibres, can make these forms of evidence extremely probative. This was expounded in the recent judgement of The State of Western Australia v Edwards [2020] WASC 339; a case in which fibre analysis and transfer played a significant role. 306 Chemometric studies of hair have generally focussed around the discrimination of dyed and non-dyed hairs, or species identification. [307][308][309] In one example, Barrett et al. investigated the discrimination of red-dyed hair samples based upon their UV-Vis absorbance spectra. 308 HCA identified three clusters of samples that were visually consistent with different shades of red. PCA identified the same groupings, and inspection of the loadings plot suggested that this separation was based on differences in the intensity of absorbance peaks related to both the hair and the dye. DA yielded an 89.1 % classification accuracy based on cross-validation, and 75 % using an external validation set. Additionally, it was found that successive washing of the hair samples led to a significant loss of dye colour within three weeks of application, which could lead to incorrect classifications.Two of the most frequently encountered forms of evidence in forensic investigations are hairs and fibres. The wide range of potential colours or morphologies, particularly for fibres, can make these forms of evidence extremely probative. This was expounded in the recent judgement of The State of Western Australia v Edwards [2020] WASC 339; a case in which fibre analysis and transfer played a significant role. 306 Chemometric studies of hair have generally focussed around the discrimination of dyed and non-dyed hairs, or species identification. [307][308][309] In one example, Barrett et al. investigated the discrimination of red-dyed hair samples based upon their UV-Vis absorbance spectra. 308 HCA identified three clusters of samples that were visually consistent with different shades of red. PCA identified the same groupings, and inspection of the loadings plot suggested that this separation was based on differences in the intensity of absorbance peaks related to both the hair and the dye. DA yielded an 89.1 % classification accuracy based on cross-validation, and 75 % using an external validation set. Additionally, it was found that successive washing of the hair samples led to a significant loss of dye colour within three weeks of application, which could lead to incorrect classifications.</p>
        <p>Chemometric methods have also been applied in several studies for the discrimination or provenance determination of fibres. 310-317 318-327 Causin et al. utilised pyrolysis GC-MS with PCA to differentiate colourless polyacrylonitrile-based fibres of similar morphological features. 310 The sample set comprised 36 fibres acquired from 11 manufacturers, with fibres from certain manufacturers differing by batch, manufacturing plant, or intended end-use. Fibres from one manufacturer were able to be uniquely identified based on the presence of methyl methacrylate as a co-monomer. The remaining samples were divided into two clusters according to the presence or absence of vinyl acetate, although no further separation according to any categorical factors was achieved.Chemometric methods have also been applied in several studies for the discrimination or provenance determination of fibres. 310-317 318-327 Causin et al. utilised pyrolysis GC-MS with PCA to differentiate colourless polyacrylonitrile-based fibres of similar morphological features. 310 The sample set comprised 36 fibres acquired from 11 manufacturers, with fibres from certain manufacturers differing by batch, manufacturing plant, or intended end-use. Fibres from one manufacturer were able to be uniquely identified based on the presence of methyl methacrylate as a co-monomer. The remaining samples were divided into two clusters according to the presence or absence of vinyl acetate, although no further separation according to any categorical factors was achieved.</p>
        <p>Rich et al. applied PCA and LDA to analyse a database of over 5,000 MSP spectra acquired from approximately 500 dyed textile fibres. 325 Both UV-Vis absorbance and fluorescence spectra were found to provide discriminating information, depending on the fibre set under analysis. In general, however, UV-Vis spectroscopy was determined to be the best single discriminating technique, allowing 78.9 % of fibres to be differentiated. It should be noted that in this study, all fibres of a particular colour were compared simultaneously, whereas casework scenarios more commonly utilise pairwise comparisons between a recovered fibre and one from a known source. It is thus possible that pairwise comparisons of this sample set would have led to improved discrimination between similar fibres.Rich et al. applied PCA and LDA to analyse a database of over 5,000 MSP spectra acquired from approximately 500 dyed textile fibres. 325 Both UV-Vis absorbance and fluorescence spectra were found to provide discriminating information, depending on the fibre set under analysis. In general, however, UV-Vis spectroscopy was determined to be the best single discriminating technique, allowing 78.9 % of fibres to be differentiated. It should be noted that in this study, all fibres of a particular colour were compared simultaneously, whereas casework scenarios more commonly utilise pairwise comparisons between a recovered fibre and one from a known source. It is thus possible that pairwise comparisons of this sample set would have led to improved discrimination between similar fibres.</p>
        <p>The analysis of biological materials such as bodily fluids, 328,329,[338][339][340][341][342][343][344][345][346][347][330][331][332][333][334][335][336][337] decomposition fluid, 348 skeletal remains [349][350][351] or fingermark residues [352][353][354] has forensic relevance for identification and timeline reconstruction. Chemometrics can assist in distinguishing various forms of biological material, creating dating models, or exploring variations associated with biological factors such as age and sex.The analysis of biological materials such as bodily fluids, 328,329,[338][339][340][341][342][343][344][345][346][347][330][331][332][333][334][335][336][337] decomposition fluid, 348 skeletal remains [349][350][351] or fingermark residues [352][353][354] has forensic relevance for identification and timeline reconstruction. Chemometrics can assist in distinguishing various forms of biological material, creating dating models, or exploring variations associated with biological factors such as age and sex.</p>
        <p>Sikirzhytski et al. used confocal Raman microscopy with DA to differentiate between dry samples of blood, semen and saliva with 100% accuracy. 342,343 With increasing advances in portable Raman spectrometers, this approach could provide the foundation for an at-scene identification method. Li et attempted to estimate the age of equine bloodstains (employed as an analogous material for human blood) based on their visible reflectance spectra. 333 Initial results using LDA produced a correct classification rate exceeding 90 %; however, this was obtained using a single blood deposit for both training and test purposes, with only 10 replicate spectra making up each set. The correct classification rate fell to 54.7 % when using an external test set obtained from a second bloodstain.Sikirzhytski et al. used confocal Raman microscopy with DA to differentiate between dry samples of blood, semen and saliva with 100% accuracy. 342,343 With increasing advances in portable Raman spectrometers, this approach could provide the foundation for an at-scene identification method. Li et attempted to estimate the age of equine bloodstains (employed as an analogous material for human blood) based on their visible reflectance spectra. 333 Initial results using LDA produced a correct classification rate exceeding 90 %; however, this was obtained using a single blood deposit for both training and test purposes, with only 10 replicate spectra making up each set. The correct classification rate fell to 54.7 % when using an external test set obtained from a second bloodstain.</p>
        <p>Doty and Lednev used Raman spectroscopy with PLS-DA to distinguish human blood taken from 49 donors (representing different ages, biological sexes and races) from those of 16 animal species known to yield false positives with certain presumptive tests such as luminol or the Kastle-Meyer reagent. 330 An external validation sensitivity of 1.00 was achieved, along with a specificity of 0.93, as some false positives were obtained when analysing blood from species with a similar haemoglobin structure to human blood. The ability to identify bloodstains originating from various species is of importance in wildlife forensics, or to hitand-run incidents that may involve both humans and animals.Doty and Lednev used Raman spectroscopy with PLS-DA to distinguish human blood taken from 49 donors (representing different ages, biological sexes and races) from those of 16 animal species known to yield false positives with certain presumptive tests such as luminol or the Kastle-Meyer reagent. 330 An external validation sensitivity of 1.00 was achieved, along with a specificity of 0.93, as some false positives were obtained when analysing blood from species with a similar haemoglobin structure to human blood. The ability to identify bloodstains originating from various species is of importance in wildlife forensics, or to hitand-run incidents that may involve both humans and animals.</p>
        <p>Exploratory chemometric tools have also been utilised to investigate the composition of latent fingermark deposits. Girod and Weyermann used GC-MS and cluster analysis to classify fingermarks from 25 donors into 'poor' or 'rich' lipid categories. 353 Fingermark replicates of selected donors were tested as a validation set, with 86 % being correctly classified. It was proposed that this model could be exploited for research purposes in order to select ideal donors for given compounds of interest. Frick et al. later employed PCA to examine the lipid composition of fingermarks collected from over 100 donors. 352 Although variations between different donors were apparent, no correlation to specific donor traits could be discerned.Exploratory chemometric tools have also been utilised to investigate the composition of latent fingermark deposits. Girod and Weyermann used GC-MS and cluster analysis to classify fingermarks from 25 donors into 'poor' or 'rich' lipid categories. 353 Fingermark replicates of selected donors were tested as a validation set, with 86 % being correctly classified. It was proposed that this model could be exploited for research purposes in order to select ideal donors for given compounds of interest. Frick et al. later employed PCA to examine the lipid composition of fingermarks collected from over 100 donors. 352 Although variations between different donors were apparent, no correlation to specific donor traits could be discerned.</p>
        <p>An emerging area of interest in forensics is the examination of cosmetics including lipsticks 26,34,[355][356][357][358][359][360] , kohl/kajal 361,362 and nail polish; 363,364 as well as 'personal products' such as fragrances 365 and lubricants. 32,[366][367][368][369][370] These traces have particular relevance in cases involving violent and/or sexual assault.An emerging area of interest in forensics is the examination of cosmetics including lipsticks 26,34,[355][356][357][358][359][360] , kohl/kajal 361,362 and nail polish; 363,364 as well as 'personal products' such as fragrances 365 and lubricants. 32,[366][367][368][369][370] These traces have particular relevance in cases involving violent and/or sexual assault.</p>
        <p>Kulikov et al. employed wavelength-dispersive X-ray fluorescence spectrometry for the elemental analysis of 39 cosmetic powders. 371 Cluster analysis and PCA were able to clearly discriminate between samples possessing traditional ingredient or mineral-based formulations, and also distinguish specific manufacturers of the latter. Salahioglu et al. later demonstrated the use of Raman spectroscopy to discriminate lipstick samples deposited on textile fibres, cigarette butts and paper tissues. 34 Thirty spectra each of ten different lipsticks were subjected to PCA and kNN, attaining accuracy rates up to 98.7 %.Kulikov et al. employed wavelength-dispersive X-ray fluorescence spectrometry for the elemental analysis of 39 cosmetic powders. 371 Cluster analysis and PCA were able to clearly discriminate between samples possessing traditional ingredient or mineral-based formulations, and also distinguish specific manufacturers of the latter. Salahioglu et al. later demonstrated the use of Raman spectroscopy to discriminate lipstick samples deposited on textile fibres, cigarette butts and paper tissues. 34 Thirty spectra each of ten different lipsticks were subjected to PCA and kNN, attaining accuracy rates up to 98.7 %.</p>
        <p>Coon, Beyramysoltan and Musah constructed a database of condom residues from 110 condoms of 16 brands using direct analysis in real time-mass spectrometry. 32 PLS-DA was then used to generate a classifier for brand prediction, which gave an accuracy of 97.4 %. This model was generally robust to air exposure or dust contamination, but failed to correctly assign residues that had been transferred to glass slides. Baumgarten et al., rather than attempting to classify by brand, established eleven classes of silicone-based lubricants according to their chemical composition. 366 Several groupings were distinguished based on the relative ratio of components such as nonoxynol-9 (a spermicide) or octylamine (an emulsifier). An LDA model validated using known and blind test samples gave highly accurate results, slightly outperforming both kNN and SVM classification.Coon, Beyramysoltan and Musah constructed a database of condom residues from 110 condoms of 16 brands using direct analysis in real time-mass spectrometry. 32 PLS-DA was then used to generate a classifier for brand prediction, which gave an accuracy of 97.4 %. This model was generally robust to air exposure or dust contamination, but failed to correctly assign residues that had been transferred to glass slides. Baumgarten et al., rather than attempting to classify by brand, established eleven classes of silicone-based lubricants according to their chemical composition. 366 Several groupings were distinguished based on the relative ratio of components such as nonoxynol-9 (a spermicide) or octylamine (an emulsifier). An LDA model validated using known and blind test samples gave highly accurate results, slightly outperforming both kNN and SVM classification.</p>
        <p>Maurer et al. used a full factorial design to investigate factors affecting the pyrolysis GC/MS analysis of PDMS, particularly in terms of repeatability. 369 Temperature and pyrolysis time were found to be highly influential on target compound variability. The factorial design was extended to a face-centred CCD to optimise these parameters, which were then applied to extracts from silicone-based condoms. Although the condoms presented similar chemical profiles to the standards, the results lacked repeatability, which may have been due to variations in the integration of minor peaks. Nonetheless, this study represents an important step in determining statistically validated protocols for condom and lubricant analysis.Maurer et al. used a full factorial design to investigate factors affecting the pyrolysis GC/MS analysis of PDMS, particularly in terms of repeatability. 369 Temperature and pyrolysis time were found to be highly influential on target compound variability. The factorial design was extended to a face-centred CCD to optimise these parameters, which were then applied to extracts from silicone-based condoms. Although the condoms presented similar chemical profiles to the standards, the results lacked repeatability, which may have been due to variations in the integration of minor peaks. Nonetheless, this study represents an important step in determining statistically validated protocols for condom and lubricant analysis.</p>
        <p>Soils may generate probative information in both criminal and environmental forensic investigations. There have hence been a number of studies applying chemometrics to discriminate or classify soils of different origins. [372][373][374][375][376][377][378][379][380][381] Thanasoulias et al. were able to distinguish soils from five different sites due to differing relative concentrations of aromatic groups in their fulvic and humic acid fractions. 380 A kmeans cluster analysis followed by PCA and LDA yielded an 85.0 % classification accuracy.Soils may generate probative information in both criminal and environmental forensic investigations. There have hence been a number of studies applying chemometrics to discriminate or classify soils of different origins. [372][373][374][375][376][377][378][379][380][381] Thanasoulias et al. were able to distinguish soils from five different sites due to differing relative concentrations of aromatic groups in their fulvic and humic acid fractions. 380 A kmeans cluster analysis followed by PCA and LDA yielded an 85.0 % classification accuracy.</p>
        <p>Dragović and Onjia described the classification of soils originating from 15 locations in Serbia and Montenegro, by applying PCA to radionuclide data collected by gamma-ray spectrometry. 376 An overall 86.0 % correct 'classification' was achieved, on par with results achieved by Thanasoulias et al. However, PCA should not generally be considered as a true classification method, as it makes no assumptions of pre-existing classes.Dragović and Onjia described the classification of soils originating from 15 locations in Serbia and Montenegro, by applying PCA to radionuclide data collected by gamma-ray spectrometry. 376 An overall 86.0 % correct 'classification' was achieved, on par with results achieved by Thanasoulias et al. However, PCA should not generally be considered as a true classification method, as it makes no assumptions of pre-existing classes.</p>
        <p>Bonetti and Quarino were able to separate soil samples collected from 12 New Jersey state parks based on their particle size distribution, pH and organic content. 373 PCA and canonical DA were initially performed solely on the particle size data, with error rates then found to decrease with the inclusion of the remaining data. Final error rates of 33.3 % and 3.3 % were obtained for soils collected during the summer and fall seasons respectively, with the high error rate of the former attributed to the collection of samples at 15 metre intervals. This indicates a high level of heterogeneity even amongst soils within a relatively limited geographical area, which may prove challenging when establishing the provenance of a questioned soil sample.Bonetti and Quarino were able to separate soil samples collected from 12 New Jersey state parks based on their particle size distribution, pH and organic content. 373 PCA and canonical DA were initially performed solely on the particle size data, with error rates then found to decrease with the inclusion of the remaining data. Final error rates of 33.3 % and 3.3 % were obtained for soils collected during the summer and fall seasons respectively, with the high error rate of the former attributed to the collection of samples at 15 metre intervals. This indicates a high level of heterogeneity even amongst soils within a relatively limited geographical area, which may prove challenging when establishing the provenance of a questioned soil sample.</p>
        <p>Glass fragments are often recovered at the scene of automotive accidents or burglaries. The ability to distinguish the source of these fragments may be vital in determining the sequence of events. Although substantial literature exists concerning the evaluation of glass using probabilistic approaches, limited work has been published examining glass using multivariate statistics. [382][383][384][385][386][387] Early work by Almirall used LDA to discriminate sources of glass according to their end-use based on elemental composition data. 386 Glass samples originating from headlamps were correctly classified, as were the majority of container glass fragments. Float glasses were distinguishable from headlamp and container glass, although some misclassifications were observed between window float glass and vehicle float glass. It was also acknowledged that the lack of an independent test set may have produced a higher proportion of correct assignments.Glass fragments are often recovered at the scene of automotive accidents or burglaries. The ability to distinguish the source of these fragments may be vital in determining the sequence of events. Although substantial literature exists concerning the evaluation of glass using probabilistic approaches, limited work has been published examining glass using multivariate statistics. [382][383][384][385][386][387] Early work by Almirall used LDA to discriminate sources of glass according to their end-use based on elemental composition data. 386 Glass samples originating from headlamps were correctly classified, as were the majority of container glass fragments. Float glasses were distinguishable from headlamp and container glass, although some misclassifications were observed between window float glass and vehicle float glass. It was also acknowledged that the lack of an independent test set may have produced a higher proportion of correct assignments.</p>
        <p>Zadora and Brozek-Mucha applied cluster analysis with scanning electron microscopy-EDX to differentiate glass samples based on their elemental content. 383 Employing a logarithmic transformation of selected element abundances revealed three clusters consistent with the glass type (car headlamp, car window or container), with only four objects lying outside of these clusters. A later study described the use of naїve Bayes classifiers and SVM models to separate glass samples originating from car or building windows and those from bulbs or headlamps. 384 Classification accuracies of 90 % or greater were achieved across ten training and validation sets using both methods, though it was recognised that classifications based solely on elemental composition should be interpreted with caution.Zadora and Brozek-Mucha applied cluster analysis with scanning electron microscopy-EDX to differentiate glass samples based on their elemental content. 383 Employing a logarithmic transformation of selected element abundances revealed three clusters consistent with the glass type (car headlamp, car window or container), with only four objects lying outside of these clusters. A later study described the use of naїve Bayes classifiers and SVM models to separate glass samples originating from car or building windows and those from bulbs or headlamps. 384 Classification accuracies of 90 % or greater were achieved across ten training and validation sets using both methods, though it was recognised that classifications based solely on elemental composition should be interpreted with caution.</p>
        <p>Later work by Teklemariam and Gotera attempted to distinguish glass from assorted food containers with other glass types such as that used in labware, electronic displays and lighting. 382 HCA found that most food container glass exhibited different elemental profiles to other types of glass based on their LIBS spectra. Some significant within-glass differences were also detected between container glass samples taken from different food brands. However, no supervised analysis was carried out to determine whether these sources could be reliably distinguished, or if unknown glass fragments could be confidently assigned to a given source type.Later work by Teklemariam and Gotera attempted to distinguish glass from assorted food containers with other glass types such as that used in labware, electronic displays and lighting. 382 HCA found that most food container glass exhibited different elemental profiles to other types of glass based on their LIBS spectra. Some significant within-glass differences were also detected between container glass samples taken from different food brands. However, no supervised analysis was carried out to determine whether these sources could be reliably distinguished, or if unknown glass fragments could be confidently assigned to a given source type.</p>
        <p>Most recently, Gupta et al. suggested an approach for integrating chemometric analysis into a probabilistic framework. 387 PCA was first used as a dimensionality reduction method, with the scored PCs then used to compute likelihood ratios to compare samples of vehicle float glass. The use of PCA resulted in shorter computation times, and also reduced the occurrence of Type I (false positive) and Type II (false negative) errors. It was suggested that this approach could be readily transferred to other types of evidence yielding multivariate data.Most recently, Gupta et al. suggested an approach for integrating chemometric analysis into a probabilistic framework. 387 PCA was first used as a dimensionality reduction method, with the scored PCs then used to compute likelihood ratios to compare samples of vehicle float glass. The use of PCA resulted in shorter computation times, and also reduced the occurrence of Type I (false positive) and Type II (false negative) errors. It was suggested that this approach could be readily transferred to other types of evidence yielding multivariate data.</p>
        <p>The 2009 National Academy of Sciences report recommended the "raising of standards for scientific examination of all forms of physical evidence". 17 This could be argued as especially important for disciplines such as impression evidence, which currently rely on direct visual comparisons between recovered (questioned) and known reference samples. Subsequently, there has been some interest into the use of pattern recognition for impression analysis.The 2009 National Academy of Sciences report recommended the "raising of standards for scientific examination of all forms of physical evidence". 17 This could be argued as especially important for disciplines such as impression evidence, which currently rely on direct visual comparisons between recovered (questioned) and known reference samples. Subsequently, there has been some interest into the use of pattern recognition for impression analysis.</p>
        <p>Petraco et al. reported the use of chemometrics to evaluate the 'uniqueness' of shoe impressions related to accidental mark or wear patterns. 388 Partial impressions left by five shoe pairs of the same brand and style, worn by a single person over 30-day periods were converted into feature vectors based on the number and location of any accidental marks. The vectors of 116 impressions were then subjected to PCA and DA to assign each impression to the corresponding shoe pair, resulting in cross-validation accuracies of 77 % to 100 %.Petraco et al. reported the use of chemometrics to evaluate the 'uniqueness' of shoe impressions related to accidental mark or wear patterns. 388 Partial impressions left by five shoe pairs of the same brand and style, worn by a single person over 30-day periods were converted into feature vectors based on the number and location of any accidental marks. The vectors of 116 impressions were then subjected to PCA and DA to assign each impression to the corresponding shoe pair, resulting in cross-validation accuracies of 77 % to 100 %.</p>
        <p>A similar methodology was later applied to the statistical discrimination of toolmark impressions. 389 An image processing program was used to convert the striation marks left by nine different screwdrivers into binary feature vectors, with PLS-DA and PCA-SVM then employed to match each screwdriver to its corresponding impression. The classification performances of each model were assessed through cross-, leave-one-out and bootstrap validation, yielding classification accuracies of 97 % or greater with both classifiers.A similar methodology was later applied to the statistical discrimination of toolmark impressions. 389 An image processing program was used to convert the striation marks left by nine different screwdrivers into binary feature vectors, with PLS-DA and PCA-SVM then employed to match each screwdriver to its corresponding impression. The classification performances of each model were assessed through cross-, leave-one-out and bootstrap validation, yielding classification accuracies of 97 % or greater with both classifiers.</p>
        <p>As evident from the examples highlighted above, chemometric methods show great potential for analytical method development, as well as obtaining, decoding and assessing analytical data gained from forensic analyses. However, there are several considerations to be made when applying these approaches, or when evaluating studies presented in the literature.As evident from the examples highlighted above, chemometric methods show great potential for analytical method development, as well as obtaining, decoding and assessing analytical data gained from forensic analyses. However, there are several considerations to be made when applying these approaches, or when evaluating studies presented in the literature.</p>
        <p>The first of these considerations is whether the models have been appropriately validated.The first of these considerations is whether the models have been appropriately validated.</p>
        <p>High classification or prediction accuracies do not necessarily imply a reliable model, particularly when dealing with limited sample sizes that may not represent the entire population. As discussed in Section 2.2, the use of appropriate test sets is essential to model validation. These sets should ideally be independent of the training data to avoid overestimating the model's capabilities, and of a sufficient size to allow a reasonable measure of likely error rates. Where possible, testing should deliberately include atypical or challenging samples to provide a more rigorous test of model performance.High classification or prediction accuracies do not necessarily imply a reliable model, particularly when dealing with limited sample sizes that may not represent the entire population. As discussed in Section 2.2, the use of appropriate test sets is essential to model validation. These sets should ideally be independent of the training data to avoid overestimating the model's capabilities, and of a sufficient size to allow a reasonable measure of likely error rates. Where possible, testing should deliberately include atypical or challenging samples to provide a more rigorous test of model performance.</p>
        <p>Similarly, the analysis of replicates is essential to evaluating constructed models and defining levels of acceptable variation. To draw conclusions without any consideration of potential sample variance fails to establish proper limits and measures of performance considering the impact of sources of variability. The ideal form of replication would be to conduct analysis of independent samples treated in the same manner, rather than simply repeating measures of a single sample. When taking replicates from a bulk material, careful consideration must be given to the sampling approach. For example, in analysing polyethylene cling films, Telford et al. recommended a stratified random sampling approach to ensure adequate representation across each roll. 35 In the case of mass manufactured products, it may also be appropriate to obtain replicates across separate lot numbers to consider batch variation.Similarly, the analysis of replicates is essential to evaluating constructed models and defining levels of acceptable variation. To draw conclusions without any consideration of potential sample variance fails to establish proper limits and measures of performance considering the impact of sources of variability. The ideal form of replication would be to conduct analysis of independent samples treated in the same manner, rather than simply repeating measures of a single sample. When taking replicates from a bulk material, careful consideration must be given to the sampling approach. For example, in analysing polyethylene cling films, Telford et al. recommended a stratified random sampling approach to ensure adequate representation across each roll. 35 In the case of mass manufactured products, it may also be appropriate to obtain replicates across separate lot numbers to consider batch variation.</p>
        <p>Another key consideration is the definition of what constitutes a 'good' or 'reliable' resultwhether it be the classification of a predictive model, estimation from a regression model, or optimal parameters given by an experimental design. It may be tempting to simply accept the immediate results shown in the model output, particularly where they seem to indicate a 'successful' outcome. However, the examples shown in this tutorial have demonstrated that more careful inspection of the underlying metrics may reveal important information.Another key consideration is the definition of what constitutes a 'good' or 'reliable' resultwhether it be the classification of a predictive model, estimation from a regression model, or optimal parameters given by an experimental design. It may be tempting to simply accept the immediate results shown in the model output, particularly where they seem to indicate a 'successful' outcome. However, the examples shown in this tutorial have demonstrated that more careful inspection of the underlying metrics may reveal important information.</p>
        <p>The importance of these factors can be demonstrated by examining work by Huang and Beauchemin, who described the use of PCA and LDA to determine ancestry (referred to in the study as ethnicity) based on multi-elemental analysis of hair. 390 Hair from thirteen donors of different ancestral backgrounds (broadly categorised as Caucasian, East Asian or South Asian) were analysed using solid sampling electrothermal vaporisation (SS-ETV) ICP-OES. However, the study design had a number of flaws, especially concerning the sampling and chemometric approach. Hair samples were collected and provided by the donors themselves, thus it was unknown whether these samples were fully representative of each individual. Only a single analysis was conducted on hair from each donor, making it difficult to assess the level of within-to between-group variation. Additionally, no external test set was used for validation of the LDA model.The importance of these factors can be demonstrated by examining work by Huang and Beauchemin, who described the use of PCA and LDA to determine ancestry (referred to in the study as ethnicity) based on multi-elemental analysis of hair. 390 Hair from thirteen donors of different ancestral backgrounds (broadly categorised as Caucasian, East Asian or South Asian) were analysed using solid sampling electrothermal vaporisation (SS-ETV) ICP-OES. However, the study design had a number of flaws, especially concerning the sampling and chemometric approach. Hair samples were collected and provided by the donors themselves, thus it was unknown whether these samples were fully representative of each individual. Only a single analysis was conducted on hair from each donor, making it difficult to assess the level of within-to between-group variation. Additionally, no external test set was used for validation of the LDA model.</p>
        <p>Although LDA gave 100 % discrimination accuracy, closer examination shows that the conclusion of reliable discrimination was not supported by the data (Figure 14a). For example, whilst three of the four Caucasian hair samples were clustered together in PCA, this cluster was more closely related to samples from East Asian and South Asian donors than the remaining Caucasian sample, which was nearest the majority of the East-Asian samples.Although LDA gave 100 % discrimination accuracy, closer examination shows that the conclusion of reliable discrimination was not supported by the data (Figure 14a). For example, whilst three of the four Caucasian hair samples were clustered together in PCA, this cluster was more closely related to samples from East Asian and South Asian donors than the remaining Caucasian sample, which was nearest the majority of the East-Asian samples.</p>
        <p>Similarly, one East-Asian sample nearest the South-Asian sample cluster. Testing the LDA model using a separate validation set would likely have given a greatly reduced predictive accuracy, revealing the lack of reliable separation. It is possible that the 'outlying' samples in the scores plot were the result of anomalous measurements, and that additional replicates would have been more closely associated with the expected clusters (theorised in Figure 14b). This could have supported the conclusion that discrimination based on ancestry could be achieved, albeit at a lower predictive accuracy.Similarly, one East-Asian sample nearest the South-Asian sample cluster. Testing the LDA model using a separate validation set would likely have given a greatly reduced predictive accuracy, revealing the lack of reliable separation. It is possible that the 'outlying' samples in the scores plot were the result of anomalous measurements, and that additional replicates would have been more closely associated with the expected clusters (theorised in Figure 14b). This could have supported the conclusion that discrimination based on ancestry could be achieved, albeit at a lower predictive accuracy.</p>
        <p>However in Figure 14c, further theorised replicates revealed even greater overlap between categories, leading to the conclusion that no such discrimination could be achieved. Sampling, replication and validation thus have significant effects on chemometric modelling. This reinforces that the reliability of any conclusions drawn are inherently dependent on the initial sampling and study design.However in Figure 14c, further theorised replicates revealed even greater overlap between categories, leading to the conclusion that no such discrimination could be achieved. Sampling, replication and validation thus have significant effects on chemometric modelling. This reinforces that the reliability of any conclusions drawn are inherently dependent on the initial sampling and study design.</p>
        <p>It must also be recognised that there are a variety of 'real' factors that may affect chemometric models. Samples encountered in casework may be subjected to a variety of ageing or weathering conditions prior to collection, which may affect chemometric classifications against new samples under controlled conditions. Likewise, models based on standard materials may not be robust to those that contain analytes contained within (or deposited on) a complex matrix. Finally, one must consider the persistence of different sample components under various conditions, which may again affect multivariate profiling. These factors are significant from an interpretative standpoint when determining whether recovered traces are consistent with a posited sequence of events.It must also be recognised that there are a variety of 'real' factors that may affect chemometric models. Samples encountered in casework may be subjected to a variety of ageing or weathering conditions prior to collection, which may affect chemometric classifications against new samples under controlled conditions. Likewise, models based on standard materials may not be robust to those that contain analytes contained within (or deposited on) a complex matrix. Finally, one must consider the persistence of different sample components under various conditions, which may again affect multivariate profiling. These factors are significant from an interpretative standpoint when determining whether recovered traces are consistent with a posited sequence of events.</p>
        <p>As well as the considerations above, integrating chemometric routines into existing forensic workflows must be planned and purposeful. Questions that must be asked include "what are the standards for interpreting the outputs?", "how can the results be presented in a way that is understood and accepted by the court?", and most importantly, "how does this add value to what is already being done?". Although statistical techniques may reduce error or bias, it is dangerous to assume that they are infallible, or even necessarily more accurate than human decision-making. The emphasis should be on the use of chemometrics to support, not replace, current analytical procedures. This was discussed by Bovens et al., who described the various steps in a forensic workflow to show where chemometric tools could fit into the process, but emphasised the need for "interconnection and coordination" in doing so. 391 The authors also introduce the 'Steps Towards a European Forensic Science Area' project coordinated by the European Network of Forensic Science Institutes; a sub-project of which is aimed at constructing best practice guidelines on how to apply chemometrics in forensic investigations, explained using specific application examples.As well as the considerations above, integrating chemometric routines into existing forensic workflows must be planned and purposeful. Questions that must be asked include "what are the standards for interpreting the outputs?", "how can the results be presented in a way that is understood and accepted by the court?", and most importantly, "how does this add value to what is already being done?". Although statistical techniques may reduce error or bias, it is dangerous to assume that they are infallible, or even necessarily more accurate than human decision-making. The emphasis should be on the use of chemometrics to support, not replace, current analytical procedures. This was discussed by Bovens et al., who described the various steps in a forensic workflow to show where chemometric tools could fit into the process, but emphasised the need for "interconnection and coordination" in doing so. 391 The authors also introduce the 'Steps Towards a European Forensic Science Area' project coordinated by the European Network of Forensic Science Institutes; a sub-project of which is aimed at constructing best practice guidelines on how to apply chemometrics in forensic investigations, explained using specific application examples.</p>
        <p>One such example by Salonen et al. described three scenarios of illicit drugs casework applying chemometric methods. 392 In one instance, Euclidean distances were calculated from physical measurements to determine whether a number of questioned tablets had been stamped with a seized tableting machine. The similarity threshold was set to yield a 5 % false positive and 9 % false negative rate, and it was noted that such decisions must be made in consultation with those requesting the analysis to ensure that it meets the required standards. It was concluded that two questioned tablets shared the same physical measurements to those from the seized tablet machine (all tablets contained MDMA), but were of a different colour indicating different production batches. In this scenario, information derived from chemometrics was combined with standard visual comparisons and chemical profiling to establish possible linkages between drug seizures.One such example by Salonen et al. described three scenarios of illicit drugs casework applying chemometric methods. 392 In one instance, Euclidean distances were calculated from physical measurements to determine whether a number of questioned tablets had been stamped with a seized tableting machine. The similarity threshold was set to yield a 5 % false positive and 9 % false negative rate, and it was noted that such decisions must be made in consultation with those requesting the analysis to ensure that it meets the required standards. It was concluded that two questioned tablets shared the same physical measurements to those from the seized tablet machine (all tablets contained MDMA), but were of a different colour indicating different production batches. In this scenario, information derived from chemometrics was combined with standard visual comparisons and chemical profiling to establish possible linkages between drug seizures.</p>
        <p>A study by Sauzier et al. compared the use of chemometrics and visual examination for detecting additions and alterations in handwritten documents. 175 Five blind validation samples were analysed using both PCA (based on visible spectra acquired with a video spectral comparator) and microscopic/alternative lighting examinations by document experts. The two approaches performed comparably, with both successfully detecting samples that contained simulated additions or alterations. In one sample, the same number of inks were detected but associated with different ink strokes (with visual examination correctly identifying the distribution). It was suggested that the scores generated from PCA could be used as statistical indicators of sample similarity to support the findings from visual examination, although no proposed threshold for an inclusion or exclusion result was not determined.A study by Sauzier et al. compared the use of chemometrics and visual examination for detecting additions and alterations in handwritten documents. 175 Five blind validation samples were analysed using both PCA (based on visible spectra acquired with a video spectral comparator) and microscopic/alternative lighting examinations by document experts. The two approaches performed comparably, with both successfully detecting samples that contained simulated additions or alterations. In one sample, the same number of inks were detected but associated with different ink strokes (with visual examination correctly identifying the distribution). It was suggested that the scores generated from PCA could be used as statistical indicators of sample similarity to support the findings from visual examination, although no proposed threshold for an inclusion or exclusion result was not determined.</p>
        <p>Other studies have examined the potential integration of chemometric techniques with probabilistic approaches, such as previously discussed work by Gupta et al. on glass comparisons using PCA with likelihood ratios (Section 3.11). Another study by Martyna et al.Other studies have examined the potential integration of chemometric techniques with probabilistic approaches, such as previously discussed work by Gupta et al. on glass comparisons using PCA with likelihood ratios (Section 3.11). Another study by Martyna et al.</p>
        <p>proposed a hybrid approach combining chemometrics with a Bayesian likelihood ratio framework to report the evidential value of vibrational spectra from automotive polymers. 393 LDA was used to generate distance representations between samples, which were then used to construct naïve LR models for sample comparisons. For each comparison, 100 LR values were generated to express the evidential value of sample similarity. The rate of false positive associations between compared samples varied from ca. 10 -25 % depending on the number of discriminants retained, whilst false negatives were consistently around 5 % for FTIR spectra and 10 % for Raman spectra. Squared Euclidean and correlation-based distance metrics were found to be optimal, yielding ≤ 10 % false positive rates using bivariate or trivariate models. Despite the limited availability of samples, which is often a limiting factor in Bayesian frameworks, the results obtained using this hybrid approach were deemed satisfactory for legal processing.proposed a hybrid approach combining chemometrics with a Bayesian likelihood ratio framework to report the evidential value of vibrational spectra from automotive polymers. 393 LDA was used to generate distance representations between samples, which were then used to construct naïve LR models for sample comparisons. For each comparison, 100 LR values were generated to express the evidential value of sample similarity. The rate of false positive associations between compared samples varied from ca. 10 -25 % depending on the number of discriminants retained, whilst false negatives were consistently around 5 % for FTIR spectra and 10 % for Raman spectra. Squared Euclidean and correlation-based distance metrics were found to be optimal, yielding ≤ 10 % false positive rates using bivariate or trivariate models. Despite the limited availability of samples, which is often a limiting factor in Bayesian frameworks, the results obtained using this hybrid approach were deemed satisfactory for legal processing.</p>
        <p>It is clear that the forensic interest in chemometrics has grown considerably over recent decades. The focus of these studies has begun to evolve from simple pattern recognition to the optimisation of analytical procedures and establishing statistical measures of evidential value. The chemometric methods applied to these problems have also shifted from 'standard' PCA and LDA to an array of alternative techniques including SVM, PLS-DA and ANNs, with the recognition that each may provide specific advantages or disadvantages depending on the question under consideration.It is clear that the forensic interest in chemometrics has grown considerably over recent decades. The focus of these studies has begun to evolve from simple pattern recognition to the optimisation of analytical procedures and establishing statistical measures of evidential value. The chemometric methods applied to these problems have also shifted from 'standard' PCA and LDA to an array of alternative techniques including SVM, PLS-DA and ANNs, with the recognition that each may provide specific advantages or disadvantages depending on the question under consideration.</p>
        <p>The increasing volume of literature in this field suggests that these methods are not all that far away from being introduced to operational forensic laboratories. However, this fundamental step first requires a further body of work probing the capabilities and limitations of these approaches. In particular, methods must be validated against known 'ground-truth' samples to establish potential error rates, documented as to provide transparency to the conclusions drawn, and established to meet the scientific standards accepted by a court.The increasing volume of literature in this field suggests that these methods are not all that far away from being introduced to operational forensic laboratories. However, this fundamental step first requires a further body of work probing the capabilities and limitations of these approaches. In particular, methods must be validated against known 'ground-truth' samples to establish potential error rates, documented as to provide transparency to the conclusions drawn, and established to meet the scientific standards accepted by a court.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:03+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Contemporary Artificial Intelligence technologies allow for the employment of Computer Vision to discern good crops from bad, providing a step in the pipeline of selecting healthy fruit from undesirable fruit, such as those which are mouldy or damaged. State-of-the-art works in the field report high accuracy results on small datasets (&lt;1000 images), which are not representative of the population regarding real-world usage. The goals of this study are to further enable real-world usage by improving generalisation with data augmentation as well as to reduce overfitting and energy usage through model pruning. In this work, we suggest a machine learning pipeline that combines the ideas of fine-tuning, transfer learning, and generative model-based training data augmentation towards improving fruit quality image classification. A linear network topology search is performed to tune a VGG16 lemon quality classification model using a publicly-available dataset of 2690 images. We find that appending a 4096 neuron fully connected layer to the convolutional layers leads to an image classification accuracy of 83.77%. We then train a Conditional Generative Adversarial Network on the training data for 2000 epochs, and it learns to generate relatively realistic images. Grad-CAM analysis of the model trained on real photographs shows that the synthetic images can exhibit classifiable characteristics such as shape, mould, and gangrene. A higher image classification accuracy of 88.75% is then attained by augmenting the training with synthetic images, arguing that Conditional Generative Adversarial Networks have the ability to produce new data to alleviate issues of data scarcity. Finally, model pruning is performed via polynomial decay, where we find that the Conditional GAN-augmented classification network can retain 81.16% classification accuracy when compressed to 50% of its original size.Contemporary Artificial Intelligence technologies allow for the employment of Computer Vision to discern good crops from bad, providing a step in the pipeline of selecting healthy fruit from undesirable fruit, such as those which are mouldy or damaged. State-of-the-art works in the field report high accuracy results on small datasets (&lt;1000 images), which are not representative of the population regarding real-world usage. The goals of this study are to further enable real-world usage by improving generalisation with data augmentation as well as to reduce overfitting and energy usage through model pruning. In this work, we suggest a machine learning pipeline that combines the ideas of fine-tuning, transfer learning, and generative model-based training data augmentation towards improving fruit quality image classification. A linear network topology search is performed to tune a VGG16 lemon quality classification model using a publicly-available dataset of 2690 images. We find that appending a 4096 neuron fully connected layer to the convolutional layers leads to an image classification accuracy of 83.77%. We then train a Conditional Generative Adversarial Network on the training data for 2000 epochs, and it learns to generate relatively realistic images. Grad-CAM analysis of the model trained on real photographs shows that the synthetic images can exhibit classifiable characteristics such as shape, mould, and gangrene. A higher image classification accuracy of 88.75% is then attained by augmenting the training with synthetic images, arguing that Conditional Generative Adversarial Networks have the ability to produce new data to alleviate issues of data scarcity. Finally, model pruning is performed via polynomial decay, where we find that the Conditional GAN-augmented classification network can retain 81.16% classification accuracy when compressed to 50% of its original size.</p>
        <p>Recognition of fruit quality is important in smart agriculture to increase production efficiency. Contemporary Artificial Intelligence technologies allow for the employment of Computer Vision to discern good crops from bad, providing a step in the pipeline of selecting healthy fruit from undesirable fruit, such as those which are mouldy or damaged.Recognition of fruit quality is important in smart agriculture to increase production efficiency. Contemporary Artificial Intelligence technologies allow for the employment of Computer Vision to discern good crops from bad, providing a step in the pipeline of selecting healthy fruit from undesirable fruit, such as those which are mouldy or damaged.</p>
        <p>Even during modern times, collecting a dataset that generally represents a species of fruit poses difficulties. For example, the widely used Fruits 360 dataset [1] contains 2134 images of apples belonging to one of thirteen cultivars. According to the United Nations, 87.2 million tonnes of apples were farmed globally in 2019 alone [2]. In terms of smart agriculture, this highlights the issue of data scarcity. On the problem of crop quality recognition, much data is required to generalise to a population and thus become apt for real-world use. Considering the yield of fruit globally compared to practical data collection, bridging this gap manually i.e., collecting more data, is simply an impossible task.Even during modern times, collecting a dataset that generally represents a species of fruit poses difficulties. For example, the widely used Fruits 360 dataset [1] contains 2134 images of apples belonging to one of thirteen cultivars. According to the United Nations, 87.2 million tonnes of apples were farmed globally in 2019 alone [2]. In terms of smart agriculture, this highlights the issue of data scarcity. On the problem of crop quality recognition, much data is required to generalise to a population and thus become apt for real-world use. Considering the yield of fruit globally compared to practical data collection, bridging this gap manually i.e., collecting more data, is simply an impossible task.</p>
        <p>Computer vision has long been noted as an important approach for fruit processing [3,4,5]. Further work also explores varying techniques for computer vision in smart agriculture such as transfer learning from pre-trained weights [6,7,8], and current research has started to explore the approach of data augmentation in improving such problems [9,10]. In this work, we focus on exploring a solution to this problem for lemon harvesting. Recording data for both lemons and limes, the United Nations noted that 20 million tonnes were harvested in 2019 [2]. According to the CIA World Factbook [11], lemon and lime fruit exports comprised over $3.3 billion USD of international exports in 2019. The largest exporters were Spain ($828 million USD), Mexico ($523 million USD), and the Netherlands ($339 million USD). Although the top 15 countries exported 93.5% of all lemons and limes in 2019, many countries are expanding their efforts; for example, Belize increased lemon and lime exports by 11,100% from 2018 to 2019, followed by Timor-Leste and Georgia which increased exports by 4,200% and 2,115%, respectively. With this information in mind, the ability to autonomously select and reject lemon fruit based on health, e.g., sorting out those which have developed mould or gangrene, would allow for further increases in an already growing market by increasing production efficiency.Computer vision has long been noted as an important approach for fruit processing [3,4,5]. Further work also explores varying techniques for computer vision in smart agriculture such as transfer learning from pre-trained weights [6,7,8], and current research has started to explore the approach of data augmentation in improving such problems [9,10]. In this work, we focus on exploring a solution to this problem for lemon harvesting. Recording data for both lemons and limes, the United Nations noted that 20 million tonnes were harvested in 2019 [2]. According to the CIA World Factbook [11], lemon and lime fruit exports comprised over $3.3 billion USD of international exports in 2019. The largest exporters were Spain ($828 million USD), Mexico ($523 million USD), and the Netherlands ($339 million USD). Although the top 15 countries exported 93.5% of all lemons and limes in 2019, many countries are expanding their efforts; for example, Belize increased lemon and lime exports by 11,100% from 2018 to 2019, followed by Timor-Leste and Georgia which increased exports by 4,200% and 2,115%, respectively. With this information in mind, the ability to autonomously select and reject lemon fruit based on health, e.g., sorting out those which have developed mould or gangrene, would allow for further increases in an already growing market by increasing production efficiency.</p>
        <p>In this article, we explore a Conditional Generative Adversarial Network-based solution to data scarcity through training data augmentation. The main scientific contributions we present in chronological order are as follows:In this article, we explore a Conditional Generative Adversarial Network-based solution to data scarcity through training data augmentation. The main scientific contributions we present in chronological order are as follows:</p>
        <p>• Exploration of Convolutional Neural Network (CNN) topologies for fruit quality recognition.• Exploration of Convolutional Neural Network (CNN) topologies for fruit quality recognition.</p>
        <p>• Implementation of a Conditional Generative Adversarial Network (Conditional GAN) for the generation of synthetic healthy and unhealthy fruit images. The trained synthetic data generation model is made available for future work1 . • Augmentation of the original dataset with synthetic images to improve the CNN performance.• Implementation of a Conditional Generative Adversarial Network (Conditional GAN) for the generation of synthetic healthy and unhealthy fruit images. The trained synthetic data generation model is made available for future work1 . • Augmentation of the original dataset with synthetic images to improve the CNN performance.</p>
        <p>• Exploration of features within synthetic images shows that the Conditional GAN learns to generate healthy synthetic fruit images with no defects, as well as unhealthy synthetic fruit images with defects such as mould and gangrene. • Model pruning shows that a Conditional GAN-augmented classification network can retain 81.16% classification accuracy when compressed to 50% of its original size.• Exploration of features within synthetic images shows that the Conditional GAN learns to generate healthy synthetic fruit images with no defects, as well as unhealthy synthetic fruit images with defects such as mould and gangrene. • Model pruning shows that a Conditional GAN-augmented classification network can retain 81.16% classification accuracy when compressed to 50% of its original size.</p>
        <p>2 Background and Related Work2 Background and Related Work</p>
        <p>Fruit Quality Recognition is a technique where a fruit can be scored or classified autonomously by an algorithm given input features such as photographs. As previously mentioned, the ability to perform this task autonomously (and non-invasively [12]) allows for an increase of production efficiency since sorting can be performed by machines endowed with such an algorithm.Fruit Quality Recognition is a technique where a fruit can be scored or classified autonomously by an algorithm given input features such as photographs. As previously mentioned, the ability to perform this task autonomously (and non-invasively [12]) allows for an increase of production efficiency since sorting can be performed by machines endowed with such an algorithm.</p>
        <p>Reduction in cost of such a system is of particular interest in the field given that several Lower Economically Developed Countries (LEDCs) are expanding the production and export of lemon fruit [11]. Solutions such as electronic noses can cost up to 100,000$ USD [13], whereas a camera and computer are a fraction of the cost, arguing that image recognition is a more viable option when cost is an issue. It is worth noting though, that low-cost electronic noses are currently an expanding line of research within the Sensors and Internet of Things (IoT) fields [14]. Electronic noses are indeed strongly performing solutions to fruit quality recognition as shown by [15], [16], and [17].Reduction in cost of such a system is of particular interest in the field given that several Lower Economically Developed Countries (LEDCs) are expanding the production and export of lemon fruit [11]. Solutions such as electronic noses can cost up to 100,000$ USD [13], whereas a camera and computer are a fraction of the cost, arguing that image recognition is a more viable option when cost is an issue. It is worth noting though, that low-cost electronic noses are currently an expanding line of research within the Sensors and Internet of Things (IoT) fields [14]. Electronic noses are indeed strongly performing solutions to fruit quality recognition as shown by [15], [16], and [17].</p>
        <p>Deep learning approaches to fruit classification are abundant, as shown by multiple literature reviews on the subject [18,19,20,21,22]. In comparison, the application of deep learning to the more fine-grained problem of quality classification are relatively rare. Bhargava and Bansal show that the field of automated fruit quality recognition is rapidly growing, in part due to the availability of newer technologies [23].Deep learning approaches to fruit classification are abundant, as shown by multiple literature reviews on the subject [18,19,20,21,22]. In comparison, the application of deep learning to the more fine-grained problem of quality classification are relatively rare. Bhargava and Bansal show that the field of automated fruit quality recognition is rapidly growing, in part due to the availability of newer technologies [23].</p>
        <p>Given that one can discern the quality of a fruit based on observation, visual features are often noted as of importance when it comes to fruit quality classification. Yamamoto et al. [24] suggested that Linear Discriminant Analysis (LDA) of colour, shape, and size enabled the formation of a distance matrix which could be used to classify both the cultivar and quality of strawberries. Usage of a single LDA led to a classification accuracy of 42% whereas combining the three analyses caused accuracy to rise to 68%. Capizzi et al. followed a similar technique through texture and gray-features with a Radial Basis Probabilistic Neural Network, which scored around 97.25% on a limited set of images of orange fruits [25]. Azizah et al. [26] provided a solution to defect classification of the mangosteen fruit, attaining a mean 97.5% 4-fold classification accuracy, albeit with a limited dataset. This pattern of data scarcity continues as would be expected in the field; in 2020, Fan, et al. [27] found that CNNs could classify apple defects with around 96.5% accuracy after processing 300 fruit images (150 per class). This study also notes the efficiency of deep learning algorithms post-training, the algorithm was capable of processing 5 fruit images per second (0.2 seconds each).Given that one can discern the quality of a fruit based on observation, visual features are often noted as of importance when it comes to fruit quality classification. Yamamoto et al. [24] suggested that Linear Discriminant Analysis (LDA) of colour, shape, and size enabled the formation of a distance matrix which could be used to classify both the cultivar and quality of strawberries. Usage of a single LDA led to a classification accuracy of 42% whereas combining the three analyses caused accuracy to rise to 68%. Capizzi et al. followed a similar technique through texture and gray-features with a Radial Basis Probabilistic Neural Network, which scored around 97.25% on a limited set of images of orange fruits [25]. Azizah et al. [26] provided a solution to defect classification of the mangosteen fruit, attaining a mean 97.5% 4-fold classification accuracy, albeit with a limited dataset. This pattern of data scarcity continues as would be expected in the field; in 2020, Fan, et al. [27] found that CNNs could classify apple defects with around 96.5% accuracy after processing 300 fruit images (150 per class). This study also notes the efficiency of deep learning algorithms post-training, the algorithm was capable of processing 5 fruit images per second (0.2 seconds each).</p>
        <p>In this work, we take inspiration from Osako et al.'s approaches to cultivar discrimination of lychee fruit [28]. The study showed the success of fruit image classification (albeit for a different task of cultivar recognition) when fine-tune transfer learning with the VGG16 CNN [29], and predictions were analysed super-imposed upon the images via 
            <rs type="software">Grad-CAM</rs> [30] in order to explain useful features for discrimination. We follow a similar approach in this work (applied to a new problem) in terms of fine-tuning of VGG16 and analysis with 
            <rs type="software">Grad-CAM</rs>, and go a step further in improving classification through data augmentation with a Conditional Generative Adversarial Network to self-regularise the network by creating new, synthetic fruit images.
        </p>
        <p>Regarding the global fruit yield statistics versus dataset size examples within the introduction, data scarcity in machine learning is the reliance of models on exhaustive labelling, providing a limitation to their real-world use [31]. Given that the use of a model is to aim towards generalisation of a population, a lack of data can lead to a situation wherein training accuracy scores are high and yet deployment to industry would lead to failure. As noted in the introduction, gathering enough data of cultivated fruit is impractical. Without enough data to properly represent the population, models will be prone to overfitting. Given this, other methods are required to prevent overfitting and encourage generalisation towards the real-world use of a machine learning model outside the realm of simply collecting more data.Regarding the global fruit yield statistics versus dataset size examples within the introduction, data scarcity in machine learning is the reliance of models on exhaustive labelling, providing a limitation to their real-world use [31]. Given that the use of a model is to aim towards generalisation of a population, a lack of data can lead to a situation wherein training accuracy scores are high and yet deployment to industry would lead to failure. As noted in the introduction, gathering enough data of cultivated fruit is impractical. Without enough data to properly represent the population, models will be prone to overfitting. Given this, other methods are required to prevent overfitting and encourage generalisation towards the real-world use of a machine learning model outside the realm of simply collecting more data.</p>
        <p>Data augmentation is the process of creating new training data by either slightly modifying the data at hand or generating new, synthetic data [32]. An augmented dataset thus provides more training examples for a given task.Data augmentation is the process of creating new training data by either slightly modifying the data at hand or generating new, synthetic data [32]. An augmented dataset thus provides more training examples for a given task.</p>
        <p>Image recognition tasks for Convolutional Neural Network image classification are affected by data scarcity due to their data requirements [33,34], where many generative models have been recommended to alleviate such issues [35,36]. Generative models have also been noted to positively impact biological signal classification [37,38], semantic Imageto-Image Translation [39], speech processing [40,41], and Human Activity Recognition [42,43] among many others.Image recognition tasks for Convolutional Neural Network image classification are affected by data scarcity due to their data requirements [33,34], where many generative models have been recommended to alleviate such issues [35,36]. Generative models have also been noted to positively impact biological signal classification [37,38], semantic Imageto-Image Translation [39], speech processing [40,41], and Human Activity Recognition [42,43] among many others.</p>
        <p>In this work, we use a Conditional GAN for data augmentation, which are described in the following section. This is based on the literature wherein GANs and Conditional GANs have been noted to perform particularly well in image classification [44,45,46,47]. We note specific inspiration from Fu et al. [48], where Conditional GANs have been noted to perform well on fine-grained images such as classification of bird and dog breeds (rather than classification of a whole species). This bares similarity to our problem, where finer details on generally similar images dictate which class they belong to.In this work, we use a Conditional GAN for data augmentation, which are described in the following section. This is based on the literature wherein GANs and Conditional GANs have been noted to perform particularly well in image classification [44,45,46,47]. We note specific inspiration from Fu et al. [48], where Conditional GANs have been noted to perform well on fine-grained images such as classification of bird and dog breeds (rather than classification of a whole species). This bares similarity to our problem, where finer details on generally similar images dictate which class they belong to.</p>
        <p>The Generative Adversarial Network (GAN) was first introduced in 2014 [49]. The idea behind the GAN is to have two neural networks compete in a zero-sum game ergo adversarial, i.e., the loss of one network is directly beneficial to the other and vice versa. To give an example of image generation, as this work performs, there are two networks; a generator network which creates images, and a discriminator network which classifies the inputs as either real or fake.The Generative Adversarial Network (GAN) was first introduced in 2014 [49]. The idea behind the GAN is to have two neural networks compete in a zero-sum game ergo adversarial, i.e., the loss of one network is directly beneficial to the other and vice versa. To give an example of image generation, as this work performs, there are two networks; a generator network which creates images, and a discriminator network which classifies the inputs as either real or fake.</p>
        <p>As with most deep learning approaches, the gradients of each network are updated after each training batch with a stochastic gradient algorithm. The output of the generator network feeds directly into the discriminator network and thus training of the two networks is automated via their competition. In terms of categorical cross-entropy, a score can be calculated as follows:As with most deep learning approaches, the gradients of each network are updated after each training batch with a stochastic gradient algorithm. The output of the generator network feeds directly into the discriminator network and thus training of the two networks is automated via their competition. In terms of categorical cross-entropy, a score can be calculated as follows:</p>
        <p>where the first part of the equation (E x [log(D(x))]) is the recognition of real images and the second partwhere the first part of the equation (E x [log(D(x))]) is the recognition of real images and the second part</p>
        <p>) is the recognition of fake images. E x and E z are the expected values over all real and fake data, respectively; e.g., x is a real input from the dataset and z may begin as a random noise input to a generator. The function D(x) is the probability that a given data is real and is thus therefore being reversed to discern fake images. Note that D(x) is replaced by G(z) in the second part of the equation, this is due to input to the discriminator being Generator G's output when presented with random input vector z. This is known as a minimax loss, since the generator's aim is to maximise Equation 1 while the discriminator aims to minimise it. A Conditional Generative Adversarial Network (CGAN or Conditional GAN) [50] is an extension of the above technology, but with a given class label. That is, the generator now aims to learn to generate images belonging to one of n classes, in this work this is a binary label of "healthy" and "unhealthy". Equation 1 can be extended as follows:) is the recognition of fake images. E x and E z are the expected values over all real and fake data, respectively; e.g., x is a real input from the dataset and z may begin as a random noise input to a generator. The function D(x) is the probability that a given data is real and is thus therefore being reversed to discern fake images. Note that D(x) is replaced by G(z) in the second part of the equation, this is due to input to the discriminator being Generator G's output when presented with random input vector z. This is known as a minimax loss, since the generator's aim is to maximise Equation 1 while the discriminator aims to minimise it. A Conditional Generative Adversarial Network (CGAN or Conditional GAN) [50] is an extension of the above technology, but with a given class label. That is, the generator now aims to learn to generate images belonging to one of n classes, in this work this is a binary label of "healthy" and "unhealthy". Equation 1 can be extended as follows:</p>
        <p>where data objects x and D(z) are given class label y. Therefore, D(x|y) is the discriminator's probability that x is real given class label y, and G(z|y) is the output of the generator with random vector z given class label y. This minute difference in topology from a GAN, as can be observed in Figure 1, allows for the generation of objects belonging to multiple classes. If the dataset in this work was presented to a vanilla GAN, the network would learn to generate fake fruit images by learning from real fruit, thus two networks would then be needed for the generation of either class. Said networks would have to train independently of one another. By using a Conditional GAN, we can specify to the network whether we want it to generate healthy or unhealthy fruit by learning not only to generate them in the general sense, but also learning from the significance of a class label.where data objects x and D(z) are given class label y. Therefore, D(x|y) is the discriminator's probability that x is real given class label y, and G(z|y) is the output of the generator with random vector z given class label y. This minute difference in topology from a GAN, as can be observed in Figure 1, allows for the generation of objects belonging to multiple classes. If the dataset in this work was presented to a vanilla GAN, the network would learn to generate fake fruit images by learning from real fruit, thus two networks would then be needed for the generation of either class. Said networks would have to train independently of one another. By using a Conditional GAN, we can specify to the network whether we want it to generate healthy or unhealthy fruit by learning not only to generate them in the general sense, but also learning from the significance of a class label.</p>
        <p>A general overview of the proposed approach can be observed in Figure 2, where the training data is augmented with a Conditional GAN approach. In this section, we describe the method for each step of the experiments performed.A general overview of the proposed approach can be observed in Figure 2, where the training data is augmented with a Conditional GAN approach. In this section, we describe the method for each step of the experiments performed.</p>
        <p>Initially, an open source dataset of lemon images were acquired from SoftwareMill [51] 2 . The dataset contains 2690 images of lemons at a resolution of 1056×1056 pixels and are annotated in COCO format. Given that each COCO annotation describes one class, i.e., a fruit that exhibits both mould and gangrene will have two individual entries, we sort through the dataset to apply a single binary class label of "healthy" or "unhealthy" to each of the fruit images.Initially, an open source dataset of lemon images were acquired from SoftwareMill [51] 2 . The dataset contains 2690 images of lemons at a resolution of 1056×1056 pixels and are annotated in COCO format. Given that each COCO annotation describes one class, i.e., a fruit that exhibits both mould and gangrene will have two individual entries, we sort through the dataset to apply a single binary class label of "healthy" or "unhealthy" to each of the fruit images.</p>
        <p>Given the computational complexity of the algorithms involved, the images are then resized to 256×256 pixels; this resolution still allows for the visualisation of undesirable features while reducing the total number of RGB pixel values from 3,345,408 (1056×1056×3) to 196,608 (256×256×3). This reduction to 5% of the original model inputs reduces the amount of memory required for training of all models, since the use of full resolution images is not feasible for consumer-grade hardware. In terms of deployment and real-world usage, robots themselves will have energy restrictions due to the processing cost and profit tradeoff regarding automation of fruit sorting. Thus, this reduction in image size increases the practicality of the approach.Given the computational complexity of the algorithms involved, the images are then resized to 256×256 pixels; this resolution still allows for the visualisation of undesirable features while reducing the total number of RGB pixel values from 3,345,408 (1056×1056×3) to 196,608 (256×256×3). This reduction to 5% of the original model inputs reduces the amount of memory required for training of all models, since the use of full resolution images is not feasible for consumer-grade hardware. In terms of deployment and real-world usage, robots themselves will have energy restrictions due to the processing cost and profit tradeoff regarding automation of fruit sorting. Thus, this reduction in image size increases the practicality of the approach.</p>
        <p>To better discern noise throughout the generative learning process, the black background is replaced with white. Although it would have no effect on the training process of the model, the background is replaced so visual glitchesTo better discern noise throughout the generative learning process, the black background is replaced with white. Although it would have no effect on the training process of the model, the background is replaced so visual glitches</p>
        <p>Generator DiscriminatorGenerator Discriminator</p>
        <p>Class Labels (0,1)Class Labels (0,1)</p>
        <p>Convolutional Neural NetworkConvolutional Neural Network</p>
        <p>Training DataTraining Data</p>
        <p>Figure 2: A general overview of our proposed approach for data augmentation towards fruit quality image classification. can be better discerned through manual observation throughout training. For example, later in Figure 7, several small glitches occur in the eighth generation of outputs that would have been more difficult to observe in the presence of a dark background.Figure 2: A general overview of our proposed approach for data augmentation towards fruit quality image classification. can be better discerned through manual observation throughout training. For example, later in Figure 7, several small glitches occur in the eighth generation of outputs that would have been more difficult to observe in the presence of a dark background.</p>
        <p>Examples of some images in the preprocessed dataset can be found in Figure 3. Healthy lemons are gathered for the healthy class, whereas mouldy, gangrenous, and those with a dark style remaining are gathered to form the unhealthy class.Examples of some images in the preprocessed dataset can be found in Figure 3. Healthy lemons are gathered for the healthy class, whereas mouldy, gangrenous, and those with a dark style remaining are gathered to form the unhealthy class.</p>
        <p>For data augmentation, a Conditional GAN is utilised. The model is selected since it supports the concatenation of the generator and discriminator networks with a second input of the class label. That is, the context of a healthy or unhealthy fruit is specified, and so the model will learn to generate images as belonging to either one of the two classes.For data augmentation, a Conditional GAN is utilised. The model is selected since it supports the concatenation of the generator and discriminator networks with a second input of the class label. That is, the context of a healthy or unhealthy fruit is specified, and so the model will learn to generate images as belonging to either one of the two classes.</p>
        <p>The initial input to the generator is a vector representing a three-channel 8×8 pixel image (8×8×3). By using Convolutional Transpose layers, this is eventually upscaled to a 256×256 RGB image. The discriminator network downsamples twice with convolutional layers of 128 neurons, a kernel size of (3, 3) and a stride of (2, 2). Each layer utilises LeakyReLU activation [52] whereas the output is set as a hyperbolic tangent for scaling, and the 
            <rs type="software">ADAM</rs> optimiser [53] is used to train. Latent space for class label interpretation is of size 100. Hyperparameter selections are based on the findings of the studies in [54].
        </p>
        <p>The Conditional GAN was first initially trained for 500 epochs, manual exploration of the produced synthetic data showed promise, but several severe visual glitches still occurred. Due to this, the training was extended and performed for 2000 epochs in total with a batch size of 64. It was also observed that batch sizes below 64 caused the generator to cease training after around 10 epochs and failed to learn any further.The Conditional GAN was first initially trained for 500 epochs, manual exploration of the produced synthetic data showed promise, but several severe visual glitches still occurred. Due to this, the training was extended and performed for 2000 epochs in total with a batch size of 64. It was also observed that batch sizes below 64 caused the generator to cease training after around 10 epochs and failed to learn any further.</p>
        <p>The image classification network itself utilises the concept of fine-tune transfer learning from a large ImageNet-trained model, VGG16 [29]. A diagram of the Convolutional Neural Network topology we use for the classification of fruit quality images can be observed in Figure 4. The final three ReLu layers and SoftMax predictions have been replaced by a single interpretation layer and a single output neuron with a sigmoid activation function for the optimisation of binary cross-entropy.The image classification network itself utilises the concept of fine-tune transfer learning from a large ImageNet-trained model, VGG16 [29]. A diagram of the Convolutional Neural Network topology we use for the classification of fruit quality images can be observed in Figure 4. The final three ReLu layers and SoftMax predictions have been replaced by a single interpretation layer and a single output neuron with a sigmoid activation function for the optimisation of binary cross-entropy.</p>
        <p>The number of neurons within the interpretation layer is optimised through a linear search of (8, 16, 32, Explainiability in AI is important, especially when such algorithms are considered for real world usage. Algorithms tend to operate in a black-box like nature, for example, the algorithm in this work would take as input an image of a lemon fruit and produce a class label output, corresponding to whether the fruit is healthy or not. With this in mind, regardless of the training accuracy scores attained, further analysis is needed to explain why decisions are made and predictions are given. We analyse several synthetic images through Gradient-weighted Class Activation Mapping (Grad-CAM) [30,55]. Class activation maps are produced by the convolutional neural network trained only on real images when given synthetic data as input. This allows us to confirm that undesirable characteristics are indeed both generated and classified as being important; since a GAN generator's goal is simply to learn to outperform the discriminator, and this may be done through other means i.e. finding methods to trick the classifier.The number of neurons within the interpretation layer is optimised through a linear search of (8, 16, 32, Explainiability in AI is important, especially when such algorithms are considered for real world usage. Algorithms tend to operate in a black-box like nature, for example, the algorithm in this work would take as input an image of a lemon fruit and produce a class label output, corresponding to whether the fruit is healthy or not. With this in mind, regardless of the training accuracy scores attained, further analysis is needed to explain why decisions are made and predictions are given. We analyse several synthetic images through Gradient-weighted Class Activation Mapping (Grad-CAM) [30,55]. Class activation maps are produced by the convolutional neural network trained only on real images when given synthetic data as input. This allows us to confirm that undesirable characteristics are indeed both generated and classified as being important; since a GAN generator's goal is simply to learn to outperform the discriminator, and this may be done through other means i.e. finding methods to trick the classifier.</p>
        <p>Considering that processing is performed on large quantities of fruit and that there may be time and energy restrictions, model pruning is performed on the network to explore the possibility of smaller model sizes, more apt for real-world usage [56,57], through polynomial decay. For each model, 9 pruning experiments were performed with weight sparsity ranging from 0.9 (10% of original size) to 0.1 (90% of original size). Pruning was performed on the whole model for 20 epochs, from a sparsity of 0 (full size) to the given value for the individual experiment (0, 0.1, 0.2, ..., 0.9).Considering that processing is performed on large quantities of fruit and that there may be time and energy restrictions, model pruning is performed on the network to explore the possibility of smaller model sizes, more apt for real-world usage [56,57], through polynomial decay. For each model, 9 pruning experiments were performed with weight sparsity ranging from 0.9 (10% of original size) to 0.1 (90% of original size). Pruning was performed on the whole model for 20 epochs, from a sparsity of 0 (full size) to the given value for the individual experiment (0, 0.1, 0.2, ..., 0.9).</p>
        <p>The models in this work were implemented in the 
            <rs type="software">Keras</rs> library with a TensorFlow backend. Models were trained on an RTX 2080Ti GPU (4352 CUDA cores).
        </p>
        <p>Table 1 shows the exploration of interpretation neurons following the VGG16 ImageNet Convolutional Neural Network. Following the three tests, the lowest observed accuracy was that from 8 interpretation neurons with a mean classification accuracy of 59.4%. The best model was the network with 4096 interpretation neurons, which scored 81.99% classification accuracy, and the best individual run was the second run of the 4096-neuron network which scored 83.77% classification accuracy.Table 1 shows the exploration of interpretation neurons following the VGG16 ImageNet Convolutional Neural Network. Following the three tests, the lowest observed accuracy was that from 8 interpretation neurons with a mean classification accuracy of 59.4%. The best model was the network with 4096 interpretation neurons, which scored 81.99% classification accuracy, and the best individual run was the second run of the 4096-neuron network which scored 83.77% classification accuracy.</p>
        <p>Separated into two graphs for readability purposes, Figures 5 and6 show the observed losses for the generator and discriminator networks of the Conditional GAN, respectively. Albeit with several anomalous spikes, it can be observed that the generator starts at a high loss of 5.5 which drops throughout the first few epochs. The discriminator, as can be expected, starts low given the quality of output by the generator. The generator can be seen to rise steadily for the first At the end of the final epoch, the discriminator losses were 0.013 for the real images and 0.005 for the fake images produced by the generator. The generator loss was 6.648. Indeed, this value of 6.648 is by far not the lowest observed, but it is important to consider the nature of GANs; the losses of the two networks are relative to one another, i.e. it is an adversarial score. To provide an example of this, Figure 7 shows a comparison of the images produced by the generator with the lowest loss (epoch 8 at 1.063) and then the generator at the final epoch. Evidently, the images produced by the final generator are of much higher quality than those output by the generator when it experiences the lowest observed loss. For this reason, the final generator is selected as the synthetic data producing model in this work, although it is suggested in the future to explore the quality at multiple stages. Figure 8 shows 18 real photographs for healthy and unhealthy classed lemons, as well as 18 examples of Conditional GAN generator output for healthy and unhealthy classed lemons. Interestingly, many of the synthetic images seem to be more reminiscent of potatoes than lemons; given the nature of GANs, this is due to the model's generalisation of the dataset which contained lemons photographed at different angles -and so this generalisation of a shape is reflected in the produced images. A more uniform colour can be observed in healthy synthetic lemons, whereas unhealthy lemons are given mould and dark styles, as well as several instances of gangrene. Similarly to the potato-like shape of the outputs, this could be attributed to the generalisation nature of GANs, several patterns have been observed during training, and these patterns are then applied while generating new images. A higher resolution example of synthetic lemon images showing features of mould and gangrene can be observed in Figure 9. The generator has seemingly learnt to cast light and shadows on the fruit as well as undesirable characteristics; both fruits have a texture and colour similar to mould on the surface, and the second fruit seems to have a darker patch towards the top.Separated into two graphs for readability purposes, Figures 5 and6 show the observed losses for the generator and discriminator networks of the Conditional GAN, respectively. Albeit with several anomalous spikes, it can be observed that the generator starts at a high loss of 5.5 which drops throughout the first few epochs. The discriminator, as can be expected, starts low given the quality of output by the generator. The generator can be seen to rise steadily for the first At the end of the final epoch, the discriminator losses were 0.013 for the real images and 0.005 for the fake images produced by the generator. The generator loss was 6.648. Indeed, this value of 6.648 is by far not the lowest observed, but it is important to consider the nature of GANs; the losses of the two networks are relative to one another, i.e. it is an adversarial score. To provide an example of this, Figure 7 shows a comparison of the images produced by the generator with the lowest loss (epoch 8 at 1.063) and then the generator at the final epoch. Evidently, the images produced by the final generator are of much higher quality than those output by the generator when it experiences the lowest observed loss. For this reason, the final generator is selected as the synthetic data producing model in this work, although it is suggested in the future to explore the quality at multiple stages. Figure 8 shows 18 real photographs for healthy and unhealthy classed lemons, as well as 18 examples of Conditional GAN generator output for healthy and unhealthy classed lemons. Interestingly, many of the synthetic images seem to be more reminiscent of potatoes than lemons; given the nature of GANs, this is due to the model's generalisation of the dataset which contained lemons photographed at different angles -and so this generalisation of a shape is reflected in the produced images. A more uniform colour can be observed in healthy synthetic lemons, whereas unhealthy lemons are given mould and dark styles, as well as several instances of gangrene. Similarly to the potato-like shape of the outputs, this could be attributed to the generalisation nature of GANs, several patterns have been observed during training, and these patterns are then applied while generating new images. A higher resolution example of synthetic lemon images showing features of mould and gangrene can be observed in Figure 9. The generator has seemingly learnt to cast light and shadows on the fruit as well as undesirable characteristics; both fruits have a texture and colour similar to mould on the surface, and the second fruit seems to have a darker patch towards the top.</p>
        <p>Figure 10 shows class activation maps on six synthetic images (three per class) from the convolutional neural network trained only on the real data. All six images were predicted to belong to their ground-truth classes. Note that on the bottom row the Convolutional Neural Network focuses on issues on the flesh of the fruit such as mould in the first two images and a dark patch which may indicate gangrene on the right-most bottom image. Additionally, the class activation maps for the healthy fruit exist more generally around the shape. These behaviours are seemingly reminiscent of how a human would analyse the images, either focusing on unhealthy characteristics if the fruit is bad or observing the general image when the fruit shows no undesirable features. This analysis further shows that both desirable and undesirable characteristics are generalised, and, to an extent reproduced by the generative model. Indeed, the synthetic images are not perfect (as can be observed when comparing them via Figure 8), but these activation maps provide insight into the useful synthetic knowledge existing within them.Figure 10 shows class activation maps on six synthetic images (three per class) from the convolutional neural network trained only on the real data. All six images were predicted to belong to their ground-truth classes. Note that on the bottom row the Convolutional Neural Network focuses on issues on the flesh of the fruit such as mould in the first two images and a dark patch which may indicate gangrene on the right-most bottom image. Additionally, the class activation maps for the healthy fruit exist more generally around the shape. These behaviours are seemingly reminiscent of how a human would analyse the images, either focusing on unhealthy characteristics if the fruit is bad or observing the general image when the fruit shows no undesirable features. This analysis further shows that both desirable and undesirable characteristics are generalised, and, to an extent reproduced by the generative model. Indeed, the synthetic images are not perfect (as can be observed when comparing them via Figure 8), but these activation maps provide insight into the useful synthetic knowledge existing within them.</p>
        <p>The results from the best CNN experiment along with the augmentation approaches can be observed in Figure 11 and Table 2. All augmentation approaches scored higher than training only on the real images, showing that augmentation has had a positive effect on the learning process.The best set of results were achieved augmenting the dataset with 400 images (13.51% of the whole dataset, 200 per class), which scored a classification accuracy of 88.75% when classifying unseen images of both healthy and unhealthy lemons. Although there was a varying number of classification accuracies recorded, note that even the weakest augmentation approach (2200 images) caused the image recognition ability to rise from 83.77% to 85.02%. That is, of the 15 trials performed, all augmentation approaches outperformed the vanilla CNN. These results thus argue that Conditional GAN-based training data augmentation is a promising approach to improve fruit quality image classification. The generator model and weights are made publicly available for further exploration.The results from the best CNN experiment along with the augmentation approaches can be observed in Figure 11 and Table 2. All augmentation approaches scored higher than training only on the real images, showing that augmentation has had a positive effect on the learning process.The best set of results were achieved augmenting the dataset with 400 images (13.51% of the whole dataset, 200 per class), which scored a classification accuracy of 88.75% when classifying unseen images of both healthy and unhealthy lemons. Although there was a varying number of classification accuracies recorded, note that even the weakest augmentation approach (2200 images) caused the image recognition ability to rise from 83.77% to 85.02%. That is, of the 15 trials performed, all augmentation approaches outperformed the vanilla CNN. These results thus argue that Conditional GAN-based training data augmentation is a promising approach to improve fruit quality image classification. The generator model and weights are made publicly available for further exploration.</p>
        <p>Table 3 and Figure 12 show a comparison of the pruning experiments from 90% to 10% compression (through sparsity i.e. zeroed weights). Although the final two models with all neurons present the best results, we note earlier that a faster model would be ideal for the problem at hand, since fruits are processed in large quantities and the models might need being ran under time and energy restrictions. It is interesting to note from these results that the data augmented network tends to outpeform the vanilla network in all cases. It is also interesting to note that a data augmentation network compressed to half of its original size can still perform at an accuracy of 81.16%, effectively doubling production at a loss of 7.59% accuracy.Table 3 and Figure 12 show a comparison of the pruning experiments from 90% to 10% compression (through sparsity i.e. zeroed weights). Although the final two models with all neurons present the best results, we note earlier that a faster model would be ideal for the problem at hand, since fruits are processed in large quantities and the models might need being ran under time and energy restrictions. It is interesting to note from these results that the data augmented network tends to outpeform the vanilla network in all cases. It is also interesting to note that a data augmentation network compressed to half of its original size can still perform at an accuracy of 81.16%, effectively doubling production at a loss of 7.59% accuracy.</p>
        <p>Lemon fruit are of growing financial importance in LDC nations, in some cases the percentage annual increase of exports were measured in the thousands. The possibility of increasing productivity and reducing the operating costs of those systems are thus particularly interesting. This work has suggested methods to both improve autonomous fruit quality recognition, followed by linear pruning to reduce the complexity of computer vision model.Lemon fruit are of growing financial importance in LDC nations, in some cases the percentage annual increase of exports were measured in the thousands. The possibility of increasing productivity and reducing the operating costs of those systems are thus particularly interesting. This work has suggested methods to both improve autonomous fruit quality recognition, followed by linear pruning to reduce the complexity of computer vision model.</p>
        <p>To conclude, we first noted that autonomous sorting of healthy fruit from undesirable fruit is possible through contemporary computer vision technologies. We explored the concepts of fine-tuning, transfer learning, and Conditional GAN-based training data augmentation; our results showed that the recognition (and thus sorting) of fruit images was improved when augmentation was introduced. We found that introducing 400 synthetic data points had the largest impact, raising the recognition accuracy from 83.77% to 88.75% via a convolutional neural network. Finally, we performed Grad-CAM analysis on the model trained only on real photographs to show that the Conditional GAN was successful in imagining the undesirable characteristics of the flesh of the synthetic fruit generated. Thus, this work argues that Conditional Generative Adversarial Networks have the ability to produce new data to alleviate issues of data scarcity in the problem of fruit health classification.To conclude, we first noted that autonomous sorting of healthy fruit from undesirable fruit is possible through contemporary computer vision technologies. We explored the concepts of fine-tuning, transfer learning, and Conditional GAN-based training data augmentation; our results showed that the recognition (and thus sorting) of fruit images was improved when augmentation was introduced. We found that introducing 400 synthetic data points had the largest impact, raising the recognition accuracy from 83.77% to 88.75% via a convolutional neural network. Finally, we performed Grad-CAM analysis on the model trained only on real photographs to show that the Conditional GAN was successful in imagining the undesirable characteristics of the flesh of the synthetic fruit generated. Thus, this work argues that Conditional Generative Adversarial Networks have the ability to produce new data to alleviate issues of data scarcity in the problem of fruit health classification.</p>
        <p>Although we explored the lowest loss of the generator compared to the final loss, and a vast improvement was made albeit with the said higher loss, future further exploration of model weights throughout the training process may be useful to explore. Given the behaviour of losses spiking, several of the later models should be explored in order to ascertain whether the final epoch did produce the best model for this approach. Although the Conditional GAN-based approach showed promise to the classical CNN, one limitation of this work is that we compare the best train-test CNN to the Conditional GAN; in future, given resource availability to quickly train several Conditional GANs, a better metric for evaluation would be to train k Conditional GANs during k-fold cross validation. This would allow for better scientific accuracy. Note that the Conditional GAN was trained for 17 hours on a leading GPU, and thus such a solution would only likely be viable in the future with better technology.Although we explored the lowest loss of the generator compared to the final loss, and a vast improvement was made albeit with the said higher loss, future further exploration of model weights throughout the training process may be useful to explore. Given the behaviour of losses spiking, several of the later models should be explored in order to ascertain whether the final epoch did produce the best model for this approach. Although the Conditional GAN-based approach showed promise to the classical CNN, one limitation of this work is that we compare the best train-test CNN to the Conditional GAN; in future, given resource availability to quickly train several Conditional GANs, a better metric for evaluation would be to train k Conditional GANs during k-fold cross validation. This would allow for better scientific accuracy. Note that the Conditional GAN was trained for 17 hours on a leading GPU, and thus such a solution would only likely be viable in the future with better technology.</p>
        <p>Following the augmentation approach, the classification accuracy rose from 83.77% to 88.75%. In future work, further exploration into the classification model should be performed to further increase this level of recognition accuracy. In addition to further improvement of binary classification, future work could also discern the type of unhealthy attribute, for example, the specificity between damaged and rotten fruit, which directs the fruit to the correct destination; in this case, a juicer or disposal, respectively.Following the augmentation approach, the classification accuracy rose from 83.77% to 88.75%. In future work, further exploration into the classification model should be performed to further increase this level of recognition accuracy. In addition to further improvement of binary classification, future work could also discern the type of unhealthy attribute, for example, the specificity between damaged and rotten fruit, which directs the fruit to the correct destination; in this case, a juicer or disposal, respectively.</p>
        <p>Since the lemon dataset is relatively new, there are no published works to directly compare our results to. In the future, once the dataset has been explored by other works, it would be useful to have a direct comparison of all results when attempting to classify the data.Since the lemon dataset is relatively new, there are no published works to directly compare our results to. In the future, once the dataset has been explored by other works, it would be useful to have a direct comparison of all results when attempting to classify the data.</p>
        <p>Pruning showed that the models could be presented at a fraction of their original size and only lose a small amount of classification ability. With this in mind, other pruning techniques could be explored in the future to further maximise the model's ability in terms of real-world usage.Pruning showed that the models could be presented at a fraction of their original size and only lose a small amount of classification ability. With this in mind, other pruning techniques could be explored in the future to further maximise the model's ability in terms of real-world usage.</p>
        <p>Some future work is also needed for real-world use of this approach. The nature of the dataset that these models are trained with dictate that image segmentation is required in order to separate the fruit from the background (such as a surface or other fruit). If a segmentation network is trained and applied prior to the preprocessing described in these techniques, then the approach can be tested on more data in a much higher volume, i.e., fruit production.Some future work is also needed for real-world use of this approach. The nature of the dataset that these models are trained with dictate that image segmentation is required in order to separate the fruit from the background (such as a surface or other fruit). If a segmentation network is trained and applied prior to the preprocessing described in these techniques, then the approach can be tested on more data in a much higher volume, i.e., fruit production.</p>
        <p>For replicability purposes as well as future research, the generator model and synthetic image generation Python 
            <rs type="software">code</rs> have been made available at: 
            <rs type="url">https://github.com/jordan-bird/synthetic-fruit-image-generator</rs>.
        </p>
        <p>Figure 4: Overview of the VGG16 topology and custom interpretation and output layer which is used for binary image classification.Figure 4: Overview of the VGG16 topology and custom interpretation and output layer which is used for binary image classification.</p>
        <p>occurs within a time of 10 epochs. Hyperparameters are set to a batch size of 64, and the Adam optimiser has a learning rate of 0.001, β 1 = 0.9, β 2 = 0.999, and = 1e -07.occurs within a time of 10 epochs. Hyperparameters are set to a batch size of 64, and the Adam optimiser has a learning rate of 0.001, β 1 = 0.9, β 2 = 0.999, and = 1e -07.</p>
        <p>Image generation model weights and code are available at: https://github.com/jordan-bird/synthetic-fruit-image-generatorImage generation model weights and code are available at: https://github.com/jordan-bird/synthetic-fruit-image-generator</p>
        <p>Note: None of the authors of this work are affiliated with SoftwareMillNote: None of the authors of this work are affiliated with SoftwareMill</p>
    </text>
</tei>

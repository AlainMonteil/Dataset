<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:12+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Schizophrenia is a severe mental disorder associated with a wide spectrum of cognitive and neurophysiological dysfunctions. Early diagnosis is still difficult and based on the manifestation of the disorder. In this study, we have evaluated whether machine learning techniques can help in the diagnosis of schizophrenia, and proposed a processing pipeline in order to obtain machine learning classifiers of schizophrenia based on resting state EEG data. We have computed well-known linear and non-linear measures on sliding windows of the EEG data, selected those measures which better differentiate between patients and healthy controls, and combined them through principal component analysis. These components were finally used as features in five standard machine learning algorithms: k-nearest neighbours (kNN), logistic regression (LR), decision trees (DT), random forest (RF) and support vector machines (SVM). Complexity measures showed a high level of ability in differentiating schizophrenia patients from healthy controls. These differences between groups were mainly located in a delimited zone of the right brain hemisphere, corresponding to the opercular area and the temporal pole. Based on the area under the curve parameter in receiver operating characteristic curve analysis, we obtained high classification power in almost all of the machine learning algorithms tested: SVM (0.89), RF (0.87), LR (0.86), kNN (0.86) and DT (0.68). Our results suggest that the proposed processing pipeline on resting state EEG data is able to easily compute and select a set of features which allow standard machine learning algorithms to perform very efficiently in differentiating schizophrenia patients from healthy subjects.Schizophrenia is a severe mental disorder associated with a wide spectrum of cognitive and neurophysiological dysfunctions. Early diagnosis is still difficult and based on the manifestation of the disorder. In this study, we have evaluated whether machine learning techniques can help in the diagnosis of schizophrenia, and proposed a processing pipeline in order to obtain machine learning classifiers of schizophrenia based on resting state EEG data. We have computed well-known linear and non-linear measures on sliding windows of the EEG data, selected those measures which better differentiate between patients and healthy controls, and combined them through principal component analysis. These components were finally used as features in five standard machine learning algorithms: k-nearest neighbours (kNN), logistic regression (LR), decision trees (DT), random forest (RF) and support vector machines (SVM). Complexity measures showed a high level of ability in differentiating schizophrenia patients from healthy controls. These differences between groups were mainly located in a delimited zone of the right brain hemisphere, corresponding to the opercular area and the temporal pole. Based on the area under the curve parameter in receiver operating characteristic curve analysis, we obtained high classification power in almost all of the machine learning algorithms tested: SVM (0.89), RF (0.87), LR (0.86), kNN (0.86) and DT (0.68). Our results suggest that the proposed processing pipeline on resting state EEG data is able to easily compute and select a set of features which allow standard machine learning algorithms to perform very efficiently in differentiating schizophrenia patients from healthy subjects.</p>
        <p>Schizophrenia is a severe mental disorder which affects approximately 1 % of the world's population. Patients with schizophrenia suffer psychotic symptoms such as hallucinations and delusions [1], along with a wide variety of cognitive and motor dysfunctions. The symptoms' onset is usually during adolescence and early adulthood (between 14 and 30), and it has been consistently found that the time between the onset of symptoms and diagnosis and treatment is one of the best predictors of later prognosis [2]. Thus, early diagnosis is of great relevance in the course of the disorder. Currently, there are no reliable biomarkers for schizophrenia and, therefore, its diagnosis relies on the subjective evaluation of the clinicians [3].Schizophrenia is a severe mental disorder which affects approximately 1 % of the world's population. Patients with schizophrenia suffer psychotic symptoms such as hallucinations and delusions [1], along with a wide variety of cognitive and motor dysfunctions. The symptoms' onset is usually during adolescence and early adulthood (between 14 and 30), and it has been consistently found that the time between the onset of symptoms and diagnosis and treatment is one of the best predictors of later prognosis [2]. Thus, early diagnosis is of great relevance in the course of the disorder. Currently, there are no reliable biomarkers for schizophrenia and, therefore, its diagnosis relies on the subjective evaluation of the clinicians [3].</p>
        <p>Machine learning is a set of artificial intelligence tools used to construct data models representing the knowledge hidden in huge data sets [4]. These data models are simplified versions of the original data constructed by using a set of features extracted from them. The machine learning tools consist of algorithms that construct these data models automatically from the set of features extracted from the original data (training data). The data models are then used to predict or classify new data (test data). In this study, we have applied five of the most widely used machine learning classification algorithms on an EEG dataset from patients with schizophrenia and healthy controls at rest: k-nearest neighbours (kNN), logistic regression (LR), decision trees (DT), support vector machines (SVM) and random forest (RF). These techniques have been used in many successful applications of machine learning in biomedicine such as identification of anticancer peptides (iACP-GAEnsC model) [5], classification of DNA-bindings proteins (DP-BINDER method) [6] and prediction of antifungal peptides (iAFPs-EnC-GA predictor) [7], among many others.Machine learning is a set of artificial intelligence tools used to construct data models representing the knowledge hidden in huge data sets [4]. These data models are simplified versions of the original data constructed by using a set of features extracted from them. The machine learning tools consist of algorithms that construct these data models automatically from the set of features extracted from the original data (training data). The data models are then used to predict or classify new data (test data). In this study, we have applied five of the most widely used machine learning classification algorithms on an EEG dataset from patients with schizophrenia and healthy controls at rest: k-nearest neighbours (kNN), logistic regression (LR), decision trees (DT), support vector machines (SVM) and random forest (RF). These techniques have been used in many successful applications of machine learning in biomedicine such as identification of anticancer peptides (iACP-GAEnsC model) [5], classification of DNA-bindings proteins (DP-BINDER method) [6] and prediction of antifungal peptides (iAFPs-EnC-GA predictor) [7], among many others.</p>
        <p>Several recent studies have shown that machine learning can also be a very useful tool in classifying schizophrenia. In [8], an ensemble method was used to classify first-episode schizophrenia from functional magnetic resonance imaging (fMRI) data. A classification accuracy of 84.7 % was obtained with that method. Ensemble methods and several neuroanatomical measures extracted from MRI data were also used in [9] to achieve a classification accuracy of 87 %. An accuracy of 85 % in classifying patients with schizophrenia and autism by using kNN and cortical features extracted from MRI data was achieved in [10]. A SVM classifier based on several structural MRI features allowed Zhu et al. to achieve an accuracy of 76 % in classifying schizophrenia at early clinical stages [11]. SVM was also used to classify schizophrenia with an accuracy of 85 % by using brain-wise functional connectivity features from MRI data [12]. The capacity of generalization and replication of machine learning for classification of schizophrenia based on functional MRI data were assessed in [13]. An accuracy of 70 % was obtained when an external validation was performed on the model trained with the main dataset. Li et al. [14] proposed a deep canonically-correlated sparse autoencoder to classify schizophrenia patients from healthy controls. Their method was applied on both single nucleotide polymorphisms and fMRI. An accuracy of 95.65 % was achieved with the genomic data, while the accuracy was 80.53 % when using fMRI. Textual data have also been used to classify schizophrenia. In [15], several linguistic features were extracted from social media posts, and then a RF method was used to obtain a classification accuracy of 96 %. Demographic data and clinical assessment questionaries were used in [16] to train a SVM classifier which achieved 90 % of accuracy in classifying schizophrenia. A review of machine learning techniques applied in classifying schizophrenia from several kinds of data such as structural and functional MRI, genomic information, interview audio records and textual clinical data can be found in [17].Several recent studies have shown that machine learning can also be a very useful tool in classifying schizophrenia. In [8], an ensemble method was used to classify first-episode schizophrenia from functional magnetic resonance imaging (fMRI) data. A classification accuracy of 84.7 % was obtained with that method. Ensemble methods and several neuroanatomical measures extracted from MRI data were also used in [9] to achieve a classification accuracy of 87 %. An accuracy of 85 % in classifying patients with schizophrenia and autism by using kNN and cortical features extracted from MRI data was achieved in [10]. A SVM classifier based on several structural MRI features allowed Zhu et al. to achieve an accuracy of 76 % in classifying schizophrenia at early clinical stages [11]. SVM was also used to classify schizophrenia with an accuracy of 85 % by using brain-wise functional connectivity features from MRI data [12]. The capacity of generalization and replication of machine learning for classification of schizophrenia based on functional MRI data were assessed in [13]. An accuracy of 70 % was obtained when an external validation was performed on the model trained with the main dataset. Li et al. [14] proposed a deep canonically-correlated sparse autoencoder to classify schizophrenia patients from healthy controls. Their method was applied on both single nucleotide polymorphisms and fMRI. An accuracy of 95.65 % was achieved with the genomic data, while the accuracy was 80.53 % when using fMRI. Textual data have also been used to classify schizophrenia. In [15], several linguistic features were extracted from social media posts, and then a RF method was used to obtain a classification accuracy of 96 %. Demographic data and clinical assessment questionaries were used in [16] to train a SVM classifier which achieved 90 % of accuracy in classifying schizophrenia. A review of machine learning techniques applied in classifying schizophrenia from several kinds of data such as structural and functional MRI, genomic information, interview audio records and textual clinical data can be found in [17].</p>
        <p>There have been several applications of machine learning techniques in schizophrenia classification based on EEG data. These methods can be grouped together taking into account the machine learning algorithm used, the measures determining the features in the algorithm and how the input EEG data was obtained. We briefly summarize now the most recent methods. For the case of event-related potential EEG signals, Zhang [18] developed a random forest algorithm for the differentiation between schizophrenia and healthy controls, using N1 and P3 features extracted when basic sensory tasks involving button press and auditory tone were performed. In this study the best classification accuracy obtained was 81 %. In [19], kNN, decision tree and SVM classifiers were used to identify children at risk of developing schizophrenia. In this study the mean amplitude of early and mid-latency event related potential were used as features during a passive auditory task. The best accuracy obtained was 44 %, improved to 72 % by using a deep learning approach based on convolutional neural networks. Several recent studies have analyzed the EEG signal obtained in resting state. Buettner et al. [20] achieved 71 % of accuracy in classifying schizophrenia using the random forest classifier based on power features extracted from several frequency bands. Twelve statistical features on five bands extracted from 5-minute resting-state EEG signal were used in a SVM model allowing Tikka et al. to achieve an accuracy of 79 % in classifying schizophrenia patients from healthy controls [21]. Non-linear measures such as entropy, largest Lyapunov exponent, Hurst exponent, and recurrence quantification analysis have also been employed in SVM algorithms, achieving an accuracy of approximately 92 % in the classification of patients [22,23]. Finally, Sun et al. [24] developed a method that, from resting state EEG data, constructed an RGB image from which a feature based on the fuzzy entropy was extracted. This feature was used in a hybrid model based on convolutional neural networks and long-short-term memory networks to obtain a schizophrenia classifier with a high accuracy of 99 %.There have been several applications of machine learning techniques in schizophrenia classification based on EEG data. These methods can be grouped together taking into account the machine learning algorithm used, the measures determining the features in the algorithm and how the input EEG data was obtained. We briefly summarize now the most recent methods. For the case of event-related potential EEG signals, Zhang [18] developed a random forest algorithm for the differentiation between schizophrenia and healthy controls, using N1 and P3 features extracted when basic sensory tasks involving button press and auditory tone were performed. In this study the best classification accuracy obtained was 81 %. In [19], kNN, decision tree and SVM classifiers were used to identify children at risk of developing schizophrenia. In this study the mean amplitude of early and mid-latency event related potential were used as features during a passive auditory task. The best accuracy obtained was 44 %, improved to 72 % by using a deep learning approach based on convolutional neural networks. Several recent studies have analyzed the EEG signal obtained in resting state. Buettner et al. [20] achieved 71 % of accuracy in classifying schizophrenia using the random forest classifier based on power features extracted from several frequency bands. Twelve statistical features on five bands extracted from 5-minute resting-state EEG signal were used in a SVM model allowing Tikka et al. to achieve an accuracy of 79 % in classifying schizophrenia patients from healthy controls [21]. Non-linear measures such as entropy, largest Lyapunov exponent, Hurst exponent, and recurrence quantification analysis have also been employed in SVM algorithms, achieving an accuracy of approximately 92 % in the classification of patients [22,23]. Finally, Sun et al. [24] developed a method that, from resting state EEG data, constructed an RGB image from which a feature based on the fuzzy entropy was extracted. This feature was used in a hybrid model based on convolutional neural networks and long-short-term memory networks to obtain a schizophrenia classifier with a high accuracy of 99 %.</p>
        <p>In the present study, we further explore the capabilities of standard machine learning algorithms in obtaining classifiers of schizophrenia based on resting state EEG data. Our approach consists of a processing pipeline whereby the EEG data is split into sliding windows over which a set of linear and non-linear measures are computed. Those measures which better differentiate between patients and controls are selected, and a principal component analysis is then performed on them. The results of the principal component analysis are finally used as the features in the machine learning algorithms. This processing pipeline allows us to select a set of features which works efficiently regardless of the machine learning algorithm used. This characteristic is not present in most of the previous studies, where good accuracy values were obtained for only one technique or a very limited number of machine learning methods. Our method is based on well-known and easily computable EEG measures which are used in standard machine learning algorithms available in common statistical software tools. This fact allows the scientific community to easily reproduce and use our processing pipeline. This is not a common characteristic in those previous studies which obtained very good accuracy results but at the cost of using complex and custom algorithms in order to extract the features and design de machine learning models.In the present study, we further explore the capabilities of standard machine learning algorithms in obtaining classifiers of schizophrenia based on resting state EEG data. Our approach consists of a processing pipeline whereby the EEG data is split into sliding windows over which a set of linear and non-linear measures are computed. Those measures which better differentiate between patients and controls are selected, and a principal component analysis is then performed on them. The results of the principal component analysis are finally used as the features in the machine learning algorithms. This processing pipeline allows us to select a set of features which works efficiently regardless of the machine learning algorithm used. This characteristic is not present in most of the previous studies, where good accuracy values were obtained for only one technique or a very limited number of machine learning methods. Our method is based on well-known and easily computable EEG measures which are used in standard machine learning algorithms available in common statistical software tools. This fact allows the scientific community to easily reproduce and use our processing pipeline. This is not a common characteristic in those previous studies which obtained very good accuracy results but at the cost of using complex and custom algorithms in order to extract the features and design de machine learning models.</p>
        <p>In the following sections, we first describe the dataset of the study and how it was obtained. Then, we briefly review the EEG measures used and the machine learning algorithms tested. Next, we show the process implemented to extract the features from the EEG data. And finally, we present and discuss the classification results obtained by the machine learning algorithms.In the following sections, we first describe the dataset of the study and how it was obtained. Then, we briefly review the EEG measures used and the machine learning algorithms tested. Next, we show the process implemented to extract the features from the EEG data. And finally, we present and discuss the classification results obtained by the machine learning algorithms.</p>
        <p>We included 31 subjects in this study (see Table 1 for demographics): 20 healthy control (HC) subjects (13 males, 7 females, mean age: 40.7 ± 11.9) and 11 subjects suffering from schizophrenia (SCZ) (9 males, 2 females, mean age: 36.2 ± 10.2). Participants in the control group were recruited from the students of the University of Jaén, the staff of the University Hospital of San Agustin Linares (Jaén) and from an Adult School. The patient group was recruited at the Hospital Universitario San Agustín (Linares, Jaén).We included 31 subjects in this study (see Table 1 for demographics): 20 healthy control (HC) subjects (13 males, 7 females, mean age: 40.7 ± 11.9) and 11 subjects suffering from schizophrenia (SCZ) (9 males, 2 females, mean age: 36.2 ± 10.2). Participants in the control group were recruited from the students of the University of Jaén, the staff of the University Hospital of San Agustin Linares (Jaén) and from an Adult School. The patient group was recruited at the Hospital Universitario San Agustín (Linares, Jaén).</p>
        <p>The inclusion criteria for participation were an ICD-10 diagnosis of schizophrenia (F20), psychotic disorder (F23) or schizophrenic disorder (F25). The diagnosis was made by the clinician in charge of the patient. The duration of the disease was, on average, 15.72 ± 10.19 years. Regarding medication, all participants were receiving antipsychotic medication (all atypical). In addition to antipsychotics, one of the participants was receiving antidepressants. Due to differences in active principles, doses and administration methods, we converted the doses of all antipsychotics into chlorpromazine equivalents (818.18 ± 407.75 mg).The inclusion criteria for participation were an ICD-10 diagnosis of schizophrenia (F20), psychotic disorder (F23) or schizophrenic disorder (F25). The diagnosis was made by the clinician in charge of the patient. The duration of the disease was, on average, 15.72 ± 10.19 years. Regarding medication, all participants were receiving antipsychotic medication (all atypical). In addition to antipsychotics, one of the participants was receiving antidepressants. Due to differences in active principles, doses and administration methods, we converted the doses of all antipsychotics into chlorpromazine equivalents (818.18 ± 407.75 mg).</p>
        <p>For both groups, the exclusion criteria were any of the following: a concurrent diagnosis of neurological disorder, a concurrent diagnosis of substance abuse, a history of developmental disability, an inability to Finally, in order to measure the cognitive functioning in both groups, we used a Spanish adaptation of the Screen for Cognitive Disability in Psychiatry (SCIP-S, [25]), which has shown satisfactory reliability and good concurrent validity with neuropsychological batteries [26]. SCIP-S has been specifically developed to detect the main cognitive deficits of people with some type of mental illness, although it can also be used to assess the cognitive status of adults without mental illness. SCIP-S is a short paper-and-pencil test that can be completed in about 15 min. It provides a score for each of the following subscales: immediate and delayed verbal learning, verbal fluency, working memory and processing speed. Descriptive statistics for each group can be found in Table 1. We performed t-tests for the scores on the five subtests of the SCIP-S. As can be seen in Table 1, the group of healthy controls scored significantly better than the patients on all dimensions of the SCIP-S except for delayed verbal learning.For both groups, the exclusion criteria were any of the following: a concurrent diagnosis of neurological disorder, a concurrent diagnosis of substance abuse, a history of developmental disability, an inability to Finally, in order to measure the cognitive functioning in both groups, we used a Spanish adaptation of the Screen for Cognitive Disability in Psychiatry (SCIP-S, [25]), which has shown satisfactory reliability and good concurrent validity with neuropsychological batteries [26]. SCIP-S has been specifically developed to detect the main cognitive deficits of people with some type of mental illness, although it can also be used to assess the cognitive status of adults without mental illness. SCIP-S is a short paper-and-pencil test that can be completed in about 15 min. It provides a score for each of the following subscales: immediate and delayed verbal learning, verbal fluency, working memory and processing speed. Descriptive statistics for each group can be found in Table 1. We performed t-tests for the scores on the five subtests of the SCIP-S. As can be seen in Table 1, the group of healthy controls scored significantly better than the patients on all dimensions of the SCIP-S except for delayed verbal learning.</p>
        <p>EEG data was acquired in a session where participants sat in a laboratory room at the hospital in a comfortable chair with a laptop computer placed on the desk in front of them. The laptop screen was located about 70 cm from the participant's eyes. Then, the experimenter proceeded to place the cap montage of 31 active electrodes in the 10-20 system with positions FP1, FP2, F7, F3, Fz, F4, F8, FT9, FC5, FC1, FC2, FC6, FT10, T7, C3, C4, T8, TP9, CP5, CP1, CP2, CP6, TP10, P7, P3, Pz, P4, P8, O1, Oz and O2. All electrodes were referenced to both mastoids, and impedances were kept below 5 kOhm. Signals were recorded at a frequency of 500 Hz. During the resting state task, participants were instructed to look at a light grey fixation cross placed in the centre of a black background for 3 min. They were asked not to move and were told that they could think about whatever they wanted, in silence. The experimenter was seated behind them, out of view of the participants, in the same room. In a second session, participants underwent cognitive evaluation (SCIP-S), and the remainder of demographic data (age, gender and educational level) were collected.EEG data was acquired in a session where participants sat in a laboratory room at the hospital in a comfortable chair with a laptop computer placed on the desk in front of them. The laptop screen was located about 70 cm from the participant's eyes. Then, the experimenter proceeded to place the cap montage of 31 active electrodes in the 10-20 system with positions FP1, FP2, F7, F3, Fz, F4, F8, FT9, FC5, FC1, FC2, FC6, FT10, T7, C3, C4, T8, TP9, CP5, CP1, CP2, CP6, TP10, P7, P3, Pz, P4, P8, O1, Oz and O2. All electrodes were referenced to both mastoids, and impedances were kept below 5 kOhm. Signals were recorded at a frequency of 500 Hz. During the resting state task, participants were instructed to look at a light grey fixation cross placed in the centre of a black background for 3 min. They were asked not to move and were told that they could think about whatever they wanted, in silence. The experimenter was seated behind them, out of view of the participants, in the same room. In a second session, participants underwent cognitive evaluation (SCIP-S), and the remainder of demographic data (age, gender and educational level) were collected.</p>
        <p>Data processing was performed with 
            <rs type="software">EEGLAB</rs> and custom 
            <rs type="software">MATLAB</rs> functions. For each participant, we selected 3 min of continuous data. Blinks and other artefacts were extracted using infomax ICA. ICA components with artefacts were selected by visual inspection of the scalp topography, power spectra and raw activity from all components. The resulting EEGs, after denoising, were used as inputs for a custom 
            <rs type="software">MATLAB</rs>
            <rs type="software">script</rs> developed to obtain the measures described in the following section.
        </p>
        <p>Seventeen different linear and non-linear measures were extracted from EEG data, namely Higuchi fractal dimension (HFD), approximate This range of measures was selected because we wanted to cover widely used metrics in both time and frequency domains as well as from non-linear analysis. Next, we briefly describe the theory behind each measure and the experimental configurations used for its computation in this study.Seventeen different linear and non-linear measures were extracted from EEG data, namely Higuchi fractal dimension (HFD), approximate This range of measures was selected because we wanted to cover widely used metrics in both time and frequency domains as well as from non-linear analysis. Next, we briefly describe the theory behind each measure and the experimental configurations used for its computation in this study.</p>
        <p>The fractal dimension is a metric of the complexity of a signal which measures how the signal occupies its geometrical target space [27]. Higuchi's algorithm calculates the fractal dimension of a temporal signal (HFD) as follows [28]:The fractal dimension is a metric of the complexity of a signal which measures how the signal occupies its geometrical target space [27]. Higuchi's algorithm calculates the fractal dimension of a temporal signal (HFD) as follows [28]:</p>
        <p>From a given signal s = {s(1), s(2), …, s(N)}, k curves s k m are constructed as:From a given signal s = {s(1), s(2), …, s(N)}, k curves s k m are constructed as:</p>
        <p>where m and k are integers and they indicate the initial time and the time interval, respectively. The length L m (k) of each curve s k m is calculated as:where m and k are integers and they indicate the initial time and the time interval, respectively. The length L m (k) of each curve s k m is calculated as:</p>
        <p>The length of the curve for time interval k, L(k), is calculated as the average of theThe length of the curve for time interval k, L(k), is calculated as the average of the</p>
        <p>Measures and channels for which HC vs SCZ comparison is statistically different. Channels Fp1, Fz, F3, FT9, C3, T7, TP9, CP1, Pz, P3, P7, O1, Oz, P8, F4 and Fp2, and measures detrended fluctuation analysis, Hjorth activity, total energy, power band alpha, power band beta and power band theta did not show significant differences for any measure nor in any channel, respectively, between HC and SCZ groups. *: t-test p-value &lt; 0.05, **: t-test p-value &lt; 0.01. HFD: Higuchi fractal dimension, ApEn: approximate entropy, SamEn: sample entropy, CD: correlation dimension, LZC: Lempel-Ziv complexity, MIMR: mutual information of multiple rhythms, HM: Hjorth mobility, HC: Hjorth complexity, DP: delta power band, LLE: largest Lyapunov exponent and DET: determinism. HFD values of the EEG signals were computed in 
            <rs type="software">MATLAB</rs> R2013a using the 
            <rs type="software">software</rs> written by Selvam and Nadu [29]. Higuchi's algorithm needs a manual-set parameter, kmax, in order to establish the maximum size of the time intervals. We set the parameter kmax with a value equal to half the number of time samples in the entry EEG. This value ensures that the curve obtained plotting HFD vs kmax, for kmax between 1 and the number of time samples, plateaus at that value of kmax [30].
        </p>
        <p>Entropy can be defined as the rate of information production in a dynamical system [31]. There are methods for a direct estimation of the entropy of time series, but these methods are not adequate for analyzing EEG data, due to its length and noisy nature. Nevertheless, approximate entropy (ApEn) and sample entropy (SamEn) are two measures, closely related to entropy, which are easily applied to EEG data and other biological signals.Entropy can be defined as the rate of information production in a dynamical system [31]. There are methods for a direct estimation of the entropy of time series, but these methods are not adequate for analyzing EEG data, due to its length and noisy nature. Nevertheless, approximate entropy (ApEn) and sample entropy (SamEn) are two measures, closely related to entropy, which are easily applied to EEG data and other biological signals.</p>
        <p>ApEn and SamEn measure the regularity of a signal by dividing it into epochs. The more frequent and more similar the epoch is, the lower the value of ApEn and the more regular the signal. Given three parameters: N, m and r, where N is the length of the EEG time series, m is the length of the epochs to be compared and r is a value of tolerance for accepting matches, ApEn(N, m, r) of the signal s = {s(1), s(2), …, s(N)} can be defined as [32]:ApEn and SamEn measure the regularity of a signal by dividing it into epochs. The more frequent and more similar the epoch is, the lower the value of ApEn and the more regular the signal. Given three parameters: N, m and r, where N is the length of the EEG time series, m is the length of the epochs to be compared and r is a value of tolerance for accepting matches, ApEn(N, m, r) of the signal s = {s(1), s(2), …, s(N)} can be defined as [32]:</p>
        <p>where C m i (r) = Bi N-m+1 , and B i are the number of epochs x m (j) within r of x m (i), wherewhere C m i (r) = Bi N-m+1 , and B i are the number of epochs x m (j) within r of x m (i), where</p>
        <p>Sample entropy (SamEn) is an optimization of ApEn in order to avoid self-matching of the epochs, and it is defined as [31]:Sample entropy (SamEn) is an optimization of ApEn in order to avoid self-matching of the epochs, and it is defined as [31]:</p>
        <p>ApEn(N, m, r) = -ln Ai Bi , where A i is the number of epochs x m+1 (j) within r of x m+1 (i).ApEn(N, m, r) = -ln Ai Bi , where A i is the number of epochs x m+1 (j) within r of x m+1 (i).</p>
        <p>In this study, ApEn and SamEn values of the EEG data were computed using the 
            <rs type="software">MATLAB</rs> code provided by Lee [33] and Martínez [34]. Parameters m and r were set to 2 and 0.1 respectively, as recommended for EEG data [31].
        </p>
        <p>The correlation dimension (CD) is a type of fractal dimension that measures the dimensionality of the space occupied by a set of points [35]. CD behaves well for signals with a small number of points, and computing CD is easy and fast.The correlation dimension (CD) is a type of fractal dimension that measures the dimensionality of the space occupied by a set of points [35]. CD behaves well for signals with a small number of points, and computing CD is easy and fast.</p>
        <p>Given the signal s = {s(1), s(2), …, s(N)}, the correlation integral C(l) is defined as [36]:Given the signal s = {s(1), s(2), …, s(N)}, the correlation integral C(l) is defined as [36]:</p>
        <p>where g is the number of pairs (i, j) whose distance |s(i) -s(j)| is less than l. For small l's, C(l) grows like a power C(l) ~ l CD . CD can be estimated easily by using a log-log plot of correlation integral versus l.where g is the number of pairs (i, j) whose distance |s(i) -s(j)| is less than l. For small l's, C(l) grows like a power C(l) ~ l CD . CD can be estimated easily by using a log-log plot of correlation integral versus l.</p>
        <p>In this study, we used the Gaussian kernel algorithm to estimate the values of CD of our EEG data using the 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> provided in [37].
        </p>
        <p>The Lempel-Ziv complexity (LZC) is a measure of the complexity of binary sequences that tests the randomness of the sequence by searching for repeated patterns in it [38]. The more irregular the sequence is, the greater the LZC. LZC can be computed easily following the algorithm proposed in [39]. The binary sequence is parsed into subsequences with different patterns. A complexity counter increases when a new subsequence appears and the following symbol is regarded as the beginning of the next subsequence. The scan continues until the end of the sequence, and finally LZC equals the complexity counter. For the case of non-binary signal like EEG, a process of binarization is required. In this study we use the mean of the EEG signal as the threshold for its binarization. The LZC was then computed using the 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> provided in [40].
        </p>
        <p>The mutual information of multiple rhythms (MIMR) is a measure of the cross frequency coupling between multiple neural rhythms present in an EEG signal [41]. 
            <rs type="software">MIMR</rs> obtains a series of binary sequences extracted from the original EEG, where each sequence includes information from a particular rhythm (i.e., alpha, beta, gamma and theta oscillations). The information from all the series is combined into a new symbolic series, from which the delayed mutual information [42] is computed as an estimation of dependence between them. The more dependent the activity from the selected rhythms is, the more coupled the rhythms are and the higher the value of MIMR.
        </p>
        <p>In order to calculate MIMR, the signal s = {s(1), s(2), …, s(N)} is smoothed using a median moving window with m window sizes {w 1 , …, w m }. The binary sequences, of size Nw m -1, are obtained from the expression [41]:In order to calculate MIMR, the signal s = {s(1), s(2), …, s(N)} is smoothed using a median moving window with m window sizes {w 1 , …, w m }. The binary sequences, of size Nw m -1, are obtained from the expression [41]:</p>
        <p>where (w m -1)/2 are the total points from the sides that are not included in the smoothing, w 0 corresponds to the original signal s(n); and H[n] is the discrete Heaviside function:where (w m -1)/2 are the total points from the sides that are not included in the smoothing, w 0 corresponds to the original signal s(n); and H[n] is the discrete Heaviside function:</p>
        <p>All binary sequences H k are transformed into the corresponding integer so that single values can identify all possible multiscale states. These integer values are used to construct the new signal {Y(n)}. Finally, MIMR is obtained as the delayed mutual information [42] of integers in {Y(n)}:All binary sequences H k are transformed into the corresponding integer so that single values can identify all possible multiscale states. These integer values are used to construct the new signal {Y(n)}. Finally, MIMR is obtained as the delayed mutual information [42] of integers in {Y(n)}:</p>
        <p>where τ is estimated as the first value that is closer to zero in the autocorrelation function of Y(n).where τ is estimated as the first value that is closer to zero in the autocorrelation function of Y(n).</p>
        <p>In this study, we use a homemade 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> to compute 
            <rs type="software">MIMR</rs>. More information about that implementation can be found in [41].
        </p>
        <p>Detrended fluctuation analysis (DFA) measures the statistical selfaffinity of a signal by detecting the long-range correlations embedded in it [43]. In order to calculate DFA, the signal s = {s(1), s(2), …, s(N)} is divided into N/l non-overlapping boxes s l of size l. For each box, a cumulative sum S l is calculated as S l = ∑ l i=1 (s i -s), where s is the mean value of the signal s. Then the local trend Y l of each box is computed as the ordinate of the least squares straight-line fit for the set of time windows of size n in S l . Next, the fluctuation of each trend is calculated as:Detrended fluctuation analysis (DFA) measures the statistical selfaffinity of a signal by detecting the long-range correlations embedded in it [43]. In order to calculate DFA, the signal s = {s(1), s(2), …, s(N)} is divided into N/l non-overlapping boxes s l of size l. For each box, a cumulative sum S l is calculated as S l = ∑ l i=1 (s i -s), where s is the mean value of the signal s. Then the local trend Y l of each box is computed as the ordinate of the least squares straight-line fit for the set of time windows of size n in S l . Next, the fluctuation of each trend is calculated as:</p>
        <p>The previous process is repeated for a range of different window lengths n, and the slope of the log-log plot of F(n) versus n is the scaling coefficient α, the result of the detrended fluctuation analysis. We used the 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> provided by 
            <rs type="creator">Magris</rs> in [44] in order to perform the detrended fluctuation analysis of our EEG data. The window lengths ranged from 20 to 500 EEG samples.
        </p>
        <p>Hjorth introduced three parameters, namely activity, mobility and complexity, to characterize EEG signals in the time domain [45].Hjorth introduced three parameters, namely activity, mobility and complexity, to characterize EEG signals in the time domain [45].</p>
        <p>Activity is computed as the standard deviation of the signal. The mobility parameter is defined as the square root of the standard deviation of the first derivative of the signal divided by the standard deviation of the signal. Finally, Hjorth complexity can be calculated as the ratio between the mobility of the first derivative of the signal and the mobility of the signal itself. These three parameters characterize the EEG signal in terms of amplitude, time scale and complexity, respectively.Activity is computed as the standard deviation of the signal. The mobility parameter is defined as the square root of the standard deviation of the first derivative of the signal divided by the standard deviation of the signal. Finally, Hjorth complexity can be calculated as the ratio between the mobility of the first derivative of the signal and the mobility of the signal itself. These three parameters characterize the EEG signal in terms of amplitude, time scale and complexity, respectively.</p>
        <p>In order to compute HA, HM and HC from our EEG data we used the 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> provided by Too in [46].
        </p>
        <p>Power spectral analysis is a conventional method in EEG analyses. It is based on decomposing the signal, using Fourier transform, into functionally distinct frequency bands: delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-12 Hz) and beta (12)(13)(14)(15)(16)(17)(18)(19)(20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30). Then an estimation of the power spectral density of the signal is performed, from which the average band power is computed for each frequency band (DP, TP, AP and BP). The total energy (TE) is easily computed as the sum of the squared amplitude of the signal in each time sample.Power spectral analysis is a conventional method in EEG analyses. It is based on decomposing the signal, using Fourier transform, into functionally distinct frequency bands: delta (0.5-4 Hz), theta (4-8 Hz), alpha (8-12 Hz) and beta (12)(13)(14)(15)(16)(17)(18)(19)(20)(21)(22)(23)(24)(25)(26)(27)(28)(29)(30). Then an estimation of the power spectral density of the signal is performed, from which the average band power is computed for each frequency band (DP, TP, AP and BP). The total energy (TE) is easily computed as the sum of the squared amplitude of the signal in each time sample.</p>
        <p>In this study, DP, TP, AP, BP and TE measures for our EEG data were computed in 
            <rs type="software">MATLAB</rs> using the function bandpower.
        </p>
        <p>The Lyapunov exponents of a dynamical system are the average exponential rates of divergence or convergence of infinitesimally close orbits in phase space [47]. A system with at least one positive Lyapunov exponent is chaotic. The value of the exponent indicates the time scale when the dynamical system becomes unpredictable. The larger the exponent, the more chaotic the system.The Lyapunov exponents of a dynamical system are the average exponential rates of divergence or convergence of infinitesimally close orbits in phase space [47]. A system with at least one positive Lyapunov exponent is chaotic. The value of the exponent indicates the time scale when the dynamical system becomes unpredictable. The larger the exponent, the more chaotic the system.</p>
        <p>For a discrete time series, the largest Lyapunov exponent (LLE) can be computed from the constructed attractor obtained by means of phase space reconstruction with delay coordinates [47]. We used the 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> provided by Wolf [48] to compute the LLE values for our EEG data.
        </p>
        <p>Recurrence quantification analysis (RQA) is a useful technique for analyzing the non-linear dynamical behaviour of EEG data [49]. This technique is based on recurrence plots, which describe the recurrence property of a deterministic dynamical system by visualizing in a binary square matrix the time-dependent behaviour of orbits in phase space. Recurrence plots can reveal large-scale and small-scale patterns of a dynamical system, such as EEG [50]. One measure of RQA is the determinism measure (DET), based on the diagonal lines of the recurrence plot. DET is defined as the ratio of recurrence points on the diagonals of the recurrence plot to all recurrence points. This measure was introduced as a determinism measure of the system, where the higher the DET value the more predictable the system is.Recurrence quantification analysis (RQA) is a useful technique for analyzing the non-linear dynamical behaviour of EEG data [49]. This technique is based on recurrence plots, which describe the recurrence property of a deterministic dynamical system by visualizing in a binary square matrix the time-dependent behaviour of orbits in phase space. Recurrence plots can reveal large-scale and small-scale patterns of a dynamical system, such as EEG [50]. One measure of RQA is the determinism measure (DET), based on the diagonal lines of the recurrence plot. DET is defined as the ratio of recurrence points on the diagonals of the recurrence plot to all recurrence points. This measure was introduced as a determinism measure of the system, where the higher the DET value the more predictable the system is.</p>
        <p>We computed the recurrence plots of our EEG data and their DET values by using the 
            <rs type="software">MATLAB</rs>
            <rs type="software">code</rs> provided by Ouyang in [51].
        </p>
        <p>In this section, we briefly describe the main features of the machine learning algorithms that we used, and how we configured them using the software tool 
            <rs type="software">KNIME</rs> [52].
        </p>
        <p>kNN is a lazy learning algorithm where the data model is simply the training data. The basic idea in classifying the test data is to find similar instances inside the training data. Similarity between data instances is computed through distance functions such as the Euclidean distance. In order to classify a new test instance, the distance from it to all the instances in the training data must be computed. Then, the most frequent class of the k nearest training instances (neighbours) is selected as the class of the new test instance.kNN is a lazy learning algorithm where the data model is simply the training data. The basic idea in classifying the test data is to find similar instances inside the training data. Similarity between data instances is computed through distance functions such as the Euclidean distance. In order to classify a new test instance, the distance from it to all the instances in the training data must be computed. Then, the most frequent class of the k nearest training instances (neighbours) is selected as the class of the new test instance.</p>
        <p>The parameter k is the key in kNN. In this study, we set k to a value of 3. Additionally, neighbors were weighted by distance, meaning that closer neighbors have greater influence on the resulting class than those farther away.The parameter k is the key in kNN. In this study, we set k to a value of 3. Additionally, neighbors were weighted by distance, meaning that closer neighbors have greater influence on the resulting class than those farther away.</p>
        <p>Logistic regression is a classification method based on approximating the output function.Logistic regression is a classification method based on approximating the output function.</p>
        <p>where w = (w 1 , …, w n ) are the coefficients (w 0 is the independent term) to be fitted based on the values of each training instance x. In order to use logistic regression for classification, the output of the function y is restricted to the values 0 and 1. The w coefficients are computed using a solver. In this study, we configured 
            <rs type="software">KNIME</rs> to use the stochastic average gradient solver because it performs well regardless of both the number of instances in the training data and the number of features of each instance [53].
        </p>
        <p>A decision tree is a classifier based on simple conditions or decision rules with the format IF-THEN-ELSE. These conditions are hierarchically chained within a tree-based structure in order to obtain the final decision [54]. Leaves of a decision tree are the output classes of the classifier. Each internal node of the tree contains a simple condition to check for a tuple &lt; feature, value &gt;. Children of a node are the possible results of the condition.A decision tree is a classifier based on simple conditions or decision rules with the format IF-THEN-ELSE. These conditions are hierarchically chained within a tree-based structure in order to obtain the final decision [54]. Leaves of a decision tree are the output classes of the classifier. Each internal node of the tree contains a simple condition to check for a tuple &lt; feature, value &gt;. Children of a node are the possible results of the condition.</p>
        <p>In the training step, the decision tree is constructed looking for a disjoint separation of the possible output classes in each node. The splitting criteria for each node is usually based on two measures: information gain and inequality (Gini index). For each node, the combination feature/value that minimizes the information gain or maximizes the Gini index is selected.In the training step, the decision tree is constructed looking for a disjoint separation of the possible output classes in each node. The splitting criteria for each node is usually based on two measures: information gain and inequality (Gini index). For each node, the combination feature/value that minimizes the information gain or maximizes the Gini index is selected.</p>
        <p>In this study, we configured KNIME by setting the Gini index as the quality measure of the node. Additionally, we used a pruning method to trim the tree in a post-processing step by replacing each node with its most popular class if the prediction accuracy did not decrease.In this study, we configured KNIME by setting the Gini index as the quality measure of the node. Additionally, we used a pruning method to trim the tree in a post-processing step by replacing each node with its most popular class if the prediction accuracy did not decrease.</p>
        <p>Random forest is a machine learning classifier that combines the results of several decision trees to produce an only output. Random forest is based on the concept of bagging, where several subsets are generated from the original training data and then used to train several decision trees independently. Each training data subset of size N is created selecting, randomly and with replacement, N instances from the original training data of size N. Each decision tree constructs a different model with an independent output class for each instance of the test data, and the final output of the random forest classifier corresponds to the most frequent class obtained.Random forest is a machine learning classifier that combines the results of several decision trees to produce an only output. Random forest is based on the concept of bagging, where several subsets are generated from the original training data and then used to train several decision trees independently. Each training data subset of size N is created selecting, randomly and with replacement, N instances from the original training data of size N. Each decision tree constructs a different model with an independent output class for each instance of the test data, and the final output of the random forest classifier corresponds to the most frequent class obtained.</p>
        <p>In this study, the random forest model configured in KNIME consisted of 100 decision trees where the Gini index was also used as the quality measure in each tree node.In this study, the random forest model configured in KNIME consisted of 100 decision trees where the Gini index was also used as the quality measure in each tree node.</p>
        <p>The goal of Support Vector Machines (SVM) is to find a hyperplane that separates the instances of two different classes. A hyperplane is a function of the form.The goal of Support Vector Machines (SVM) is to find a hyperplane that separates the instances of two different classes. A hyperplane is a function of the form.</p>
        <p>where w = (w 1 , …, w n ) are the coefficients (w 0 is the independent term) to be fitted based on the values of the features of each training instance x. From all possible hyperplanes dividing the training instances into two classes, SVM computes the one that obtains the maximum margin. The margin of a hyperplane is the maximum distance between border instances, and the training instances closest to the hyperplane are called the support vector (see Fig. 1).where w = (w 1 , …, w n ) are the coefficients (w 0 is the independent term) to be fitted based on the values of the features of each training instance x. From all possible hyperplanes dividing the training instances into two classes, SVM computes the one that obtains the maximum margin. The margin of a hyperplane is the maximum distance between border instances, and the training instances closest to the hyperplane are called the support vector (see Fig. 1).</p>
        <p>Usually, a hyperplane is not a good solution when the two classes are not linearly separable. To solve this problem, SVM uses kernel functions that increment the dimensionality of the training data by adding more variables through non-linear transformations. Typical examples of kernel functions are the polynomial kernel (the hyperplane is transformed into a polynomial of arbitrary grade) and the radial base function kernel (the hyperplane is transformed into a circular-shape function).Usually, a hyperplane is not a good solution when the two classes are not linearly separable. To solve this problem, SVM uses kernel functions that increment the dimensionality of the training data by adding more variables through non-linear transformations. Typical examples of kernel functions are the polynomial kernel (the hyperplane is transformed into a polynomial of arbitrary grade) and the radial base function kernel (the hyperplane is transformed into a circular-shape function).</p>
        <p>In this study, we used in KNIME a polynomial kernel of the form (γ &lt; w, x &gt; +r) d with d = 1, γ = 1.5 and r = 1.In this study, we used in KNIME a polynomial kernel of the form (γ &lt; w, x &gt; +r) d with d = 1, γ = 1.5 and r = 1.</p>
        <p>The pipeline proposed for computing and selecting the features in order to train the classifiers and test them using our schizophrenia EEG dataset is described graphically in Fig. 2.The pipeline proposed for computing and selecting the features in order to train the classifiers and test them using our schizophrenia EEG dataset is described graphically in Fig. 2.</p>
        <p>Firstly, we computed all the measures for each channel of the EEG data. In order to accomplish that computation, each EEG epoch of 160 s was split into sliding windows of 2 s with 90 % of overlapping [55] (see Fig. 2.A). The measures were computed for each sliding window and then the median of all of them was considered as the representative value of each measure in each channel of the epoch. In this way, we obtained 527 values (17 measures × 31 channels) for each EEG epoch (see Fig. 2Firstly, we computed all the measures for each channel of the EEG data. In order to accomplish that computation, each EEG epoch of 160 s was split into sliding windows of 2 s with 90 % of overlapping [55] (see Fig. 2.A). The measures were computed for each sliding window and then the median of all of them was considered as the representative value of each measure in each channel of the epoch. In this way, we obtained 527 values (17 measures × 31 channels) for each EEG epoch (see Fig. 2</p>
        <p>Using 527 different measures as features for each epoch in the machine learning models would not be a good strategy for a dataset of only 31 subjects. Moreover, such a large set of features could present additional problems, such as the appearance of confounding and correlated variables. So a dimensionality reduction process was performed in order to adjust the number of features to the size of our dataset. First, we selected those measures and channels where significant differences between groups (HC vs SCZ) were found in a t-test (see Table 2). In this way, the number of variables (pairs measure-channel) was reduced to 38. The scale of these variables is not the same, so a normalization process to [-1, 1] was also applied in order to adequately combine and compare values from different measures (see Fig. 2.C).Using 527 different measures as features for each epoch in the machine learning models would not be a good strategy for a dataset of only 31 subjects. Moreover, such a large set of features could present additional problems, such as the appearance of confounding and correlated variables. So a dimensionality reduction process was performed in order to adjust the number of features to the size of our dataset. First, we selected those measures and channels where significant differences between groups (HC vs SCZ) were found in a t-test (see Table 2). In this way, the number of variables (pairs measure-channel) was reduced to 38. The scale of these variables is not the same, so a normalization process to [-1, 1] was also applied in order to adequately combine and compare values from different measures (see Fig. 2.C).</p>
        <p>Finally, we used principal component analysis (PCA) [56] to identify those variables which preserved the maximum variance in the data. Fig. 3 shows the percentage of variance explained by the first ten principal components. The first and second principal components explain 41 % and 19 % of the variance, respectively. However, the third principal component only explains 9 % of the variance, and this percentage decreases for subsequent principal components. Therefore, we finally selected principal components one and two as the features characterizing each subject.Finally, we used principal component analysis (PCA) [56] to identify those variables which preserved the maximum variance in the data. Fig. 3 shows the percentage of variance explained by the first ten principal components. The first and second principal components explain 41 % and 19 % of the variance, respectively. However, the third principal component only explains 9 % of the variance, and this percentage decreases for subsequent principal components. Therefore, we finally selected principal components one and two as the features characterizing each subject.</p>
        <p>The transformed data with 31 subjects (20 healthy controls and 11 schizophrenia patients) and two features (two principal components of 38 variables showing significant differences between groups) was then divided into train and test sets by using a leave-one-out cross validation [57] (see Fig. 2.C).The transformed data with 31 subjects (20 healthy controls and 11 schizophrenia patients) and two features (two principal components of 38 variables showing significant differences between groups) was then divided into train and test sets by using a leave-one-out cross validation [57] (see Fig. 2.C).</p>
        <p>Table 3 shows the classification results obtained for each subject using the five machine learning algorithms, configured in KNIME as previously described in Sections 2.4.1 to 2.4.5. The classification result (SCZ or HC) was assigned when the probability returned by the crossvalidation process of the classification algorithm for that group was greater than 50 %. Three subjects (numbers five, ten and twenty-five) were misclassified with all algorithms. Fig. 4 shows a graphical representation of the dataset based on their two principal components. In this figure, subjects five, ten and twenty-five have been highlighted for a better understanding of their misclassification with all algorithms.Table 3 shows the classification results obtained for each subject using the five machine learning algorithms, configured in KNIME as previously described in Sections 2.4.1 to 2.4.5. The classification result (SCZ or HC) was assigned when the probability returned by the crossvalidation process of the classification algorithm for that group was greater than 50 %. Three subjects (numbers five, ten and twenty-five) were misclassified with all algorithms. Fig. 4 shows a graphical representation of the dataset based on their two principal components. In this figure, subjects five, ten and twenty-five have been highlighted for a better understanding of their misclassification with all algorithms.</p>
        <p>Table 4 shows the main scores obtained for each classification algorithm. The best accuracy was obtained for kNN. Nevertheless, SVM and Random Forest were the classification algorithms which obtained the best values for the AUC (area under curve) parameter of the ROC (Receiver Operating Characteristic) curve [58]. ROC curves of all classification algorithms are shown in Fig. 5.Table 4 shows the main scores obtained for each classification algorithm. The best accuracy was obtained for kNN. Nevertheless, SVM and Random Forest were the classification algorithms which obtained the best values for the AUC (area under curve) parameter of the ROC (Receiver Operating Characteristic) curve [58]. ROC curves of all classification algorithms are shown in Fig. 5.</p>
        <p>As well as EEG data analysis, we performed an SCIP-S test on both schizophrenia patients and healthy controls (see Table 1), so we have four additional cognitive measures which also showed significant differences between groups. In order to assess the effectiveness of the proposed classification method based on complexity measures extracted from EEG data, we repeated the classification pipeline shown in Fig. 2.C, but using only those four cognitive measures as input features. The scores obtained for each classification algorithm are shown in Table 5 and Fig. 6.As well as EEG data analysis, we performed an SCIP-S test on both schizophrenia patients and healthy controls (see Table 1), so we have four additional cognitive measures which also showed significant differences between groups. In order to assess the effectiveness of the proposed classification method based on complexity measures extracted from EEG data, we repeated the classification pipeline shown in Fig. 2.C, but using only those four cognitive measures as input features. The scores obtained for each classification algorithm are shown in Table 5 and Fig. 6.</p>
        <p>Classification results using cognitive measures as features are clearly worse than those obtained for measures extracted from the EEG data (see Table 4 and Fig. 5). The best AUC value achieved was 0.75 LR classifier, while an AUC of up to 0.89 was obtained for SVM when EEG measures were used, as previously seen.Classification results using cognitive measures as features are clearly worse than those obtained for measures extracted from the EEG data (see Table 4 and Fig. 5). The best AUC value achieved was 0.75 LR classifier, while an AUC of up to 0.89 was obtained for SVM when EEG measures were used, as previously seen.</p>
        <p>Table 6 shows, in a comparative way, the performance scores obtained in recent studies on machine learning classifiers for schizophrenia based on features extracted from EEG signals and those obtained with our methodology.Table 6 shows, in a comparative way, the performance scores obtained in recent studies on machine learning classifiers for schizophrenia based on features extracted from EEG signals and those obtained with our methodology.</p>
        <p>Finally, in order to analyze the relationship between cognitive and EEG measures we first calculated the average across the EEG channels for the variables in which we found significant differences between patients and healthy controls (see Table 2). We then calculated the Spearman correlation coefficient between these EEG variables and the four dimensions of the SCIP-S test (VLi, WM, VF and PS) which showed significant differences between groups (see Table 1). To control the type I error rate due to multiple comparisons, we used the false discovery rate approach [59]. After adjustment, only the correlations that were significant at a 95 % confidence level were considered. The results are presented in Fig. 7. No strong correlations were found between cognitive measures and EEG features in any group. VF showed weak correlations with CD, LZC, ApEn, SamEn, HC and DET in the HC group. For the SZC group, WM correlated weakly with LZC and HC.Finally, in order to analyze the relationship between cognitive and EEG measures we first calculated the average across the EEG channels for the variables in which we found significant differences between patients and healthy controls (see Table 2). We then calculated the Spearman correlation coefficient between these EEG variables and the four dimensions of the SCIP-S test (VLi, WM, VF and PS) which showed significant differences between groups (see Table 1). To control the type I error rate due to multiple comparisons, we used the false discovery rate approach [59]. After adjustment, only the correlations that were significant at a 95 % confidence level were considered. The results are presented in Fig. 7. No strong correlations were found between cognitive measures and EEG features in any group. VF showed weak correlations with CD, LZC, ApEn, SamEn, HC and DET in the HC group. For the SZC group, WM correlated weakly with LZC and HC.</p>
        <p>In the last few years, several studies have proposed using machine learning algorithms as a tool for distinguishing between patients with schizophrenia and healthy controls using EEG data. A wide range of measures have been used as features for those algorithms, including linear and non-linear measures. In the present study we have focused on providing a processing pipeline for computing and selecting those features which better differentiate schizophrenia patients from healthy controls. These features are then used as inputs in traditional machine learning techniques.In the last few years, several studies have proposed using machine learning algorithms as a tool for distinguishing between patients with schizophrenia and healthy controls using EEG data. A wide range of measures have been used as features for those algorithms, including linear and non-linear measures. In the present study we have focused on providing a processing pipeline for computing and selecting those features which better differentiate schizophrenia patients from healthy controls. These features are then used as inputs in traditional machine learning techniques.</p>
        <p>Our results show that complexity measures are the most effective measures in differentiating schizophrenia (see Table 2). HFD, MIMR, CD, LZC and especially LLE were the measures which showed significant differences in the highest number of channels. On the contrary, DFA, HA, TP, AP, BP and TE did not show significant differences in any channel. These findings are in line with recent studies, such as [22,23], which also showed the capability of complexity measures in distinguishing schizophrenia from healthy controls.Our results show that complexity measures are the most effective measures in differentiating schizophrenia (see Table 2). HFD, MIMR, CD, LZC and especially LLE were the measures which showed significant differences in the highest number of channels. On the contrary, DFA, HA, TP, AP, BP and TE did not show significant differences in any channel. These findings are in line with recent studies, such as [22,23], which also showed the capability of complexity measures in distinguishing schizophrenia from healthy controls.</p>
        <p>Channels FC6 and F8 are clearly the channels which present significant differences between subjects in a high number of measures, followed by channels FT10 and T8 (see Table 2). This finding implies that, according to our results, the main differences in brain activity between patients and healthy subjects are located in a clearly delimited area of the right hemisphere: the zone covered by the opercular area and the temporal pole. Some other studies [60] also reported those differences for complexity measures in the right brain hemisphere, but not delimiting the affected zone with the precision of the present study.Channels FC6 and F8 are clearly the channels which present significant differences between subjects in a high number of measures, followed by channels FT10 and T8 (see Table 2). This finding implies that, according to our results, the main differences in brain activity between patients and healthy subjects are located in a clearly delimited area of the right hemisphere: the zone covered by the opercular area and the temporal pole. Some other studies [60] also reported those differences for complexity measures in the right brain hemisphere, but not delimiting the affected zone with the precision of the present study.</p>
        <p>According to ROC curve analysis based on the AUC parameter (see Fig. 5), SVM (0.89), random forest (0.87), logistic regression (0.86) and kNN (0.86) performed very well. Only the decision tree algorithm performed a little worse, with an AUC of 0.68. This result suggests that our processing pipeline for computing and selecting the features works very efficiently regardless of the machine learning algorithm used. We believe that this fact is relevant, since it indicates that a good selection of the features would guarantee a good classification rate for use in other new or more sophisticated classification algorithms. This was not so common in several previous studies, where good accuracy values were obtained but only for a certain technique [8,18,20,60], performing poorly with the other algorithms tested [19,23].According to ROC curve analysis based on the AUC parameter (see Fig. 5), SVM (0.89), random forest (0.87), logistic regression (0.86) and kNN (0.86) performed very well. Only the decision tree algorithm performed a little worse, with an AUC of 0.68. This result suggests that our processing pipeline for computing and selecting the features works very efficiently regardless of the machine learning algorithm used. We believe that this fact is relevant, since it indicates that a good selection of the features would guarantee a good classification rate for use in other new or more sophisticated classification algorithms. This was not so common in several previous studies, where good accuracy values were obtained but only for a certain technique [8,18,20,60], performing poorly with the other algorithms tested [19,23].</p>
        <p>Cognitive deficits in all cognitive domains are a central and persistent feature of schizophrenia [61][62][63][64] and have traditionally been used as an endophenotype to diagnose the underlying pathogenesis [65]. For this reason, we compared the discriminative capacity of EEG-based measures with respect to measures of cognitive functioning (VLi, VF, WM and PS). This comparison revealed that features extracted from the EEG data allowed classifiers to perform better, as shown in Table 5 and Fig. 6. Several authors defend the hypothesis that cognitive impairment is a consequence of schizophrenia and, therefore, cognitive measures would be reliable criteria for its diagnosis [65,66]. Nevertheless, our results suggest that neurophysiological measures extracted from EEG data provide better features for classification algorithms. These results would be in line with research with schizophrenia patients, which shows that neurocognition only accounts for a moderate association with variance in functional outcome [67,68]. Furthermore, the analysis of correlations between cognitive scores and complexity measures (see Fig. 7) pointed in the direction that these two groups of measures are not related to each other, and provide different information. Only verbal fluency (VF) was weakly correlated with several complexity measures (CD, LZC, ApEn, SamEn, HC and DET) for the HC group. For the case of the SCZ group, only working memory (WM) correlated with LZC and HC, but this correlation was weak as well. Although we do not have an explanation for these results at the moment, it is interesting to note that different metrics are associated with different aspects in both groups; and this should be further explored in future studies.Cognitive deficits in all cognitive domains are a central and persistent feature of schizophrenia [61][62][63][64] and have traditionally been used as an endophenotype to diagnose the underlying pathogenesis [65]. For this reason, we compared the discriminative capacity of EEG-based measures with respect to measures of cognitive functioning (VLi, VF, WM and PS). This comparison revealed that features extracted from the EEG data allowed classifiers to perform better, as shown in Table 5 and Fig. 6. Several authors defend the hypothesis that cognitive impairment is a consequence of schizophrenia and, therefore, cognitive measures would be reliable criteria for its diagnosis [65,66]. Nevertheless, our results suggest that neurophysiological measures extracted from EEG data provide better features for classification algorithms. These results would be in line with research with schizophrenia patients, which shows that neurocognition only accounts for a moderate association with variance in functional outcome [67,68]. Furthermore, the analysis of correlations between cognitive scores and complexity measures (see Fig. 7) pointed in the direction that these two groups of measures are not related to each other, and provide different information. Only verbal fluency (VF) was weakly correlated with several complexity measures (CD, LZC, ApEn, SamEn, HC and DET) for the HC group. For the case of the SCZ group, only working memory (WM) correlated with LZC and HC, but this correlation was weak as well. Although we do not have an explanation for these results at the moment, it is interesting to note that different metrics are associated with different aspects in both groups; and this should be further explored in future studies.</p>
        <p>The main limitation of our study is not being able to provide the best classification accuracy among the state-of-the-art EEG schizophrenia classification tools. To the best of our knowledge, this rank is led by Sun et al [24] with their hybrid model, based on convolutional neural networks and long-short-term memory networks, that achieved an accuracy of 99 % in classifying schizophrenia (see Table 6). Nevertheless, we believe that our method, even while not performing as well as Sun's, shows a valuable benefit for most scientists: it is much simpler to reproduce. No image construction is needed and no complex neural network has to be trained. Our method relies on well-known EEG measures easily computable with public source code. These measures are used in standard machine learning algorithms, which are available and easily configurable in common statistical software tools.The main limitation of our study is not being able to provide the best classification accuracy among the state-of-the-art EEG schizophrenia classification tools. To the best of our knowledge, this rank is led by Sun et al [24] with their hybrid model, based on convolutional neural networks and long-short-term memory networks, that achieved an accuracy of 99 % in classifying schizophrenia (see Table 6). Nevertheless, we believe that our method, even while not performing as well as Sun's, shows a valuable benefit for most scientists: it is much simpler to reproduce. No image construction is needed and no complex neural network has to be trained. Our method relies on well-known EEG measures easily computable with public source code. These measures are used in standard machine learning algorithms, which are available and easily configurable in common statistical software tools.</p>
        <p>Another limitation of our study is the relatively small size of the dataset, so caution should be observed in generalizing our results to other datasets where patients underwent different treatment.Another limitation of our study is the relatively small size of the dataset, so caution should be observed in generalizing our results to other datasets where patients underwent different treatment.</p>
        <p>signal s is fractal-like with the dimension D.signal s is fractal-like with the dimension D.</p>
        <p>This work is part of the research project PID2019-105145RB-I00 supported by the Spanish Government (MCIN/AEI/10.13039/ 501100011033).This work is part of the research project PID2019-105145RB-I00 supported by the Spanish Government (MCIN/AEI/10.13039/ 501100011033).</p>
        <p>Data will be made available on request.Data will be made available on request.</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </text>
</tei>

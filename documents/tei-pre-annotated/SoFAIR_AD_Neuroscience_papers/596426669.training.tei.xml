<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:04+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Relationships between self-assembly phenomena and classical neural network concepts motivated our work. Pattern recognition by nucleation kinetics in multicomponent molecular self-assembly shares many conceptual features that have been well understood in the context of neural networks and machine learning. However, the self-assembling system is not simply a neural network by another name: in self-assembly there is no exact equivalent of a 'neuron' that performs a weighted linear sum with a threshold. Consequently, understanding the similarities and differences between these alternative implementations can provide insight into the nature of computation in distributed highly interconnected systems.Relationships between self-assembly phenomena and classical neural network concepts motivated our work. Pattern recognition by nucleation kinetics in multicomponent molecular self-assembly shares many conceptual features that have been well understood in the context of neural networks and machine learning. However, the self-assembling system is not simply a neural network by another name: in self-assembly there is no exact equivalent of a 'neuron' that performs a weighted linear sum with a threshold. Consequently, understanding the similarities and differences between these alternative implementations can provide insight into the nature of computation in distributed highly interconnected systems.</p>
        <p>In this supplemental discussion, we attempt to highlight the similarities while delineating important differences. This section is not necessary for understanding the content of our results, but may be helpful for appreciating the wider context of our work, especially for those who are not already immersed in the machine learning and neuroscience literature. Since we do not rely on advanced modern notions for these comparisons, a useful reference is Hertz, Krogh, and Palmer's "Introduction to the Theory of Neural Computation" 1 (1991). This text is notable for emphasizing connections to physics and biology. More modern texts that focus more on the mathematical and algorithmic perspectives include Bishop's "Pattern Recognition and Machine Learning" 2 (2006) and Goodfellow, Bengio, and Courville's "Deep Learning" 3 (2018). Our aim here is to isolate a few relevant themes and illustrate them with (perhaps over-simplified) examples, thus helping connect the dots between disparate fields, even though the relationships are not always precise.In this supplemental discussion, we attempt to highlight the similarities while delineating important differences. This section is not necessary for understanding the content of our results, but may be helpful for appreciating the wider context of our work, especially for those who are not already immersed in the machine learning and neuroscience literature. Since we do not rely on advanced modern notions for these comparisons, a useful reference is Hertz, Krogh, and Palmer's "Introduction to the Theory of Neural Computation" 1 (1991). This text is notable for emphasizing connections to physics and biology. More modern texts that focus more on the mathematical and algorithmic perspectives include Bishop's "Pattern Recognition and Machine Learning" 2 (2006) and Goodfellow, Bengio, and Courville's "Deep Learning" 3 (2018). Our aim here is to isolate a few relevant themes and illustrate them with (perhaps over-simplified) examples, thus helping connect the dots between disparate fields, even though the relationships are not always precise.</p>
        <p>Connecting the dots between the theory of neural computation and non-neural systems in biology and engineering has a long history. It was a relatively short path from early mathematical formalisms for neural function 4,5 to special-purpose electronic systems implementing neural architectures with discrete components 6 and VLSI transistor physics. 7 A prominent (though not universal) theme in such work was to engineer explicit circuit components for weights (e.g. resistors), summation (e.g. via Kirchoff's current law) and thresholds (e.g. transistors) so as to match the mathematical weighted linear sum and (hard or soft) threshold as developed in the theory. A similar "engineering" approach was used to establish that well-mixed chemistry is capable of mimicking neural computation, first by by Rössler 8 demonstrating how electrical circuits can be converted into theoretical chemical reactions with equivalent dynamics and in particular how excitable "spiking" behavior analogous to conduction in neural axons could be designed in the reaction-diffusion context, and later by Hjelmfelt et al 9 who showed, again theoretically, that a system of coupled enzymatic conversions of metabolites behaved like Hopfield neural networks, where weights correspond to enzyme concentrations, summation is of concentrations, and the sharp threshold arises from substrate interconversion. The general form of these observations was also reflected in theoretical models of genetic regulatory networks, wherein weights correspond to promiscuous transcription factor binding constants, the summation occurs at the probabilistic level to govern the occupancy of active transcriptional apparatus, and the nonlinearity arises from statistical mechanical considerations. 10,11 In the case of cellular signal transduction cascades, Bray 12 argued that despite their dynamics not having the exact form of a traditional neural network, they shared important features such as each unit integrating input from many other elements (perhaps nonlinearly) and having many parameters (such as reaction rate constants) that can tune the overall system function. Phenomenological modeling of genetic regulatory and signal transduction networks argued that, independent of explicit mechanisms, such systems share generic behaviors with neural networks, such as characteristic global dynamics 13 and emergent decision-making. 14 The prospect and potential of synthetic biology, in living cells or in cell-free systems, led to efforts to experimentally demonstrate biochemical analogs of neural networks. Two simplified cell-free systemsformulated as simplified genetic regulatory architectures that use RNA [15][16][17] or DNA [18][19][20] molecules as signals and just two or three enzymes (RNA polymerase and RNase, or DNA polymerase, nickase, and exonuclease) to produce Self-assembly and neural networks and destroy signals -were developed to implement neural computing units wherein each weight corresponds to a transcription template and thresholding involves competitive hybridization, and have been scaled (with limited connectivity) to about 15 units so far. Enzyme-free DNA strand displacement circuitry has scaled better, from a 4-bit Hopfield network 21 to a 100-bit winner-take-all classifier 22 but still, in existing designs, has required a new molecule specific to each weight in the neural network. These cell-free and enzyme-free biochemical circuits are explicit mechanistic implementations of neural network models, such that a theoretical parameters (weights) for the neural network can be translated into experimental parameters (concentrations) for the biochemical circuit that will, in principle, compute the same function. Cell-free neural networks using the full transcription/translation (TX-TL) system of the central dogma 23 and synthetic metabolic circuits 24 and genetic circuits [25][26][27] within living cells have not yet scaled beyond 4 units, reflecting the challenges of engineering with complex pre-existing biochemical components.Connecting the dots between the theory of neural computation and non-neural systems in biology and engineering has a long history. It was a relatively short path from early mathematical formalisms for neural function 4,5 to special-purpose electronic systems implementing neural architectures with discrete components 6 and VLSI transistor physics. 7 A prominent (though not universal) theme in such work was to engineer explicit circuit components for weights (e.g. resistors), summation (e.g. via Kirchoff's current law) and thresholds (e.g. transistors) so as to match the mathematical weighted linear sum and (hard or soft) threshold as developed in the theory. A similar "engineering" approach was used to establish that well-mixed chemistry is capable of mimicking neural computation, first by by Rössler 8 demonstrating how electrical circuits can be converted into theoretical chemical reactions with equivalent dynamics and in particular how excitable "spiking" behavior analogous to conduction in neural axons could be designed in the reaction-diffusion context, and later by Hjelmfelt et al 9 who showed, again theoretically, that a system of coupled enzymatic conversions of metabolites behaved like Hopfield neural networks, where weights correspond to enzyme concentrations, summation is of concentrations, and the sharp threshold arises from substrate interconversion. The general form of these observations was also reflected in theoretical models of genetic regulatory networks, wherein weights correspond to promiscuous transcription factor binding constants, the summation occurs at the probabilistic level to govern the occupancy of active transcriptional apparatus, and the nonlinearity arises from statistical mechanical considerations. 10,11 In the case of cellular signal transduction cascades, Bray 12 argued that despite their dynamics not having the exact form of a traditional neural network, they shared important features such as each unit integrating input from many other elements (perhaps nonlinearly) and having many parameters (such as reaction rate constants) that can tune the overall system function. Phenomenological modeling of genetic regulatory and signal transduction networks argued that, independent of explicit mechanisms, such systems share generic behaviors with neural networks, such as characteristic global dynamics 13 and emergent decision-making. 14 The prospect and potential of synthetic biology, in living cells or in cell-free systems, led to efforts to experimentally demonstrate biochemical analogs of neural networks. Two simplified cell-free systemsformulated as simplified genetic regulatory architectures that use RNA [15][16][17] or DNA [18][19][20] molecules as signals and just two or three enzymes (RNA polymerase and RNase, or DNA polymerase, nickase, and exonuclease) to produce Self-assembly and neural networks and destroy signals -were developed to implement neural computing units wherein each weight corresponds to a transcription template and thresholding involves competitive hybridization, and have been scaled (with limited connectivity) to about 15 units so far. Enzyme-free DNA strand displacement circuitry has scaled better, from a 4-bit Hopfield network 21 to a 100-bit winner-take-all classifier 22 but still, in existing designs, has required a new molecule specific to each weight in the neural network. These cell-free and enzyme-free biochemical circuits are explicit mechanistic implementations of neural network models, such that a theoretical parameters (weights) for the neural network can be translated into experimental parameters (concentrations) for the biochemical circuit that will, in principle, compute the same function. Cell-free neural networks using the full transcription/translation (TX-TL) system of the central dogma 23 and synthetic metabolic circuits 24 and genetic circuits [25][26][27] within living cells have not yet scaled beyond 4 units, reflecting the challenges of engineering with complex pre-existing biochemical components.</p>
        <p>In contrast to the foregoing mechanistic designs for neural computation in well-mixed chemical reaction networks, the basis for neural computation in molecular self-assembly was not established based on detailed mechanistic correspondences, but rather on general shared principles such as energy landscapes with multiple deep minima, Hebbian learning of interaction strengths, robustness of random wiring, and colocalization as a computational element. To reiterate: we are not saying that neural network models and self-assembly models are mathematically identical through a change of variables, or that there is an embedding such that one system can simulate an arbitrary instance of the other, so we make no claims that their range of capabilities coincide precisely. We are saying that they exhibit a variety of related phenomena, that similar concepts can be used to design and understand them, and that the natural questions in one domain are likely to stimulate related productive questions in the other domain.In contrast to the foregoing mechanistic designs for neural computation in well-mixed chemical reaction networks, the basis for neural computation in molecular self-assembly was not established based on detailed mechanistic correspondences, but rather on general shared principles such as energy landscapes with multiple deep minima, Hebbian learning of interaction strengths, robustness of random wiring, and colocalization as a computational element. To reiterate: we are not saying that neural network models and self-assembly models are mathematically identical through a change of variables, or that there is an embedding such that one system can simulate an arbitrary instance of the other, so we make no claims that their range of capabilities coincide precisely. We are saying that they exhibit a variety of related phenomena, that similar concepts can be used to design and understand them, and that the natural questions in one domain are likely to stimulate related productive questions in the other domain.</p>
        <p>Central to the broader perspective for neural networks is the theme of high-dimensional input to each neural computing unit -perhaps even all-to-all connectivity. In a trained neural network, it may be that most connections are of similar weak magnitude, or it may be that there a few dominant strong interactions that drive the behavior, or (more surprisingly) there can be a few easily-identified strong interactions yet it is the profusion of many weak interactions that actually drives the behavior 28 -that is to say, distinct collective dynamics may arise from the global interaction patterns. Multifarious self-assembly provides a concrete example of similar phenomena in engineered molecular self-assembly. However, the principles behind our work also hold lessons for biology. Molecular biology is often presented as a story of intentional highly-specific interactions between numerous molecular agents. As a consequence, promiscuous interactions between components are often seen as deleterious perturbations that degrade performance; for example, signals in one signaling pathway might leak into another and activate the wrong output or an attempted assembly of the ribosome from 60 different proteins might result in an uncontrolled toxic aggregate of ribosomal proteins. Nonetheless, in other contexts it has been appreciated that many promiscuous interactions may collectively provide a function if the interactions are cooperative, like neural networks. 12 While promiscuous interactions are pervasive and even inevitable [29][30][31] at the molecular scale, it is not yet well understood in what contexts and to what extent promiscuous interactions can be a blessing for molecular systems. Our work adds to a small but growing list of examples [32][33][34][35][36] of functionality that exploits molecular promiscuity and cannot be understood as deleterious perturbations of a conventional picture with specific interactions.Central to the broader perspective for neural networks is the theme of high-dimensional input to each neural computing unit -perhaps even all-to-all connectivity. In a trained neural network, it may be that most connections are of similar weak magnitude, or it may be that there a few dominant strong interactions that drive the behavior, or (more surprisingly) there can be a few easily-identified strong interactions yet it is the profusion of many weak interactions that actually drives the behavior 28 -that is to say, distinct collective dynamics may arise from the global interaction patterns. Multifarious self-assembly provides a concrete example of similar phenomena in engineered molecular self-assembly. However, the principles behind our work also hold lessons for biology. Molecular biology is often presented as a story of intentional highly-specific interactions between numerous molecular agents. As a consequence, promiscuous interactions between components are often seen as deleterious perturbations that degrade performance; for example, signals in one signaling pathway might leak into another and activate the wrong output or an attempted assembly of the ribosome from 60 different proteins might result in an uncontrolled toxic aggregate of ribosomal proteins. Nonetheless, in other contexts it has been appreciated that many promiscuous interactions may collectively provide a function if the interactions are cooperative, like neural networks. 12 While promiscuous interactions are pervasive and even inevitable [29][30][31] at the molecular scale, it is not yet well understood in what contexts and to what extent promiscuous interactions can be a blessing for molecular systems. Our work adds to a small but growing list of examples [32][33][34][35][36] of functionality that exploits molecular promiscuity and cannot be understood as deleterious perturbations of a conventional picture with specific interactions.</p>
        <p>Beyond biology, our work also provides a concrete example of neural computing principles within physical systems. From this perspective, neural networks can be seen as a special class of algorithms that effectively simulate a many-body system with two key characteristics: disordered interactions between the agents and strong non-linearities. Here we use competitive nucleation between different stable structures of a large collective of interacting molecules to perform high dimensional pattern recognition. But more generally, disordered interactions help sculpt complex decision surfaces in high dimensional input space while non-linearities then map these different regions to different outputs. Thus, the fact that certain physical and molecular systems can have disordered interactions and non-linearities in their collective phenomena suggests that neural network-like behavior might naturally emerge in these systems.Beyond biology, our work also provides a concrete example of neural computing principles within physical systems. From this perspective, neural networks can be seen as a special class of algorithms that effectively simulate a many-body system with two key characteristics: disordered interactions between the agents and strong non-linearities. Here we use competitive nucleation between different stable structures of a large collective of interacting molecules to perform high dimensional pattern recognition. But more generally, disordered interactions help sculpt complex decision surfaces in high dimensional input space while non-linearities then map these different regions to different outputs. Thus, the fact that certain physical and molecular systems can have disordered interactions and non-linearities in their collective phenomena suggests that neural network-like behavior might naturally emerge in these systems.</p>
        <p>The following technical discussion presumes familiarity with the main paper. Section 1.1 discusses connections to the seminal Hopfield associative memory model for recurrent neural networks, and Section 1.2 presents the generalization to Boltzmann machines. Section 1.3 illustrates a concrete relation between the architecture of our self-assembling system and a real neural system first identified in rodents. Section 1.4 then provides a broader machine learning perspective on the computation achieved by the self-assembling system. Finally, Section 1.5 discusses how our training procedure relates to phenomena underlying reservoir computing.The following technical discussion presumes familiarity with the main paper. Section 1.1 discusses connections to the seminal Hopfield associative memory model for recurrent neural networks, and Section 1.2 presents the generalization to Boltzmann machines. Section 1.3 illustrates a concrete relation between the architecture of our self-assembling system and a real neural system first identified in rodents. Section 1.4 then provides a broader machine learning perspective on the computation achieved by the self-assembling system. Finally, Section 1.5 discusses how our training procedure relates to phenomena underlying reservoir computing.</p>
        <p>The Hopfield associative memory is a simple Ising-like model for the memorization of binary patterns and their recall from partial or imperfect information. Given a set of M target patterns (aka memories) {x 1 , . . . , x M }, each being a vector of N spins so x α ∈ {-1, +1} N , weights for a neural network are set by a direct Hebbian learning rule:The Hopfield associative memory is a simple Ising-like model for the memorization of binary patterns and their recall from partial or imperfect information. Given a set of M target patterns (aka memories) {x 1 , . . . , x M }, each being a vector of N spins so x α ∈ {-1, +1} N , weights for a neural network are set by a direct Hebbian learning rule:</p>
        <p>w α i,j = x α i x α j that implements a "fire together, wire together" principle for memory stimulus α being active that increases the strength of synaptic connections between neurons that are active at the same time. For multiple memories, simple Hebbian learning linearly accumulates these synaptic updates and results in the sum:w α i,j = x α i x α j that implements a "fire together, wire together" principle for memory stimulus α being active that increases the strength of synaptic connections between neurons that are active at the same time. For multiple memories, simple Hebbian learning linearly accumulates these synaptic updates and results in the sum:</p>
        <p>for i = j and w i,i = 0.for i = j and w i,i = 0.</p>
        <p>In addition to coupling between units via w i,j , Hopfield networks can include a unit-specific bias term b i that provides a threshold for activation. Hebbian learning sets b α i = x i and thus b i = α b α i , which is equivalent to standard Hebbian learning of coupling to a special unit that is always clamped on.In addition to coupling between units via w i,j , Hopfield networks can include a unit-specific bias term b i that provides a threshold for activation. Hebbian learning sets b α i = x i and thus b i = α b α i , which is equivalent to standard Hebbian learning of coupling to a special unit that is always clamped on.</p>
        <p>Dynamics are as follows. Starting from an arbitrary initial state x, updates proceed one-at-a-time asynchronously according to linear threshold unit semantics:Dynamics are as follows. Starting from an arbitrary initial state x, updates proceed one-at-a-time asynchronously according to linear threshold unit semantics:</p>
        <p>The dynamics respect an energy functionThe dynamics respect an energy function</p>
        <p>that is strictly decreasing every time a unit changes state, and is bounded below so that convergence to a stable local minimum is guaranteed. This implies that, if the target memories are the only stable local minima, then initializing the network with an arbitrary pattern will result in convergence to one or another of the target memories -which we may call associative memory recall because there is a sense in which the "most similar" memory is identified. This is guaranteed to be the case if there is just a single memory (along with its exact inverse if biases are not used) that corresponds to a single basin in the energy landscape. As more memories are learned, the Hebbian rule results in additional basins being dug into the landscape, which do not significantly overlap so long as the memories are sufficiently distinct. Thus, the dynamics can be characterized as having point attractors. However, as the number of memories grows, for a fixed N , it becomes possible and even likely that spurious memories (local minima that are not target memories used in training) will appear and even that target memories will become unstable (perhaps defaulting to a similar spurious memory nearby).that is strictly decreasing every time a unit changes state, and is bounded below so that convergence to a stable local minimum is guaranteed. This implies that, if the target memories are the only stable local minima, then initializing the network with an arbitrary pattern will result in convergence to one or another of the target memories -which we may call associative memory recall because there is a sense in which the "most similar" memory is identified. This is guaranteed to be the case if there is just a single memory (along with its exact inverse if biases are not used) that corresponds to a single basin in the energy landscape. As more memories are learned, the Hebbian rule results in additional basins being dug into the landscape, which do not significantly overlap so long as the memories are sufficiently distinct. Thus, the dynamics can be characterized as having point attractors. However, as the number of memories grows, for a fixed N , it becomes possible and even likely that spurious memories (local minima that are not target memories used in training) will appear and even that target memories will become unstable (perhaps defaulting to a similar spurious memory nearby).</p>
        <p>Problems with unstable target memories and spurious memories are exacerbated by correlations between the target memories, for example, if two memory patterns are identical for a substantial number of bits. For the case of uniform random memory patterns, which will be statistically near-orthogonal, it is possible to determine a capacity for how many memories the Hopfield network can store reliably, depending on criteria for "reliable": if a few bit-errors in recalled memories can be tolerated, then M ≤ 0.138N memories can be stored with high probability; for perfect recall with high probability, the limit is O(N/ log N ). That said, improvements on the basic Hebbian learning rule are possible, for example using the Perceptron learning rule that only changes weights when they are not already sufficient for correct behavior, or more advanced global linear algebra methods to set weights analytically.Problems with unstable target memories and spurious memories are exacerbated by correlations between the target memories, for example, if two memory patterns are identical for a substantial number of bits. For the case of uniform random memory patterns, which will be statistically near-orthogonal, it is possible to determine a capacity for how many memories the Hopfield network can store reliably, depending on criteria for "reliable": if a few bit-errors in recalled memories can be tolerated, then M ≤ 0.138N memories can be stored with high probability; for perfect recall with high probability, the limit is O(N/ log N ). That said, improvements on the basic Hebbian learning rule are possible, for example using the Perceptron learning rule that only changes weights when they are not already sufficient for correct behavior, or more advanced global linear algebra methods to set weights analytically.</p>
        <p>Parallels with multifarious self-assembly: In the original work on multifarious self-assembly 37 the M target shapes could be considered uniform random memories -the same N tiles arranged randomly within a square. Promiscuous pairwise binding energies are determined by a Hebbian learning rule we call "get together, glue together", which is analogous to the familiar "fire together, wire together". Without postulating a specific molecular mechanism, we examine the consequence of any process whose net effect over time is to increase the binding affinities of molecules that have been brought together into particular geometric arrangements by external circumstances. (For example, consider membrane proteins in a fluid mosaic that bind to an external surface exhibiting a particular spatial arrangement of ligands, which thus re-arrange the membrane proteins correspondingly. Each environmentally-driven arrangement would be called a "memory", and the "get together, glue together" rule would result in certain membrane protein species developing specific affinities for each other, by the unknown hypothetical mechanism. The consequence is that after learning, the membrane proteins would tend to associate with each other in geometrical arrangements matching or similar to the memories, even in the absence of environmentally-driven stimuli.) Formally, for in-silico training, the Hebbian self-assembly learning rule is applied to each memory α and then summed over memories, J α,δ i,j = E 0 if tile i is the δ-neighbor of tile j in memory α, for direction δ ∈ {north, east, south, west} 0 otherwise where E 0 &gt; 0 is a standard interaction strength; thus, in total,Parallels with multifarious self-assembly: In the original work on multifarious self-assembly 37 the M target shapes could be considered uniform random memories -the same N tiles arranged randomly within a square. Promiscuous pairwise binding energies are determined by a Hebbian learning rule we call "get together, glue together", which is analogous to the familiar "fire together, wire together". Without postulating a specific molecular mechanism, we examine the consequence of any process whose net effect over time is to increase the binding affinities of molecules that have been brought together into particular geometric arrangements by external circumstances. (For example, consider membrane proteins in a fluid mosaic that bind to an external surface exhibiting a particular spatial arrangement of ligands, which thus re-arrange the membrane proteins correspondingly. Each environmentally-driven arrangement would be called a "memory", and the "get together, glue together" rule would result in certain membrane protein species developing specific affinities for each other, by the unknown hypothetical mechanism. The consequence is that after learning, the membrane proteins would tend to associate with each other in geometrical arrangements matching or similar to the memories, even in the absence of environmentally-driven stimuli.) Formally, for in-silico training, the Hebbian self-assembly learning rule is applied to each memory α and then summed over memories, J α,δ i,j = E 0 if tile i is the δ-neighbor of tile j in memory α, for direction δ ∈ {north, east, south, west} 0 otherwise where E 0 &gt; 0 is a standard interaction strength; thus, in total,</p>
        <p>..</p>
        <p>(The main text of the original work 37 oversimplifies this equation by ignoring orientations; here we are following the explanation from equations S14 and S15 of the SI, which is necessary for correct function.) The resulting interaction matrix can potentially contain entries of size 2E 0 , 3E 0 , . . . for i, j that are neighbors in multiple memories. As uniform binding energies yields better performance, this matrix was 'clipped', 37 J clipped,δ i,j = min(J δ i,j , E 0 ), giving a Perceptron-like learning rule that avoids reinforcing interactions that appear in multiple stored memories. Thus, the interaction matrix is essentially binary, with energies being either 0 or E 0 based on whether the tiles should interact or not interact. In this model, it is assumed that each of the N molecules are capable of being adjusted to have arbitrary promiscuous interactions with any chosen subset of the other molecules; i.e., there are no constraints on which interaction matrices are considered to be implementable.(The main text of the original work 37 oversimplifies this equation by ignoring orientations; here we are following the explanation from equations S14 and S15 of the SI, which is necessary for correct function.) The resulting interaction matrix can potentially contain entries of size 2E 0 , 3E 0 , . . . for i, j that are neighbors in multiple memories. As uniform binding energies yields better performance, this matrix was 'clipped', 37 J clipped,δ i,j = min(J δ i,j , E 0 ), giving a Perceptron-like learning rule that avoids reinforcing interactions that appear in multiple stored memories. Thus, the interaction matrix is essentially binary, with energies being either 0 or E 0 based on whether the tiles should interact or not interact. In this model, it is assumed that each of the N molecules are capable of being adjusted to have arbitrary promiscuous interactions with any chosen subset of the other molecules; i.e., there are no constraints on which interaction matrices are considered to be implementable.</p>
        <p>DNA implementation of interactions derived from Hebbian learning: Because our DNA tile architecture relies on perfect Watson-Crick complementarity for matching domains, we cannot use this Hebbian rule directly: only interaction matrices that respect a certain transitivity constraint can be directly implemented. For example, if the "north" of tile i can bind to the "south" of tiles k and l, and if the "south" of tile k can bind to the "north" of tiles i and j, then by Watson-Crick transitivity it must also be that the "north" of tile j can bind to the "south" of tile l, even though this binding may not be dictated by the Hebbian learning rule (Figure S1.1). More generally, with M shapes, interior tiles will have M (probably distinct) neighbors in each direction δ, all of whom will have to have the same domain sequence, leading to a percolation-like system of constraints that force too many (possibly all) domain sequences to be identical or complementary.DNA implementation of interactions derived from Hebbian learning: Because our DNA tile architecture relies on perfect Watson-Crick complementarity for matching domains, we cannot use this Hebbian rule directly: only interaction matrices that respect a certain transitivity constraint can be directly implemented. For example, if the "north" of tile i can bind to the "south" of tiles k and l, and if the "south" of tile k can bind to the "north" of tiles i and j, then by Watson-Crick transitivity it must also be that the "north" of tile j can bind to the "south" of tile l, even though this binding may not be dictated by the Hebbian learning rule (Figure S1.1). More generally, with M shapes, interior tiles will have M (probably distinct) neighbors in each direction δ, all of whom will have to have the same domain sequence, leading to a percolation-like system of constraints that force too many (possibly all) domain sequences to be identical or complementary.</p>
        <p>Designing a multifarious DNA system based on Hebbian learning is nonetheless possible, with a slight change of perspective. Consider M shapes on a square lattice, superimposing a black and white checkerboard pattern, and suppose that each shape has N black squares. These will be the shared tiles, and all their 4N domains will be distinct. In each shape, we randomly assign each of N shared tiles to the N black squares so that each shared tile appears in M locations, and we introduce a new unique tile for each white square, whose domains are determined by its neighboring black tiles. Thus, its black and white squares are balanced, there will be N shared black tiles and M N shape-specific white tiles, utilizing 4N binding domains. In this construction, we can consider the white tiles to implement a form of Hebbian learning that "memorizes" the M shapes based on the shared-tile adjacency interactions 1 . This construction is called the "simple checkerboard" in Extended Data Fig. E3, which also discusses 1 Beyond noting that the black shared tiles and white shape-specific tiles of the checkerboard construction could correspond to pattern species and weight species, respectively, required to implement the results of a conceptual Hebbian learning analog, the checkerboard construction also suggest hypothetical mechanisms that could physically implement the learning process itself at the molecular level. In this footnote, we loosely consider two such hypothetical implementations that could lead to "get together, glue together" dynamics. The first would be an explicit direct mechanism in which some proximity-directed ligation or synthesis reaction 38,39 creates weight species W ij when pattern species X i and X j have been brought near each other by environmentally-driven events. For example, there might be a full set of single-domain oligonucleotides that have been activated so as to covalently link with each other at a very slow rate that is accelerated if they are bound to adjacent SST tiles (the pattern species). While the resulting two-domain "tiles" (the synthesized weight species) would not have the standard four-domain SST format, they would still be likely to stabilize a multifarious set of shapes that geometrically arrange the shared pattern species similarly to the environmentally-driven arrangements. A conceptual mechanism of this type is illustrated in Extended Data Fig. E1. The second hypothetical would be an indirect mechanism that nonetheless also ends up stabilizing a multifarious pattern of binding affinities. In this case our pattern species will be arbitrary, perhaps proteins, and there will be no weight species; promiscuous binding affinities between the protein pattern species will be evolved to stabilize the environmentally-driven geometrical arrangements. For example, Hochberg et al 40 have argued that evolution naturally stabilizes and entrenches hydrophobic interactions between proteins in multicomponent complexes, as a consequence of the need to avoid harmful aggregation. If the relevant proteins participate in multiple distinct multicomponent complexes, as documented by Sartori and Leibler, 41 a multifarious system could effectively be learned by evolution. Such approaches could be compared to prior approaches to brain-like learning in DNA computing. [42][43][44] (a) Target shape layouts If a north-south pair (e.g. tile 5 being above tile 3, denoted 5S:3N for the glues that must match) is in an equivalence class, then so must any other pair that appears in any shape and contains either of the two glues. Here, this leads to four north-south equivalence classes, but -remarkably, due to full percolation of constraints -just one east-west equivalence class. (c) Each (color-coded) equivalence class will correspond to a specific glue type (i.e. a Watson-Crick complementary pair of DNA domain sequences). Together they specify the maximally-distinct design of tiles consistent with the layout. (d) The percolation of transitive Watson-Crick binding allows tiles to be adjacent that were not adjacent in any of the three target shape layouts. This gives rise to alternate possible layouts -not only 3 × 3 but also larger assemblies that are not shown.Designing a multifarious DNA system based on Hebbian learning is nonetheless possible, with a slight change of perspective. Consider M shapes on a square lattice, superimposing a black and white checkerboard pattern, and suppose that each shape has N black squares. These will be the shared tiles, and all their 4N domains will be distinct. In each shape, we randomly assign each of N shared tiles to the N black squares so that each shared tile appears in M locations, and we introduce a new unique tile for each white square, whose domains are determined by its neighboring black tiles. Thus, its black and white squares are balanced, there will be N shared black tiles and M N shape-specific white tiles, utilizing 4N binding domains. In this construction, we can consider the white tiles to implement a form of Hebbian learning that "memorizes" the M shapes based on the shared-tile adjacency interactions 1 . This construction is called the "simple checkerboard" in Extended Data Fig. E3, which also discusses 1 Beyond noting that the black shared tiles and white shape-specific tiles of the checkerboard construction could correspond to pattern species and weight species, respectively, required to implement the results of a conceptual Hebbian learning analog, the checkerboard construction also suggest hypothetical mechanisms that could physically implement the learning process itself at the molecular level. In this footnote, we loosely consider two such hypothetical implementations that could lead to "get together, glue together" dynamics. The first would be an explicit direct mechanism in which some proximity-directed ligation or synthesis reaction 38,39 creates weight species W ij when pattern species X i and X j have been brought near each other by environmentally-driven events. For example, there might be a full set of single-domain oligonucleotides that have been activated so as to covalently link with each other at a very slow rate that is accelerated if they are bound to adjacent SST tiles (the pattern species). While the resulting two-domain "tiles" (the synthesized weight species) would not have the standard four-domain SST format, they would still be likely to stabilize a multifarious set of shapes that geometrically arrange the shared pattern species similarly to the environmentally-driven arrangements. A conceptual mechanism of this type is illustrated in Extended Data Fig. E1. The second hypothetical would be an indirect mechanism that nonetheless also ends up stabilizing a multifarious pattern of binding affinities. In this case our pattern species will be arbitrary, perhaps proteins, and there will be no weight species; promiscuous binding affinities between the protein pattern species will be evolved to stabilize the environmentally-driven geometrical arrangements. For example, Hochberg et al 40 have argued that evolution naturally stabilizes and entrenches hydrophobic interactions between proteins in multicomponent complexes, as a consequence of the need to avoid harmful aggregation. If the relevant proteins participate in multiple distinct multicomponent complexes, as documented by Sartori and Leibler, 41 a multifarious system could effectively be learned by evolution. Such approaches could be compared to prior approaches to brain-like learning in DNA computing. [42][43][44] (a) Target shape layouts If a north-south pair (e.g. tile 5 being above tile 3, denoted 5S:3N for the glues that must match) is in an equivalence class, then so must any other pair that appears in any shape and contains either of the two glues. Here, this leads to four north-south equivalence classes, but -remarkably, due to full percolation of constraints -just one east-west equivalence class. (c) Each (color-coded) equivalence class will correspond to a specific glue type (i.e. a Watson-Crick complementary pair of DNA domain sequences). Together they specify the maximally-distinct design of tiles consistent with the layout. (d) The percolation of transitive Watson-Crick binding allows tiles to be adjacent that were not adjacent in any of the three target shape layouts. This gives rise to alternate possible layouts -not only 3 × 3 but also larger assemblies that are not shown.</p>
        <p>an improvement ("guarded edges") that ensures shared tiles on the boundary of a shape will also be on a similar boundary in other shapes, so that the boundary domains of all completed shapes can be implemented in DNA using minimally-interacting poly-T sequences that reduce molecular aggregation. Our actual DNA implementation used a more systematic algorithm to optimize self-assembly robustness to rigorously ensure a proofreading self-assembly property, which is only statistically ensured by the checkerboard construction. (Note that while the checkerboard design ensures that tiles can bind to each other if and only if they are adjacent in one of the target memory shapes, our optimization does not ensure that property, and therefore the transitivity of Watson-Crick binding may give rise to additional possible interactions (see Figure S3.3). Because of proofreading, these interactions do not induce assembly errors. The putative advantage of the optimization is that fewer DNA domain sequences are needed -698 versus the 992 that a checkerboard design would entail -and thus sequence design can ensure more uniform binding energies and better orthogonality.an improvement ("guarded edges") that ensures shared tiles on the boundary of a shape will also be on a similar boundary in other shapes, so that the boundary domains of all completed shapes can be implemented in DNA using minimally-interacting poly-T sequences that reduce molecular aggregation. Our actual DNA implementation used a more systematic algorithm to optimize self-assembly robustness to rigorously ensure a proofreading self-assembly property, which is only statistically ensured by the checkerboard construction. (Note that while the checkerboard design ensures that tiles can bind to each other if and only if they are adjacent in one of the target memory shapes, our optimization does not ensure that property, and therefore the transitivity of Watson-Crick binding may give rise to additional possible interactions (see Figure S3.3). Because of proofreading, these interactions do not induce assembly errors. The putative advantage of the optimization is that fewer DNA domain sequences are needed -698 versus the 992 that a checkerboard design would entail -and thus sequence design can ensure more uniform binding energies and better orthogonality.</p>
        <p>Energy landscape: Regardless of how the tile set is obtained, the tiles and their binding energies induce an energy landscape that governs the self-assembly process. In our idealized model, the chemical potential for an assembly A isEnergy landscape: Regardless of how the tile set is obtained, the tiles and their binding energies induce an energy landscape that governs the self-assembly process. In our idealized model, the chemical potential for an assembly A is</p>
        <p>for binary variables x i p ∈ {0, 1} that indicate position p in assembly A is occupied by tile type i, with δ(p, p ) being the relative orientation of neighboring positions p and p or else zero, for which J 0 i,j = 0, and with Θ i = RT ln [ti] u0 being the concentration-dependent cost of tile addition. (Typically, J δ i,j ≥ 0 while Θ i &lt; 0, although this latter depends on the reference concentration e.g. u 0 = 1 M.) This can be seen as a quadratic (pairwise) energy function like the Hopfield model with biases, with the coupling energies J playing the role of the synaptic weights w and the (inverted) chemical potentials Θ playing the role of the biases b. Additionally, however, the self-assembly system must observe constraints on x i p imposing that at most a single tile type can be in a given location, and that the assembly remains as a single connected component. From the perspective of a single assembly growing in a solution where tile concentrations are unchanging, thermodynamically favorable tile additions correspond to downhill steps in this energy function. Ideally, the Hebbian learning principle would ensure that each local energy minimum corresponds to the correct assembly of one of the target memorized shapes -and this is almost true. However, "off-pathway" assembly such as the chimeric structures illustrated in Extended Data Fig. E3 can result in even lower energies. The "checkerboard with guarded edges" design introduces a large barrier to the formation of chimera by enforcing that tiles on the boundary of each shape present non-binding domains on the outside, while the optimized design of the experimental system additionally ensures a local proofreading property during growth. For such designs, it's reasonable then to assume that the assembly process remain "on-pathway" with respect to growing one of the target memorized shapes, and with that restriction (together with there not being too many shapes to be memorized), the dominant energy basins are indeed the target memories -that is, they are point attractors.for binary variables x i p ∈ {0, 1} that indicate position p in assembly A is occupied by tile type i, with δ(p, p ) being the relative orientation of neighboring positions p and p or else zero, for which J 0 i,j = 0, and with Θ i = RT ln [ti] u0 being the concentration-dependent cost of tile addition. (Typically, J δ i,j ≥ 0 while Θ i &lt; 0, although this latter depends on the reference concentration e.g. u 0 = 1 M.) This can be seen as a quadratic (pairwise) energy function like the Hopfield model with biases, with the coupling energies J playing the role of the synaptic weights w and the (inverted) chemical potentials Θ playing the role of the biases b. Additionally, however, the self-assembly system must observe constraints on x i p imposing that at most a single tile type can be in a given location, and that the assembly remains as a single connected component. From the perspective of a single assembly growing in a solution where tile concentrations are unchanging, thermodynamically favorable tile additions correspond to downhill steps in this energy function. Ideally, the Hebbian learning principle would ensure that each local energy minimum corresponds to the correct assembly of one of the target memorized shapes -and this is almost true. However, "off-pathway" assembly such as the chimeric structures illustrated in Extended Data Fig. E3 can result in even lower energies. The "checkerboard with guarded edges" design introduces a large barrier to the formation of chimera by enforcing that tiles on the boundary of each shape present non-binding domains on the outside, while the optimized design of the experimental system additionally ensures a local proofreading property during growth. For such designs, it's reasonable then to assume that the assembly process remain "on-pathway" with respect to growing one of the target memorized shapes, and with that restriction (together with there not being too many shapes to be memorized), the dominant energy basins are indeed the target memories -that is, they are point attractors.</p>
        <p>With this restriction, thermodynamically favorable tile additions have a direct correspondence to a neural linear threshold unit. To see this more clearly, note that the energy landscape assumes a simpler form when the assembly A is a (not necessarily proper) subset of one of the memorized shapes, 1 ≤ α ≤ M . For well-designed multifarious systems such as ours, where no multi-tile subassembly of a memorized shape is also a subassembly of another memorized shape and where proofreading largely prevents assembly pathways that form chimeras and other erroneous assemblies, the simpler form is sufficient to treat on-pathway processes. In this case, since we are assuming that no tile occurs more than once in a given shape, its position in the shape is implied and we only need the binary variables x i that indicate tile i is present. For the same reason, we can use a shape-specific coupling energy J α i,j that is zero except for tiles that, in α, are adjacent. Thus, the chemical potential for assembly A that is a subassembly of shape α is:With this restriction, thermodynamically favorable tile additions have a direct correspondence to a neural linear threshold unit. To see this more clearly, note that the energy landscape assumes a simpler form when the assembly A is a (not necessarily proper) subset of one of the memorized shapes, 1 ≤ α ≤ M . For well-designed multifarious systems such as ours, where no multi-tile subassembly of a memorized shape is also a subassembly of another memorized shape and where proofreading largely prevents assembly pathways that form chimeras and other erroneous assemblies, the simpler form is sufficient to treat on-pathway processes. In this case, since we are assuming that no tile occurs more than once in a given shape, its position in the shape is implied and we only need the binary variables x i that indicate tile i is present. For the same reason, we can use a shape-specific coupling energy J α i,j that is zero except for tiles that, in α, are adjacent. Thus, the chemical potential for assembly A that is a subassembly of shape α is:</p>
        <p>which has the exact form of the Hopfield model (eq. 1.1 with w i,j = J i,j and b i = Θ i ) except that the coupling matrix depends on which shape is growing. Furthermore, the dynamics is not identical to the Hopfield model: there, any neuron could potentially flip state at any time, whereas in multifarious self-assembly, the only allowed attachments are positions on the edge of the assembly, as it must remain connected. (Reversible self-assembly, in which tiles sometimes detach, would have a similar correspondence with the Boltzmann machine perspective discussed below.) That said, when a tile does attach, the energy change is the exact same linear weighted sum as in the Hopfield model:which has the exact form of the Hopfield model (eq. 1.1 with w i,j = J i,j and b i = Θ i ) except that the coupling matrix depends on which shape is growing. Furthermore, the dynamics is not identical to the Hopfield model: there, any neuron could potentially flip state at any time, whereas in multifarious self-assembly, the only allowed attachments are positions on the edge of the assembly, as it must remain connected. (Reversible self-assembly, in which tiles sometimes detach, would have a similar correspondence with the Boltzmann machine perspective discussed below.) That said, when a tile does attach, the energy change is the exact same linear weighted sum as in the Hopfield model:</p>
        <p>which is downhill exactly when the sum is positive. (If a tile detaches, the energy change is the opposite.) This is interesting because there is no explicit molecular representation of the summation process (unlike well-mixed biochemical neural networks where distinct molecules and reactions are employed for each multiplication, addition, and thresholding step [15, 21, 22]) -rather, the summation is implicit in the energy landscape. Successfully trained multifarious self-assembly performs associative memory recall from an initial seed assembly by taking a series of downhill (and occasional uphill) steps until a deep local minimum is obtained, corresponding to a completed shape. (Note that the potential for arbitrarily large self-assembled structures, such as tubes or repeated chimeric errors, implies that there could be infinitely many x i p in a full model of the physics, so our metaphor only applies if we restrict the size by fiat.)which is downhill exactly when the sum is positive. (If a tile detaches, the energy change is the opposite.) This is interesting because there is no explicit molecular representation of the summation process (unlike well-mixed biochemical neural networks where distinct molecules and reactions are employed for each multiplication, addition, and thresholding step [15, 21, 22]) -rather, the summation is implicit in the energy landscape. Successfully trained multifarious self-assembly performs associative memory recall from an initial seed assembly by taking a series of downhill (and occasional uphill) steps until a deep local minimum is obtained, corresponding to a completed shape. (Note that the potential for arbitrarily large self-assembled structures, such as tubes or repeated chimeric errors, implies that there could be infinitely many x i p in a full model of the physics, so our metaphor only applies if we restrict the size by fiat.)</p>
        <p>Our pattern recognition experiments do not involve seed assemblies; instead, concentration patterns are presented by setting biases Θ i in the above energy functional. (Hopfield models can also be used in such a modality, with inputs presented as biases rather than initial conditions for firing rates.) In this modality, the self-assembly model shows robustness to noise in pattern recognition, much like a Hopfield model. For example, if a small fraction of tiles have perturbed concentrations, nucleation rates are not strongly affected unless all of those tiles are strongly colocalized. Consequently, pattern recognition is robust, especially to random 'speckle' noise.Our pattern recognition experiments do not involve seed assemblies; instead, concentration patterns are presented by setting biases Θ i in the above energy functional. (Hopfield models can also be used in such a modality, with inputs presented as biases rather than initial conditions for firing rates.) In this modality, the self-assembly model shows robustness to noise in pattern recognition, much like a Hopfield model. For example, if a small fraction of tiles have perturbed concentrations, nucleation rates are not strongly affected unless all of those tiles are strongly colocalized. Consequently, pattern recognition is robust, especially to random 'speckle' noise.</p>
        <p>The network of interactions between species in the self-assembly model reflect the 2-dimensional (2D) geometry of structures. Each species potentially interacts with ∼ 4M other species where M is the number of memories stored. A subset of the connections (e.g., blue in Extended Data Fig. E1) will form a 2-dimensional lattice for a specific spatial ordering of the species with the other connections (red in Extended Data Fig. E1) being long range. An alternative ordering of species will reveal that the red connections also form a 2D lattice. In contrast, Hopfield Associative memory is typically based on fully connected networks where each neuron has O(N ) connections where N is the number of neurons; in fact, restricting Hopfield associative memories to a finite lattice does not allow for storing an extensive number (i.e., growing linearly with N ) of memories. 45,46 The concept of a capacity is a novel but natural question about self-assembly inspired by the Hopfield associative memory connection. While earlier work has explored limits on function due to non-specific interactions, [47][48][49][50][51] here, promiscuous interactions are not failures of design or oddities of evolution. Instead, promiscuous interactions enable alternate functions. Nevertheless, these interactions needed for other functions do get in each other's way and set a capacity on the number of stored memories. The capacity for self-assembly was shown to be ∼ N (z-2)/z where z is the coordination number (z = 4 for our 2D structures) and N is the number of distinct molecular species; this scaling is lower than the linear ∼ N scaling of fully connected Hopfield models.The network of interactions between species in the self-assembly model reflect the 2-dimensional (2D) geometry of structures. Each species potentially interacts with ∼ 4M other species where M is the number of memories stored. A subset of the connections (e.g., blue in Extended Data Fig. E1) will form a 2-dimensional lattice for a specific spatial ordering of the species with the other connections (red in Extended Data Fig. E1) being long range. An alternative ordering of species will reveal that the red connections also form a 2D lattice. In contrast, Hopfield Associative memory is typically based on fully connected networks where each neuron has O(N ) connections where N is the number of neurons; in fact, restricting Hopfield associative memories to a finite lattice does not allow for storing an extensive number (i.e., growing linearly with N ) of memories. 45,46 The concept of a capacity is a novel but natural question about self-assembly inspired by the Hopfield associative memory connection. While earlier work has explored limits on function due to non-specific interactions, [47][48][49][50][51] here, promiscuous interactions are not failures of design or oddities of evolution. Instead, promiscuous interactions enable alternate functions. Nevertheless, these interactions needed for other functions do get in each other's way and set a capacity on the number of stored memories. The capacity for self-assembly was shown to be ∼ N (z-2)/z where z is the coordination number (z = 4 for our 2D structures) and N is the number of distinct molecular species; this scaling is lower than the linear ∼ N scaling of fully connected Hopfield models.</p>
        <p>A phase diagram for the self-assembly system 37 as a function of temperature and the number of memories M reveals three phases, similar to the Hopfield model: a successful associative memory phase (low temperature, low M ), a dissolved-in-solution phase (high temperatures), and a spin glass-like phase with numerous spurious memories (low temperature, large M ). The topology of the relative placement of the phases is distinct from the Hopfield model but resembles the place cell network model discussed below.A phase diagram for the self-assembly system 37 as a function of temperature and the number of memories M reveals three phases, similar to the Hopfield model: a successful associative memory phase (low temperature, low M ), a dissolved-in-solution phase (high temperatures), and a spin glass-like phase with numerous spurious memories (low temperature, large M ). The topology of the relative placement of the phases is distinct from the Hopfield model but resembles the place cell network model discussed below.</p>
        <p>Finally, self-assembly plays out in real space which has no analog in the Hopfield model. Each molecular species might be present at copy number ∼ 10 10 in a 10 µL sample but each neuron is present only once in a neural network. Thus, molecules carry out the same pattern recognition computation in parallel, with interactions only through depletion effects described in the discussion of winner-take-all selection in Section 2.3. As a consequence, in self-assembly, multiple distinct nucleation events will typically grow into distinct structures in different parts of real space in a test tube. In contrast, multiple parallel 'nucleation' events amongst different neurons in a Hopfield model (more precisely, in place cell models described below) will interact strongly and must either merge or compete with each other.Finally, self-assembly plays out in real space which has no analog in the Hopfield model. Each molecular species might be present at copy number ∼ 10 10 in a 10 µL sample but each neuron is present only once in a neural network. Thus, molecules carry out the same pattern recognition computation in parallel, with interactions only through depletion effects described in the discussion of winner-take-all selection in Section 2.3. As a consequence, in self-assembly, multiple distinct nucleation events will typically grow into distinct structures in different parts of real space in a test tube. In contrast, multiple parallel 'nucleation' events amongst different neurons in a Hopfield model (more precisely, in place cell models described below) will interact strongly and must either merge or compete with each other.</p>
        <p>Boltzmann machines generalize Hopfield networks from "temperature-zero" strictly downhill steps that define memories as attractor basins, to "finite-temperature" dynamics that define a probability distribution over the state space. We now have (ignoring bias terms for simplicity):Boltzmann machines generalize Hopfield networks from "temperature-zero" strictly downhill steps that define memories as attractor basins, to "finite-temperature" dynamics that define a probability distribution over the state space. We now have (ignoring bias terms for simplicity):</p>
        <p>as the equilibrium for a stochastic dynamicsas the equilibrium for a stochastic dynamics</p>
        <p>that involves both uphill and downhill steps that follow detailed balance with respect to the energy function. Given known values of some of the x i , such a network can infer the exact conditional probability distribution P (x|v) by "clamping" the known units v, i.e. just never updating them. Importantly there is a learning rule for adjusting the weights w i,j so as to approximate a target distribution that is given by examples. Considering the case where the target distribution Q(v) is specified on a given subset of the variables x, we can consider the network to define P (v) = h P (hv) where x ≡ hv by concatenation and thus v represent the "visible" units and h represent the "hidden" units. More hidden units allow Boltzmann machines to approximate more complex probability distributions; remarkably, the learning rule (though not computationally efficient) is very simply expressed as a combination of Hebbian learning during a "wake" phase where the visible units are clamped to samples from Q(v), balanced by anti-Hebbian unlearning during a "sleep" phase during which no units are clamped:that involves both uphill and downhill steps that follow detailed balance with respect to the energy function. Given known values of some of the x i , such a network can infer the exact conditional probability distribution P (x|v) by "clamping" the known units v, i.e. just never updating them. Importantly there is a learning rule for adjusting the weights w i,j so as to approximate a target distribution that is given by examples. Considering the case where the target distribution Q(v) is specified on a given subset of the variables x, we can consider the network to define P (v) = h P (hv) where x ≡ hv by concatenation and thus v represent the "visible" units and h represent the "hidden" units. More hidden units allow Boltzmann machines to approximate more complex probability distributions; remarkably, the learning rule (though not computationally efficient) is very simply expressed as a combination of Hebbian learning during a "wake" phase where the visible units are clamped to samples from Q(v), balanced by anti-Hebbian unlearning during a "sleep" phase during which no units are clamped:</p>
        <p>where Q(x) = Q(v)P (x|v), and this simple expression exactly corresponds to gradient descent minimization of the relative entropy of Q(v) and P (v), i.e.,where Q(x) = Q(v)P (x|v), and this simple expression exactly corresponds to gradient descent minimization of the relative entropy of Q(v) and P (v), i.e.,</p>
        <p>wherewhere</p>
        <p>is the relative entropy, aka the Kullback-Leibler divergence.is the relative entropy, aka the Kullback-Leibler divergence.</p>
        <p>The result of this wake/sleep phase alternation between Hebbian and anti-Hebbian learning is that Boltzmann machines fail to suffer the catastrophic failure experienced by Hopfield networks when attempting to store too many memories. Instead, Boltzmann machines transition naturally between inducing an energy landscape with point attractors (when there are relatively few distinct memories in the training set) and one with continuous attractors (when the training set contains extensive variations, such as images of an object seen from many angles). The continuous attractor may have any dimensionality, depending on the nature of variation in the training data. Because the Boltzmann machine state space is discrete, we are using the term "continuous attractor" to refer a contiguous set of states that may be easily explored by the Boltzmann machine's random-walk stochastic dynamics at the given temperature; i.e. there are no high-energy barriers disconnecting the set of reasonably-high-probability states. See Figure S1.2.The result of this wake/sleep phase alternation between Hebbian and anti-Hebbian learning is that Boltzmann machines fail to suffer the catastrophic failure experienced by Hopfield networks when attempting to store too many memories. Instead, Boltzmann machines transition naturally between inducing an energy landscape with point attractors (when there are relatively few distinct memories in the training set) and one with continuous attractors (when the training set contains extensive variations, such as images of an object seen from many angles). The continuous attractor may have any dimensionality, depending on the nature of variation in the training data. Because the Boltzmann machine state space is discrete, we are using the term "continuous attractor" to refer a contiguous set of states that may be easily explored by the Boltzmann machine's random-walk stochastic dynamics at the given temperature; i.e. there are no high-energy barriers disconnecting the set of reasonably-high-probability states. See Figure S1.2.</p>
        <p>The relevance to self-assembly begins with statistical mechanics at finite temperature, in contrast to the Hopfield network's zero-temperature dynamics. We do not have a general interpretation of multifarious self-assembly in terms of generative probability distributions, however, so that is an open question. Further, we are not aware of a connection between the elegant Boltzmann machine learning rule and any method for design of molecular self-assembling systems -another avenue to explore. That said, some key features of multifarious self-assembly have direct interpretations -and were inspired by -a neural architecture for so-called "place cells" in rodents, as described in the next section, where the Boltzmann machine perspective provides an elegant way to articulate a toy model. Furthermore, we will see that while the energy landscape of multifarious self-assembly is akin to the point attractors of Hopfield associative memories, the structure of the kinetic barriers relevant to nucleation and pattern recognition is akin to the continuous attractor structure of place cell models.The relevance to self-assembly begins with statistical mechanics at finite temperature, in contrast to the Hopfield network's zero-temperature dynamics. We do not have a general interpretation of multifarious self-assembly in terms of generative probability distributions, however, so that is an open question. Further, we are not aware of a connection between the elegant Boltzmann machine learning rule and any method for design of molecular self-assembling systems -another avenue to explore. That said, some key features of multifarious self-assembly have direct interpretations -and were inspired by -a neural architecture for so-called "place cells" in rodents, as described in the next section, where the Boltzmann machine perspective provides an elegant way to articulate a toy model. Furthermore, we will see that while the energy landscape of multifarious self-assembly is akin to the point attractors of Hopfield associative memories, the structure of the kinetic barriers relevant to nucleation and pattern recognition is akin to the continuous attractor structure of place cell models.</p>
        <p>Place cells and grid cells were discovered by John O'Keefe, May-Britt Moser, Edvard Moser and colleagues 52,53 to be key components of the rodent brain's navigational system. Parts of the computational architecture they discovered re-appear in the principles of multifarious self-assemnbly. 37,54 We review those connections here.Place cells and grid cells were discovered by John O'Keefe, May-Britt Moser, Edvard Moser and colleagues 52,53 to be key components of the rodent brain's navigational system. Parts of the computational architecture they discovered re-appear in the principles of multifarious self-assemnbly. 37,54 We review those connections here.</p>
        <p>Let us begin with a story, adapted from Hopfield's musings about place cells. 55 You are recording from a grid of electrodes in the hippocampus of an awake behaving rat as it explores a small room or a maze, which as illustrated in Figure S1.3 is in the shape of an H. You find that some neurons don't fire at all, but most neurons fire every now and again, stochastically -and each one seems to prefer a particular location in the maze. Plotting where the rat was when neuron i fired, you see that it fires most frequently near position p H i and that its firing rate decreases with the rat's distance from that location. So, having computed the mean position p H i for each neuron, you can now re-examine your data (or analyze newly-taken data) at any given instant in time, plotting the positions p H i for all neurons that fired during that instant. This will give a scatter of points around the true position of the rat: a population code for where the rat is. A neuron elsewhere in the rat's brain could look at this population of neurons in the hippocampus, and know where the rat is, and perhaps perform other computations based on the rat's position encoded this way.Let us begin with a story, adapted from Hopfield's musings about place cells. 55 You are recording from a grid of electrodes in the hippocampus of an awake behaving rat as it explores a small room or a maze, which as illustrated in Figure S1.3 is in the shape of an H. You find that some neurons don't fire at all, but most neurons fire every now and again, stochastically -and each one seems to prefer a particular location in the maze. Plotting where the rat was when neuron i fired, you see that it fires most frequently near position p H i and that its firing rate decreases with the rat's distance from that location. So, having computed the mean position p H i for each neuron, you can now re-examine your data (or analyze newly-taken data) at any given instant in time, plotting the positions p H i for all neurons that fired during that instant. This will give a scatter of points around the true position of the rat: a population code for where the rat is. A neuron elsewhere in the rat's brain could look at this population of neurons in the hippocampus, and know where the rat is, and perhaps perform other computations based on the rat's position encoded this way.</p>
        <p>An interesting thing occurs when you pick up the rat and drop it in a new maze, which perhaps has the shape of an A. The same electrodes are still recording from the same neurons. To encode where the rat is in A, will the neurons that responded while in H now be silent, and the neurons that were silent in H now be active and encode positional information in A? What if we were to drop the rat into yet another environment, M? In fact, what you see is that most of the same neurons, as well as some of the previously-silent ones, are active in A, but the positions that they encode for are apparently randomly distributed in the new maze. Let's call these positions p A i . So this means, if the rat is wandering around in H, at any given moment the firing neurons can have their p H i coordinates plotted, and they will form a roughly colocalized clump in the H maze; but if the same firing neurons are plotted according to p A i in the A maze, they will be randomly scattered all over the place. Consequently, the same neurons can underlie population codes for the rat's position in H, the rat's position in A, and even the rat's position in M. That is, neurons elsewhere in the brain can easily compute not only "If I am somewhere in H, where exactly am I?" and "If I am somewhere in A, where exactly am I?" and "If I am somewhere in M, where exactly am I?" just by averaging the associated positions for each neuron that's firing, but they can (a) A rat may be placed in one of three mazes, which it explores as an array of electrodes records the activity of a few hundred neurons. (b) If we record the location of the rat every time that a certain cell fired (black circles), we see that it has a selective "place field" within each maze (center plotted as orange dot). (c) If at a given instant we look at which neurons are firing, we see that those neurons' respective place fields (colored dots) are colocalized near each other within the maze the rat is exploring (at position indicated by black circle), but they are distributed randomly in the other mazes. also easily compute "Am I in H?" and "Am I in A?" and "Am I in M?" by considering the pairwise activity of cells that are nearby in one map or another. The rat is likely to be in H if the firing neurons are roughly colocalized, anywhere in H. This is a pattern recognition problem: given the set of neurons currently firing, which maze is the rat in? The answer comes from examining how the neurons' associated positions are colocalized, or not, in each of the candidate mazes.An interesting thing occurs when you pick up the rat and drop it in a new maze, which perhaps has the shape of an A. The same electrodes are still recording from the same neurons. To encode where the rat is in A, will the neurons that responded while in H now be silent, and the neurons that were silent in H now be active and encode positional information in A? What if we were to drop the rat into yet another environment, M? In fact, what you see is that most of the same neurons, as well as some of the previously-silent ones, are active in A, but the positions that they encode for are apparently randomly distributed in the new maze. Let's call these positions p A i . So this means, if the rat is wandering around in H, at any given moment the firing neurons can have their p H i coordinates plotted, and they will form a roughly colocalized clump in the H maze; but if the same firing neurons are plotted according to p A i in the A maze, they will be randomly scattered all over the place. Consequently, the same neurons can underlie population codes for the rat's position in H, the rat's position in A, and even the rat's position in M. That is, neurons elsewhere in the brain can easily compute not only "If I am somewhere in H, where exactly am I?" and "If I am somewhere in A, where exactly am I?" and "If I am somewhere in M, where exactly am I?" just by averaging the associated positions for each neuron that's firing, but they can (a) A rat may be placed in one of three mazes, which it explores as an array of electrodes records the activity of a few hundred neurons. (b) If we record the location of the rat every time that a certain cell fired (black circles), we see that it has a selective "place field" within each maze (center plotted as orange dot). (c) If at a given instant we look at which neurons are firing, we see that those neurons' respective place fields (colored dots) are colocalized near each other within the maze the rat is exploring (at position indicated by black circle), but they are distributed randomly in the other mazes. also easily compute "Am I in H?" and "Am I in A?" and "Am I in M?" by considering the pairwise activity of cells that are nearby in one map or another. The rat is likely to be in H if the firing neurons are roughly colocalized, anywhere in H. This is a pattern recognition problem: given the set of neurons currently firing, which maze is the rat in? The answer comes from examining how the neurons' associated positions are colocalized, or not, in each of the candidate mazes.</p>
        <p>The usefulness of this computational architecture comes from the fact that, obviously, an animal needs to be able to navigate through many environments over its lifetime, and it would seem to be wasteful to have a neuron that only fired in exactly one position in the entire world, say three inches to the left of the stove in your grandmother's kitchen. Utilizing a randomized map between sensory perceptions (which in their entirety might be unique to a position in the world, but which considered partially will recur in many environments) provides a way for the same neurons to underlie populations codes in different environments based on the effective orthogonality of colocalization vs scattered activity.The usefulness of this computational architecture comes from the fact that, obviously, an animal needs to be able to navigate through many environments over its lifetime, and it would seem to be wasteful to have a neuron that only fired in exactly one position in the entire world, say three inches to the left of the stove in your grandmother's kitchen. Utilizing a randomized map between sensory perceptions (which in their entirety might be unique to a position in the world, but which considered partially will recur in many environments) provides a way for the same neurons to underlie populations codes in different environments based on the effective orthogonality of colocalization vs scattered activity.</p>
        <p>Standard Boltzmann machine learning serves to illustrate how such place cell maps could function in a toy model. We consider N neurons in the hippocampus; the rest of the brain, which provides input to the hippocampus or reads the output of the hippocampus, is not explicitly modeled as a neural network -but the input driving the interaction pattern, except that each cell will have at most four local neighbors in each environment, and no inhibitory long-range interactions. This is illustrated for tile S291: a line is drawn to every other tile in shape H that it can bind to (via either its N, E, S, or W sides) using color red for tiles that are adjacent in H, green for tiles adjacent in A, and blue for tiles adjacent in M. Gray is used for tiles that are not adjacent in any shape, but for which binding ensues from the transitivity of Watson-Crick binding. (Also c.f. Extended Data Fig. E1, where tile binding affinities due to colocalization within the same shape are shown in one color, while tile binding affinities due to colocalization with respect to another shape are shown in another color). See Figure S3.3 for a representation of the full SHAM tile wiring diagram.Standard Boltzmann machine learning serves to illustrate how such place cell maps could function in a toy model. We consider N neurons in the hippocampus; the rest of the brain, which provides input to the hippocampus or reads the output of the hippocampus, is not explicitly modeled as a neural network -but the input driving the interaction pattern, except that each cell will have at most four local neighbors in each environment, and no inhibitory long-range interactions. This is illustrated for tile S291: a line is drawn to every other tile in shape H that it can bind to (via either its N, E, S, or W sides) using color red for tiles that are adjacent in H, green for tiles adjacent in A, and blue for tiles adjacent in M. Gray is used for tiles that are not adjacent in any shape, but for which binding ensues from the transitivity of Watson-Crick binding. (Also c.f. Extended Data Fig. E1, where tile binding affinities due to colocalization within the same shape are shown in one color, while tile binding affinities due to colocalization with respect to another shape are shown in another color). See Figure S3.3 for a representation of the full SHAM tile wiring diagram.</p>
        <p>hippocampus will be given as training data. Our perspective (appropriate for Boltzmann machine models) is that the purpose of our N neurons is to encode a probabilistic generative model that matches the distribution of the training data. That is, in the "wake" phase, the model rat wanders around randomly inside one of the mazes, with the hippocampal neurons being driven by the environment. Specifically, each neuron is given a fixed-for-all-time random preferred location in each maze, p H i , p A i , and p M i , and when the rat is at position p in maze H, neuron i fires with Gaussian probability N (p H i , σ), i.e. it is driven by sensory perception. During the "sleep" phase, neurons fire according to Boltzmann machine stochastic dynamics, as described above, based on the current weights w i,j between the hippocampal neurons. Learning occurs by slow Hebbian weight changes during the wake phase and slow anti-Hebbian weight changes during the sleep phase, exactly as in a classical Boltzmann machine. After sufficient learning, the free-running (sleeping, dreaming) network encodes a probability distribution much like a continuous attractor: high probability states are those in which the firing neurons are clumped with radius roughly σ in either H, A, or M, centered around any point within the continuous maze area. Due to the stochasticity of firing, the clump randomly moves around the maze, occasionally decohering and "teleporting" to become clumped in a different maze. Looking at the learned weights, we see that they are strongest between neurons whose receptive fields are near each other (in any maze), but also with negative interactions well balanced such that the clump of co-activated neurons does not grow larger than the typical size during the wake phase (Figure S1.4). Roughly, we can describe the results of Boltzmann machine learning as:hippocampus will be given as training data. Our perspective (appropriate for Boltzmann machine models) is that the purpose of our N neurons is to encode a probabilistic generative model that matches the distribution of the training data. That is, in the "wake" phase, the model rat wanders around randomly inside one of the mazes, with the hippocampal neurons being driven by the environment. Specifically, each neuron is given a fixed-for-all-time random preferred location in each maze, p H i , p A i , and p M i , and when the rat is at position p in maze H, neuron i fires with Gaussian probability N (p H i , σ), i.e. it is driven by sensory perception. During the "sleep" phase, neurons fire according to Boltzmann machine stochastic dynamics, as described above, based on the current weights w i,j between the hippocampal neurons. Learning occurs by slow Hebbian weight changes during the wake phase and slow anti-Hebbian weight changes during the sleep phase, exactly as in a classical Boltzmann machine. After sufficient learning, the free-running (sleeping, dreaming) network encodes a probability distribution much like a continuous attractor: high probability states are those in which the firing neurons are clumped with radius roughly σ in either H, A, or M, centered around any point within the continuous maze area. Due to the stochasticity of firing, the clump randomly moves around the maze, occasionally decohering and "teleporting" to become clumped in a different maze. Looking at the learned weights, we see that they are strongest between neurons whose receptive fields are near each other (in any maze), but also with negative interactions well balanced such that the clump of co-activated neurons does not grow larger than the typical size during the wake phase (Figure S1.4). Roughly, we can describe the results of Boltzmann machine learning as:</p>
        <p>where α ∈ {H, A, M } specifies an environment. The value of this behavior -reproducing the probability distribution of the training data -comes from how a Boltzmann machine can use the distribution to perform inference based on partial knowledge: clamping (driving) neurons with known values and letting the others evolve by stochastic dynamics generates exactly the conditional probability distribution. Suppose the rat is in a perceptually challenging environment, such that only a fraction of hippocampal neurons are driven by the environment, and some neurons are erroneously active. With the environment clamping these neurons but the internal stochastic dynamics controlling the activity of the environmentallyunconstrained neurons, generation according to the conditional probability distribution will infer a highly probable "fleshed out" population code that other parts of the brain can read in order to infer which maze the rat is in, as well as where within the maze the rat is -whereas without the inference, the poor-quality directly stimulated activity may not be sufficient.where α ∈ {H, A, M } specifies an environment. The value of this behavior -reproducing the probability distribution of the training data -comes from how a Boltzmann machine can use the distribution to perform inference based on partial knowledge: clamping (driving) neurons with known values and letting the others evolve by stochastic dynamics generates exactly the conditional probability distribution. Suppose the rat is in a perceptually challenging environment, such that only a fraction of hippocampal neurons are driven by the environment, and some neurons are erroneously active. With the environment clamping these neurons but the internal stochastic dynamics controlling the activity of the environmentallyunconstrained neurons, generation according to the conditional probability distribution will infer a highly probable "fleshed out" population code that other parts of the brain can read in order to infer which maze the rat is in, as well as where within the maze the rat is -whereas without the inference, the poor-quality directly stimulated activity may not be sufficient.</p>
        <p>While this toy model is oversimplified even with respect to other rudimentary models of hippocampal place cells, 55,56 it serves well as a reference point for considering natural parallels to pattern recognition by multifarious self-assembly. Here, each tile corresponds to a hippocampal neuron; its position within the self-assembled shapes H, A, and M correspond to its receptive fields in those mazes; and its initial concentration corresponds to the (possibly low-quality) environmental stimulation. The binding interactions between neighboring tiles correspond to weights between hippocampal neurons with neighboring receptive fields (although the neurons may have longerrange interactions). Nucleation and self-assembly of some low-concentration tiles together with high-concentration tiles corresponds to formation of a clump of active neurons during inference that activates neurons that were not environmentally driven.While this toy model is oversimplified even with respect to other rudimentary models of hippocampal place cells, 55,56 it serves well as a reference point for considering natural parallels to pattern recognition by multifarious self-assembly. Here, each tile corresponds to a hippocampal neuron; its position within the self-assembled shapes H, A, and M correspond to its receptive fields in those mazes; and its initial concentration corresponds to the (possibly low-quality) environmental stimulation. The binding interactions between neighboring tiles correspond to weights between hippocampal neurons with neighboring receptive fields (although the neurons may have longerrange interactions). Nucleation and self-assembly of some low-concentration tiles together with high-concentration tiles corresponds to formation of a clump of active neurons during inference that activates neurons that were not environmentally driven.</p>
        <p>We can now clearly see how multifarious self-assembly shares aspects both of Hopfield associative memories based on point attractors, and of Boltzmann machine pattern recognition based on continuous attractors. The energy landscape for assemblies in a multifarious system has point-attractor basins; in our system these correspond to the three shapes H, A, and M, which are our "memories". In contrast, the energy barriers for nucleation of assemblies in a multifarious system share the structure of place cell continuous attractors; the set of critical nuclei, closely corresponding to the set of "flag" patterns that stimulate nucleation due to colocalization of high-concentration tiles, form a continuous set. In other words, the thermodynamics of self-assembly has point-attractor structure reflected in the energy basins, while the kinetics of self-assembly has continuous-attractor structure reflected in the nucleation barriers. Note that Figure 1 of the main paper illustrates the three-basin point-attractor nature of the energy landscape, but does not illustrate the continuous-attractor nature of the multifarious nucleation pathways: while two possible nucleation and growth pathways are illustrated for the green shape, a more realistic illustration would include a whole continuous space of possible nucleation and growth pathways.We can now clearly see how multifarious self-assembly shares aspects both of Hopfield associative memories based on point attractors, and of Boltzmann machine pattern recognition based on continuous attractors. The energy landscape for assemblies in a multifarious system has point-attractor basins; in our system these correspond to the three shapes H, A, and M, which are our "memories". In contrast, the energy barriers for nucleation of assemblies in a multifarious system share the structure of place cell continuous attractors; the set of critical nuclei, closely corresponding to the set of "flag" patterns that stimulate nucleation due to colocalization of high-concentration tiles, form a continuous set. In other words, the thermodynamics of self-assembly has point-attractor structure reflected in the energy basins, while the kinetics of self-assembly has continuous-attractor structure reflected in the nucleation barriers. Note that Figure 1 of the main paper illustrates the three-basin point-attractor nature of the energy landscape, but does not illustrate the continuous-attractor nature of the multifarious nucleation pathways: while two possible nucleation and growth pathways are illustrated for the green shape, a more realistic illustration would include a whole continuous space of possible nucleation and growth pathways.</p>
        <p>That said, the analogy with place cells is far from exact. A notable difference is that in self-assembly, once seed has formed, the associated shape will grow to completion -whereas for a rat in a fixed position in the maze, a clump of neurons will remain firing but will not grow to include neurons that have place fields within that maze, due to long-range inhibitory connections. Consequently, unlike the free-running neural network model of place cells, which "dreams" of moving throughout the maze as the clump of currently-active neurons stochastically diffuses, the self-assembly system as explored here does not have a corresponding "mental exploration" behavior. 54 It is natural to ask what physical modification to the self-assembly process would correspond better to "mental exploration": it would have to be an assembly that stays roughly the same size, growing in some places while shrinking in other places, thus at each moment remaining a "clump" of tiles of roughly constant size, but positioned in different places within the full shape. In principle, could the necessary feedback to limit assembly size come, for example, from depletion of a limited shared resource that all tiles consume/release upon assembly/disassembly, or alternatively, from accumulated strain due to geometrical frustration? 57,58 This is meant as an example of the kind of novel question that arises from serious consideration of the metaphors between neural networks and molecular self-assembly.That said, the analogy with place cells is far from exact. A notable difference is that in self-assembly, once seed has formed, the associated shape will grow to completion -whereas for a rat in a fixed position in the maze, a clump of neurons will remain firing but will not grow to include neurons that have place fields within that maze, due to long-range inhibitory connections. Consequently, unlike the free-running neural network model of place cells, which "dreams" of moving throughout the maze as the clump of currently-active neurons stochastically diffuses, the self-assembly system as explored here does not have a corresponding "mental exploration" behavior. 54 It is natural to ask what physical modification to the self-assembly process would correspond better to "mental exploration": it would have to be an assembly that stays roughly the same size, growing in some places while shrinking in other places, thus at each moment remaining a "clump" of tiles of roughly constant size, but positioned in different places within the full shape. In principle, could the necessary feedback to limit assembly size come, for example, from depletion of a limited shared resource that all tiles consume/release upon assembly/disassembly, or alternatively, from accumulated strain due to geometrical frustration? 57,58 This is meant as an example of the kind of novel question that arises from serious consideration of the metaphors between neural networks and molecular self-assembly.</p>
        <p>A general framework for supervised learning considers training a parameterized function class f (x; θ) to minimize a cost function L(D; θ) on a training set of data D, with the objective of performing well on out-ofsample data as assessed by an independent test set D . A common choice for the function class is a multilayer neural network with a certain number of input units, hidden units, and output units. In a basic set-up, D and D are independent identically distributed (iid) samples from an unknown probability distribution over input-output pairs, and L is the mean squared error with respect to a target function, e.g.,A general framework for supervised learning considers training a parameterized function class f (x; θ) to minimize a cost function L(D; θ) on a training set of data D, with the objective of performing well on out-ofsample data as assessed by an independent test set D . A common choice for the function class is a multilayer neural network with a certain number of input units, hidden units, and output units. In a basic set-up, D and D are independent identically distributed (iid) samples from an unknown probability distribution over input-output pairs, and L is the mean squared error with respect to a target function, e.g.,</p>
        <p>Attempts to find a value θ * that performs well (finding the global minimum, or even a guaranteed local minimum, is often too much to hope for) may proceed by gradient descent, hill-climbing, genetic algorithms, or other optimization algorithms. If the training error L(D; θ * ) is small, then we say that the training data has been learned well. If L(D ; θ * ) is close to L(D; θ * ) then we say that the learned function has generalized well, without overfitting the training data. The power of a function class can be characterized by the range of functions that can be obtained by varying the parameters, e.g., the VC-dimension describes when all N -dimensional Boolean functions can be implemented and has strong implications with respect to generalization. All else being equal, weaker function classes generalize better given the same data.Attempts to find a value θ * that performs well (finding the global minimum, or even a guaranteed local minimum, is often too much to hope for) may proceed by gradient descent, hill-climbing, genetic algorithms, or other optimization algorithms. If the training error L(D; θ * ) is small, then we say that the training data has been learned well. If L(D ; θ * ) is close to L(D; θ * ) then we say that the learned function has generalized well, without overfitting the training data. The power of a function class can be characterized by the range of functions that can be obtained by varying the parameters, e.g., the VC-dimension describes when all N -dimensional Boolean functions can be implemented and has strong implications with respect to generalization. All else being equal, weaker function classes generalize better given the same data.</p>
        <p>Viewing the SHAM experiments through this lens, we can easily establish a correspondence. The parameters θ specify the pixel-to-tile map. The input x will be 900-pixel gray-scale images. The function class f (x; θ) returns the output vector y = (h, a, m) of percentages of the respective types of self-assembled shapes after experimental annealing with initial tile concentrations specified by x and the pixel-to-tile map θ. This description takes the function class to be defined by the experimental reality, but of course theoretical analysis would have to adopt a mathematical model thereof, such as the Stochastic Greedy Model developed in Section 2.2 or the simpler abstract models described below. Our training data is just the 18 specified images together with their target classes, e.g., for x α being "Hopfield", y α = (1, 0, 0). Since our function class is not analytically specified but rather defined by a physical process, for training optimization we work with a surrogate cost function Score(x) based on our model of nucleation kinetics; this surrogate function tallies the estimated log-difference in nucleation rates between the target shape and its best competitor. Although not directly optimizing the mean squared error of shape percentages, a low Score(x α ) would imply (in the model, at least) that the target shape nucleates much faster than the other shapes, and thus the output shape percentage will approach being 100% the target shape. Other differences are that we minimized the maximum (i.e. worst) image score rather than the mean, which was sensible given that our training set only had 18 images and we wanted all of our experiments to work, and that the optimization used the simplified (but much faster) Window Nucleation Model for the first stage of optimization. The optimization algorithm itself was a discrete stochastic hill-climbing algorithm rather than a continuous gradient descent algorithm (such as back-propagation) due the non-differentiable nature of the pixel-to-tile map θ. Finally, it would be unreasonable to claim that our 18 test images are additional iid samples from the same source distribution as the training samples, as the training images were hand-chosen for interest and the testing images were concocted to test robustness to certain kinds of noise and variation. (The exception is the "Harom" training image of a 3, which was taken from the same MNIST database of digit images as the six testing images of 3s.) Despite these differences, we can calculate the mean squared error of the analog classification probabilities from our experimental results, with the training set error L(D; θ * ) = 0.042 and the test set error L(D ; θ * ) = 0.202 based on the AFM count fractions reported in Figure 5g (substitutingViewing the SHAM experiments through this lens, we can easily establish a correspondence. The parameters θ specify the pixel-to-tile map. The input x will be 900-pixel gray-scale images. The function class f (x; θ) returns the output vector y = (h, a, m) of percentages of the respective types of self-assembled shapes after experimental annealing with initial tile concentrations specified by x and the pixel-to-tile map θ. This description takes the function class to be defined by the experimental reality, but of course theoretical analysis would have to adopt a mathematical model thereof, such as the Stochastic Greedy Model developed in Section 2.2 or the simpler abstract models described below. Our training data is just the 18 specified images together with their target classes, e.g., for x α being "Hopfield", y α = (1, 0, 0). Since our function class is not analytically specified but rather defined by a physical process, for training optimization we work with a surrogate cost function Score(x) based on our model of nucleation kinetics; this surrogate function tallies the estimated log-difference in nucleation rates between the target shape and its best competitor. Although not directly optimizing the mean squared error of shape percentages, a low Score(x α ) would imply (in the model, at least) that the target shape nucleates much faster than the other shapes, and thus the output shape percentage will approach being 100% the target shape. Other differences are that we minimized the maximum (i.e. worst) image score rather than the mean, which was sensible given that our training set only had 18 images and we wanted all of our experiments to work, and that the optimization used the simplified (but much faster) Window Nucleation Model for the first stage of optimization. The optimization algorithm itself was a discrete stochastic hill-climbing algorithm rather than a continuous gradient descent algorithm (such as back-propagation) due the non-differentiable nature of the pixel-to-tile map θ. Finally, it would be unreasonable to claim that our 18 test images are additional iid samples from the same source distribution as the training samples, as the training images were hand-chosen for interest and the testing images were concocted to test robustness to certain kinds of noise and variation. (The exception is the "Harom" training image of a 3, which was taken from the same MNIST database of digit images as the six testing images of 3s.) Despite these differences, we can calculate the mean squared error of the analog classification probabilities from our experimental results, with the training set error L(D; θ * ) = 0.042 and the test set error L(D ; θ * ) = 0.202 based on the AFM count fractions reported in Figure 5g (substituting</p>
        <p>3 ) for the 7 test images with no AFM counts). These parallels suggest several questions for further study. While we used the analog probabilities of nucleating the H, A, and M shapes to approximate a discrete classification task with 3 categories, could multifarious selfassembly be trained to approximate an arbitrary real-valued function g : R 900 → R 3 whose outputs represent probabilities? What are the limitations to the range of functions that can approximated by nucleation kinetics, and are there variations on the molecular system design that help or hinder the ability approximate complex multidimensional functions -analogous to the differences in representational capacity of single-layer vs multilayer neural networks, with linear vs locally-nonlinear vs globally-nonlinear activation functions? What would be analogous to hidden units? Does pattern recognition by self-assembly nucleation benefit from generalization and suffer from overfitting analogously to neural networks and other machine learning models? Can one train not just the pixel-to-tile map, but train the tile set itself to improve pattern recognition performance? Can the pixel-to-tile map, and/or the tile set, be generalized to be parameterized with continuous variables, so as to allow differentiable gradient descent optimization algorithms?3 ) for the 7 test images with no AFM counts). These parallels suggest several questions for further study. While we used the analog probabilities of nucleating the H, A, and M shapes to approximate a discrete classification task with 3 categories, could multifarious selfassembly be trained to approximate an arbitrary real-valued function g : R 900 → R 3 whose outputs represent probabilities? What are the limitations to the range of functions that can approximated by nucleation kinetics, and are there variations on the molecular system design that help or hinder the ability approximate complex multidimensional functions -analogous to the differences in representational capacity of single-layer vs multilayer neural networks, with linear vs locally-nonlinear vs globally-nonlinear activation functions? What would be analogous to hidden units? Does pattern recognition by self-assembly nucleation benefit from generalization and suffer from overfitting analogously to neural networks and other machine learning models? Can one train not just the pixel-to-tile map, but train the tile set itself to improve pattern recognition performance? Can the pixel-to-tile map, and/or the tile set, be generalized to be parameterized with continuous variables, so as to allow differentiable gradient descent optimization algorithms?</p>
        <p>Notable reference points for neural network classification and function approximation models are linear threshold units (LTU), polynomial threshold functions (PTF), winner-take-all units (WTA), their soft-transition analogs, and multilayer networks thereof. For dimension n analog real input vectors x, an LTU computes a single binary scalar output based on weight vector w and bias b:Notable reference points for neural network classification and function approximation models are linear threshold units (LTU), polynomial threshold functions (PTF), winner-take-all units (WTA), their soft-transition analogs, and multilayer networks thereof. For dimension n analog real input vectors x, an LTU computes a single binary scalar output based on weight vector w and bias b:</p>
        <p>a PTF (also known as a sigma-pi unit) specifies a polynomial by a set of terms using variables V (which may be the empty set for a bias) and their coefficients w:a PTF (also known as a sigma-pi unit) specifies a polynomial by a set of terms using variables V (which may be the empty set for a bias) and their coefficients w:</p>
        <p>and a weighted WTA is typically formulated as having a vector output calculated using a weight matrix W and bias vector B:and a weighted WTA is typically formulated as having a vector output calculated using a weight matrix W and bias vector B:</p>
        <p>where ϕ is the 0/1 indicator function for a Boolean truth value. (Using ±1 to indicate Boolean input and output values, and changing ϕ accordingly, is also common and can affect the complexity of function computation under resource constraints, [59][60][61] such as network depth or degree or number of polynomial terms. Remarkably, using {1,2} for False and True, which may correspond better to low and high concentrations, gives polynomial threshold functions even more computational power. 62 ) Comparing LTU to PTF and WTA, key results are that a single weighted WTA can compute functions that require networks of LTUs with a single layer of hidden units 63 and that a single PTF is similarly strictly more powerful than a single LTU but less powerful than a network of LTUs with a single layer of hidden units; 59 the increased computational power of single PTFs with monomial degree bounded by d or with the number of monomials bounded by m have also been studied. 64 An abstract model of pattern recognition by tile self-assembly nucleation can be formulated based on the Window Nucleation Model that is defined in Section 2.5. Ignoring the heuristic weights used to bias the WNM scores to favor experimentally preferred regions, the nucleation rate estimate (from equation 2.25) for k ×k windows can be written aswhere ϕ is the 0/1 indicator function for a Boolean truth value. (Using ±1 to indicate Boolean input and output values, and changing ϕ accordingly, is also common and can affect the complexity of function computation under resource constraints, [59][60][61] such as network depth or degree or number of polynomial terms. Remarkably, using {1,2} for False and True, which may correspond better to low and high concentrations, gives polynomial threshold functions even more computational power. 62 ) Comparing LTU to PTF and WTA, key results are that a single weighted WTA can compute functions that require networks of LTUs with a single layer of hidden units 63 and that a single PTF is similarly strictly more powerful than a single LTU but less powerful than a network of LTUs with a single layer of hidden units; 59 the increased computational power of single PTFs with monomial degree bounded by d or with the number of monomials bounded by m have also been studied. 64 An abstract model of pattern recognition by tile self-assembly nucleation can be formulated based on the Window Nucleation Model that is defined in Section 2.5. Ignoring the heuristic weights used to bias the WNM scores to favor experimentally preferred regions, the nucleation rate estimate (from equation 2.25) for k ×k windows can be written as</p>
        <p>A is k×k in shape i∈AA is k×k in shape i∈A</p>
        <p>x i for a given shape, where x i = c i /c gives the concentration of tile i normalized to the base concentration c, and γ and γ lump physical constants. If we define recognition to occur if the initial nucleation rate (considering the tile concentrations to be unchanging) exceeds a threshold, then the classification functionx i for a given shape, where x i = c i /c gives the concentration of tile i normalized to the base concentration c, and γ and γ lump physical constants. If we define recognition to occur if the initial nucleation rate (considering the tile concentrations to be unchanging) exceeds a threshold, then the classification function</p>
        <p>is formally a PTF with polynomial degree k 2 . It is not presently clear, however, how to characterize the power of this subset of PTF functions, where the set of monomials derives from placement of tiles in the shape, and all weights are 1. That said, perhaps a more interesting abstract model would take into account the fact that tile concentrations deplete, leading to a WTA effect. Using a simplified idealization of this effect, we can abstract the classification function for a multifarious set of shapes asis formally a PTF with polynomial degree k 2 . It is not presently clear, however, how to characterize the power of this subset of PTF functions, where the set of monomials derives from placement of tiles in the shape, and all weights are 1. That said, perhaps a more interesting abstract model would take into account the fact that tile concentrations deplete, leading to a WTA effect. Using a simplified idealization of this effect, we can abstract the classification function for a multifarious set of shapes as</p>
        <p>which might be expected to combine the power of PTF and WTA, but again is restricted by the shape layout constraints.which might be expected to combine the power of PTF and WTA, but again is restricted by the shape layout constraints.</p>
        <p>As an informal empirical assessment of the power of pattern recognition by the nucleation kinetics of multifarious self-assembly, we trained the f W N M :W T A (x; shapes) model to classify the MNIST database of handwritten digits. 65 The 28×28 pixel grayscale images were used as concentration patterns with the white background being interpreted as the base concentration c and the darker digit pixels linearly scaled up to 10c. The tile layout within 10 shapes, one for each digit class, was optimized by stochastic hill-climbing (tile position swaps and mutations) to maximize the fraction of training set images that were correctly classified. The standard training set of 60, 000 images was used, and after training the performance on a test set of 10, 000 images was assessed. The WNM window size was k = 3. Three types of constraints on the tile layouts were considered. (1) Each shape is a 28 × 28 square containing exactly one copy of each tile. This corresponds to the assumption that arbitrary promiscuous interactions can be designed, as in the original work on multifarious self-assembly. 37 A training error of 7.4% and test error of 10.8% was achieved. (2) Each shape is a 10 × 10 square containing whatever tiles work best, including possibly multiple copies of the same tile within the same shape. Again, implementation would require arbitrary promiscuous interactions to be designable. A training error of 7.7% and test error of 9.8% was achieved. (3) Each shape is a 12 × 12 square in which half the tiles, arranged as a checkerboard, are shared among all 10 shapes, while the other half are unique to the given shape. This "simple checkerboard" design could be implemented with SST DNA tiles and Watson-Crick domain interactions. A training error of 12.8% and test error of 13.9% was achieved. Presumably, slightly worse performance could be expected if further restrictions for "guarded edges" were imposed. Furthermore, unlike the shapes designed for the experimental system, no check was performed to ensure that the proofreading property is respected so malformed assemblies such as chimera are largely prevented; enforcing this property could in principle reduce performance somewhat as well.As an informal empirical assessment of the power of pattern recognition by the nucleation kinetics of multifarious self-assembly, we trained the f W N M :W T A (x; shapes) model to classify the MNIST database of handwritten digits. 65 The 28×28 pixel grayscale images were used as concentration patterns with the white background being interpreted as the base concentration c and the darker digit pixels linearly scaled up to 10c. The tile layout within 10 shapes, one for each digit class, was optimized by stochastic hill-climbing (tile position swaps and mutations) to maximize the fraction of training set images that were correctly classified. The standard training set of 60, 000 images was used, and after training the performance on a test set of 10, 000 images was assessed. The WNM window size was k = 3. Three types of constraints on the tile layouts were considered. (1) Each shape is a 28 × 28 square containing exactly one copy of each tile. This corresponds to the assumption that arbitrary promiscuous interactions can be designed, as in the original work on multifarious self-assembly. 37 A training error of 7.4% and test error of 10.8% was achieved. (2) Each shape is a 10 × 10 square containing whatever tiles work best, including possibly multiple copies of the same tile within the same shape. Again, implementation would require arbitrary promiscuous interactions to be designable. A training error of 7.7% and test error of 9.8% was achieved. (3) Each shape is a 12 × 12 square in which half the tiles, arranged as a checkerboard, are shared among all 10 shapes, while the other half are unique to the given shape. This "simple checkerboard" design could be implemented with SST DNA tiles and Watson-Crick domain interactions. A training error of 12.8% and test error of 13.9% was achieved. Presumably, slightly worse performance could be expected if further restrictions for "guarded edges" were imposed. Furthermore, unlike the shapes designed for the experimental system, no check was performed to ensure that the proofreading property is respected so malformed assemblies such as chimera are largely prevented; enforcing this property could in principle reduce performance somewhat as well.</p>
        <p>These results are not interesting in and of themselves -there is no need to solve image recognition problems at the molecular level -but they can serve as an initial calibration of the computational power of nucleation in molecular self-assembly. Is it more like a linear classifier? Or worse? Or better, perhaps giving similar performance as a two-layer neural network? Or even a multilayer deep neural network? Those familiar with the MNIST problem already know the answer: In LeCun's 1998 study, 65 and a multilayer convolutional neural network achieved a 0.8% test set error. This comparison places the readily implementable checkerboard design as roughly comparable with a single-layer WTA, but even though it's a reach goal, less-constrained multifarious layouts still perform worse than two-layer neural networks. Thus one might imagine that when intracellular decision-making requires high-dimensional information-processing comparable to a single-layer WTA, multicomponent nucleation might be a suitable mechanism for accomplishing the task.These results are not interesting in and of themselves -there is no need to solve image recognition problems at the molecular level -but they can serve as an initial calibration of the computational power of nucleation in molecular self-assembly. Is it more like a linear classifier? Or worse? Or better, perhaps giving similar performance as a two-layer neural network? Or even a multilayer deep neural network? Those familiar with the MNIST problem already know the answer: In LeCun's 1998 study, 65 and a multilayer convolutional neural network achieved a 0.8% test set error. This comparison places the readily implementable checkerboard design as roughly comparable with a single-layer WTA, but even though it's a reach goal, less-constrained multifarious layouts still perform worse than two-layer neural networks. Thus one might imagine that when intracellular decision-making requires high-dimensional information-processing comparable to a single-layer WTA, multicomponent nucleation might be a suitable mechanism for accomplishing the task.</p>
        <p>Training on the MNIST database also reveals a further connection to place cell and Boltzmann machine architectures: training sculpts the energy barrier landscape to be structured analogously to a continuous attractor. To maximize performance, each part of a layout must take responsibility for nucleation in response to different subsets of class images, and overlapping parts of the layout respond to overlapping subsets of image pixels, encouraging neighboring parts of the layout to respond to similar images. This is illustrated in Figure S1.5 for the checkerboard design, which shows the average of images "claimed" by each tile (in the sense that the tile is a part of the fastest-nucleating k × k window within the fastest-nucleating shape when the image's tile concentrations are used).Training on the MNIST database also reveals a further connection to place cell and Boltzmann machine architectures: training sculpts the energy barrier landscape to be structured analogously to a continuous attractor. To maximize performance, each part of a layout must take responsibility for nucleation in response to different subsets of class images, and overlapping parts of the layout respond to overlapping subsets of image pixels, encouraging neighboring parts of the layout to respond to similar images. This is illustrated in Figure S1.5 for the checkerboard design, which shows the average of images "claimed" by each tile (in the sense that the tile is a part of the fastest-nucleating k × k window within the fastest-nucleating shape when the image's tile concentrations are used).</p>
        <p>One might question whether the simplifications inherent in the above abstractions are appropriate. For example, for the winner-take-all effect during annealing, one might be more concerned with comparing which shape experiences significant nucleation first, at the highest temperature, rather than which shape experiences the most nucleation at a given fixed temperature. In this case, the choice of window size k coarsely reflects the choice of annealing speed. Beyond that, as is discussed in Section 2.5, the Window Nucleation Model is only a coarse approximation of nucleation rates for multifarious self-assembly, and as is shown in Section 2.3, the dynamics of tile concentration depletion only imperfectly results in a winner-take-all effect -thus highly abstracted mathematical models can only serve as a coarse guide for understanding the computational power of real molecular systems, and the question is whether their study can lead to insights and decisions that prove useful when working with the real thing. This bottom line, unsurprisingly, is also very familiar in neural computation, where the LTU, PTF, and WTA models (and special cases thereof, such as the clusteron variant of sigma-pi units) naturally arise as abstractions of single-cell and network computational capability, despite the real thing being incredibly more complex. 66-68One might question whether the simplifications inherent in the above abstractions are appropriate. For example, for the winner-take-all effect during annealing, one might be more concerned with comparing which shape experiences significant nucleation first, at the highest temperature, rather than which shape experiences the most nucleation at a given fixed temperature. In this case, the choice of window size k coarsely reflects the choice of annealing speed. Beyond that, as is discussed in Section 2.5, the Window Nucleation Model is only a coarse approximation of nucleation rates for multifarious self-assembly, and as is shown in Section 2.3, the dynamics of tile concentration depletion only imperfectly results in a winner-take-all effect -thus highly abstracted mathematical models can only serve as a coarse guide for understanding the computational power of real molecular systems, and the question is whether their study can lead to insights and decisions that prove useful when working with the real thing. This bottom line, unsurprisingly, is also very familiar in neural computation, where the LTU, PTF, and WTA models (and special cases thereof, such as the clusteron variant of sigma-pi units) naturally arise as abstractions of single-cell and network computational capability, despite the real thing being incredibly more complex. 66-68</p>
        <p>Our experimental system was first designed to robustly demonstrate multifarious self-assembly of the three shapes H, A, and M, and only later, after that was successfully demonstrated, did we choose a set of images and the system to recognize and classify the images based on nucleation rates with winner-take-all competition. It may at first seem implausible that having first designed a multifarious system with three shapes, agnostic of any specific pattern recognition problem, it just so happened that this set of molecules, with no modifications, could be re-interpreted (via the pixel-to-tile map) as correctly classifying our later choice of 18 training images. Further, we claim it could have performed equally well for most any other set of 18 training images (with similar pixel intensity histograms). Shouldn't the actual physical system -the molecules we use for computing -have at least something to do with with the problem being solved? But in fact, our approach and claims are neither implausible nor unreasonablethey are actually consistent with the essential findings of reservoir computing. 69 Our first perspective will frame the conundrum in the context of DNA computing and molecular programming. One might be more comfortable if there were no pixel-to-tile map, and the task were simply stated as "given the following training set of images using pixels 1 through N , design corresponding DNA tiles 1 through N that will correctly classify each image, when using the pixel intensity to set the corresponding tile concentration." This task allows the designer complete freedom in sequence choice, so that an entirely new set of molecules can be synthesized and used for each new pattern recognition problem. Despite not designing new molecules, our results do solve this task as posed, but additionally we implicitly show that the full design freedom is often not necessary: we restrict not only to using SST but also to using the exact sequences from the SHAM design, just renumbering the tiles.Our experimental system was first designed to robustly demonstrate multifarious self-assembly of the three shapes H, A, and M, and only later, after that was successfully demonstrated, did we choose a set of images and the system to recognize and classify the images based on nucleation rates with winner-take-all competition. It may at first seem implausible that having first designed a multifarious system with three shapes, agnostic of any specific pattern recognition problem, it just so happened that this set of molecules, with no modifications, could be re-interpreted (via the pixel-to-tile map) as correctly classifying our later choice of 18 training images. Further, we claim it could have performed equally well for most any other set of 18 training images (with similar pixel intensity histograms). Shouldn't the actual physical system -the molecules we use for computing -have at least something to do with with the problem being solved? But in fact, our approach and claims are neither implausible nor unreasonablethey are actually consistent with the essential findings of reservoir computing. 69 Our first perspective will frame the conundrum in the context of DNA computing and molecular programming. One might be more comfortable if there were no pixel-to-tile map, and the task were simply stated as "given the following training set of images using pixels 1 through N , design corresponding DNA tiles 1 through N that will correctly classify each image, when using the pixel intensity to set the corresponding tile concentration." This task allows the designer complete freedom in sequence choice, so that an entirely new set of molecules can be synthesized and used for each new pattern recognition problem. Despite not designing new molecules, our results do solve this task as posed, but additionally we implicitly show that the full design freedom is often not necessary: we restrict not only to using SST but also to using the exact sequences from the SHAM design, just renumbering the tiles.</p>
        <p>As a second perspective, note that it is standard in the field for work that focuses on molecular information processing to allow computational signals to be encoded however the system designer wishes, rather than requiring the system to interface with pre-determined input and output molecules. This is analogous to the situation in robotics where sensors and actuators may have electronic components whose construction is specialized to interact with the physical world (photodiodes, electromagnetic motors) but within the programmable microprocessor, all wires and logic gates process information (0s and 1s, low and high voltages) using a representation that is independent of the physical meaning. In molecular systems, we may also have processes that translate various chemical signals into a form compatible with the information processing core, as well as those that translate the output of information processing to actuate some chemical response. For example, the DNA strand displacement circuits may use "translator" molecules to convert the presence or absence of a free biological RNA strand into the the presence or absence of a free DNA strand with unrelated sequence that is suitable for circuit computation. 70,71 Thus, work on circuit computation principles often ignores the translation step, and assumes that input signals come in whatever molecular format is needed for the computing subsystem.As a second perspective, note that it is standard in the field for work that focuses on molecular information processing to allow computational signals to be encoded however the system designer wishes, rather than requiring the system to interface with pre-determined input and output molecules. This is analogous to the situation in robotics where sensors and actuators may have electronic components whose construction is specialized to interact with the physical world (photodiodes, electromagnetic motors) but within the programmable microprocessor, all wires and logic gates process information (0s and 1s, low and high voltages) using a representation that is independent of the physical meaning. In molecular systems, we may also have processes that translate various chemical signals into a form compatible with the information processing core, as well as those that translate the output of information processing to actuate some chemical response. For example, the DNA strand displacement circuits may use "translator" molecules to convert the presence or absence of a free biological RNA strand into the the presence or absence of a free DNA strand with unrelated sequence that is suitable for circuit computation. 70,71 Thus, work on circuit computation principles often ignores the translation step, and assumes that input signals come in whatever molecular format is needed for the computing subsystem.</p>
        <p>From this perspective, we could imagine an input sensor layer upstream of the actual multifarious self-assembly that performs pattern recognition: a set of N translator molecules converts each target input (a DNA or RNA strand, protein, small molecule, ...) to a comparable amount of the corresponding SST strand, which will nucleate and grow into one shape or another once the concentrations of key components become large enough (Figure S1.6). It's clear that the computation for recognizing patterns is not taking place within the translator layer, as this layer just performs a 1-to-1 conversion of molecular sequence. Thus, in our work, training the pixel-to-tile map θ corresponds directly to designing a (virtual) translator layer upstream of the self-assembly process. On the output side, relative positions of neighboring tiles are unique to each shape, and thus could directly support cooperative binding of scaffolding to activate e.g. an enzyme cascade 72 or trigger other shape-selective downstream processes.From this perspective, we could imagine an input sensor layer upstream of the actual multifarious self-assembly that performs pattern recognition: a set of N translator molecules converts each target input (a DNA or RNA strand, protein, small molecule, ...) to a comparable amount of the corresponding SST strand, which will nucleate and grow into one shape or another once the concentrations of key components become large enough (Figure S1.6). It's clear that the computation for recognizing patterns is not taking place within the translator layer, as this layer just performs a 1-to-1 conversion of molecular sequence. Thus, in our work, training the pixel-to-tile map θ corresponds directly to designing a (virtual) translator layer upstream of the self-assembly process. On the output side, relative positions of neighboring tiles are unique to each shape, and thus could directly support cooperative binding of scaffolding to activate e.g. an enzyme cascade 72 or trigger other shape-selective downstream processes.</p>
        <p>A third perspective supposes that we want to classify a set of N target DNA strands, with specific sequences, according to their concentration patterns -and we don't want a separate input translation layer. Our work here suggests that the following approach should be sufficient: Lay out the target strands within three shapes (if we have 3 categories of output) by assigning each strand to a random 'white' checkerboard of positions, then for each shape design an additional ∼ N shape-specific tiles to fill the 'black' checkerboard positions. The sequences of the 'black' strands will be dictated by the sequences of the target 'white' strands. For pattern recognition experiments, keep the shape-specific tile concentrations constant at the minimum standard concentration, and add the target An input layer of translator gates detects analytes (here nucleic acid strands, but they could be proteins, small organic chemicals, or other molecules that can be recognized by aptamers, for example) by triggering the release of a sequestered tile strand. Design of the translator gate determines which analyte produces which tile strand, analogous to the pixel-to-tile map θ, and thus selects what pattern recognition task the nucleation-and-self-assembly-based computational reservoir will perform. In the presence of an output layer of proteins that cooperatively bind to pairs of tiles that are specific to each shape, the assembly of a specific structure results in activation of a specific enzyme cascade. Other modalities of output could of course be envisioned as well.A third perspective supposes that we want to classify a set of N target DNA strands, with specific sequences, according to their concentration patterns -and we don't want a separate input translation layer. Our work here suggests that the following approach should be sufficient: Lay out the target strands within three shapes (if we have 3 categories of output) by assigning each strand to a random 'white' checkerboard of positions, then for each shape design an additional ∼ N shape-specific tiles to fill the 'black' checkerboard positions. The sequences of the 'black' strands will be dictated by the sequences of the target 'white' strands. For pattern recognition experiments, keep the shape-specific tile concentrations constant at the minimum standard concentration, and add the target An input layer of translator gates detects analytes (here nucleic acid strands, but they could be proteins, small organic chemicals, or other molecules that can be recognized by aptamers, for example) by triggering the release of a sequestered tile strand. Design of the translator gate determines which analyte produces which tile strand, analogous to the pixel-to-tile map θ, and thus selects what pattern recognition task the nucleation-and-self-assembly-based computational reservoir will perform. In the presence of an output layer of proteins that cooperatively bind to pairs of tiles that are specific to each shape, the assembly of a specific structure results in activation of a specific enzyme cascade. Other modalities of output could of course be envisioned as well.</p>
        <p>DNA according to the pattern to be classified. Computation done this way is maximally 'entangled'; it does not have a separate sensor layer and a separate compute layer, and thus it would be more compact. Now let's return to why one might not be too surprised that using the same SHAM molecules, but just reassigning the pixel-to-tile map, is likely to be successful for many different pattern recognition training sets. As we've said, we consider the phenomenon to be analogous to -or an example of -reservoir computing.DNA according to the pattern to be classified. Computation done this way is maximally 'entangled'; it does not have a separate sensor layer and a separate compute layer, and thus it would be more compact. Now let's return to why one might not be too surprised that using the same SHAM molecules, but just reassigning the pixel-to-tile map, is likely to be successful for many different pattern recognition training sets. As we've said, we consider the phenomenon to be analogous to -or an example of -reservoir computing.</p>
        <p>Reservoir computing was initially developed as "echo state networks" 73 and "liquid state machines", 74 and later generalized. The common motivation was to find an easier and faster way to train recurrent neural networks to predict or classify time-series data, compared to standard approaches such as backpropagation-through-time, which can be very slow because of the difficulty of adjusting network weights that control its dynamical behavior. So instead of modifying the network's dynamical behavior during training, reservoir computing takes a fixed recurrent network that has fixed dynamical behavior, and only trains a linear input layer and a linear output layer -in fact, the standard set-up only trains the output layer, as a simple and fast learning algorithm suffices when there is a direct error signal.Reservoir computing was initially developed as "echo state networks" 73 and "liquid state machines", 74 and later generalized. The common motivation was to find an easier and faster way to train recurrent neural networks to predict or classify time-series data, compared to standard approaches such as backpropagation-through-time, which can be very slow because of the difficulty of adjusting network weights that control its dynamical behavior. So instead of modifying the network's dynamical behavior during training, reservoir computing takes a fixed recurrent network that has fixed dynamical behavior, and only trains a linear input layer and a linear output layer -in fact, the standard set-up only trains the output layer, as a simple and fast learning algorithm suffices when there is a direct error signal.</p>
        <p>Remarkably, this often works. Perhaps more remarkably, the fixed recurrent network may often be generated randomly, so long as the generation method ensures with high probability that certain properties will hold, such as having damped non-chaotic dynamics, having modes with a broad range of time scales, and having sufficient diversity of nonlinearities that guarantee better approximations for larger networks. Once chosen, this fixed network is referred to as a "reservoir" for dynamical behavior that can be stimulated by the input layer mapping and read out by the output layer mapping. Furthermore, the reservoir needn't be a neural network, but could be other forms of dynamical systems or even physical systems -indeed, an early example of a reservoir was waves in a pond stimulated by pebbles dropped into it. The fundamental observation is that there exist classes of randomlygenerated networks that contain within them the potential for a remarkable diversity of behaviors, which can be accessed easily via training simple (e.g. linear) output and/or input encodings. Although typically used for spatiotemporal pattern recognition, reservoir computing can also be applied to static pattern recognition problems, where the fundamental observation still holds (although seldom being competitive with standard backprop).Remarkably, this often works. Perhaps more remarkably, the fixed recurrent network may often be generated randomly, so long as the generation method ensures with high probability that certain properties will hold, such as having damped non-chaotic dynamics, having modes with a broad range of time scales, and having sufficient diversity of nonlinearities that guarantee better approximations for larger networks. Once chosen, this fixed network is referred to as a "reservoir" for dynamical behavior that can be stimulated by the input layer mapping and read out by the output layer mapping. Furthermore, the reservoir needn't be a neural network, but could be other forms of dynamical systems or even physical systems -indeed, an early example of a reservoir was waves in a pond stimulated by pebbles dropped into it. The fundamental observation is that there exist classes of randomlygenerated networks that contain within them the potential for a remarkable diversity of behaviors, which can be accessed easily via training simple (e.g. linear) output and/or input encodings. Although typically used for spatiotemporal pattern recognition, reservoir computing can also be applied to static pattern recognition problems, where the fundamental observation still holds (although seldom being competitive with standard backprop).</p>
        <p>In the molecular programming field, reservoir computing has previously been proposed as an architecture for temporal pattern recognition by programmable coupled biochemical oscillators. 75 To view the SHAM self-assembly system through the lens of reservoir computing, we ask, "What is the task? What is the reservoir? What is the input layer? What is the output layer?" Our task is static pattern recognition, so we don't need complex temporal dynamics in the reservoir. The reservoir itself is the fixed design of the three-shape multifarious tile set, together with the standard experimental protocol of a constant temperature hold or a temperature anneal. The dynamics stabilize and yield an output that is the distribution of self-assembled shapes; the output encoding is trivial: shape H means class "H", shape A means class "A", and shape M means class "M". The reservoir is trained to solve different problems by training the input encoding, i.e. the pixel-to-tile map. Unlike the standard reservoir computing set-up, where only the output layer is trained and a simple and fast learning algorithm suffices, the dependence of the output on the input encoding is complicated in the SHAM system, so a brute-force hill-climbing optimization was needed in order to train the system for a particular pattern recognition task. Thus, the characteristic ease of training reservoir systems does not carry over to the SHAM system. However, the fact that a single fixed core system contains within itself the potential to solve nearly arbitrary pattern recognition problems (within the system's capacity) is very much in line with the findings and principles of reservoir computing. Furthermore, it helps explain the at-first paradoxical-seeming fact that we do not need to design new molecules in order to use the SHAM system to solve new pattern recognition tasks.In the molecular programming field, reservoir computing has previously been proposed as an architecture for temporal pattern recognition by programmable coupled biochemical oscillators. 75 To view the SHAM self-assembly system through the lens of reservoir computing, we ask, "What is the task? What is the reservoir? What is the input layer? What is the output layer?" Our task is static pattern recognition, so we don't need complex temporal dynamics in the reservoir. The reservoir itself is the fixed design of the three-shape multifarious tile set, together with the standard experimental protocol of a constant temperature hold or a temperature anneal. The dynamics stabilize and yield an output that is the distribution of self-assembled shapes; the output encoding is trivial: shape H means class "H", shape A means class "A", and shape M means class "M". The reservoir is trained to solve different problems by training the input encoding, i.e. the pixel-to-tile map. Unlike the standard reservoir computing set-up, where only the output layer is trained and a simple and fast learning algorithm suffices, the dependence of the output on the input encoding is complicated in the SHAM system, so a brute-force hill-climbing optimization was needed in order to train the system for a particular pattern recognition task. Thus, the characteristic ease of training reservoir systems does not carry over to the SHAM system. However, the fact that a single fixed core system contains within itself the potential to solve nearly arbitrary pattern recognition problems (within the system's capacity) is very much in line with the findings and principles of reservoir computing. Furthermore, it helps explain the at-first paradoxical-seeming fact that we do not need to design new molecules in order to use the SHAM system to solve new pattern recognition tasks.</p>
        <p>We consider the multifarious self-assembly architecture explored in this paper to be a remarkably compact, robust, and scalable molecular architecture for solving pattern recognition and classification problems. However, this claim is difficult to quantify, and solid conclusions will have to wait for future investigations. Nonetheless, comparisons to several other experimentally-demonstrated molecular implementations of neural network computation may serve as useful reference points.We consider the multifarious self-assembly architecture explored in this paper to be a remarkably compact, robust, and scalable molecular architecture for solving pattern recognition and classification problems. However, this claim is difficult to quantify, and solid conclusions will have to wait for future investigations. Nonetheless, comparisons to several other experimentally-demonstrated molecular implementations of neural network computation may serve as useful reference points.</p>
        <p>As for our work, the architecture is compact in the sense that processing 900-variable input vectors required only 900 distinct DNA strands, which are themselves the input strands; it is robust in the sense that the strands could be used without purification and in that speckle errors did not hinder recognition; it is scalable in the sense that processing N input variables would entail just N strands, and prior demonstrations of uniquely-addressed SST assemblies (in 3D) utilised as many as N = 30,000 SST tiles. 76 That said, as the computational power of this architecture is not yet characterized, and the tile assembly geometry imposes limitations, we cannot at this time assess how general the pattern recognition would be for N = 30,000.As for our work, the architecture is compact in the sense that processing 900-variable input vectors required only 900 distinct DNA strands, which are themselves the input strands; it is robust in the sense that the strands could be used without purification and in that speckle errors did not hinder recognition; it is scalable in the sense that processing N input variables would entail just N strands, and prior demonstrations of uniquely-addressed SST assemblies (in 3D) utilised as many as N = 30,000 SST tiles. 76 That said, as the computational power of this architecture is not yet characterized, and the tile assembly geometry imposes limitations, we cannot at this time assess how general the pattern recognition would be for N = 30,000.</p>
        <p>The first reference point is the earliest attempt to synthesize a biochemical Hopfield network using DNA 'genelet' templates and 2 essential enzymes. 15,16 Here, a 2-neuron bistable memory required 6 DNA strands; it established a dynamic steady-state that could correct small errors. As each genelet template directly implemented a neural synapse, generalizing to a fully connected N -unit Hopfield memory would require 3N 2 strands. However, imperfect enzyme activity leads to an accumulation of 'waste' strands that limit function. Modern improvements on the genelet architecture has allowed scaling up to ∼ 15-genelet systems. 17 A second reference point are enzyme-free DNA strand displacement implementations of neural network computation. A use-once implementation of a fully-connected 4-neuron Hopfield memory 21 required 112 strands, which were prepared and purified as 72 separate complexes to avoid performance-limiting leak reactions. Scaling to N -bit patterns would entail O(N 2 ) purified complexes if all O(N 2 ) weights are used, but presumably many relevant pattern recognition tasks would not need a fully-connected network. A use-once implementation of a 100-bit input, 6-neuron winner-take-all computation 22 used 225 purified species to classify downscaled and binarized 10 × 10 MNIST digits using just 20% of available weights. Using the same architecture for a 3-neuron WTA that classifies 900-bit input, analogous to our 900-variable analog pattern classification task, would require two strands per network weight; if just 20% of all possible weights were required to obtain adequate performance, then roughly 1200 strands would be needed, in addition to the 900 input strands. As an alternate comparison, Section 1.4 makes the case that the full-scale MNIST images (28 × 28 grayscale) could be classified by a multifarious system of ten 12 × 12 tile shapes, using just 792 DNA strands -again suggesting more bang for the buck. Most recently, a convolutional neural network was implemented with DNA strand displacement circuits, 77 using 512 strands to classify 144-bit input into 32 categories. An impediment to drawing conclusions from these numbers is the hard-to-compare quality of each computation, such as the accuracy performance of the actual experimental system, or its performance on out-of-sample inputs -for example, the convolutional network exhibited robustness to rotated patterns.The first reference point is the earliest attempt to synthesize a biochemical Hopfield network using DNA 'genelet' templates and 2 essential enzymes. 15,16 Here, a 2-neuron bistable memory required 6 DNA strands; it established a dynamic steady-state that could correct small errors. As each genelet template directly implemented a neural synapse, generalizing to a fully connected N -unit Hopfield memory would require 3N 2 strands. However, imperfect enzyme activity leads to an accumulation of 'waste' strands that limit function. Modern improvements on the genelet architecture has allowed scaling up to ∼ 15-genelet systems. 17 A second reference point are enzyme-free DNA strand displacement implementations of neural network computation. A use-once implementation of a fully-connected 4-neuron Hopfield memory 21 required 112 strands, which were prepared and purified as 72 separate complexes to avoid performance-limiting leak reactions. Scaling to N -bit patterns would entail O(N 2 ) purified complexes if all O(N 2 ) weights are used, but presumably many relevant pattern recognition tasks would not need a fully-connected network. A use-once implementation of a 100-bit input, 6-neuron winner-take-all computation 22 used 225 purified species to classify downscaled and binarized 10 × 10 MNIST digits using just 20% of available weights. Using the same architecture for a 3-neuron WTA that classifies 900-bit input, analogous to our 900-variable analog pattern classification task, would require two strands per network weight; if just 20% of all possible weights were required to obtain adequate performance, then roughly 1200 strands would be needed, in addition to the 900 input strands. As an alternate comparison, Section 1.4 makes the case that the full-scale MNIST images (28 × 28 grayscale) could be classified by a multifarious system of ten 12 × 12 tile shapes, using just 792 DNA strands -again suggesting more bang for the buck. Most recently, a convolutional neural network was implemented with DNA strand displacement circuits, 77 using 512 strands to classify 144-bit input into 32 categories. An impediment to drawing conclusions from these numbers is the hard-to-compare quality of each computation, such as the accuracy performance of the actual experimental system, or its performance on out-of-sample inputs -for example, the convolutional network exhibited robustness to rotated patterns.</p>
        <p>As a third reference point, due to the difficulties of protein design and genetic engineering and the intracellular environment, synthetic neuromorphic computation in living cells has not yet advanced beyond three inputs and two neurons. [24][25][26][27] A major consideration for system robustness is the effect of imperfect molecules. Most large strand displacement circuits and enzyme circuits, including the ones mentioned above, require PAGE or HPLC purified strands and complexes in order to reduce failures due to leak and side reactions. Remarkably, our system based on multifarious self-assembly worked using unpurified synthetic DNA strands for the SST, similar to the prior demonstration of algorithmic self-assembly using unpurified SST. 78 There, the robustness was partially attributed to the ability of crystal growth to exclude impurities under only slightly supersaturated conditions (i.e. only slightly below the melting temperature) -a phenomenon thought to be enhanced by the proofreading tile set design. Since DNA oligonucleotide synthesis errors are dominated by truncations that would cause affected tiles to have weaker binding to other tiles, it is possible that critical nuclei are systematically enriched for non-erroneous strands, conferring an extra degree of robustness in the nucleation energy barrier heights and thus in the pattern recognition kinetics. Furthermore, although not studied here, it is reasonable to expect that theoretical connections between multifarious nucleation and neural networks confers some additional degree of robustness to imperfect inputs, as this is often found in distributed neural computation.As a third reference point, due to the difficulties of protein design and genetic engineering and the intracellular environment, synthetic neuromorphic computation in living cells has not yet advanced beyond three inputs and two neurons. [24][25][26][27] A major consideration for system robustness is the effect of imperfect molecules. Most large strand displacement circuits and enzyme circuits, including the ones mentioned above, require PAGE or HPLC purified strands and complexes in order to reduce failures due to leak and side reactions. Remarkably, our system based on multifarious self-assembly worked using unpurified synthetic DNA strands for the SST, similar to the prior demonstration of algorithmic self-assembly using unpurified SST. 78 There, the robustness was partially attributed to the ability of crystal growth to exclude impurities under only slightly supersaturated conditions (i.e. only slightly below the melting temperature) -a phenomenon thought to be enhanced by the proofreading tile set design. Since DNA oligonucleotide synthesis errors are dominated by truncations that would cause affected tiles to have weaker binding to other tiles, it is possible that critical nuclei are systematically enriched for non-erroneous strands, conferring an extra degree of robustness in the nucleation energy barrier heights and thus in the pattern recognition kinetics. Furthermore, although not studied here, it is reasonable to expect that theoretical connections between multifarious nucleation and neural networks confers some additional degree of robustness to imperfect inputs, as this is often found in distributed neural computation.</p>
        <p>With respect to speed, the DNA strand displacement circuit for classifying MNIST digits operated in 10-hour experiments, which is faster than our 150-hour experiments for multifarious nucleation. However, neither of these represent the limits for the molecular computing. Subsequent work on DNA strand displacement circuits identified new principles, new molecular architectures, and new experimental protocols that reduced the computation time for comparably-sized circuits to just a few minutes. 71,79 The speed limits for computation by nucleation remain to be studied in detail; while general arguments (such as those outlined in Extended Data Fig. E10) suggest an exponential slowdown for problems that require larger critical nucleus sizes, it is not yet clear exactly how much extra computational power the larger critical nucleus sizes would provide -nor do we yet understand the trade-offs with other factors such as concentration ranges, temperature, and the potential for active regulation of nucleation such as is seen in microtubules. 80 These distinctions in compactness, robustness and scalability between prior work on DNA neural networks and ours are partly explained by our not designing molecules explicitly to carry out each steo of the weighted sum and thresholding mechanisms for individual neurons. Rather, we exploit what molecules do naturally and note the implicit effect on the assembly energy, thereby exploiting their collective behavior to carry out a pattern recognition task.With respect to speed, the DNA strand displacement circuit for classifying MNIST digits operated in 10-hour experiments, which is faster than our 150-hour experiments for multifarious nucleation. However, neither of these represent the limits for the molecular computing. Subsequent work on DNA strand displacement circuits identified new principles, new molecular architectures, and new experimental protocols that reduced the computation time for comparably-sized circuits to just a few minutes. 71,79 The speed limits for computation by nucleation remain to be studied in detail; while general arguments (such as those outlined in Extended Data Fig. E10) suggest an exponential slowdown for problems that require larger critical nucleus sizes, it is not yet clear exactly how much extra computational power the larger critical nucleus sizes would provide -nor do we yet understand the trade-offs with other factors such as concentration ranges, temperature, and the potential for active regulation of nucleation such as is seen in microtubules. 80 These distinctions in compactness, robustness and scalability between prior work on DNA neural networks and ours are partly explained by our not designing molecules explicitly to carry out each steo of the weighted sum and thresholding mechanisms for individual neurons. Rather, we exploit what molecules do naturally and note the implicit effect on the assembly energy, thereby exploiting their collective behavior to carry out a pattern recognition task.</p>
        <p>Put another way, our molecular pattern recognition architecture is founded on uncovering the latent capabilities of inevitable molecular processes. Any system that self-assembles must go through a nucleation phase, so the question is what features does the process's energy landscape contain and how can they be programmed? In this case the self-assembly energy landscape and nucleation energy barriers share similarities with Hopfield networks, Boltzmann machines, and place cell models, and they can be programmed by geometric layout of tiles within target shapes. It is natural to assume that energy landscapes and energy barriers for other "inevitable" physical processes will also be seen to have inherent computational capabilities in the multicomponent limit where "more is different".Put another way, our molecular pattern recognition architecture is founded on uncovering the latent capabilities of inevitable molecular processes. Any system that self-assembles must go through a nucleation phase, so the question is what features does the process's energy landscape contain and how can they be programmed? In this case the self-assembly energy landscape and nucleation energy barriers share similarities with Hopfield networks, Boltzmann machines, and place cell models, and they can be programmed by geometric layout of tiles within target shapes. It is natural to assume that energy landscapes and energy barriers for other "inevitable" physical processes will also be seen to have inherent computational capabilities in the multicomponent limit where "more is different".</p>
        <p>Prior work on programmable self-assembly of DNA tiles primarily focused on a different inevitable process in the multicomponent limit: layer by layer crystalline growth, rather than crystalline nucleation. In that work, nucleation is dictated by an explicit seed, such as a long strand or DNA origami structure to which futher tiles can bind. 78,[81][82][83][84] There, the seemingly natural connection was to one-dimensional (usually deterministic) cellular automata, whose space-time history pattern is constructed as the algorithmic crystal grows by selecting the bestfitting tile at each growth site. While one-dimensional cellular automata are a powerful model of computation, and they can directly simulate Turing machines and even a class of parallel multihead Turing machines, programming them has a discrete symbolic flavor as it is usually done, which can come across as brittle and belabored. For example, it is currently unclear how to program a cellular automaton to perform the same pattern recognition task as was done here (classifying 18 arbitrary images into three arbitrary classes).Prior work on programmable self-assembly of DNA tiles primarily focused on a different inevitable process in the multicomponent limit: layer by layer crystalline growth, rather than crystalline nucleation. In that work, nucleation is dictated by an explicit seed, such as a long strand or DNA origami structure to which futher tiles can bind. 78,[81][82][83][84] There, the seemingly natural connection was to one-dimensional (usually deterministic) cellular automata, whose space-time history pattern is constructed as the algorithmic crystal grows by selecting the bestfitting tile at each growth site. While one-dimensional cellular automata are a powerful model of computation, and they can directly simulate Turing machines and even a class of parallel multihead Turing machines, programming them has a discrete symbolic flavor as it is usually done, which can come across as brittle and belabored. For example, it is currently unclear how to program a cellular automaton to perform the same pattern recognition task as was done here (classifying 18 arbitrary images into three arbitrary classes).</p>
        <p>We have demonstrated high dimensional pattern recognition by exploiting competitive nucleation in a system of molecules with multiple crystal structures i.e., crystal polymorphism, first described by Mitscherlich 85 and a bane to crystallographers such as Hodgkin. 86 Indeed, many scientific and industrial applications rely on biasing nucleation towards one of the crystal polymorphs by tuning annealing protocols. Our work updates these classic ideas for heterogeneous crystals where the number of distinct components is of the size of the crystal itself. 37,49,87 This new heterogeneous context introduces novel elements such as pattern recognition and winner-take-all nucleation: in our system, depletion of components can lower nucleation rates of off-target structures more so than for the desired structure while in classical crystal polymorphism, depletion lowers nucleation rates for all crystalline forms.We have demonstrated high dimensional pattern recognition by exploiting competitive nucleation in a system of molecules with multiple crystal structures i.e., crystal polymorphism, first described by Mitscherlich 85 and a bane to crystallographers such as Hodgkin. 86 Indeed, many scientific and industrial applications rely on biasing nucleation towards one of the crystal polymorphs by tuning annealing protocols. Our work updates these classic ideas for heterogeneous crystals where the number of distinct components is of the size of the crystal itself. 37,49,87 This new heterogeneous context introduces novel elements such as pattern recognition and winner-take-all nucleation: in our system, depletion of components can lower nucleation rates of off-target structures more so than for the desired structure while in classical crystal polymorphism, depletion lowers nucleation rates for all crystalline forms.</p>
        <p>Selection of crystal polymorph formed during nucleation and growth is inherently governed by kinetic, rather than thermodynamic, principles. In the limit of an infinitely slow anneal, of course a finite-sized system will eventually adopt the energetically most favorable polymorph, but the basins in the energy landscape corresponding to the polymorphs may be so deep and with such high barriers between them that this limit is of little or no relevance to experimental systems. For experimentally-relevant time scales, the kinetics determining which basin becomes occupied most quickly.Selection of crystal polymorph formed during nucleation and growth is inherently governed by kinetic, rather than thermodynamic, principles. In the limit of an infinitely slow anneal, of course a finite-sized system will eventually adopt the energetically most favorable polymorph, but the basins in the energy landscape corresponding to the polymorphs may be so deep and with such high barriers between them that this limit is of little or no relevance to experimental systems. For experimentally-relevant time scales, the kinetics determining which basin becomes occupied most quickly.</p>
        <p>In the SHAM system, there are four natural kinetic regimes to consider. First, on the fastest time scale where most nucleation occurs at a low enough temperature that the critical nuclei are just dimers of two tiles, the amount of nucleation will be roughly proportional to the number of critical nucleation pathways, i.e. the number of distinct dimers that could form. In this case, structure A is slightly favored, as it has an area of 496 tiles compared to 480 for the other two shapes (c.f. Figure 2b). Second, a slower anneal will allow significant nucleation before the temperature gets so low, so critical nuclei will be large enough that their energy barrier will depend on the integrated contributions of perhaps dozens of tiles, and thus on the spatial colocalization of high-concentration tiles. Once nucleated, continued growth will trap the assembly within a deep energy basin, regardless of which structure has formed. This is the regime explored in detail in our work here. Third, an anneal may so slow that the relative melting temperatures of each structure determines which is observed: During the temperature interval between the highest-melting-temperature structure and the next highest, only one structure can form, and given enough time, it will, to the exclusion of the others. As shown e.g. in Section S5.3.38, the model we discuss below predicts that structure H has the highest melting temperature. Finally, the fourth regime is an anneal slow enough to reach thermodynamic equilibrium, in which case the structure H will almost completely dominate, as it has the highest area-to-perimeter ratio (c.f. Figure 3e). Although in this case the third and fourth regimes select for the same structure, it need not in general be the case, because the DNA hybridization and entropic effects can give different energy-vs-temperature slopes for different structures. These conclusions still generally hold, with some adjustments and within certain limits, when the tile concentrations are not uniform.In the SHAM system, there are four natural kinetic regimes to consider. First, on the fastest time scale where most nucleation occurs at a low enough temperature that the critical nuclei are just dimers of two tiles, the amount of nucleation will be roughly proportional to the number of critical nucleation pathways, i.e. the number of distinct dimers that could form. In this case, structure A is slightly favored, as it has an area of 496 tiles compared to 480 for the other two shapes (c.f. Figure 2b). Second, a slower anneal will allow significant nucleation before the temperature gets so low, so critical nuclei will be large enough that their energy barrier will depend on the integrated contributions of perhaps dozens of tiles, and thus on the spatial colocalization of high-concentration tiles. Once nucleated, continued growth will trap the assembly within a deep energy basin, regardless of which structure has formed. This is the regime explored in detail in our work here. Third, an anneal may so slow that the relative melting temperatures of each structure determines which is observed: During the temperature interval between the highest-melting-temperature structure and the next highest, only one structure can form, and given enough time, it will, to the exclusion of the others. As shown e.g. in Section S5.3.38, the model we discuss below predicts that structure H has the highest melting temperature. Finally, the fourth regime is an anneal slow enough to reach thermodynamic equilibrium, in which case the structure H will almost completely dominate, as it has the highest area-to-perimeter ratio (c.f. Figure 3e). Although in this case the third and fourth regimes select for the same structure, it need not in general be the case, because the DNA hybridization and entropic effects can give different energy-vs-temperature slopes for different structures. These conclusions still generally hold, with some adjustments and within certain limits, when the tile concentrations are not uniform.</p>
        <p>Similar kinetic selection of non-equilibrium trapped states has been seen more broadly in polymer folding, e.g., in synthetic DNA origami, 88 natural co-transcriptional folding of RNA, 89 and annealing of DNA hairpin systems. 90,91 Further, such non-equilibrium self-assembly could be made contingent on a complex chemical context, defined by combinations of molecules rather than single species -illustrating how such systems could process information and make decisions.Similar kinetic selection of non-equilibrium trapped states has been seen more broadly in polymer folding, e.g., in synthetic DNA origami, 88 natural co-transcriptional folding of RNA, 89 and annealing of DNA hairpin systems. 90,91 Further, such non-equilibrium self-assembly could be made contingent on a complex chemical context, defined by combinations of molecules rather than single species -illustrating how such systems could process information and make decisions.</p>
        <p>The following section develops rudimentary mathematical models and simulation algorithms for quantitatively treating these issues in the context of multifarious self-assembly of DNA tiles.The following section develops rudimentary mathematical models and simulation algorithms for quantitatively treating these issues in the context of multifarious self-assembly of DNA tiles.</p>
        <p>Models of nucleation and growth in self-assembling systems must strike a balance between a closeness to physical reality that allows them to capture details of complex behaviors, and an abstraction that allows broader theoretical reasoning. To investigate different questions about our system, we used four different models of nucleation at different points in this spectrum.Models of nucleation and growth in self-assembling systems must strike a balance between a closeness to physical reality that allows them to capture details of complex behaviors, and an abstraction that allows broader theoretical reasoning. To investigate different questions about our system, we used four different models of nucleation at different points in this spectrum.</p>
        <p>More detailed and physically realistic than any of the models we used would be atomic-level 92 and coarsegrained 93,94 molecular dynamics simulations of the DNA molecules themselves. Although we do not consider any such models explicitly here, we make use of conclusions derived from such studies by others. 95 The lowest-level model we use directly is the single-assembly kinetic Tile Assembly Model (kTAM), 96 for which a stochastic simulator, Xgrow, is available (Section 2.1). This model abstracts the details of DNA tiles -whose implementations involve strands that interact through hybridization and folding processes -into abstract tiles that attach and detach in single steps and form perfectly rigid lattices. We use the kTAM to analyze the growth of assemblies in our system, and the potential for the formation of chimeric assemblies. To bring the simulation predictions into the ballpark of experimental reality, we attempted to tune the model parameters to fit the singlestranded tile (SST) motif, using experimental and computational results on DNA tile energetics. 95,97,98 While simulations in the single-assembly kTAM are useful for favorable growth processes, where assemblies are likely to grow through similar pathways into similar structures and thus the simulation of one, or a few, individual assemblies can be instructive, nucleation involves the exploration of many unfavorable pathways by many small assemblies at once, many of which will melt, and so direct simulation of nucleation by the single-assembly kTAM would be computationally expensive. However, the model was generalized by Schulman &amp; Winfree to consider multiple assemblies growing according to mass-action kinetics; they used this to prove theoretical bounds on nucleation rates that can be calculated with some knowledge of the energy landscape of a system's possible assemblies and assembly pathways. 99 To sample assembly pathways for this bound without needing to exhaustively enumerate them, which would be computationally intractable, we developed the Stochastic Greedy Model, which seeks to find likely assembly pathways for assemblies in our system through greedily making favorable attachments and stochastically choosing unfavorable ones. The assembly pathways thus enumerated are then evaluated with respect to the reactions in the generalized kTAM, allowing estimation of pattern-dependent differences in nucleation rates. We used the Stochastic Greedy Model (Section 2.2) for predicting pattern-dependent nucleation rates in our experiments (Figures 3,4, and 5) as well as for final optimization of pixel-to-tile mapping for pattern recognition.More detailed and physically realistic than any of the models we used would be atomic-level 92 and coarsegrained 93,94 molecular dynamics simulations of the DNA molecules themselves. Although we do not consider any such models explicitly here, we make use of conclusions derived from such studies by others. 95 The lowest-level model we use directly is the single-assembly kinetic Tile Assembly Model (kTAM), 96 for which a stochastic simulator, Xgrow, is available (Section 2.1). This model abstracts the details of DNA tiles -whose implementations involve strands that interact through hybridization and folding processes -into abstract tiles that attach and detach in single steps and form perfectly rigid lattices. We use the kTAM to analyze the growth of assemblies in our system, and the potential for the formation of chimeric assemblies. To bring the simulation predictions into the ballpark of experimental reality, we attempted to tune the model parameters to fit the singlestranded tile (SST) motif, using experimental and computational results on DNA tile energetics. 95,97,98 While simulations in the single-assembly kTAM are useful for favorable growth processes, where assemblies are likely to grow through similar pathways into similar structures and thus the simulation of one, or a few, individual assemblies can be instructive, nucleation involves the exploration of many unfavorable pathways by many small assemblies at once, many of which will melt, and so direct simulation of nucleation by the single-assembly kTAM would be computationally expensive. However, the model was generalized by Schulman &amp; Winfree to consider multiple assemblies growing according to mass-action kinetics; they used this to prove theoretical bounds on nucleation rates that can be calculated with some knowledge of the energy landscape of a system's possible assemblies and assembly pathways. 99 To sample assembly pathways for this bound without needing to exhaustively enumerate them, which would be computationally intractable, we developed the Stochastic Greedy Model, which seeks to find likely assembly pathways for assemblies in our system through greedily making favorable attachments and stochastically choosing unfavorable ones. The assembly pathways thus enumerated are then evaluated with respect to the reactions in the generalized kTAM, allowing estimation of pattern-dependent differences in nucleation rates. We used the Stochastic Greedy Model (Section 2.2) for predicting pattern-dependent nucleation rates in our experiments (Figures 3,4, and 5) as well as for final optimization of pixel-to-tile mapping for pattern recognition.</p>
        <p>However, as a stochastic model requiring the collection of many trajectories to produce low-variance estimates of nucleation rates, the Stochastic Greedy Model was either too slow (with more trajectories) or too noisy (with fewer trajectories) to provide the fast, stable comparisons of nucleation rates between similar concentration patterns needed for optimization algorithms searching within the large space of pixel to tile mappings. For this purpose, we instead developed a simple, deterministic, heuristic model, the Window Nucleation Model (Section 2.5), based on intuitive reasoning about the thermodynamics of small assemblies and potential critical nuclei along assembly pathways. This model does not provide a nucleation rate, but instead provides a score value for each concentration pattern such that, if one pattern has a higher score than another, then that first pattern is likely, though not guaranteed, to have a higher nucleation rate in the Stochastic Greedy Model than the second. Using only simple functions on arrays, the calculations for this model were used for the optimization of pattern-to-tile mappings in our pattern recognition experiments, starting from random assignments; once optimization with the Window Nucleation Model was ended, further optimization was performed with the Stochastic Greedy Model, to fine-tune the pixel-to-tile map and to verify nucleation rates under that more physically-meaningful model.However, as a stochastic model requiring the collection of many trajectories to produce low-variance estimates of nucleation rates, the Stochastic Greedy Model was either too slow (with more trajectories) or too noisy (with fewer trajectories) to provide the fast, stable comparisons of nucleation rates between similar concentration patterns needed for optimization algorithms searching within the large space of pixel to tile mappings. For this purpose, we instead developed a simple, deterministic, heuristic model, the Window Nucleation Model (Section 2.5), based on intuitive reasoning about the thermodynamics of small assemblies and potential critical nuclei along assembly pathways. This model does not provide a nucleation rate, but instead provides a score value for each concentration pattern such that, if one pattern has a higher score than another, then that first pattern is likely, though not guaranteed, to have a higher nucleation rate in the Stochastic Greedy Model than the second. Using only simple functions on arrays, the calculations for this model were used for the optimization of pattern-to-tile mappings in our pattern recognition experiments, starting from random assignments; once optimization with the Window Nucleation Model was ended, further optimization was performed with the Stochastic Greedy Model, to fine-tune the pixel-to-tile map and to verify nucleation rates under that more physically-meaningful model.</p>
        <p>The Stochastic Greedy Model and Window Nucleation Model, as well as kTAM simulations in Xgrow, consider monomer tile concentrations as being constant over time and not being depleted by assembly, as though controlled by a chemostat. In reality, monomer concentrations deplete as structures nucleate and grow, potentially changing nucleation and growth rates and leading to a winner-take-all effect seen in experiments. We build a simplified model of such a winner-take-all phenomena using a simplified 1-dimensional Markov chain model with representing nucleation and growth, modeled with time-dependent transition rates (Section 2.3).The Stochastic Greedy Model and Window Nucleation Model, as well as kTAM simulations in Xgrow, consider monomer tile concentrations as being constant over time and not being depleted by assembly, as though controlled by a chemostat. In reality, monomer concentrations deplete as structures nucleate and grow, potentially changing nucleation and growth rates and leading to a winner-take-all effect seen in experiments. We build a simplified model of such a winner-take-all phenomena using a simplified 1-dimensional Markov chain model with representing nucleation and growth, modeled with time-dependent transition rates (Section 2.3).</p>
        <p>The kinetic Tile Assembly Model (kTAM) abstracts DNA tiles as oriented squares with labeled sides. 96,100 It assumes topologically correct self-assembly, so an assembled object is a connected arrangement of tiles on the square lattice. Tiles are "connected" if they have abutting sides, in which case we may speak of the "bond" between those tiles, whose strength may depend on the pair of labels indicating bond type.The kinetic Tile Assembly Model (kTAM) abstracts DNA tiles as oriented squares with labeled sides. 96,100 It assumes topologically correct self-assembly, so an assembled object is a connected arrangement of tiles on the square lattice. Tiles are "connected" if they have abutting sides, in which case we may speak of the "bond" between those tiles, whose strength may depend on the pair of labels indicating bond type.</p>
        <p>Thermodynamics is specified by associating an energy to each possible assembly based on the tiles involved and how well their side labels match; kinetics is specified by associating rates for each possible tile addition or removal, satisfying detailed balance. Association between two multi-tile assemblies will not be considered in the basic kTAM model, nor will splitting of an assembly into two multi-tile assemblies. (Such interactions, along with lattice defects, can be important for DNA tile assembly, 82,83 but we did not encounter strong evidence for them in this work.) Thus, considered as a chemical reaction network (CRN) with infinitely many possible species, the reactions are of the form:Thermodynamics is specified by associating an energy to each possible assembly based on the tiles involved and how well their side labels match; kinetics is specified by associating rates for each possible tile addition or removal, satisfying detailed balance. Association between two multi-tile assemblies will not be considered in the basic kTAM model, nor will splitting of an assembly into two multi-tile assemblies. (Such interactions, along with lattice defects, can be important for DNA tile assembly, 82,83 but we did not encounter strong evidence for them in this work.) Thus, considered as a chemical reaction network (CRN) with infinitely many possible species, the reactions are of the form:</p>
        <p>where A and A are assemblies, t i is a tile of type i, and A is the same as A but for the addition of tile t at some location. The base model only considers monomer addition reactions, which must be reversible in order to satisfy physical detailed balance, and thus single tile detachments that would break the assembly are not considered. Except when explicitly specified otherwise, kTAM models do not incorporate any notion of globally correct tile placement; they allow any tile to bind in any location so long as at least one bond matches. Thus, tile attachments and assemblies that contain errors with respect to the target complete structure will be considered.where A and A are assemblies, t i is a tile of type i, and A is the same as A but for the addition of tile t at some location. The base model only considers monomer addition reactions, which must be reversible in order to satisfy physical detailed balance, and thus single tile detachments that would break the assembly are not considered. Except when explicitly specified otherwise, kTAM models do not incorporate any notion of globally correct tile placement; they allow any tile to bind in any location so long as at least one bond matches. Thus, tile attachments and assemblies that contain errors with respect to the target complete structure will be considered.</p>
        <p>The simplest model for the thermodynamic energy of an assembly considers just the summed interaction strength of all side-to-side bonds based on whether the labels match, and assumes identical bonding strength for matching labels with non-matching labels contributing nothing:The simplest model for the thermodynamic energy of an assembly considers just the summed interaction strength of all side-to-side bonds based on whether the labels match, and assumes identical bonding strength for matching labels with non-matching labels contributing nothing:</p>
        <p>where B is the total number of matching bonds and G se is the generic bond strength for a suitable choice of units. With respect to these units, detailed balance is expressed, for equilibrium concentrations, aswhere B is the total number of matching bonds and G se is the generic bond strength for a suitable choice of units. With respect to these units, detailed balance is expressed, for equilibrium concentrations, as</p>
        <p>where b is the number of new matching bonds in A that were not already in A, and û0 is the reference concentration for these units. We will see later that û0 must be chosen carefully, and will most likely not be standard units. Note that G se ≥ 0 and more positive values correspond to stronger bonds. Our simplifying assumption for kinetics is that k f does not depend on assembly size or tile type. Thus it can be considered as a parameter, and k r can be calculated for a given tile dissociation from a given assembly based on the detailed balance equation.where b is the number of new matching bonds in A that were not already in A, and û0 is the reference concentration for these units. We will see later that û0 must be chosen carefully, and will most likely not be standard units. Note that G se ≥ 0 and more positive values correspond to stronger bonds. Our simplifying assumption for kinetics is that k f does not depend on assembly size or tile type. Thus it can be considered as a parameter, and k r can be calculated for a given tile dissociation from a given assembly based on the detailed balance equation.</p>
        <p>Further, it is often simpler and sufficiently insightful to consider assemblies growing in a bath of tiles whose concentrations are held constant. (This scenario is relevant, for example, during the time before significant nucleation has depleted the monomer concentrations.) Here, we will need to allow different tile types to have different concentrations, and we use the notationFurther, it is often simpler and sufficiently insightful to consider assemblies growing in a bath of tiles whose concentrations are held constant. (This scenario is relevant, for example, during the time before significant nucleation has depleted the monomer concentrations.) Here, we will need to allow different tile types to have different concentrations, and we use the notation</p>
        <p>where G i mc is the parameter controlling the concentration of tile type t i , with larger values corresponding to lower concentrations (and thus greater positional entropy per molecule). With tile monomer concentrations held constant, telescoping detailed balance immediately gives us the equilibrium concentration for each assembly:where G i mc is the parameter controlling the concentration of tile type t i , with larger values corresponding to lower concentrations (and thus greater positional entropy per molecule). With tile monomer concentrations held constant, telescoping detailed balance immediately gives us the equilibrium concentration for each assembly:</p>
        <p>where the assembly energywhere the assembly energy</p>
        <p>3) is a chemical potential with respect to the tile monomer concentrations. Because our model does not incorporate interactions between assemblies, each assembly's behavior is independent from the others in the powered scenario, so a single assembly's growth can be modeled as a continuous-time Markov chain (CTMC) where each state is a possible assembly, transitions correspond to the addition or removal of a single tile t i that is connected by b bonds, and transition rates are given by:3) is a chemical potential with respect to the tile monomer concentrations. Because our model does not incorporate interactions between assemblies, each assembly's behavior is independent from the others in the powered scenario, so a single assembly's growth can be modeled as a continuous-time Markov chain (CTMC) where each state is a possible assembly, transitions correspond to the addition or removal of a single tile t i that is connected by b bonds, and transition rates are given by:</p>
        <p>With every reaction being reversible, this CTMC also satisfies detailed balance, and if it reaches equilibrium (which is only possible for parameters below the melting temperature) then the probability of observing assembly A is:With every reaction being reversible, this CTMC also satisfies detailed balance, and if it reaches equilibrium (which is only possible for parameters below the melting temperature) then the probability of observing assembly A is:</p>
        <p>Note that if a tile is removed from an assembly of size 2, then either tile can be considered the 'remaining' singletile assembly. As mentioned above, tile removals that would result in multiple, disconnected assemblies cannot be allowed if detailed balance is to be respected. This simple model can be efficiently simulated by the stochastic Gillespie algorithm 101,102 using a k-d tree to store rates for log-time steps, as done in 
            <rs type="software">Xgrow</rs>. 103 The Xgrow simulator has options to allow a limited variety of reverse reactions that break the assembly into multiple pieces, at the cost of also breaking detailed balance. Specifically, the 'chunk fission' option can be roughly described as: (a) if after a single tile is removed, the remaining assembly is not connected, an irreversible reaction that removes this tile and keeps only the largest remaining assembly is used, with a rate k r,b set by the bonds broken when removing just that tile, and (b) multi-tile subassemblies up to size 2 × 2 are considered for irreversible removal with rate k r,b where now b is the number of bonds along the perimeter of the subassembly. Without this option, unrealistic surface roughening occurs near the melting temperature due to the inability of small clusters to fall off when they are well-connected internally but have only 1 bond with the main assembly. Although 
            <rs type="software">Xgrow</rs> does not implement arbitrary fragmentation of large assemblies, during growth of chimeric multifarious structures wherein partial assemblies of two distinct shapes are connected in just one place, circumstances can arise where removal of a single subassembly of size 2 × 2 or smaller will result in separation of the two sides. This is observed in Extended Data Figure E2.
        </p>
        <p>Xgrow has been used extensively in the literature 82,104,105 as it provides a simplifying clarity, but quantitative comparison with experimental studies of DNA tile systems requires further adjustments, as discussed below. 100Xgrow has been used extensively in the literature 82,104,105 as it provides a simplifying clarity, but quantitative comparison with experimental studies of DNA tile systems requires further adjustments, as discussed below. 100</p>
        <p>There are now extensive experimental demonstrations of tile-based DNA self-assembly of linear (tubes and ribbons [106][107][108] ), two-dimensional (finite and unbounded [109][110][111] ), and three-dimensional (finite and unbounded [112][113][114] ) structures using double-crossover tiles, 115 single-stranded tiles 108 as well as other DNA tile motifs. Each has its own features that make the kTAM and its variants more or less accurate.There are now extensive experimental demonstrations of tile-based DNA self-assembly of linear (tubes and ribbons [106][107][108] ), two-dimensional (finite and unbounded [109][110][111] ), and three-dimensional (finite and unbounded [112][113][114] ) structures using double-crossover tiles, 115 single-stranded tiles 108 as well as other DNA tile motifs. Each has its own features that make the kTAM and its variants more or less accurate.</p>
        <p>Suppose we have experimentally measured free energies and rate constants for assembly, given for natural units of concentration, time, and energy. We will show that if a few plausible conditions hold, exactly or approximately, then we can find corresponding parameters for the simple kTAM model -thus allowing us use parameters extracted from the literature as a baseline for our simulations of nucleation kinetics.Suppose we have experimentally measured free energies and rate constants for assembly, given for natural units of concentration, time, and energy. We will show that if a few plausible conditions hold, exactly or approximately, then we can find corresponding parameters for the simple kTAM model -thus allowing us use parameters extracted from the literature as a baseline for our simulations of nucleation kinetics.</p>
        <p>Specifically, let us assume (1) that the rate constant for tile attachments is identical for all locations and all tile types, so that always r f = k f c i as before; and (2) that the standard free energy of attachment depends only on the the number of matching bonds formed, b, and is linear. Therefore, we can extract the offset and slope, and write:Specifically, let us assume (1) that the rate constant for tile attachments is identical for all locations and all tile types, so that always r f = k f c i as before; and (2) that the standard free energy of attachment depends only on the the number of matching bonds formed, b, and is linear. Therefore, we can extract the offset and slope, and write:</p>
        <p>where α and G se are unitless, R is the gas constant, T is the temperature in Kelvin, and the energies use standard units with respect to a standard concentration such as u 0 = 1 M. Considering the per-second rates of tile attachment and detachment at a specific site in an assembly, the simple basic model and the experimentally parameterized model should agree exactly, sowhere α and G se are unitless, R is the gas constant, T is the temperature in Kelvin, and the energies use standard units with respect to a standard concentration such as u 0 = 1 M. Considering the per-second rates of tile attachment and detachment at a specific site in an assembly, the simple basic model and the experimentally parameterized model should agree exactly, so</p>
        <p>where α = ln û0 u0 and thus the choice of units for the simple model are determined by the offset α via û0 = u 0 e α . Unfortunately (though not surprisingly) experimental and computational studies 95,97,98 report that the assumptions we used are only approximately correct for DNA tiles:where α = ln û0 u0 and thus the choice of units for the simple model are determined by the offset α via û0 = u 0 e α . Unfortunately (though not surprisingly) experimental and computational studies 95,97,98 report that the assumptions we used are only approximately correct for DNA tiles:</p>
        <p>1. For DNA double-crossover tiles assembling into lattices on mica, as observed by atomic force microscopy movies, 97 ∆G • b was linear in b (within error bars) over a 100-fold range of off-rates, while on-rates were constant within a factor of 2 (except where fewer than 10 data points were collected, e.g., for b = 4).1. For DNA double-crossover tiles assembling into lattices on mica, as observed by atomic force microscopy movies, 97 ∆G • b was linear in b (within error bars) over a 100-fold range of off-rates, while on-rates were constant within a factor of 2 (except where fewer than 10 data points were collected, e.g., for b = 4).</p>
        <p>2. For DNA double-crossover tiles assembling in solution, as measured by fluorescence, 98 a roughly linear dependence on b was also found for ∆G • b , now covering a 10 9 -fold range of rates, with on-rates remaining constant within a factor of about 4. Importantly, this work also introduced additional loop energy terms that the basic kTAM cannot accommodate, but fortunately these only come into play for "non-convex" assemblies that are rare during nucleation and growth near the melting temperature. In particular, tiles attaching or detaching by 4 bonds involve rare non-convex assemblies, and we are less concerned about inaccuracies of energies and rates for such reactions.2. For DNA double-crossover tiles assembling in solution, as measured by fluorescence, 98 a roughly linear dependence on b was also found for ∆G • b , now covering a 10 9 -fold range of rates, with on-rates remaining constant within a factor of about 4. Importantly, this work also introduced additional loop energy terms that the basic kTAM cannot accommodate, but fortunately these only come into play for "non-convex" assemblies that are rare during nucleation and growth near the melting temperature. In particular, tiles attaching or detaching by 4 bonds involve rare non-convex assemblies, and we are less concerned about inaccuracies of energies and rates for such reactions.</p>
        <p>3. For DNA single-stranded tiles assembling in solution, examined computationally by coarse-grained molecular dynamics of 32 distinct local contexts, 95 and averaging over contexts with the same b, again we see a roughly linear dependence on b for ∆G • b covering again a roughly 10 9 -fold range of rates. Although they don't calculate rates, they too assume a constant for the on-rate. Again, their value for b = 4 is anomalous (by ∼ 2 R T ), and again we are content to ignore it.3. For DNA single-stranded tiles assembling in solution, examined computationally by coarse-grained molecular dynamics of 32 distinct local contexts, 95 and averaging over contexts with the same b, again we see a roughly linear dependence on b for ∆G • b covering again a roughly 10 9 -fold range of rates. Although they don't calculate rates, they too assume a constant for the on-rate. Again, their value for b = 4 is anomalous (by ∼ 2 R T ), and again we are content to ignore it.</p>
        <p>To find reasonable parameters for the kTAM to model our SST system, we will make use of the computational work 95 because the underlying oxDNA model 116 has been well calibrated, and detailed experimental measurements for SST are not yet available. From pages 7-18 of the supplementary materials, the rightmost value of F/kT gives the value ofTo find reasonable parameters for the kTAM to model our SST system, we will make use of the computational work 95 because the underlying oxDNA model 116 has been well calibrated, and detailed experimental measurements for SST are not yet available. From pages 7-18 of the supplementary materials, the rightmost value of F/kT gives the value of</p>
        <p>RT and in the computational work [t i ] = 100 nM and u 0 = 1 M, we take G se = 6 and α = -7.1, and use k f = 10 6 /M/s. Given α and [t i ], G i mc is determined, completing the parameterization needed for simulation. (Note that recent work 78 with SSTs did not model an α-like effect, resulting in an estimated G se ≈ 8 for 100 nM tiles that may be too high.)RT and in the computational work [t i ] = 100 nM and u 0 = 1 M, we take G se = 6 and α = -7.1, and use k f = 10 6 /M/s. Given α and [t i ], G i mc is determined, completing the parameterization needed for simulation. (Note that recent work 78 with SSTs did not model an α-like effect, resulting in an estimated G se ≈ 8 for 100 nM tiles that may be too high.)</p>
        <p>Note, however, that these parameters are for T = 50 • C, as that was the temperature used in the simulations. For our purposes, we are interested in nucleation and growth rates at different temperatures, in particular the critical range between 53 • C and 45 • C. Therefore, we must consider the temperature-dependence of parameters k f , α, G se , and G i mc . A roughly 50% increase in the association rate constant was observed 98 for DNA double-crossover molecules between 12 • C and 21 • C, but we will be satisfied holding k f constant. We also make the simplifying assumption that α is temperature independent, i.e. that it represents purely entropic factors. Consequently, G i mc will also be temperature independent.Note, however, that these parameters are for T = 50 • C, as that was the temperature used in the simulations. For our purposes, we are interested in nucleation and growth rates at different temperatures, in particular the critical range between 53 • C and 45 • C. Therefore, we must consider the temperature-dependence of parameters k f , α, G se , and G i mc . A roughly 50% increase in the association rate constant was observed 98 for DNA double-crossover molecules between 12 • C and 21 • C, but we will be satisfied holding k f constant. We also make the simplifying assumption that α is temperature independent, i.e. that it represents purely entropic factors. Consequently, G i mc will also be temperature independent.</p>
        <p>To provide a temperature dependence for G se , we note that association b = 1 corresponds to uncomplicated hybridization of two single-stranded species, for which the thermodynamics is well-understood. 117 In particular,To provide a temperature dependence for G se , we note that association b = 1 corresponds to uncomplicated hybridization of two single-stranded species, for which the thermodynamics is well-understood. 117 In particular,</p>
        <p>We used an implementation of nearest-neighbor energy calculations with SantaLucia 2004 parameters 117,118 to calculate ∆H • and ∆S • averaged over all binding domain sequences in the SHAM system (excluding the poly-T domains used on shape edges): ∆H • = -72.76 kcal/mol and ∆S • = -0.2013 kcal/(mol• K). For α = -7.1, this is effectively linear over the range of interest, withWe used an implementation of nearest-neighbor energy calculations with SantaLucia 2004 parameters 117,118 to calculate ∆H • and ∆S • averaged over all binding domain sequences in the SHAM system (excluding the poly-T domains used on shape edges): ∆H • = -72.76 kcal/mol and ∆S • = -0.2013 kcal/(mol• K). For α = -7.1, this is effectively linear over the range of interest, with</p>
        <p>For reference, T = 50 The above discussion makes it clear that the parameterization of the nucleation model is fairly rough, and we do not expect quantitative agreement with experimental results. Our aim for the model is to provide quantitative insight into the nucleation phenomena with this highly heterogenous system, to observe qualitative effects that may also be present in the real system, and to provide a baseline against which to assess the effectiveness of our sequence design and pixel-to-tile mapping.For reference, T = 50 The above discussion makes it clear that the parameterization of the nucleation model is fairly rough, and we do not expect quantitative agreement with experimental results. Our aim for the model is to provide quantitative insight into the nucleation phenomena with this highly heterogenous system, to observe qualitative effects that may also be present in the real system, and to provide a baseline against which to assess the effectiveness of our sequence design and pixel-to-tile mapping.</p>
        <p>With respect to these standard units, we define the free energy (chemical potential)With respect to these standard units, we define the free energy (chemical potential)</p>
        <p>where as before B is the total number of matching bonds between tiles in the assembly. Now, at equilibrium, with monomer concentrations held constant at their initial values,where as before B is the total number of matching bonds between tiles in the assembly. Now, at equilibrium, with monomer concentrations held constant at their initial values,</p>
        <p>In the following, we will sometimes find it convenient to refer to a base concentration c = û0 e -Gmc = u 0 e -Gmc+α such that individual tiles have concentrationsIn the following, we will sometimes find it convenient to refer to a base concentration c = û0 e -Gmc = u 0 e -Gmc+α such that individual tiles have concentrations</p>
        <p>The equilibrium concentration of a particular assembly given by Equation 2.14 does not generally hold while the system is out of equilibrium. In particular, as in the experiments considered here, in an isolated system that initially only contains monomers and does not replenish them as they are incorporated into structures, the total tile concentrations will remain constant, and thus free monomer concentrations will deplete. When there is a significant barrier to nucleation, the subset of pre-nucleation structures may quickly approach equilibrium with respect to each other, contributing to some tile depletion, while further growth of nucleated structures continues the depletion on a longer time scale. Let us first review classical nucleation theory of homogeneous crystals before delving into the more complex heterogeneous case. 48,119 Using the above formalism for homogeneous crystals, there is is a single tile type and thus a single concentration, c. To estimate the nucleation rate we find the size (in number of tiles) of the critical nuclei, as their concentration limits nucleation. For a given number of tiles, K, the highest concentration assembly A K will have a shape as close as possible to being square, as that maximizes the number of bonds. (This can be interpreted as minimizing perimeter for a given area, which generalizes as surface area versus volume.) For simplicity, we will just treat K = n 2 for integer n and then interpolate results as if K were a continuous variable. In this case,The equilibrium concentration of a particular assembly given by Equation 2.14 does not generally hold while the system is out of equilibrium. In particular, as in the experiments considered here, in an isolated system that initially only contains monomers and does not replenish them as they are incorporated into structures, the total tile concentrations will remain constant, and thus free monomer concentrations will deplete. When there is a significant barrier to nucleation, the subset of pre-nucleation structures may quickly approach equilibrium with respect to each other, contributing to some tile depletion, while further growth of nucleated structures continues the depletion on a longer time scale. Let us first review classical nucleation theory of homogeneous crystals before delving into the more complex heterogeneous case. 48,119 Using the above formalism for homogeneous crystals, there is is a single tile type and thus a single concentration, c. To estimate the nucleation rate we find the size (in number of tiles) of the critical nuclei, as their concentration limits nucleation. For a given number of tiles, K, the highest concentration assembly A K will have a shape as close as possible to being square, as that maximizes the number of bonds. (This can be interpreted as minimizing perimeter for a given area, which generalizes as surface area versus volume.) For simplicity, we will just treat K = n 2 for integer n and then interpolate results as if K were a continuous variable. In this case,</p>
        <p>since the number of bonds in an n × n square is B = 2n(n -1). For continuous n, we find the critical nucleus size by minimizing [A K ] with respect to n:since the number of bonds in an n × n square is B = 2n(n -1). For continuous n, we find the critical nucleus size by minimizing [A K ] with respect to n:</p>
        <p>As the crystal melting temperature, in this notation, occurs when G mc = 2G se , we see that the critical nucleus size becomes arbitrarily large as the system approaches arbitrarily close to the melting temperature. More concretely, a little algebra yieldsAs the crystal melting temperature, in this notation, occurs when G mc = 2G se , we see that the critical nucleus size becomes arbitrarily large as the system approaches arbitrarily close to the melting temperature. More concretely, a little algebra yields</p>
        <p>KGse . The nucleation rate η measures the production of larger-than-critical assemblies; ignoring smaller factors, such as the role of non-square critical assemblies and the multiplicities of possible nucleation pathways, we can approximateKGse . The nucleation rate η measures the production of larger-than-critical assemblies; ignoring smaller factors, such as the role of non-square critical assemblies and the multiplicities of possible nucleation pathways, we can approximate</p>
        <p>as the order-of-magnitude rate of nucleation, in molar per second. Correspondingly, the time scale for nucleation, τ , will be inversely proportional to the critical nucleus concentration, so τ ∼ e √ KGse . This already suggests an insight relevant to pattern recognition by multifarious self-assembly: to make use of large critical nuclei that integrate more information, we can change reaction conditions to increase K, for example by decreasing the base concentration (increasing G mc ) while keeping the temperature constant (fixed G se ), but this comes with a corresponding exponential slow-down in the computing speed. The result is essentially the same if we hold the concentration (via G mc ) constant and adjust the temperature (via G se ) to obtain larger K. That said, it can't be taken for granted that insights derived from classical nucleation theory of homogeneous crystals will apply unchanged to the heterogeneous multicomponent, as has been highlighted by significant theoretical and experimental work that extends this framework. 50,51,87,99,107,[120][121][122][123] For multifarious self-assembly of single-stranded tiles, we are interested in the overall rate of nucleation η shape for each possible final assembly shape. With multiple tile types, each having a different concentration and specific binding interactions, there may be many distinct nucleation pathways involving many types of critical nuclei with particular shapes and compositions, and the situation becomes far more complex than for homogeneous systems. Nonetheless, Schulman &amp; Winfree [99] show that the equilibrium concentration for an assembly, with respect to the initial monomer tile concentrations, is an upper bound for the concentration of that assembly at any time in the time evolution of a mass-action model with monomer concentrations that deplete during growth, if the initial state consists of monomers and assemblies grow by reversible monomer addition only. As a result, the total rate at which larger assemblies form from a particular assembly A, which we will call its growth flux, has an upper bound ofas the order-of-magnitude rate of nucleation, in molar per second. Correspondingly, the time scale for nucleation, τ , will be inversely proportional to the critical nucleus concentration, so τ ∼ e √ KGse . This already suggests an insight relevant to pattern recognition by multifarious self-assembly: to make use of large critical nuclei that integrate more information, we can change reaction conditions to increase K, for example by decreasing the base concentration (increasing G mc ) while keeping the temperature constant (fixed G se ), but this comes with a corresponding exponential slow-down in the computing speed. The result is essentially the same if we hold the concentration (via G mc ) constant and adjust the temperature (via G se ) to obtain larger K. That said, it can't be taken for granted that insights derived from classical nucleation theory of homogeneous crystals will apply unchanged to the heterogeneous multicomponent, as has been highlighted by significant theoretical and experimental work that extends this framework. 50,51,87,99,107,[120][121][122][123] For multifarious self-assembly of single-stranded tiles, we are interested in the overall rate of nucleation η shape for each possible final assembly shape. With multiple tile types, each having a different concentration and specific binding interactions, there may be many distinct nucleation pathways involving many types of critical nuclei with particular shapes and compositions, and the situation becomes far more complex than for homogeneous systems. Nonetheless, Schulman &amp; Winfree [99] show that the equilibrium concentration for an assembly, with respect to the initial monomer tile concentrations, is an upper bound for the concentration of that assembly at any time in the time evolution of a mass-action model with monomer concentrations that deplete during growth, if the initial state consists of monomers and assemblies grow by reversible monomer addition only. As a result, the total rate at which larger assemblies form from a particular assembly A, which we will call its growth flux, has an upper bound of</p>
        <p>where J(A) is the set of possible tile attachments to A and we use that for all times t,where J(A) is the set of possible tile attachments to A and we use that for all times t,</p>
        <p>From this per-assembly growth flux, we can derive an upper bound for nucleation, or more generally, a limiting rate for growth from monomers to a set of particular target assemblies. Consider any set of assemblies S in the space of all possible assemblies, such that every series of tile attachment and detachment events going from an individual tile monomer to the set of target assemblies must contain at least one of the assemblies in S as an intermediate assembly. We might want to consider S to be a 'boundary surface' that separates pre-nucleated structures from post-nucleated structures. For example, for a target assembly of size N , the set of all assemblies with size exactly K (for some K &lt; N ) would satisfy this property. However, in general S can be any set of assemblies that must be crossed during assembly, and this gives us considerable flexibility when finding suitable S. For any such S, considering all series of attachments and detachments, the total flux through the 'surface' S, and thus from monomers to target assemblies, has an upper bound ofFrom this per-assembly growth flux, we can derive an upper bound for nucleation, or more generally, a limiting rate for growth from monomers to a set of particular target assemblies. Consider any set of assemblies S in the space of all possible assemblies, such that every series of tile attachment and detachment events going from an individual tile monomer to the set of target assemblies must contain at least one of the assemblies in S as an intermediate assembly. We might want to consider S to be a 'boundary surface' that separates pre-nucleated structures from post-nucleated structures. For example, for a target assembly of size N , the set of all assemblies with size exactly K (for some K &lt; N ) would satisfy this property. However, in general S can be any set of assemblies that must be crossed during assembly, and this gives us considerable flexibility when finding suitable S. For any such S, considering all series of attachments and detachments, the total flux through the 'surface' S, and thus from monomers to target assemblies, has an upper bound of</p>
        <p>This result is considered more rigorously by Schulman &amp; Winfree [99]. It relies only on S containing a complete surface that all growth must pass through. A poor choice of surface (or a set with extraneous members) will simply result in an unnecessarily high upper bound: for example, in conditions where G(A) does not begin to decrease until assembly sizes much larger than dimers, the set of all dimers D would still result in η + D constraining the total rate of target production, but the bound would be very high. Generally, a surface that all growth must pass through where G(A) is high for each A ∈ S will tend to result in the smallest η + S , and thus the tightest bound on nucleation. It would be desirable to find an optimal surface, i.e. one that strictly minimizes η + S . Even in cases where there are few tile types, regular periodic order, and homogeneous concentrations, enumeration of a complete minimal surface may be infeasible, but excellent approximations can be articulated by taking advantage of symmetry and considering just the most dominant terms. For example, in such cases S may be all assemblies of a well-chosen critical size K, for which the dominant species may be easy to describe. Specifically, as assemblies with higher G(A) within a set S will contribute exponentially less to η + S , it often suffices to consider the assemblies within the set with the smallest G(A) values, which will contribute the most to η + S . For homogeneous systems, these will simply be those that have the most bonds with the fewest tiles, often corresponding geometrically to having the largest area for the smallest perimeter.This result is considered more rigorously by Schulman &amp; Winfree [99]. It relies only on S containing a complete surface that all growth must pass through. A poor choice of surface (or a set with extraneous members) will simply result in an unnecessarily high upper bound: for example, in conditions where G(A) does not begin to decrease until assembly sizes much larger than dimers, the set of all dimers D would still result in η + D constraining the total rate of target production, but the bound would be very high. Generally, a surface that all growth must pass through where G(A) is high for each A ∈ S will tend to result in the smallest η + S , and thus the tightest bound on nucleation. It would be desirable to find an optimal surface, i.e. one that strictly minimizes η + S . Even in cases where there are few tile types, regular periodic order, and homogeneous concentrations, enumeration of a complete minimal surface may be infeasible, but excellent approximations can be articulated by taking advantage of symmetry and considering just the most dominant terms. For example, in such cases S may be all assemblies of a well-chosen critical size K, for which the dominant species may be easy to describe. Specifically, as assemblies with higher G(A) within a set S will contribute exponentially less to η + S , it often suffices to consider the assemblies within the set with the smallest G(A) values, which will contribute the most to η + S . For homogeneous systems, these will simply be those that have the most bonds with the fewest tiles, often corresponding geometrically to having the largest area for the smallest perimeter.</p>
        <p>In the case of unequal concentrations and inhomogenous designs, however, symmetry breaks. Each possible assembly must be considered separately, as each tile may have a different concentration, resulting in a different G(A) despite geometric similarity, and the incorporation of more high concentration tiles at the expense of increasing perimeter may result in G(A) being smaller, breaking the link between geometric compactness and free energy. As a result, simple approaches to finding the largest contributors to a particular η + S , for example, considering squares of increasing size in a homogeneous, single-tile-type system, 100 cannot be easily adapted to a non-homogeneous system with many tile types.In the case of unequal concentrations and inhomogenous designs, however, symmetry breaks. Each possible assembly must be considered separately, as each tile may have a different concentration, resulting in a different G(A) despite geometric similarity, and the incorporation of more high concentration tiles at the expense of increasing perimeter may result in G(A) being smaller, breaking the link between geometric compactness and free energy. As a result, simple approaches to finding the largest contributors to a particular η + S , for example, considering squares of increasing size in a homogeneous, single-tile-type system, 100 cannot be easily adapted to a non-homogeneous system with many tile types.</p>
        <p>More generally, consider the set of all possible trajectories from a single tile to a full assembly by monomer addition, and collect -without duplication -the highest-energy point along each trajectory, which we may call the 'critical nucleus' for that trajectory. (Using 'critical nucleus' for these assemblies is a bit of an abuse of terminology, but please bear with us.) The set S of all such critical nuclei contains a complete separating surface (along with much else) and its lowest-energy point must be reached or exceeded by any trajectory. Pruning this conceptual S down to a smaller subset that contains just enough of the lower-energy assemblies could provide a tractable approximation for the nucleation rate.More generally, consider the set of all possible trajectories from a single tile to a full assembly by monomer addition, and collect -without duplication -the highest-energy point along each trajectory, which we may call the 'critical nucleus' for that trajectory. (Using 'critical nucleus' for these assemblies is a bit of an abuse of terminology, but please bear with us.) The set S of all such critical nuclei contains a complete separating surface (along with much else) and its lowest-energy point must be reached or exceeded by any trajectory. Pruning this conceptual S down to a smaller subset that contains just enough of the lower-energy assemblies could provide a tractable approximation for the nucleation rate.</p>
        <p>Thus, we developed the Stochastic Greedy Model to stochastically generate a set S of 'critical nuclei' for the nucleation of a single shape, essentially considering the tile system as a uniquely-addressed tile system assembling that one shape. Only correct tile attachments on the uniquely-addressed lattice are considered: attachments of incorrect tiles by one bond are ignored, as are any attachments of tiles that aren't present in the shape, and each tile i will only appear at a particular site (x, y) on the lattice. The algorithm functions by taking some series of probabilistic trajectories using a 'two step' process for each change in assembly: taking an unfavorable step probabilistically, and then deterministically taking the resulting series of possible favorable attachments, while recording the free energy of the assembly immediately after the unfavorable step to try to find a critical nucleus.Thus, we developed the Stochastic Greedy Model to stochastically generate a set S of 'critical nuclei' for the nucleation of a single shape, essentially considering the tile system as a uniquely-addressed tile system assembling that one shape. Only correct tile attachments on the uniquely-addressed lattice are considered: attachments of incorrect tiles by one bond are ignored, as are any attachments of tiles that aren't present in the shape, and each tile i will only appear at a particular site (x, y) on the lattice. The algorithm functions by taking some series of probabilistic trajectories using a 'two step' process for each change in assembly: taking an unfavorable step probabilistically, and then deterministically taking the resulting series of possible favorable attachments, while recording the free energy of the assembly immediately after the unfavorable step to try to find a critical nucleus.</p>
        <p>Given a set S thus enumerated by the Stochastic Greedy Model, the nucleation rate estimated by Equation 2.16 may be above or below the actual nucleation rate for monomer addition assembly in the bulk mass-action kTAM. To the extent that there are gaps in S such that some kTAM assembly trajectories do not go through S, the estimate will be too low. Both having too few sampled trajectories, and considering only greedy trajectories, contribute to this effect. To the extent that some kTAM assembly trajectories go through multiple states in S, the estimate will be too high. Both enumerating a "thick" surface due to stochastic variability, and the fact that kTAM trajectories may reversibly cross a perfect surface multiple times, contribute to this effect. Finally, to the extent that assembly concentrations for the non-equilibrium kinetics remain lower than the equilibrium concentrations given fixed monomer concentrations, the estimate will be too high. Both tile depletion during growth, and the inclusion of "critical nuclei" whose concentrations are drained by nearly-irreversible post-critical growth, contribute to this effect. Accepting these limitations, we characterize the estimated effective nucleation rate, given the enumerated set of critical nuclei S, in terms of an effective ensemble barrier energy G ce such thatGiven a set S thus enumerated by the Stochastic Greedy Model, the nucleation rate estimated by Equation 2.16 may be above or below the actual nucleation rate for monomer addition assembly in the bulk mass-action kTAM. To the extent that there are gaps in S such that some kTAM assembly trajectories do not go through S, the estimate will be too low. Both having too few sampled trajectories, and considering only greedy trajectories, contribute to this effect. To the extent that some kTAM assembly trajectories go through multiple states in S, the estimate will be too high. Both enumerating a "thick" surface due to stochastic variability, and the fact that kTAM trajectories may reversibly cross a perfect surface multiple times, contribute to this effect. Finally, to the extent that assembly concentrations for the non-equilibrium kinetics remain lower than the equilibrium concentrations given fixed monomer concentrations, the estimate will be too high. Both tile depletion during growth, and the inclusion of "critical nuclei" whose concentrations are drained by nearly-irreversible post-critical growth, contribute to this effect. Accepting these limitations, we characterize the estimated effective nucleation rate, given the enumerated set of critical nuclei S, in terms of an effective ensemble barrier energy G ce such that</p>
        <p>(2.17)(2.17)</p>
        <p>We can interpret G ce as the macrostate free energy for a set of microstates that consist of the identified critical nuclei together with an arriving tile that has lost its translational entropy by colocalizing at a prospective site of attachment, but which has not yet formed bonds. Since our multifarious self-assembly design has the property that a pair of tiles that are adjacent (in a given orientation) in one shape are guaranteed to be non-adjacent (in that orientation) in the other shapes, any multi-tile critical nucleus can be expected to grow specifically into the complete shape that it is consistent with. Therefore, when S was enumerated based on a specific shape, the nucleation rate estimate for that shape can be writtenWe can interpret G ce as the macrostate free energy for a set of microstates that consist of the identified critical nuclei together with an arriving tile that has lost its translational entropy by colocalizing at a prospective site of attachment, but which has not yet formed bonds. Since our multifarious self-assembly design has the property that a pair of tiles that are adjacent (in a given orientation) in one shape are guaranteed to be non-adjacent (in that orientation) in the other shapes, any multi-tile critical nucleus can be expected to grow specifically into the complete shape that it is consistent with. Therefore, when S was enumerated based on a specific shape, the nucleation rate estimate for that shape can be written</p>
        <p>While our algorithm is sufficient for the concentration patterns used in this work, more subtle concentration patterns that are nearly equally localized in multiple structures will require solving the issues raised here. One approach to addressing many of these issues is to adapt a transition path sampling method, 124 like forward flux sampling 125 as a wrapper around the kinetic Tile Assembly Model described above. These statistical sampling methods are explicitly designed to determine the rate of processes controlled by rare events and have been applied to nucleation, 126 albeit not in the multicomponent limit with unequal concentrations needed here.While our algorithm is sufficient for the concentration patterns used in this work, more subtle concentration patterns that are nearly equally localized in multiple structures will require solving the issues raised here. One approach to addressing many of these issues is to adapt a transition path sampling method, 124 like forward flux sampling 125 as a wrapper around the kinetic Tile Assembly Model described above. These statistical sampling methods are explicitly designed to determine the rate of processes controlled by rare events and have been applied to nucleation, 126 albeit not in the multicomponent limit with unequal concentrations needed here.</p>
        <p>We now provide details of the Stochastic Greedy Model.We now provide details of the Stochastic Greedy Model.</p>
        <p>For each target shape (H, A, M), we generated 40,000 trajectories of possible assembly paths from a single tile all the way to a complete shape. Each trajectory starts from a single tile, used as the initial seed for growth, that is chosen probabilistically by concentration, such that the probability of choosing tile i is P i start = c i / ( i c i ). The trajectory will exclusively involve tile additions, such that assembly A 0 is just tile t i and after s tile addition steps, assembly A s consists of (s + 1) tiles in total and has free energy (chemical potential) G(A s ) as defined above.For each target shape (H, A, M), we generated 40,000 trajectories of possible assembly paths from a single tile all the way to a complete shape. Each trajectory starts from a single tile, used as the initial seed for growth, that is chosen probabilistically by concentration, such that the probability of choosing tile i is P i start = c i / ( i c i ). The trajectory will exclusively involve tile additions, such that assembly A 0 is just tile t i and after s tile addition steps, assembly A s consists of (s + 1) tiles in total and has free energy (chemical potential) G(A s ) as defined above.</p>
        <p>At each point in trajectory construction, where A s is the current assembly, the algorithm calculates ∆G att (x, y) = G(A s ) -G(A s ) for every point (x, y), where A s reflects the potential addition of the correct tile at position (x, y). We further calculate a Boltzmann weight p att (x, y) = e -∆Gatt(x,y) , which we use for the probability that the algorithm will select this site for the next tile attachment. For speed, this calculation is done by keeping values in two arrays, and only changing values adjacent to the change in the assembly array: i.e., for every tile attachment, these two values are recalculated for whichever of the four locations adjacent to that tile attachment are empty. For a given site (x, y) where tile i can attach and form b bonds, we useAt each point in trajectory construction, where A s is the current assembly, the algorithm calculates ∆G att (x, y) = G(A s ) -G(A s ) for every point (x, y), where A s reflects the potential addition of the correct tile at position (x, y). We further calculate a Boltzmann weight p att (x, y) = e -∆Gatt(x,y) , which we use for the probability that the algorithm will select this site for the next tile attachment. For speed, this calculation is done by keeping values in two arrays, and only changing values adjacent to the change in the assembly array: i.e., for every tile attachment, these two values are recalculated for whichever of the four locations adjacent to that tile attachment are empty. For a given site (x, y) where tile i can attach and form b bonds, we use</p>
        <p>For the first (presumed unfavorable) step, and subsequently whenever the assembly is again in a state where only ∆G att &gt; 0 attachments are possible, the algorithm chooses a step to take with a probability of attachment P (x, y) = p att (x, y)/ X,Y p att (X, Y ). Note that this is not the actual probability of that tile attachment taking place in the kTAM, which would be determined only by tile concentrations. If there is no attachment possible at all, i.e. the assembly is complete, then the trajectory ends.For the first (presumed unfavorable) step, and subsequently whenever the assembly is again in a state where only ∆G att &gt; 0 attachments are possible, the algorithm chooses a step to take with a probability of attachment P (x, y) = p att (x, y)/ X,Y p att (X, Y ). Note that this is not the actual probability of that tile attachment taking place in the kTAM, which would be determined only by tile concentrations. If there is no attachment possible at all, i.e. the assembly is complete, then the trajectory ends.</p>
        <p>After selecting (x, y), the algorithm makes the selected attachment (the first of the step's two parts) and updates the ∆G att and p att arrays. The G of the assembly is stored as G(s), where s is the step number. Note that this is stored after the ∆G att &gt; 0 (unfavorable, and probably b = 1) attachment. The maximum value of G(s) is stored as the current best guess for the critical nucleus free energy G cn along the trajectory, and the assembly state that resulted in that maximum is stored until replaced by a higher-G(s) assembly.After selecting (x, y), the algorithm makes the selected attachment (the first of the step's two parts) and updates the ∆G att and p att arrays. The G of the assembly is stored as G(s), where s is the step number. Note that this is stored after the ∆G att &gt; 0 (unfavorable, and probably b = 1) attachment. The maximum value of G(s) is stored as the current best guess for the critical nucleus free energy G cn along the trajectory, and the assembly state that resulted in that maximum is stored until replaced by a higher-G(s) assembly.</p>
        <p>After the ∆G att &gt; 0 attachment, the algorithm runs a filling routine that repeatedly attempts to make an arbitrary ∆G att &lt; 0 attachment, until no such attachments are possible. Because of the properties of the assembly (all correct tile attachments, only one possible tile per site, no detachment, no negative glues), the order of attachments is not important. This is the second part of the step: making every possible subsequent favorable step after the initial unfavorable step.After the ∆G att &gt; 0 attachment, the algorithm runs a filling routine that repeatedly attempts to make an arbitrary ∆G att &lt; 0 attachment, until no such attachments are possible. Because of the properties of the assembly (all correct tile attachments, only one possible tile per site, no detachment, no negative glues), the order of attachments is not important. This is the second part of the step: making every possible subsequent favorable step after the initial unfavorable step.</p>
        <p>As used in this work, the algorithm always continues until the final complete shape is formed and thus no more correct attachments are possible; however, for increased efficiency, it is also defined such that it may stop after a given number of tiles have formed, which the user may believe to be sufficient to guarantee nucleation has occurred.As used in this work, the algorithm always continues until the final complete shape is formed and thus no more correct attachments are possible; however, for increased efficiency, it is also defined such that it may stop after a given number of tiles have formed, which the user may believe to be sufficient to guarantee nucleation has occurred.</p>
        <p>At the end of a trajectory, the algorithm checks to see whether the final assembly free energy G(end) &lt; G(base). In other words, it checks whether the final assembly is more favorable than a low-concentration monomer tile, for equilibrium with respect to fixed tile concentrations. If this is not the case, then the algorithm reports the trajectory as a failure. As used in this work, where trajectories are computed all the way to the complete shape, either all trajectories are successful or all trajectories are failures, given a concentration pattern, temperature, and shape. With respect to the fixed tile concentrations, we define the melting temperature for a given pattern and shape as point where G(complete) = G(base). (Melting temperatures would not be pattern-dependent if all tiles were shared in all shapes, but as it is, different patterns may place more high-concentration tiles in different shapes; the additional shape-dependence comes from the different area-to-perimeter ratios.)At the end of a trajectory, the algorithm checks to see whether the final assembly free energy G(end) &lt; G(base). In other words, it checks whether the final assembly is more favorable than a low-concentration monomer tile, for equilibrium with respect to fixed tile concentrations. If this is not the case, then the algorithm reports the trajectory as a failure. As used in this work, where trajectories are computed all the way to the complete shape, either all trajectories are successful or all trajectories are failures, given a concentration pattern, temperature, and shape. With respect to the fixed tile concentrations, we define the melting temperature for a given pattern and shape as point where G(complete) = G(base). (Melting temperatures would not be pattern-dependent if all tiles were shared in all shapes, but as it is, different patterns may place more high-concentration tiles in different shapes; the additional shape-dependence comes from the different area-to-perimeter ratios.)</p>
        <p>The trajectory algorithm then returns the following information to the nucleation rate algorithm:The trajectory algorithm then returns the following information to the nucleation rate algorithm:</p>
        <p>• The G cn found by the algorithm: the highest G(s) along the trajectory.• The G cn found by the algorithm: the highest G(s) along the trajectory.</p>
        <p>• The critical nucleus assembly that corresponds to that G cn .• The critical nucleus assembly that corresponds to that G cn .</p>
        <p>• G(s) for every step s.• G(s) for every step s.</p>
        <p>• The step s where G cn is found.• The step s where G cn is found.</p>
        <p>• Whether G(end) &lt; G(base).• Whether G(end) &lt; G(base).</p>
        <p>Example greedy trajectories for H flag 6 are shown in Fig. 3c. Figure S2.1 shows a Boltzmann-weighted sample of critical nuclei in the enumerated sets S for each shape, for the H flag 6 concentrations at T = 49.2 • C, at T = 48.3 • C, and at T = 45.5 • C. Given the quantitative differences between the theoretical model and experimental results, we only look to interpret qualitative features of the critical nucleus sets. It is immediately clear that (a) most critical nuclei in H are near the flag, while critical nuclei in the other shapes are scattered around; and (b) the typical critical nucleus size is strongly dependent on temperature in the samples shown, varying from 3 to 16 for the on-target shape and from 3 to 73 for the off-target shapes, with the greatest difference being in the off-target shapes. Figure S2.2 shows an analogous Boltzmann-weighted sample of critical nuclei for the Avogadro pattern recognition experiment. The phenomena are similar here, although the on-target critical nuclei are somewhat larger on average, off-target critical nuclei are somewhat smaller, and estimated nucleation rates are somewhat closer to each other, consistent with the narrower range of temperatures used. As the critical nuclei for the on-target shape can be quite small, there is a sense in which the selectivity of pattern recognition relies on the rejection of nucleation attempts in the off-target shapes, where the amount of information being considered is governed by the size of their critical nuclei at the relevant temperature.Example greedy trajectories for H flag 6 are shown in Fig. 3c. Figure S2.1 shows a Boltzmann-weighted sample of critical nuclei in the enumerated sets S for each shape, for the H flag 6 concentrations at T = 49.2 • C, at T = 48.3 • C, and at T = 45.5 • C. Given the quantitative differences between the theoretical model and experimental results, we only look to interpret qualitative features of the critical nucleus sets. It is immediately clear that (a) most critical nuclei in H are near the flag, while critical nuclei in the other shapes are scattered around; and (b) the typical critical nucleus size is strongly dependent on temperature in the samples shown, varying from 3 to 16 for the on-target shape and from 3 to 73 for the off-target shapes, with the greatest difference being in the off-target shapes. Figure S2.2 shows an analogous Boltzmann-weighted sample of critical nuclei for the Avogadro pattern recognition experiment. The phenomena are similar here, although the on-target critical nuclei are somewhat larger on average, off-target critical nuclei are somewhat smaller, and estimated nucleation rates are somewhat closer to each other, consistent with the narrower range of temperatures used. As the critical nuclei for the on-target shape can be quite small, there is a sense in which the selectivity of pattern recognition relies on the rejection of nucleation attempts in the off-target shapes, where the amount of information being considered is governed by the size of their critical nuclei at the relevant temperature.</p>
        <p>Fig. 3e shows macrostate ensemble free energy for assemblies of a given size, using the ensemble of trajectories generated by the Stochastic Greedy Model. Let S K be the set of size-K assemblies that appear in some enumerated trajectory. Then we calculate and plot G SGM (K) =ln A∈S K e -G(A) . Because the greedy trajectories are biased toward the low-energy assemblies that dominate this Boltzmann-weighted sum, G SGM is an adequate approximation of the macrostate ensemble free energy for the set of all size-K assemblies for the given shape.Fig. 3e shows macrostate ensemble free energy for assemblies of a given size, using the ensemble of trajectories generated by the Stochastic Greedy Model. Let S K be the set of size-K assemblies that appear in some enumerated trajectory. Then we calculate and plot G SGM (K) =ln A∈S K e -G(A) . Because the greedy trajectories are biased toward the low-energy assemblies that dominate this Boltzmann-weighted sum, G SGM is an adequate approximation of the macrostate ensemble free energy for the set of all size-K assemblies for the given shape.</p>
        <p>The Stochastic Greedy Model generates a set number of trajectories. Once these trajectories have been generated, the algorithm has a set of distinct critical nuclei, S, and associated information. Using the enumerated set S, the algorithm estimates the ensemble nucleation barrier energy G ce and the nucleation rate η + according to equations 2.15, 2.16, and 2.17, with one adjustment: rather than summing over all possible tile additions from each critical nucleus, J(A), we sum over only the favorable (energetically downhill) tile additions, which we call J + (A). While the full set J(A) is required for a rigorous upper bound in the general case, for the set of critical nuclei enumerated by the Stochastic Greedy Model, which are guaranteed to have at least one downhill tile addition step available, neglecting the uphill tile additions avoids counting flux that is likely to be immediately reversed, while keeping theThe Stochastic Greedy Model generates a set number of trajectories. Once these trajectories have been generated, the algorithm has a set of distinct critical nuclei, S, and associated information. Using the enumerated set S, the algorithm estimates the ensemble nucleation barrier energy G ce and the nucleation rate η + according to equations 2.15, 2.16, and 2.17, with one adjustment: rather than summing over all possible tile additions from each critical nucleus, J(A), we sum over only the favorable (energetically downhill) tile additions, which we call J + (A). While the full set J(A) is required for a rigorous upper bound in the general case, for the set of critical nuclei enumerated by the Stochastic Greedy Model, which are guaranteed to have at least one downhill tile addition step available, neglecting the uphill tile additions avoids counting flux that is likely to be immediately reversed, while keeping the</p>
        <p>.1: A Boltzmann-weighted sample of 6 'critical nuclei' for each shape, at three temperatures, for H flag 6. Orange and red indicate low-and high-concentration tiles that are part of the critical nucleus, respectively; grey and green are used analogously for tiles not in the nucleus. η + is the estimated nucleation rate for the shape; Gce is the corresponding critical nucleus ensemble free energy; Gse is per-bond differential sticky-end energy at the shown temperature; and Gcn is the chemical potential of the given critical nucleus..1: A Boltzmann-weighted sample of 6 'critical nuclei' for each shape, at three temperatures, for H flag 6. Orange and red indicate low-and high-concentration tiles that are part of the critical nucleus, respectively; grey and green are used analogously for tiles not in the nucleus. η + is the estimated nucleation rate for the shape; Gce is the corresponding critical nucleus ensemble free energy; Gse is per-bond differential sticky-end energy at the shown temperature; and Gcn is the chemical potential of the given critical nucleus.</p>
        <p>.2: A Boltzmann-weighted sample of 6 'critical nuclei' for each shape, at three temperatures, for Avogadro. The color scale from orange to red scales linearly from the lowest to highest concentration, and is used for tiles that are part of the critical nucleus; the color scale from black to white is used analogously for tiles not in the nucleus. η + , Gce, Gse, and Gcn are as in Figure S2.1.2: A Boltzmann-weighted sample of 6 'critical nuclei' for each shape, at three temperatures, for Avogadro. The color scale from orange to red scales linearly from the lowest to highest concentration, and is used for tiles that are part of the critical nucleus; the color scale from black to white is used analogously for tiles not in the nucleus. η + , Gce, Gse, and Gcn are as in Figure S2.1</p>
        <p>dominant favorable tile additions. Thus, we replace the previous formulas with:dominant favorable tile additions. Thus, we replace the previous formulas with:</p>
        <p>The total nucleation rate for a shape gives no information on the region where nucleation began, even though, with unequal tile concentrations, some regions may be much more likely than others to participate in nucleation rather than to grow by tile attachments after a structure has nucleated. While trajectories in the Stochastic Greedy Model start from individual tiles, the trajectories are used to find distinct critical nuclei, and a particular critical nucleus may arise from trajectories starting from many different positions. An alternative measure of the contribution of a particular location to the nucleation rate of a shape, calculated only from the set of distinct critical nuclei S that was enumerated for the given shape, is the sum over all critical nuclei that include the given location, of the nucleation flux through the nucleus divided by its size:The total nucleation rate for a shape gives no information on the region where nucleation began, even though, with unequal tile concentrations, some regions may be much more likely than others to participate in nucleation rather than to grow by tile attachments after a structure has nucleated. While trajectories in the Stochastic Greedy Model start from individual tiles, the trajectories are used to find distinct critical nuclei, and a particular critical nucleus may arise from trajectories starting from many different positions. An alternative measure of the contribution of a particular location to the nucleation rate of a shape, calculated only from the set of distinct critical nuclei S that was enumerated for the given shape, is the sum over all critical nuclei that include the given location, of the nucleation flux through the nucleus divided by its size:</p>
        <p>where N (A) is the number of tiles in A. By weighting the nucleation rate in this way, the contributions of regions to the total nucleation rate of a shape can be calculated by adding the contributions of each location, and the sum of contributions for all locations in a shape equals the total nucleation rate:where N (A) is the number of tiles in A. By weighting the nucleation rate in this way, the contributions of regions to the total nucleation rate of a shape can be calculated by adding the contributions of each location, and the sum of contributions for all locations in a shape equals the total nucleation rate:</p>
        <p>x,y ∈shape η + shape:x,y = η + shape Because we performed a rough calibration of the kTAM parameters in Section 2.1.1, we can use the Stochastic Greedy Model to estimate the temperature dependence of nucleation rates for any given shape and pattern of tile concentrations. Simultaneously, for each temperature we can calculate the Boltzmann-weighted (i.e. weighted proportional to equilibrium concentration) average critical nucleus size. In Sections 5.3 and 6.4, we perform this analysis for each sample from the flag scan and pattern recognition experiments, respectively. These calculations use the initial concentrations of each tile, and do not model the depletion process. Although both nucleation rate and average critical nucleus size can be calculated above the shape-and pattern-dependent melting temperature (where G(complete) = G(base)), their interpretation becomes more fraught, so we mark the melting temperature with a dashed vertical line and plot only at lower temperatures.x,y ∈shape η + shape:x,y = η + shape Because we performed a rough calibration of the kTAM parameters in Section 2.1.1, we can use the Stochastic Greedy Model to estimate the temperature dependence of nucleation rates for any given shape and pattern of tile concentrations. Simultaneously, for each temperature we can calculate the Boltzmann-weighted (i.e. weighted proportional to equilibrium concentration) average critical nucleus size. In Sections 5.3 and 6.4, we perform this analysis for each sample from the flag scan and pattern recognition experiments, respectively. These calculations use the initial concentrations of each tile, and do not model the depletion process. Although both nucleation rate and average critical nucleus size can be calculated above the shape-and pattern-dependent melting temperature (where G(complete) = G(base)), their interpretation becomes more fraught, so we mark the melting temperature with a dashed vertical line and plot only at lower temperatures.</p>
        <p>It should be emphasized that our nucleation rate estimates are not intended to be quantitatively accurate: the kTAM model and its calibration only roughly capture the thermodynamics and kinetics of our system; the further approximations used in the Stochastic Greedy Model are also rough; and nucleation kinetics are notoriously sensitive with order-of-magnitude differences between theory and experiment being common and often difficult to explain. 48,127 (Example deficiencies in our estimates can be seen in the plots for flag scan experiments, which contain abrupt jumps in the nucleation rate estimate that are artifacts of the Stochastic Greedy Model's enumeration process for 'critical nuclei'.)It should be emphasized that our nucleation rate estimates are not intended to be quantitatively accurate: the kTAM model and its calibration only roughly capture the thermodynamics and kinetics of our system; the further approximations used in the Stochastic Greedy Model are also rough; and nucleation kinetics are notoriously sensitive with order-of-magnitude differences between theory and experiment being common and often difficult to explain. 48,127 (Example deficiencies in our estimates can be seen in the plots for flag scan experiments, which contain abrupt jumps in the nucleation rate estimate that are artifacts of the Stochastic Greedy Model's enumeration process for 'critical nuclei'.)</p>
        <p>In particular, we do not expect the model to accurately predict the temperature (during a ramp experiment) at which nucleation becomes measurable, nor the absolute or relative nucleation rates at a given temperature. For example, in the pattern recognition experiments, the time to go from about 5% to 10% quenched was at least about an hour, indicating that nucleation was never faster than about 5 × 10 -13 M/s, and this mostly occurred between 47 • C and 48 • C. The nucleation model, on the other hand, estimated that this rate would occur between 48.4 • C and 48.6 • C for most patterns, and that at this temperature, the on-target shape would nucleate at least 2 (and often more than 5) orders of magnitude faster than the off-target shape -which fails to explain cases such as Mockingbird where the off-target shape nucleates at nearly the same temperature. This suggests that the nucleation rate estimates may be a few orders of magnitude too high; were k f decreased, the experimentallyobserved nucleation rates would correspond to a lower temperature where on-target and off-target rates are more similar in the model.In particular, we do not expect the model to accurately predict the temperature (during a ramp experiment) at which nucleation becomes measurable, nor the absolute or relative nucleation rates at a given temperature. For example, in the pattern recognition experiments, the time to go from about 5% to 10% quenched was at least about an hour, indicating that nucleation was never faster than about 5 × 10 -13 M/s, and this mostly occurred between 47 • C and 48 • C. The nucleation model, on the other hand, estimated that this rate would occur between 48.4 • C and 48.6 • C for most patterns, and that at this temperature, the on-target shape would nucleate at least 2 (and often more than 5) orders of magnitude faster than the off-target shape -which fails to explain cases such as Mockingbird where the off-target shape nucleates at nearly the same temperature. This suggests that the nucleation rate estimates may be a few orders of magnitude too high; were k f decreased, the experimentallyobserved nucleation rates would correspond to a lower temperature where on-target and off-target rates are more similar in the model.</p>
        <p>Nonetheless, our model provides several important insights (perhaps better phrased as hypotheses) that help us appreciate features of nucleation in multifarious self-assembly systems and general characteristics of their ability to perform pattern recognition. Most notable is the sharp increase in nucleation rate for the on-target shape as the temperature drops below its melting temperature, in contrast to the much more gradual increase for the off-target shapes. This is accompanied by a fast transition (i.e. over a narrow temperature range) from large critical nuclei to small critical nuclei for the on-target shape, whereas the transition occurs much more slowly for the off-target shapes.Nonetheless, our model provides several important insights (perhaps better phrased as hypotheses) that help us appreciate features of nucleation in multifarious self-assembly systems and general characteristics of their ability to perform pattern recognition. Most notable is the sharp increase in nucleation rate for the on-target shape as the temperature drops below its melting temperature, in contrast to the much more gradual increase for the off-target shapes. This is accompanied by a fast transition (i.e. over a narrow temperature range) from large critical nuclei to small critical nuclei for the on-target shape, whereas the transition occurs much more slowly for the off-target shapes.</p>
        <p>Consequently, selectivity for the correct target shape (i.e. the ratio of nucleation rates) increases as one considers increasing temperatures, with the best selectivity near the melting temperature where absolute nucleation rates are low. However, consistent with the shapes having different melting temperatures (M has the lowest melting temperature due it having the highest perimeter-to-area ratio), it is possible that as one gets too close to their melting temperatures, the target shapes' nucleation rate may plummet below the off-target shapes (as occurs for all the M flag scan patterns). That said, the relative stability of on-target nucleation rates sufficiently below the melting temperatures provides another perspective on the previously-mentioned interpretation that pattern recognition selectivity comes primarily from rejection of off-target nucleation attempts.Consequently, selectivity for the correct target shape (i.e. the ratio of nucleation rates) increases as one considers increasing temperatures, with the best selectivity near the melting temperature where absolute nucleation rates are low. However, consistent with the shapes having different melting temperatures (M has the lowest melting temperature due it having the highest perimeter-to-area ratio), it is possible that as one gets too close to their melting temperatures, the target shapes' nucleation rate may plummet below the off-target shapes (as occurs for all the M flag scan patterns). That said, the relative stability of on-target nucleation rates sufficiently below the melting temperatures provides another perspective on the previously-mentioned interpretation that pattern recognition selectivity comes primarily from rejection of off-target nucleation attempts.</p>
        <p>The above model of nucleation in multicomponent systems provides a basis for understanding the dependence of nucleation rate on concentration patterns, and thus the ability of self-assembly to perform pattern recognition, under the assumption that the temperature and monomer tile concentrations are fixed. The model considers each shape independently, and thus could be used to ask whether a single uniquely-addressed shape could recognize patterns via a nucleation rate that is above a "YES" threshold or below a "NO" threshold, depending on whether the pattern should be recognized. To classify patterns into multiple groups (such as H, A, and M) one could use three distinct uniquely-addressed shapes, sharing no tiles, if the patterns contain enough redundant information that a YES/NO decision for each shape is possible based on a partitioning of pixels into three groups. We expect this would be possible for the training set of 18 images used in this work, for example, using 3 new shapes each containing 300 tiles.The above model of nucleation in multicomponent systems provides a basis for understanding the dependence of nucleation rate on concentration patterns, and thus the ability of self-assembly to perform pattern recognition, under the assumption that the temperature and monomer tile concentrations are fixed. The model considers each shape independently, and thus could be used to ask whether a single uniquely-addressed shape could recognize patterns via a nucleation rate that is above a "YES" threshold or below a "NO" threshold, depending on whether the pattern should be recognized. To classify patterns into multiple groups (such as H, A, and M) one could use three distinct uniquely-addressed shapes, sharing no tiles, if the patterns contain enough redundant information that a YES/NO decision for each shape is possible based on a partitioning of pixels into three groups. We expect this would be possible for the training set of 18 images used in this work, for example, using 3 new shapes each containing 300 tiles.</p>
        <p>What then is the advantage of a multifarious self-assembly system with shared tiles, if any? Intuitively, we expect two advantages. First, a system with fully-shared tiles would enable all shapes to be responsive to all pixels, and thus redundancy in the pattern information would not be necessary. Our system, with a partially shared tile set, partially addresses this issue. Second, when one shape nucleates first, it will deplete the concentrations of the shared tiles, and this may limit the nucleation of other shapes, resulting in a winner-take-all effect. Such a winner-take-all effect allows experiments with a temperature ramp that does not require precise knowledge of the nucleation temperatures for on-and off-target structures. In contrast, in systems without shared components, experiments would have to be carried out at a specific temperature that allows specificity; such a temperature may be hard to know a priori and will not be universal, varying with the specific patterns and structures in question.What then is the advantage of a multifarious self-assembly system with shared tiles, if any? Intuitively, we expect two advantages. First, a system with fully-shared tiles would enable all shapes to be responsive to all pixels, and thus redundancy in the pattern information would not be necessary. Our system, with a partially shared tile set, partially addresses this issue. Second, when one shape nucleates first, it will deplete the concentrations of the shared tiles, and this may limit the nucleation of other shapes, resulting in a winner-take-all effect. Such a winner-take-all effect allows experiments with a temperature ramp that does not require precise knowledge of the nucleation temperatures for on-and off-target structures. In contrast, in systems without shared components, experiments would have to be carried out at a specific temperature that allows specificity; such a temperature may be hard to know a priori and will not be universal, varying with the specific patterns and structures in question.</p>
        <p>Here, we wish to quantify this winner-take-all (WTA) effect and provide some understanding of under what circumstances it will or will not occur, and how it can aid the pattern-recognition process. These questions could presumably be addressed by building on the kTAM or SGM, adding equations to model the depletion of each tile type during the nucleation and growth of self-assembling structures, but doing so would be prohibitively expensive computationally. Further, it is hoped that simplifying to the essential ingredients provides more insight.Here, we wish to quantify this winner-take-all (WTA) effect and provide some understanding of under what circumstances it will or will not occur, and how it can aid the pattern-recognition process. These questions could presumably be addressed by building on the kTAM or SGM, adding equations to model the depletion of each tile type during the nucleation and growth of self-assembling structures, but doing so would be prohibitively expensive computationally. Further, it is hoped that simplifying to the essential ingredients provides more insight.</p>
        <p>Absence of winner-take-all in homogeneous systems. Before examining how monomer depletion can lead to a winner-take-all effect in multifarious systems, it is worth commenting on why this effect would not be expected in simpler systems. Crystal polymorphism has been well-studied since the time of Mitscherlich; the ability to nucleate only the most stable form of several polymorphs is an industrially relevant problem, in domains ranging from making chocolate to drugs. Consequently, numerous annealing protocols have been developed to enhance the yield of, say, the most stable polymorph while minimizing nucleation and growth of the competing phases.Absence of winner-take-all in homogeneous systems. Before examining how monomer depletion can lead to a winner-take-all effect in multifarious systems, it is worth commenting on why this effect would not be expected in simpler systems. Crystal polymorphism has been well-studied since the time of Mitscherlich; the ability to nucleate only the most stable form of several polymorphs is an industrially relevant problem, in domains ranging from making chocolate to drugs. Consequently, numerous annealing protocols have been developed to enhance the yield of, say, the most stable polymorph while minimizing nucleation and growth of the competing phases.</p>
        <p>The winner-take-all effect described here makes the selectivity problem easier in the multicomponent limit and is absent in the well-studied case of homogeneous crystals. In the multicomponent structures studied here, the on-target structure, e.g., H in the models above, can nucleate from a region of high concentration tiles while critical seeds of the off-target structure, say A, predominantly involve low concentration tiles (since the same high concentration tiles are not spatially localized on A). Depletion affects the low concentration tiles much more significantly than the high concentration tiles; hence depletion will more significantly affect the nucleation rate of A (the off-target structure) than the nucleation rate of H (the on-target structure). In contrast, in homogeneous crystal polymorphs, depletion will affect nucleation of both on and off target structures to similar extents because both nucleation rates are determined by the concentration of the same component(s). Hence, we do not expect a winner-take-all effect for crystal polymorphic structures involving only one or a few kinds of components. On the right we indicate the assemblies that are considered in the models: The CG CRN just uses three assemblies for each shape, e.g. Hnuc, H mid , and H f ull . The SGM-lite CRN considers all assemblies that occur in M pre-computed stochastic greedy trajectories for each shape, starting from a single high-concentration tile and proceeding by monomer addition to the full shape. Outlines of sample assemblies along three color-coded trajectories are shown for each shape.The winner-take-all effect described here makes the selectivity problem easier in the multicomponent limit and is absent in the well-studied case of homogeneous crystals. In the multicomponent structures studied here, the on-target structure, e.g., H in the models above, can nucleate from a region of high concentration tiles while critical seeds of the off-target structure, say A, predominantly involve low concentration tiles (since the same high concentration tiles are not spatially localized on A). Depletion affects the low concentration tiles much more significantly than the high concentration tiles; hence depletion will more significantly affect the nucleation rate of A (the off-target structure) than the nucleation rate of H (the on-target structure). In contrast, in homogeneous crystal polymorphs, depletion will affect nucleation of both on and off target structures to similar extents because both nucleation rates are determined by the concentration of the same component(s). Hence, we do not expect a winner-take-all effect for crystal polymorphic structures involving only one or a few kinds of components. On the right we indicate the assemblies that are considered in the models: The CG CRN just uses three assemblies for each shape, e.g. Hnuc, H mid , and H f ull . The SGM-lite CRN considers all assemblies that occur in M pre-computed stochastic greedy trajectories for each shape, starting from a single high-concentration tile and proceeding by monomer addition to the full shape. Outlines of sample assemblies along three color-coded trajectories are shown for each shape.</p>
        <p>Presence of winner-take-all in multifarious systems with shared components but not in multicomponent systems with independent components. We will consider the assembly of two structures, say 'H' and 'A', from a pool of monomers that deplete over time as monomers get incorporated into the structures. The structures may be competing for shared monomers, or may not be competing. Two models will be developed. The first is a highly simplified model that just tries to capture the notion of classical Arrhenius nucleation kinetics combined with uniform growth and maturation using shared components. We call it the "coarse-grained" (CG) CRN model. The second model is still oversimplified, but it adopts more details and an overall framework similar to the SGM, with individual tiles and individual assemblies growing (or shrinking) by reversible monomer addition reactions that satisfy detailed balance. We call it the "SGM-lite" CRN model. They are both illustrated schematically in Figure S2. 3.Presence of winner-take-all in multifarious systems with shared components but not in multicomponent systems with independent components. We will consider the assembly of two structures, say 'H' and 'A', from a pool of monomers that deplete over time as monomers get incorporated into the structures. The structures may be competing for shared monomers, or may not be competing. Two models will be developed. The first is a highly simplified model that just tries to capture the notion of classical Arrhenius nucleation kinetics combined with uniform growth and maturation using shared components. We call it the "coarse-grained" (CG) CRN model. The second model is still oversimplified, but it adopts more details and an overall framework similar to the SGM, with individual tiles and individual assemblies growing (or shrinking) by reversible monomer addition reactions that satisfy detailed balance. We call it the "SGM-lite" CRN model. They are both illustrated schematically in Figure S2. 3.</p>
        <p>The coarse-grained model. The CG model has two variants, one considering all tiles being shared between the two shapes, and the other considering all tiles to be distinct. The shared-tiles (S) variant has the following 6 reactions:The coarse-grained model. The CG model has two variants, one considering all tiles being shared between the two shapes, and the other considering all tiles to be distinct. The shared-tiles (S) variant has the following 6 reactions:</p>
        <p>where N = 500 is the total number of tiles in each shape, s = 25 is the number of high concentration tiles and also the number of tiles in the assemblies H nuc and A nuc , and species c H n , c A n , and c g respectively represent the tiles in two nucleation regions and the growth region (black, intermediate grey, and light grey in Figure S2.3). Because our reactions represent coarse-grained pathways involving many individual tile addition steps, in each class the concentrations of all tiles will be the same, and we can model it using a single species concentration. That is, the first pair of reactions represent the nucleation and growth of all s tiles in the nucleation regions; the second pair of reactions represent the growth of the assembly to include all but the tiles that could nucleate the opposing shape; and the third pair of reactions completes the shape. Thus, one copy of each relevant tile is consumed in each coarse-grained reaction step.where N = 500 is the total number of tiles in each shape, s = 25 is the number of high concentration tiles and also the number of tiles in the assemblies H nuc and A nuc , and species c H n , c A n , and c g respectively represent the tiles in two nucleation regions and the growth region (black, intermediate grey, and light grey in Figure S2.3). Because our reactions represent coarse-grained pathways involving many individual tile addition steps, in each class the concentrations of all tiles will be the same, and we can model it using a single species concentration. That is, the first pair of reactions represent the nucleation and growth of all s tiles in the nucleation regions; the second pair of reactions represent the growth of the assembly to include all but the tiles that could nucleate the opposing shape; and the third pair of reactions completes the shape. Thus, one copy of each relevant tile is consumed in each coarse-grained reaction step.</p>
        <p>The rates for these coarse-grained reactions are calculated from approximations of kTAM kinetics incorporating classical nucleation theory. Specifically, we assume that in the nucleation region there is a critical nucleus whose size depends on temperature (via the sticky-end binding strength G se ) and tile concentration (via the energy parameter G mc = α-ln c/u 0 ), and the nucleation rate is proportional to its equilibrium concentration. (As usual, α = -7.1 andThe rates for these coarse-grained reactions are calculated from approximations of kTAM kinetics incorporating classical nucleation theory. Specifically, we assume that in the nucleation region there is a critical nucleus whose size depends on temperature (via the sticky-end binding strength G se ) and tile concentration (via the energy parameter G mc = α-ln c/u 0 ), and the nucleation rate is proportional to its equilibrium concentration. (As usual, α = -7.1 and</p>
        <p>is determined by maximizing G(A critical ) to find the free energy barrier. We interpolate k to continuous real values, rather than integers, for convenience. We set k n = 0.01 /s to roughly match SGM nucleation rates for the range of concentrations considered. The time for growth of the rest of the nucleation region is assumed to be negligible with respect to nucleation itself. (Note that H nuc and A nuc do not represent the critical nuclei themselves, but rather the entire region that is considered prone to nucleation.) A weakness of this simple model is that it does not consider entropic effects due to there being many places where nucleation could initiate and likewise many alternative critical assemblies that could contribute to the overall nucleation rate.is determined by maximizing G(A critical ) to find the free energy barrier. We interpolate k to continuous real values, rather than integers, for convenience. We set k n = 0.01 /s to roughly match SGM nucleation rates for the range of concentrations considered. The time for growth of the rest of the nucleation region is assumed to be negligible with respect to nucleation itself. (Note that H nuc and A nuc do not represent the critical nuclei themselves, but rather the entire region that is considered prone to nucleation.) A weakness of this simple model is that it does not consider entropic effects due to there being many places where nucleation could initiate and likewise many alternative critical assemblies that could contribute to the overall nucleation rate.</p>
        <p>For growth of assemblies, our reference rate considers the expected time for a biased random walk to complete N steps, with forward steps occurring at rate k f c and reverse steps occurring at rate k r,2 , where as per the kTAM, k r,b = k f u 0 e -b Gse+α and k f = 10 6 /M/s. That is, it is the net rate for completing the growth of the entire shape assuming each reversible step involves forming or breaking 2 bonds. For coarse-grained steps that involve fewer net steps, the rate is accordingly rescaled. Note that barriers for initiating a new layer of tiles on a facet are not considered here, although they can be very significant near the melting temperature when 2 G se ≈ G mc .For growth of assemblies, our reference rate considers the expected time for a biased random walk to complete N steps, with forward steps occurring at rate k f c and reverse steps occurring at rate k r,2 , where as per the kTAM, k r,b = k f u 0 e -b Gse+α and k f = 10 6 /M/s. That is, it is the net rate for completing the growth of the entire shape assuming each reversible step involves forming or breaking 2 bonds. For coarse-grained steps that involve fewer net steps, the rate is accordingly rescaled. Note that barriers for initiating a new layer of tiles on a facet are not considered here, although they can be very significant near the melting temperature when 2 G se ≈ G mc .</p>
        <p>Altogether, this gives:Altogether, this gives:</p>
        <p>where G mc is a function of c and G se is a function of the temperature T as per Equation 2.12. We will consider an anneal from well above the nucleation temperature for either shape (for our range of concentrations) down to a temperature where growth is strongly favored, specifically, from 55 • C to 30 • C over some time t ramp during which G se increases linearly with time.where G mc is a function of c and G se is a function of the temperature T as per Equation 2.12. We will consider an anneal from well above the nucleation temperature for either shape (for our range of concentrations) down to a temperature where growth is strongly favored, specifically, from 55 • C to 30 • C over some time t ramp during which G se increases linearly with time.</p>
        <p>The no-shared-tiles (NS) variant has just the following 4 reactions, as there is no need to distinguish two populations of growth tiles:The no-shared-tiles (NS) variant has just the following 4 reactions, as there is no need to distinguish two populations of growth tiles:</p>
        <p>Results from simulations of the CG model are shown on the left in Figure S2.4. For comparison, the flag experiments of Figure 4 cooled from 48 • C to 46 • C over 100 hours, for a rate of 0.02 • C per hour; the slowest simulation in the CG model traverses 25 • C over the same time, i.e., it is 12.5 times faster. We consider this range because the CG model's approximations appear to overestimate reaction rates, making the CG results "too perfect" for slower ramps. Similarly, the fact that the CG model considers high-concentration tiles within a solid region, rather than in a checkerboard pattern, makes the model more sensitive to concentration enhancements, and thus we consider a maximum 6-fold enhancement instead of the 17.6-fold enhancement used in the flag experiments.Results from simulations of the CG model are shown on the left in Figure S2.4. For comparison, the flag experiments of Figure 4 cooled from 48 • C to 46 • C over 100 hours, for a rate of 0.02 • C per hour; the slowest simulation in the CG model traverses 25 • C over the same time, i.e., it is 12.5 times faster. We consider this range because the CG model's approximations appear to overestimate reaction rates, making the CG results "too perfect" for slower ramps. Similarly, the fact that the CG model considers high-concentration tiles within a solid region, rather than in a checkerboard pattern, makes the model more sensitive to concentration enhancements, and thus we consider a maximum 6-fold enhancement instead of the 17.6-fold enhancement used in the flag experiments.</p>
        <p>The general trends are as expected: For model variant (S) with shared tiles, greater concentration enhancements and/or slower temperature ramps result in both higher selectivity and greater completion. In contrast, while the model variant (NS) with no shared tiles similarly obtains greater completion for greater concentration enhancements and/or slower temperature ramps, it only obtains high selectivity for very fast ramps (for which completion is poor). This is because, without competition for tiles, both shapes will eventually grow to completion in the NS variant of the model; observing the results before it has had time to grow is the only way to obtain selectivity, which results from significant nucleation from colocalized high-concentration tiles occurring at an earlier time in the ramp than nucleation from low-concentration tiles in the off-target shape.The general trends are as expected: For model variant (S) with shared tiles, greater concentration enhancements and/or slower temperature ramps result in both higher selectivity and greater completion. In contrast, while the model variant (NS) with no shared tiles similarly obtains greater completion for greater concentration enhancements and/or slower temperature ramps, it only obtains high selectivity for very fast ramps (for which completion is poor). This is because, without competition for tiles, both shapes will eventually grow to completion in the NS variant of the model; observing the results before it has had time to grow is the only way to obtain selectivity, which results from significant nucleation from colocalized high-concentration tiles occurring at an earlier time in the ramp than nucleation from low-concentration tiles in the off-target shape.</p>
        <p>(Note that if we considered a temperature hold instead of a ramp, then we could work at a temperature that is initially very selective for the target shape, if only we knew what temperature that should be. But due to concentration depletion, nucleation and growth would come to a halt before reaching completion.)(Note that if we considered a temperature hold instead of a ramp, then we could work at a temperature that is initially very selective for the target shape, if only we knew what temperature that should be. But due to concentration depletion, nucleation and growth would come to a halt before reaching completion.)</p>
        <p>Given the simplification inherent in this model, it is worth considering which phenomena are likely to hold up in more accurate models, differing quantitatively but not qualitatively, and which phenomena might change substantially. A few concerns may be considered. The CG behavior for fast ramps may not transfer to the In the lower plots, selectivity is computed as the fraction of final complete assemblies that are the target shape, H, and completion is computed as the final concentration of the target shape as a fraction of the limiting base concentration. Nucleation is computed as the rate of production of Hnuc or Anuc in the CG model, and as the net rate of production of assemblies containing at least 25 tiles in the SGM-lite model.Given the simplification inherent in this model, it is worth considering which phenomena are likely to hold up in more accurate models, differing quantitatively but not qualitatively, and which phenomena might change substantially. A few concerns may be considered. The CG behavior for fast ramps may not transfer to the In the lower plots, selectivity is computed as the fraction of final complete assemblies that are the target shape, H, and completion is computed as the final concentration of the target shape as a fraction of the limiting base concentration. Nucleation is computed as the rate of production of Hnuc or Anuc in the CG model, and as the net rate of production of assemblies containing at least 25 tiles in the SGM-lite model.</p>
        <p>experimental system because: (1) SST form unstructured aggregates rather than well-defined crystalline assemblies when cooled too fast; (2) classical nucleation theory assumes that the critical nuclei are at equilibrium instantly, whereas our experimental results suggests that nucleation and growth kinetics are not well separated. (Specifically, when a single shape contains multiple labels at different locations, experimental fluorescence trajectories commonly exhibit a temporal order of quenching consistent with the distance between the expected nucleating center and the fluorophore position within the shape, indicating that timescales for creating critical nuclei and for completing the growth process are overlapping.) The CG behavior for slow ramps may not transfer to the experimental system because: (1) near the melting temperature, facet nucleation barriers will limit growth, but these are ignored by the model; (2) nucleation and growth are modeled with irreversible reactions, but as concentrations drop, small assemblies will become unfavorable and melt due to Ostwald ripening. To gauge the severity of these issues, we explore a slightly less oversimplified model.experimental system because: (1) SST form unstructured aggregates rather than well-defined crystalline assemblies when cooled too fast; (2) classical nucleation theory assumes that the critical nuclei are at equilibrium instantly, whereas our experimental results suggests that nucleation and growth kinetics are not well separated. (Specifically, when a single shape contains multiple labels at different locations, experimental fluorescence trajectories commonly exhibit a temporal order of quenching consistent with the distance between the expected nucleating center and the fluorophore position within the shape, indicating that timescales for creating critical nuclei and for completing the growth process are overlapping.) The CG behavior for slow ramps may not transfer to the experimental system because: (1) near the melting temperature, facet nucleation barriers will limit growth, but these are ignored by the model; (2) nucleation and growth are modeled with irreversible reactions, but as concentrations drop, small assemblies will become unfavorable and melt due to Ostwald ripening. To gauge the severity of these issues, we explore a slightly less oversimplified model.</p>
        <p>The more detailed model. The SGM-lite model also has two variants, one considering tiles being shared between the two shapes identically as in the experimental system, and the other considering all tiles to be distinct. All reactions in this CRN are of the form:The more detailed model. The SGM-lite model also has two variants, one considering tiles being shared between the two shapes identically as in the experimental system, and the other considering all tiles to be distinct. All reactions in this CRN are of the form:</p>
        <p>where A is an assembly (or single tile), t i is a single tile, and A is assembly A with tile t i added in the correct position with respect to either the on-target or off-target shape, forming b ≥ 1 bonds. Due to computational constraints, not all such reactions are considered. Rather, for each shape we generate M = 8 trajectories starting with a single high-concentration tile, adding a random tile that attaches by at least 2 bonds if that is possible, else adding a random tile that attaches by just 1 bond, until the full shape has been grown. As we use the actual tile arrangement for the experimental H and A shapes, which have respectively 480 and 496 tiles, this leads to roughly 8000 reactions and a comparable number of species. Because nucleation in the model is confined to start with the high-concentration tiles for a particular pattern, in this case H flag 6, the resulting CRN is only suitable for simulating that concentration pattern -but for any level of concentration enhancement and at any temperature. Thus, by letting G se be a function of time, as in the CG model, we may use a standard simulator for mass-action chemical kinetics [128].where A is an assembly (or single tile), t i is a single tile, and A is assembly A with tile t i added in the correct position with respect to either the on-target or off-target shape, forming b ≥ 1 bonds. Due to computational constraints, not all such reactions are considered. Rather, for each shape we generate M = 8 trajectories starting with a single high-concentration tile, adding a random tile that attaches by at least 2 bonds if that is possible, else adding a random tile that attaches by just 1 bond, until the full shape has been grown. As we use the actual tile arrangement for the experimental H and A shapes, which have respectively 480 and 496 tiles, this leads to roughly 8000 reactions and a comparable number of species. Because nucleation in the model is confined to start with the high-concentration tiles for a particular pattern, in this case H flag 6, the resulting CRN is only suitable for simulating that concentration pattern -but for any level of concentration enhancement and at any temperature. Thus, by letting G se be a function of time, as in the CG model, we may use a standard simulator for mass-action chemical kinetics [128].</p>
        <p>Results from simulations of the SGM-lite model are shown on the right in Figure S2. 4. The simulated cooling rate is now ten times slower than for the CG model, 0.025 • C/h compared to the experimental 0.02 • C/h, but over a wider range, again from 55 • C to 30 • C.Results from simulations of the SGM-lite model are shown on the right in Figure S2. 4. The simulated cooling rate is now ten times slower than for the CG model, 0.025 • C/h compared to the experimental 0.02 • C/h, but over a wider range, again from 55 • C to 30 • C.</p>
        <p>Many trends are similar as in the CG model, but there are some noticeable differences. Perhaps most noticeable is that in the shared-tile model variant, the yield of complete on-target shapes never exceeds 60% for our parameters, even under circumstances where the selectivity is near perfect. This is related to another striking difference: that the off-target nucleation rates are delayed from but not suppressed by the preceding on-target nucleation and growth -both peak at about 1 nM/h although the off-target nucleation occurs later, unlike in the CG model where the maximum off-target nucleation rate is more than 10-fold lower than the off-target rate. What we observe happening is that while the on-target shape nucleates and slowly grows to its full size, the off-target shape nucleates but grows to only on average a little less than 100 tiles in size. Sensitivity to slow growth kinetics can also be seen in the surprising reduction of selectivity in the shared-tile model as the ramp gets slower, for low concentration enhancements. Slowing the annealing further by 10-fold only moderately ameliorates this effect.Many trends are similar as in the CG model, but there are some noticeable differences. Perhaps most noticeable is that in the shared-tile model variant, the yield of complete on-target shapes never exceeds 60% for our parameters, even under circumstances where the selectivity is near perfect. This is related to another striking difference: that the off-target nucleation rates are delayed from but not suppressed by the preceding on-target nucleation and growth -both peak at about 1 nM/h although the off-target nucleation occurs later, unlike in the CG model where the maximum off-target nucleation rate is more than 10-fold lower than the off-target rate. What we observe happening is that while the on-target shape nucleates and slowly grows to its full size, the off-target shape nucleates but grows to only on average a little less than 100 tiles in size. Sensitivity to slow growth kinetics can also be seen in the surprising reduction of selectivity in the shared-tile model as the ramp gets slower, for low concentration enhancements. Slowing the annealing further by 10-fold only moderately ameliorates this effect.</p>
        <p>A fuller picture of the winner-take-all effect in pattern recognition by nucleation in multifarious self-assembly will require further investigation.A fuller picture of the winner-take-all effect in pattern recognition by nucleation in multifarious self-assembly will require further investigation.</p>
        <p>The goal of pattern recognition training is to optimize the pixel-to-tile map θ so as to enhance nucleation of the target shape for each training image, while suppressing nucleation of the off-target shapes. The tile set itself stays the same, so with the SHAM system, the set of images will be classified into three groups according to the resulting shapes H, A, and M. However, generalization to a greater number of classes/shapes is straightforward, just requiring the design of a multifarious system with more possible shapes.The goal of pattern recognition training is to optimize the pixel-to-tile map θ so as to enhance nucleation of the target shape for each training image, while suppressing nucleation of the off-target shapes. The tile set itself stays the same, so with the SHAM system, the set of images will be classified into three groups according to the resulting shapes H, A, and M. However, generalization to a greater number of classes/shapes is straightforward, just requiring the design of a multifarious system with more possible shapes.</p>
        <p>Several decisions must be made in order to numerically define the objective function for optimization. How should nucleation rates be predicted? By what metric should deviations from the ideal be measured? Should additional criteria be incorporated to address experimental concerns? We will use a score based on maximizing the ratio of predicted nucleation rates using a fast and simple approximation, adjusted to impose preferences for locating nucleation near areas that worked well in flag experiments, and to impose penalties against using high concentrations for fluorophore / quencher strands and their neighbors.Several decisions must be made in order to numerically define the objective function for optimization. How should nucleation rates be predicted? By what metric should deviations from the ideal be measured? Should additional criteria be incorporated to address experimental concerns? We will use a score based on maximizing the ratio of predicted nucleation rates using a fast and simple approximation, adjusted to impose preferences for locating nucleation near areas that worked well in flag experiments, and to impose penalties against using high concentrations for fluorophore / quencher strands and their neighbors.</p>
        <p>The overall score for optimization will be based on a modification of the Stochastic Greedy Model's critical ensemble nucleation barrier energy estimate G ce , simplified and adjusted (using penalties w x,y ) to predict low nucleation rates for empirically poor areas in each shape. This modification is described below as the Window Nucleation Model. Thus, given a pixel-to-tile map,The overall score for optimization will be based on a modification of the Stochastic Greedy Model's critical ensemble nucleation barrier energy estimate G ce , simplified and adjusted (using penalties w x,y ) to predict low nucleation rates for empirically poor areas in each shape. This modification is described below as the Window Nucleation Model. Thus, given a pixel-to-tile map,</p>
        <p>gives a score that is lower for faster-nucleating cases. Since the nucleation rate is exponential in G ce , the difference in scores between the target shape and its closest competitor is the log of the ratio of their nucleation rates. This gives a score for the image, in which we also add penalty for tiles that should remain at low concentrations.gives a score that is lower for faster-nucleating cases. Since the nucleation rate is exponential in G ce , the difference in scores between the target shape and its closest competitor is the log of the ratio of their nucleation rates. This gives a score for the image, in which we also add penalty for tiles that should remain at low concentrations.</p>
        <p>Score(image) = Score(image, target shape)min Score(image, other shape)Score(image) = Score(image, target shape)min Score(image, other shape)</p>
        <p>where f i = c i /c is the concentration of tile i relative to the base concentration c, so 1 ≤ f i ≤ 27. This per-image score is also lower for images predicted to perform better. The penalties d i are detailed below. Finally, these scores were combined for all 18 training images to give a total score for that assignment map:where f i = c i /c is the concentration of tile i relative to the base concentration c, so 1 ≤ f i ≤ 27. This per-image score is also lower for images predicted to perform better. The penalties d i are detailed below. Finally, these scores were combined for all 18 training images to give a total score for that assignment map:</p>
        <p>That is, we try to optimize the pixel-to-tile map by improving the worst-performing image, in hope of obtaining a system whose performance is reasonably consistent across the training images. Specifically, the overall score was minimized using a naive, parallel "hill-climbing" (in our case, hill-descending) algorithm. Moves consisted of attempted swaps of randomly-chosen pairs tiles, accepted when the score decreases. 72 parallel hill-climbing processes attempted to minimize the score, but every 100,000 steps, all hill-climbing processes switched their current state to the best state any of them had found so far.That is, we try to optimize the pixel-to-tile map by improving the worst-performing image, in hope of obtaining a system whose performance is reasonably consistent across the training images. Specifically, the overall score was minimized using a naive, parallel "hill-climbing" (in our case, hill-descending) algorithm. Moves consisted of attempted swaps of randomly-chosen pairs tiles, accepted when the score decreases. 72 parallel hill-climbing processes attempted to minimize the score, but every 100,000 steps, all hill-climbing processes switched their current state to the best state any of them had found so far.</p>
        <p>After minimization with the Window Nucleation Model, we continued with a slower fine-tuning optimization stage using the Stochastic Greedy Model's estimate for G ce instead of G ≈ ce . (Thus, the position-dependent w x,y penalties were no longer applied, although the tile-specific concentration penalties d i were still applied.) At the time of systems design, this process used an older version of model, in which (i) G se = 7.0 and G mc = 12.75, (ii) G ce was estimated from just 50 trajectories, each of which took only 14 unfavorable steps rather than completing the entire shape, and (iii) there was no cutoff for the final G of the trajectory. Additionally, each parallel hill-climbing process synchronized with others every step, as each step was quite slow so synchronization adds little delay. During the fine-tuning step, ∼ 25 tiles were swapped.After minimization with the Window Nucleation Model, we continued with a slower fine-tuning optimization stage using the Stochastic Greedy Model's estimate for G ce instead of G ≈ ce . (Thus, the position-dependent w x,y penalties were no longer applied, although the tile-specific concentration penalties d i were still applied.) At the time of systems design, this process used an older version of model, in which (i) G se = 7.0 and G mc = 12.75, (ii) G ce was estimated from just 50 trajectories, each of which took only 14 unfavorable steps rather than completing the entire shape, and (iii) there was no cutoff for the final G of the trajectory. Additionally, each parallel hill-climbing process synchronized with others every step, as each step was quite slow so synchronization adds little delay. During the fine-tuning step, ∼ 25 tiles were swapped.</p>
        <p>To promote consistent levels and behavior of fluorescence signals, we planned to use fluorophore-and quencherlabeled strands at the base (i.e. lowest) concentration for each experiment; therefore, we wanted to restrict the pixel-to-tile map so as to accommodate this choice. The tiles with non-zero tile-concentration penalty d i were as follows: locations with fluorophore labels (d i = 1,000), locations with quenchers (d i = 1), and tiles adjacent to flourophore-quencher pairs that bound to both the fluorophore and quencher (d i = 0.5). Because the system used 917 tiles and 900 pixel locations, there were 17 assignments that were, for any pattern, always at base concentration. Consequently, the 12 fluorophores, with such a high penalty, always ended up with those assignments, while some quenchers and adjacent tiles were not always at base concentration, but were optimized toward the lower end of concentrations.To promote consistent levels and behavior of fluorescence signals, we planned to use fluorophore-and quencherlabeled strands at the base (i.e. lowest) concentration for each experiment; therefore, we wanted to restrict the pixel-to-tile map so as to accommodate this choice. The tiles with non-zero tile-concentration penalty d i were as follows: locations with fluorophore labels (d i = 1,000), locations with quenchers (d i = 1), and tiles adjacent to flourophore-quencher pairs that bound to both the fluorophore and quencher (d i = 0.5). Because the system used 917 tiles and 900 pixel locations, there were 17 assignments that were, for any pattern, always at base concentration. Consequently, the 12 fluorophores, with such a high penalty, always ended up with those assignments, while some quenchers and adjacent tiles were not always at base concentration, but were optimized toward the lower end of concentrations.</p>
        <p>In experiments, both fluorophore-and quencher-labeled strands were used at the base concentration, regardless of the concentration implied by the corresponding image pixel (but adjacent tiles were used at the pixel-specified concentration). Accordingly, during training the nucleation rate estimates used base concentrations for fluorophoreand quencher-labeled strands, regardless of the value of the pixel indicated by the current pixel-to-tile map (but again adjacent tiles used pixel-specified concentrations).In experiments, both fluorophore-and quencher-labeled strands were used at the base concentration, regardless of the concentration implied by the corresponding image pixel (but adjacent tiles were used at the pixel-specified concentration). Accordingly, during training the nucleation rate estimates used base concentrations for fluorophoreand quencher-labeled strands, regardless of the value of the pixel indicated by the current pixel-to-tile map (but again adjacent tiles used pixel-specified concentrations).</p>
        <p>To optimize the pixel-to-tile map for training pattern recognition by nucleation, we need a fast inner loop, and thus it is infeasible to use the full Stochastic Greedy Model. Finding a pixel-to-tile map that will work well experimentally does not necessarily require an accurate nucleation rate estimate; any function that provides a sufficiently similar rank ordering should be usable. Our choice was to use the simpler and faster Window Nucleation Model, which can be interpreted as calculating G ce as in equations 2.18, but for a boundary set of 'critical nuclei' S that consist of every k×k region ('window') in the shape, and with the sum over J + (A) replaced by a single constant concentration, the base concentration c. In that case, we approximateTo optimize the pixel-to-tile map for training pattern recognition by nucleation, we need a fast inner loop, and thus it is infeasible to use the full Stochastic Greedy Model. Finding a pixel-to-tile map that will work well experimentally does not necessarily require an accurate nucleation rate estimate; any function that provides a sufficiently similar rank ordering should be usable. Our choice was to use the simpler and faster Window Nucleation Model, which can be interpreted as calculating G ce as in equations 2.18, but for a boundary set of 'critical nuclei' S that consist of every k×k region ('window') in the shape, and with the sum over J + (A) replaced by a single constant concentration, the base concentration c. In that case, we approximate</p>
        <p>where c i = u 0 e -G i mc +α = c f i and c, B, G se , and α are constants that can be factored out as an additive offset to G se .where c i = u 0 e -G i mc +α = c f i and c, B, G se , and α are constants that can be factored out as an additive offset to G se .</p>
        <p>The Window Nucleation Model calculates a score related to this approximation for G ce , but also including position-dependent weights w x,y to encourage pattern recognition to make use of tiles that were demonstrated to work well in the flag scan experiments. Given a pixel-to-tile map θ to be evaluated, tile concentrations are stored in a zero-indexed L × L array as f (x, y) = c i /c where i is the tile at position (x, y) within the shape, or f (x, y) = 0 if the position is outside the shape. The concentration c i = c e 3 pn ln 3 is determined from the pattern image being evaluated based on i = θ(n), with p n being the grayscale level for image pixel n, and the base concentration is c = 16.67 nM. Then the score for window size k is calculated as:The Window Nucleation Model calculates a score related to this approximation for G ce , but also including position-dependent weights w x,y to encourage pattern recognition to make use of tiles that were demonstrated to work well in the flag scan experiments. Given a pixel-to-tile map θ to be evaluated, tile concentrations are stored in a zero-indexed L × L array as f (x, y) = c i /c where i is the tile at position (x, y) within the shape, or f (x, y) = 0 if the position is outside the shape. The concentration c i = c e 3 pn ln 3 is determined from the pattern image being evaluated based on i = θ(n), with p n being the grayscale level for image pixel n, and the base concentration is c = 16.67 nM. Then the score for window size k is calculated as:</p>
        <p>where w x,y is a weight (by default equal to 1) provided for each location in the shape to roughly incorporate experimental observations about which areas nucleated better than others. Because f (x, y) ≥ 1, a weight w x,y &gt; 1 will result in a more negative score, i.e. a prediction of faster nucleation, whenever the relevant tile is above the base concentration. Note that the contribution to the nucleation score is zero for window regions that extend beyond the boundaries of the shape. Also note that temperature, in the form of its influence on G se , does not come into play in this score, as bond energies can be factored out as a constant independent of position and pixel-to-tile map. Because temperature does influence the size of critical nuclei, our choice of k has implications on the range of temperatures for which the Window Nucleation Model will be reasonably well correlated with actual nucleation. The area K discussed in the main text as the size of a putative critical nucleus here corresponds to K = k 2 . For pattern recognition training, the window size was k = 4. This was chosen to be slightly smaller than the 5 × 5 flag size, which we knew could be sufficient for selective nucleation, because in the pattern recognition setup the high-concentration tiles need not be restricted to a checkerboard arrangement within the window; however, we make no claims of optimality here. The position-dependent weight was 2 when considering tiles in a target shape for a particular pattern within the 5 × 5 contiguous region of the following flags: H flags 2, 6, 8, and 10; A flags 1, 9, 10, and 12, and M flags 1, 5, 9, and 11, and was otherwise 1. When considering tiles in an off-target shape, all weights were 1. The goal of this weighting was to push high concentration regions for nucleation of the target shape into regions shown to work well in the flag-scan experiments (Fig. 4d), in the sense of having strong on-target nucleation and growth, but low off-target nucleation and growth.where w x,y is a weight (by default equal to 1) provided for each location in the shape to roughly incorporate experimental observations about which areas nucleated better than others. Because f (x, y) ≥ 1, a weight w x,y &gt; 1 will result in a more negative score, i.e. a prediction of faster nucleation, whenever the relevant tile is above the base concentration. Note that the contribution to the nucleation score is zero for window regions that extend beyond the boundaries of the shape. Also note that temperature, in the form of its influence on G se , does not come into play in this score, as bond energies can be factored out as a constant independent of position and pixel-to-tile map. Because temperature does influence the size of critical nuclei, our choice of k has implications on the range of temperatures for which the Window Nucleation Model will be reasonably well correlated with actual nucleation. The area K discussed in the main text as the size of a putative critical nucleus here corresponds to K = k 2 . For pattern recognition training, the window size was k = 4. This was chosen to be slightly smaller than the 5 × 5 flag size, which we knew could be sufficient for selective nucleation, because in the pattern recognition setup the high-concentration tiles need not be restricted to a checkerboard arrangement within the window; however, we make no claims of optimality here. The position-dependent weight was 2 when considering tiles in a target shape for a particular pattern within the 5 × 5 contiguous region of the following flags: H flags 2, 6, 8, and 10; A flags 1, 9, 10, and 12, and M flags 1, 5, 9, and 11, and was otherwise 1. When considering tiles in an off-target shape, all weights were 1. The goal of this weighting was to push high concentration regions for nucleation of the target shape into regions shown to work well in the flag-scan experiments (Fig. 4d), in the sense of having strong on-target nucleation and growth, but low off-target nucleation and growth.</p>
        <p>The training and weighting was effective at locating pattern-specific nucleation to the preferred regions, as estimated by the Stochastic Greedy Model. Figure S2.5 show color-coded and superimposed nucleation heatmaps of η +The training and weighting was effective at locating pattern-specific nucleation to the preferred regions, as estimated by the Stochastic Greedy Model. Figure S2.5 show color-coded and superimposed nucleation heatmaps of η +</p>
        <p>x,y for each training pattern on its target shape. (Individual nucleation heatmaps can be found in Section YYY, including heatmaps for nucleation on the off-target shapes.) Because there were 4 preferred regions for each shape, but 6 training patterns targeted to each shape, the strong effect of the position-dependent weights resulted in confining nucleation to the preferred regions, and thus forcing more than one shape to share the same nucleation region in some cases. Examination of how these patterns embed in the shapes (see Section 6.4), we see that although the same or overlapping nucleation regions are used, the highest-concentration tiles used by each pattern are generally different (c.f. Avogadro vs Abbott). This suggests that if all locations worked well for nucleation, there would be considerable potential for storing and recognizing more patterns per class in future improved designs that distribute nucleation throughout the shapes. (At this point we do not know whether the inferior nucleation locations were due to strand synthesis, sequence design, or other factors.)x,y for each training pattern on its target shape. (Individual nucleation heatmaps can be found in Section YYY, including heatmaps for nucleation on the off-target shapes.) Because there were 4 preferred regions for each shape, but 6 training patterns targeted to each shape, the strong effect of the position-dependent weights resulted in confining nucleation to the preferred regions, and thus forcing more than one shape to share the same nucleation region in some cases. Examination of how these patterns embed in the shapes (see Section 6.4), we see that although the same or overlapping nucleation regions are used, the highest-concentration tiles used by each pattern are generally different (c.f. Avogadro vs Abbott). This suggests that if all locations worked well for nucleation, there would be considerable potential for storing and recognizing more patterns per class in future improved designs that distribute nucleation throughout the shapes. (At this point we do not know whether the inferior nucleation locations were due to strand synthesis, sequence design, or other factors.)</p>
        <p>The training method discussed in Section 2.4 uses scores based on models of nucleation, together with an hillclimbing algorithm, to try to optimize the nucleation rate of the target shape for each training image compared to the nucleation rates of off-target shapes. When the method was developed, we were uncertain as to whether pattern recognition by nucleation, especially in this reservoir computing context, would be possible at all, and therefore we aimed for a method that used our most accurate nucleation models to attempt to ensure the highest discrimination ratio. It worked for our purposes, but the method is very expensive computationally and has no guarantees of finding the optimal pixel-to-tile map.The training method discussed in Section 2.4 uses scores based on models of nucleation, together with an hillclimbing algorithm, to try to optimize the nucleation rate of the target shape for each training image compared to the nucleation rates of off-target shapes. When the method was developed, we were uncertain as to whether pattern recognition by nucleation, especially in this reservoir computing context, would be possible at all, and therefore we aimed for a method that used our most accurate nucleation models to attempt to ensure the highest discrimination ratio. It worked for our purposes, but the method is very expensive computationally and has no guarantees of finding the optimal pixel-to-tile map.</p>
        <p>A kind reviewer suggested a simpler and faster algorithm, which we have evaluated and confirmed ought to be effective in many cases. The method is predicated on the assumption that the training patterns can be recognized based on just the co-occurrence of their few highest-intensity pixels. Specifically, for each training image the highest w 2 pixels will be assigned to tiles in a unique, contiguous w × w region of the target shape, if possible. Unlike the method used for our experimental implementation, this method does not rely on any model of nucleation, or exploration of the space of pixel-to-tile maps. In fact, the method is simple enough that the training could be done by hand:A kind reviewer suggested a simpler and faster algorithm, which we have evaluated and confirmed ought to be effective in many cases. The method is predicated on the assumption that the training patterns can be recognized based on just the co-occurrence of their few highest-intensity pixels. Specifically, for each training image the highest w 2 pixels will be assigned to tiles in a unique, contiguous w × w region of the target shape, if possible. Unlike the method used for our experimental implementation, this method does not rely on any model of nucleation, or exploration of the space of pixel-to-tile maps. In fact, the method is simple enough that the training could be done by hand:</p>
        <p>• For each training image, pixel locations are sorted by pixel value, from highest to lowest. The w 2 highest value pixel locations that are not already assigned to tiles are used for assignment.• For each training image, pixel locations are sorted by pixel value, from highest to lowest. The w 2 highest value pixel locations that are not already assigned to tiles are used for assignment.</p>
        <p>• For each training image, all possible contiguous w × w regions in the target shape are checked, in random order, for whether the tiles in the region are all unassigned. When a region with no previously-assigned tiles is found, the unassigned high-value pixel locations in the training image are assigned, in left-to-right, top-to-bottom order, to the tiles in the region. If no suitable region is found after all possibilities are checked, regions with only one previously-assigned tile are searched for, then two, etc, until a region is found. When a region with some previously-assigned tiles is used, these tiles are skipped during assignment, and not all w 2 pixels are assigned at this point.• For each training image, all possible contiguous w × w regions in the target shape are checked, in random order, for whether the tiles in the region are all unassigned. When a region with no previously-assigned tiles is found, the unassigned high-value pixel locations in the training image are assigned, in left-to-right, top-to-bottom order, to the tiles in the region. If no suitable region is found after all possibilities are checked, regions with only one previously-assigned tile are searched for, then two, etc, until a region is found. When a region with some previously-assigned tiles is used, these tiles are skipped during assignment, and not all w 2 pixels are assigned at this point.</p>
        <p>• Once all images have had pixels assigned to tiles with this process, any remaining unassigned pixels and tiles are assigned randomly.• Once all images have had pixels assigned to tiles with this process, any remaining unassigned pixels and tiles are assigned randomly.</p>
        <p>The parameter w is chosen by the user to reflect the expected critical nucleus size for the concentrations and annealing speed to be used.The parameter w is chosen by the user to reflect the expected critical nucleus size for the concentrations and annealing speed to be used.</p>
        <p>Unlike the nucleation-model-based training methods, this simple method has straightforward limits on the number of images that can be successfully trained if using the full complement of w 2 pixel-to-tile assignments per image is required, because no pixel or tile can be used to recognize more than one image and each trained image will have a unique intended nucleation region. Specifically, for images with P pixels and with shape s having N s tiles, the number of training images is limited by P/w 2 (as each image assigns w 2 pixels) and the number of training images classified as shape s is limited by N s /w 2 (because each such image assigns w 2 tiles). A more sophisticated algorithm could attempt to re-use pixel-to-tile assignments for multiple images and shapes.Unlike the nucleation-model-based training methods, this simple method has straightforward limits on the number of images that can be successfully trained if using the full complement of w 2 pixel-to-tile assignments per image is required, because no pixel or tile can be used to recognize more than one image and each trained image will have a unique intended nucleation region. Specifically, for images with P pixels and with shape s having N s tiles, the number of training images is limited by P/w 2 (as each image assigns w 2 pixels) and the number of training images classified as shape s is limited by N s /w 2 (because each such image assigns w 2 tiles). A more sophisticated algorithm could attempt to re-use pixel-to-tile assignments for multiple images and shapes.</p>
        <p>We have no theoretical performance guarantee for this algorithm, and we assume that its effectiveness depends not only on the nature of the images but also the particularities of how tiles are arranged in the shapes. Analysis might be possible for random images and shapes with random arrangements of tiles. One would expect, for example, that even before the above hard limits are reached, the last-to-be-added images will have many of its highest-value pixels already assigned by the processing of previous images, so only lower-value pixels will be used in its assigned nucleation region. Furthermore, as shared tiles are assigned, their locations spread throughout other shapes will also mean that later images in the process will be less likely to have a full w 2 region of high concentration tiles assigned for nucleation. Thus the expected performance of pattern recognition will smoothly degrade as more images are added.We have no theoretical performance guarantee for this algorithm, and we assume that its effectiveness depends not only on the nature of the images but also the particularities of how tiles are arranged in the shapes. Analysis might be possible for random images and shapes with random arrangements of tiles. One would expect, for example, that even before the above hard limits are reached, the last-to-be-added images will have many of its highest-value pixels already assigned by the processing of previous images, so only lower-value pixels will be used in its assigned nucleation region. Furthermore, as shared tiles are assigned, their locations spread throughout other shapes will also mean that later images in the process will be less likely to have a full w 2 region of high concentration tiles assigned for nucleation. Thus the expected performance of pattern recognition will smoothly degrade as more images are added.</p>
        <p>Surprisingly, however, this simple assignment method compares favorably to the model-based method in the capacity measurements of Extended Data Fig. E8. Note however that the algorithm is very sensitive to the choice of w with respect to the experimental conditions, or their approximation by the Stochastic Greedy Model with a specific choice of parameters. For this reason, we increased G se to 5.5 for assessing the selectivity in Extended Data Fig. E8. Even so, the case of w = 2 is not plotted because accuracy never exceeds 70% even with just one image, as the "experimental conditions" here require larger critical nuclei. On the other side, for w larger than the experimentally relevant size, the number of images that can be recognized is bounded by the use of w 2 tiles and pixels per shape, as per the hard limits discussed above. Thus, unlike the naive hill-climbing optimization, this simpler assignment algorithm performs worse as w is increased beyond the ideal value.Surprisingly, however, this simple assignment method compares favorably to the model-based method in the capacity measurements of Extended Data Fig. E8. Note however that the algorithm is very sensitive to the choice of w with respect to the experimental conditions, or their approximation by the Stochastic Greedy Model with a specific choice of parameters. For this reason, we increased G se to 5.5 for assessing the selectivity in Extended Data Fig. E8. Even so, the case of w = 2 is not plotted because accuracy never exceeds 70% even with just one image, as the "experimental conditions" here require larger critical nuclei. On the other side, for w larger than the experimentally relevant size, the number of images that can be recognized is bounded by the use of w 2 tiles and pixels per shape, as per the hard limits discussed above. Thus, unlike the naive hill-climbing optimization, this simpler assignment algorithm performs worse as w is increased beyond the ideal value.</p>
        <p>As a caveat to the interpretation of the Extended Data Fig. E8 capacity measurements, it should be appreciated that the choice of a constant temperature (G se = 5.4 for the WNM-trained pixel-to-tile maps and G se = 5.5 for the alternate assignment method presented in this section) does not perfectly reflect the principles governing selectivity in an experiment where the temperature is ramped down slowly. In that scenario, the winning shape would be the one that first achieves a nucleate rate that is sufficiently high relative to the annealing rate. Future analysis of pattern recognition capacity should incorporate this consideration.As a caveat to the interpretation of the Extended Data Fig. E8 capacity measurements, it should be appreciated that the choice of a constant temperature (G se = 5.4 for the WNM-trained pixel-to-tile maps and G se = 5.5 for the alternate assignment method presented in this section) does not perfectly reflect the principles governing selectivity in an experiment where the temperature is ramped down slowly. In that scenario, the winning shape would be the one that first achieves a nucleate rate that is sufficiently high relative to the annealing rate. Future analysis of pattern recognition capacity should incorporate this consideration.</p>
        <p>It is also worth noting that this simple algorithm does not perform well for the MNIST pattern recognition task discussed in Section 1.4, as that task involves balancing trade-offs for classifying a large number of similar patterns, so memorizing them individually is insufficient.It is also worth noting that this simple algorithm does not perform well for the MNIST pattern recognition task discussed in Section 1.4, as that task involves balancing trade-offs for classifying a large number of similar patterns, so memorizing them individually is insufficient.</p>
        <p>In our work here, we also placed restrictions on the images to be recognized. First, all images used for training and testing should make use of the same total concentration, so as to control for the strong effect that absolute concentration has on nucleation -we are interested in conditions where the pattern of relative concentrations is the deciding factor. Second, all images should have the same histogram of pixel values, so as to control for the fact that a combination of high concentration and low concentration tiles can be more effective than uniform concentrations with the same total. This second restriction is more stringent and implies the first. Neither is necessary in principle for pattern recognition by nucleation, but eliminate some possible confounds for interpreting this first foray into the phenomenon. Expanding our understanding of pattern recognition by nucleation into the full space of possible images could reveal novel properties; for example, we might expect dilutions of a concentration pattern to be classified equivalently for a suitably slow anneal until the first nucleation occurs.In our work here, we also placed restrictions on the images to be recognized. First, all images used for training and testing should make use of the same total concentration, so as to control for the strong effect that absolute concentration has on nucleation -we are interested in conditions where the pattern of relative concentrations is the deciding factor. Second, all images should have the same histogram of pixel values, so as to control for the fact that a combination of high concentration and low concentration tiles can be more effective than uniform concentrations with the same total. This second restriction is more stringent and implies the first. Neither is necessary in principle for pattern recognition by nucleation, but eliminate some possible confounds for interpreting this first foray into the phenomenon. Expanding our understanding of pattern recognition by nucleation into the full space of possible images could reveal novel properties; for example, we might expect dilutions of a concentration pattern to be classified equivalently for a suitably slow anneal until the first nucleation occurs.</p>
        <p>The particulars of image processing in this work are as follows. To allow reasonable mixing of reagents by the acoustic liquid handler we used, which had a discrete volume step of 25 nL, pixel values for each image were binned into 10 bins. As described in the Methods, pixel values 0 ≤ p n ≤ 1 were mapped exponentially to tile concentrations, using c i = c e 3 pn ln 3 , where base concentration c = 16.67 nM, c i is a tile concentration and i = θ(n). Pixel value 0 corresponds to black and 1 to white. Because of pixel value binning, the pixel values were x/9 for integer x ∈ [0, 9], which similarly resulted in 10 bins for tile concentrations, between 16.67 and 450 nM.The particulars of image processing in this work are as follows. To allow reasonable mixing of reagents by the acoustic liquid handler we used, which had a discrete volume step of 25 nL, pixel values for each image were binned into 10 bins. As described in the Methods, pixel values 0 ≤ p n ≤ 1 were mapped exponentially to tile concentrations, using c i = c e 3 pn ln 3 , where base concentration c = 16.67 nM, c i is a tile concentration and i = θ(n). Pixel value 0 corresponds to black and 1 to white. Because of pixel value binning, the pixel values were x/9 for integer x ∈ [0, 9], which similarly resulted in 10 bins for tile concentrations, between 16.67 and 450 nM.</p>
        <p>Images for the pattern recognition experiments came from a wide variety of sources. As described in Methods, each was cropped and rescaled to a 30 × 30 array, with grayscale levels adjusted to match a consistent histogram. We selected images that could be categorized into three classes based on the first letter of their names, with class labels and shapes 'H', 'A', and 'M' chosen to reflect 'Hopfield Associative Memory', the foundational inspiration for our perspective on multifarious self-assembly. Images were downloaded and processed in September, 2019. Quotes are from the given link or from Wikipedia.Images for the pattern recognition experiments came from a wide variety of sources. As described in Methods, each was cropped and rescaled to a 30 × 30 array, with grayscale levels adjusted to match a consistent histogram. We selected images that could be categorized into three classes based on the first letter of their names, with class labels and shapes 'H', 'A', and 'M' chosen to reflect 'Hopfield Associative Memory', the foundational inspiration for our perspective on multifarious self-assembly. Images were downloaded and processed in September, 2019. Quotes are from the given link or from Wikipedia.</p>
        <p>Hodgkin. https://www.gettyimages.com/detail/2672529.Hodgkin. https://www.gettyimages.com/detail/2672529.</p>
        <p>Image of Dorothy Mary Crowfoot Hodgkin licensed from Keystone / Getty Images. Hodgkin "was a Nobel Prize-winning British chemist who advanced the technique of X-ray crystallography to determine the structure of biomolecules, which became essential for structural biology." Our self-assembly work relies on the formation of geometrically ordered structures that would produce point-based diffraction patterns in the limit of large structures, much like classical homogeneous crystals. However, our structures have different DNA sequences at different sites and hence are not periodic if the sequence identity is taken into account. Our structures can be seen as a generalization of crystals to the multicomponent limit where the number of distinct species is comparable to the system size.Image of Dorothy Mary Crowfoot Hodgkin licensed from Keystone / Getty Images. Hodgkin "was a Nobel Prize-winning British chemist who advanced the technique of X-ray crystallography to determine the structure of biomolecules, which became essential for structural biology." Our self-assembly work relies on the formation of geometrically ordered structures that would produce point-based diffraction patterns in the limit of large structures, much like classical homogeneous crystals. However, our structures have different DNA sequences at different sites and hence are not periodic if the sequence identity is taken into account. Our structures can be seen as a generalization of crystals to the multicomponent limit where the number of distinct species is comparable to the system size.</p>
        <p>Hopfield. https://alchetron.com/John-Hopfield.Hopfield. https://alchetron.com/John-Hopfield.</p>
        <p>Permission for using this image obtained directly from John Joseph Hopfield and from Princeton University.Permission for using this image obtained directly from John Joseph Hopfield and from Princeton University.</p>
        <p>Hopfield "is an American scientist most widely known for his invention of an associative neural network in 1982." Hopfield's Associative Memory inspired the concepts underlying this work, even though the our work exploits an inevitable physical process (nucleation) to perform pattern recognition without mimicking Hopfield neural networks element-by-element.Hopfield "is an American scientist most widely known for his invention of an associative neural network in 1982." Hopfield's Associative Memory inspired the concepts underlying this work, even though the our work exploits an inevitable physical process (nucleation) to perform pattern recognition without mimicking Hopfield neural networks element-by-element.</p>
        <p>Horse. https://pixabay.com/photos/horse-mold-thoroughbred-arabian-2063672. Pixabay's license is "free for commercial use, with no attribution required." The Arabian horse is "is also one of the oldest breeds, with archaeological evidence of horses in the Middle East that resemble modern Arabians dating back 4,500 years."Horse. https://pixabay.com/photos/horse-mold-thoroughbred-arabian-2063672. Pixabay's license is "free for commercial use, with no attribution required." The Arabian horse is "is also one of the oldest breeds, with archaeological evidence of horses in the Middle East that resemble modern Arabians dating back 4,500 years."</p>
        <p>Hazelnuts. https://pixabay.com/photos/hazelnut-nuts-legume-nut-healthy-3357096. Pixabay's license is "free for commercial use, with no attribution required." As of this writing, the oldest "evidence of large-scale Mesolithic nut processing, some 8,000 years old, was found in a midden pit on the island of Colonsay in Scotland." for work concerning the grid cells in the entorhinal cortex, as well as several additional space-representing cell types in the same circuit that make up the positioning system in the brain." The collective dynamics of the multifarious self-assembly model explored here is mathematically related to place cell network models that simultaneously encode multiple spatial memories through different colocalization of place fields.Hazelnuts. https://pixabay.com/photos/hazelnut-nuts-legume-nut-healthy-3357096. Pixabay's license is "free for commercial use, with no attribution required." As of this writing, the oldest "evidence of large-scale Mesolithic nut processing, some 8,000 years old, was found in a midden pit on the island of Colonsay in Scotland." for work concerning the grid cells in the entorhinal cortex, as well as several additional space-representing cell types in the same circuit that make up the positioning system in the brain." The collective dynamics of the multifarious self-assembly model explored here is mathematically related to place cell network models that simultaneously encode multiple spatial memories through different colocalization of place fields.</p>
        <p>Mockingbird. https://pixabay.com/photos/polyphonic-mockingbird-2232092. Pixabay's license is "free for commercial use, with no attribution required." The northern mockingbird, commonly found in North America, "is known for its mimicking ability, as reflected by the meaning of its scientific name, 'many-tongued thrush'."Mockingbird. https://pixabay.com/photos/polyphonic-mockingbird-2232092. Pixabay's license is "free for commercial use, with no attribution required." The northern mockingbird, commonly found in North America, "is known for its mimicking ability, as reflected by the meaning of its scientific name, 'many-tongued thrush'."</p>
        <p>Mbili. http://www.pymvpa.org/datadb/mnist.html. "Mbili" is the word for "2" in Swahili. The image is from the MNIST database.Mbili. http://www.pymvpa.org/datadb/mnist.html. "Mbili" is the word for "2" in Swahili. The image is from the MNIST database.</p>
        <p>M. https://www.nist.gov/itl/products-and-services/emnist-dataset. The image of the letter "M" is from the EMNIST database.M. https://www.nist.gov/itl/products-and-services/emnist-dataset. The image of the letter "M" is from the EMNIST database.</p>
        <p>Sample mixes were designed to be possible using a particular order of automated intermediate mixes, first creating mixes of tiles at each concentration value (discretized by the pixel value bins) for each pattern, then mixing these to obtain a full sample mix for each pattern. Absolute tile concentrations were set to have a target average tile concentration of 60 nM. This resulted in slightly different discrete tile concentration bins than were used in the nucleation model. Additionally, the discrete volume steps used by the acoustic liquid handler resulted in slightly different concentrations in experiments than the target concentrations, as shown in Table S2.1. The actual concentrations (nominally) dispensed by the liquid handling robot are different from the model concentrations by no more than 3.2 %.Sample mixes were designed to be possible using a particular order of automated intermediate mixes, first creating mixes of tiles at each concentration value (discretized by the pixel value bins) for each pattern, then mixing these to obtain a full sample mix for each pattern. Absolute tile concentrations were set to have a target average tile concentration of 60 nM. This resulted in slightly different discrete tile concentration bins than were used in the nucleation model. Additionally, the discrete volume steps used by the acoustic liquid handler resulted in slightly different concentrations in experiments than the target concentrations, as shown in Table S2.1. The actual concentrations (nominally) dispensed by the liquid handling robot are different from the model concentrations by no more than 3.2 %.</p>
        <p>To assess the progress of nucleation and growth in our system, we used a combination of atomic force microscopy (AFM) and real-time fluorescence, each of which comes with strengths and limitations. The obvious strength of AFM is that we can see individual assemblies and collect statistics on their morphologies, in some cases with singletile resolution; limitations include that it can be time-consuming to prepare samples and take images, that analysis can be hard to automate, and that deposition onto mica introduces biases: in our case, we adjusted conditions such that single tiles and small assemblies did not stick well and were washed away. For these reasons we limited AFM to end-point measurements. The obvious strength of real-time fluorescence is that it reports bulk averages as a function of time, thus providing insight into kinetics; limitations include that exactly what the fluorescence level is reporting sometimes can be hard to discern, and that modifications required to enable fluorescent readout may alter the behavior of the system. Addressing and understanding these limitations, for our experimental system, required us to explore and characterize several designs for fluorescent readout. Due to the wide availability of real-time quantitative PCR (qPCR) machines, with their ability to precisely control temperature while measuring multicolor fluorescence in real time on parallel samples, they have become a useful tool for quantitative studies of thermodynamics and kinetics in nucleic acid biophysics and molecular programming. 22,129,130 Ideally, the use of a qPCR machine would allow the quantitative measurement of the absolute or relative concentrations of free and bound tiles of multiple chosen types simultaneously throughout the course of self-assembly. Our particular aim was to simultaneously monitor four locations per shape, so as to be able to observe and distinguish both the timing of initial nucleation and the progress of further growth. However, there are several phenomena that present obstacles to converting from fluorescence measurements to absolute (or even relative) concentrations in our experimental conditions. Previously used methods for fluorescent monitoring of DNA self-assembly did not appear to be suitable for our needs. Self-assembly of 2D and 3D DNA nanostructures made of double-crossover and single-stranded tiles has been monitored using the SYBER Green I intercalating dye, 111,121 but this method generically assesses single-stranded versus double-stranded rather than being easily targetable to specific locations within a structure. Fluorescence resonance energy transfer (FRET) between fluorophore pairs covalently attached to specific strands can provide proximity-dependent signals, 98,114 but for multiplexing, each signal occupies more wavelength real estate than a single-fluorophore setup, and these studies monitored only a single position within the DNA object.To assess the progress of nucleation and growth in our system, we used a combination of atomic force microscopy (AFM) and real-time fluorescence, each of which comes with strengths and limitations. The obvious strength of AFM is that we can see individual assemblies and collect statistics on their morphologies, in some cases with singletile resolution; limitations include that it can be time-consuming to prepare samples and take images, that analysis can be hard to automate, and that deposition onto mica introduces biases: in our case, we adjusted conditions such that single tiles and small assemblies did not stick well and were washed away. For these reasons we limited AFM to end-point measurements. The obvious strength of real-time fluorescence is that it reports bulk averages as a function of time, thus providing insight into kinetics; limitations include that exactly what the fluorescence level is reporting sometimes can be hard to discern, and that modifications required to enable fluorescent readout may alter the behavior of the system. Addressing and understanding these limitations, for our experimental system, required us to explore and characterize several designs for fluorescent readout. Due to the wide availability of real-time quantitative PCR (qPCR) machines, with their ability to precisely control temperature while measuring multicolor fluorescence in real time on parallel samples, they have become a useful tool for quantitative studies of thermodynamics and kinetics in nucleic acid biophysics and molecular programming. 22,129,130 Ideally, the use of a qPCR machine would allow the quantitative measurement of the absolute or relative concentrations of free and bound tiles of multiple chosen types simultaneously throughout the course of self-assembly. Our particular aim was to simultaneously monitor four locations per shape, so as to be able to observe and distinguish both the timing of initial nucleation and the progress of further growth. However, there are several phenomena that present obstacles to converting from fluorescence measurements to absolute (or even relative) concentrations in our experimental conditions. Previously used methods for fluorescent monitoring of DNA self-assembly did not appear to be suitable for our needs. Self-assembly of 2D and 3D DNA nanostructures made of double-crossover and single-stranded tiles has been monitored using the SYBER Green I intercalating dye, 111,121 but this method generically assesses single-stranded versus double-stranded rather than being easily targetable to specific locations within a structure. Fluorescence resonance energy transfer (FRET) between fluorophore pairs covalently attached to specific strands can provide proximity-dependent signals, 98,114 but for multiplexing, each signal occupies more wavelength real estate than a single-fluorophore setup, and these studies monitored only a single position within the DNA object.</p>
        <p>Inspired by dual-labeled fluorescent probes such as TaqMan probes and molecular beacons, 131 we initially used a single tile modified to contain a 5 fluorophore and a 3 quencher, as shown in Figure S3.1(a). When unbound, the modified strand was expected to allow free movement of the fluorophore and quencher without confining them to be near enough to cause significant quenching. When bound in a lattice, however, the the two ends would be on adjacent helices and oriented so as to be near each other. This constrained position would only result from a sufficiently large structure and not, for example, from dimers: thus quenching was expected to indicate the presence of a well-formed lattice structure around the labelled tile. By modifying only a single tile, we also hoped to minimize changes to nucleation and growth of the overall structure. We refer to a tile with a fluorophore and quencher as a 'label tile'. Using this 'label tile' design in initial experiments, we found that while changes to fluorescence occurred near expected growth temperatures, the changes were not as simple as a transition from a bright unbound state, to a quenched bound state. Using 5 × 5 sets of tiles around label tiles to characterize their behaviour by growing small structures during annealing experiments, we found that fluorescence significantly increased when ramping from temperatures well above melting to temperatures around those favourable for growth, before quenching at lower temperatures, but only to levels slightly below those seen at high temperatures (Figure S3.1(b), 'checker' and 'uniform'). In contrast, samples missing half the tiles required for structure formation, but with adjacent tiles that could bind to the label tile, showed an increase in fluorescence as temperature decreased (Figure S3.1(b), 'half').Inspired by dual-labeled fluorescent probes such as TaqMan probes and molecular beacons, 131 we initially used a single tile modified to contain a 5 fluorophore and a 3 quencher, as shown in Figure S3.1(a). When unbound, the modified strand was expected to allow free movement of the fluorophore and quencher without confining them to be near enough to cause significant quenching. When bound in a lattice, however, the the two ends would be on adjacent helices and oriented so as to be near each other. This constrained position would only result from a sufficiently large structure and not, for example, from dimers: thus quenching was expected to indicate the presence of a well-formed lattice structure around the labelled tile. By modifying only a single tile, we also hoped to minimize changes to nucleation and growth of the overall structure. We refer to a tile with a fluorophore and quencher as a 'label tile'. Using this 'label tile' design in initial experiments, we found that while changes to fluorescence occurred near expected growth temperatures, the changes were not as simple as a transition from a bright unbound state, to a quenched bound state. Using 5 × 5 sets of tiles around label tiles to characterize their behaviour by growing small structures during annealing experiments, we found that fluorescence significantly increased when ramping from temperatures well above melting to temperatures around those favourable for growth, before quenching at lower temperatures, but only to levels slightly below those seen at high temperatures (Figure S3.1(b), 'checker' and 'uniform'). In contrast, samples missing half the tiles required for structure formation, but with adjacent tiles that could bind to the label tile, showed an increase in fluorescence as temperature decreased (Figure S3.1(b), 'half').</p>
        <p>These results led us to suspect that, when unbound and single-stranded, the fluorophore and quencher on the label tile were both not constrained by the DNA to be close enough to quench fully, but were also not constrained to be far apart, and thus tended to interact and quench at an intermediate level. However, when some domains on the tile were bound to those on other tiles, whether in the course of nucleating a larger lattice structure or simply binding to complementary regions, the increased stiffness from the double-stranded regions did force the fluorophore and quencher apart, stopping this quenching and raising the fluorescence signal. Figure S3.1(a) shows an example intermediate structure that can occur in all the samples, but which will continue to assemble into a fully-quenched 5 × 5 square in all but the 'half' sample. This interpretation is consistent with the understood mechanism by which TaqMan probes, which typically have a baseline fluorescence several fold higher than molecular beacons that use a hairpin stem to bring fluorophore and quencher together, enhance their fluorescence upon binding to a target. 131 To verify this interpretation, we used 14-nucleotide 'guard strands' complementary to the label tiles as shown in Figure S3.1(c), such that when added to a sample containing only the matching label tile, they would cause the tile to be double-stranded at low temperatures. For all our label tiles, this addition resulted in higher fluorescence at temperatures lower than the guard strands' ∼ 50 • C predicted 132 melting point, while the label tiles by themselves showed relatively little temperature dependence (Figure S3.1(d)).These results led us to suspect that, when unbound and single-stranded, the fluorophore and quencher on the label tile were both not constrained by the DNA to be close enough to quench fully, but were also not constrained to be far apart, and thus tended to interact and quench at an intermediate level. However, when some domains on the tile were bound to those on other tiles, whether in the course of nucleating a larger lattice structure or simply binding to complementary regions, the increased stiffness from the double-stranded regions did force the fluorophore and quencher apart, stopping this quenching and raising the fluorescence signal. Figure S3.1(a) shows an example intermediate structure that can occur in all the samples, but which will continue to assemble into a fully-quenched 5 × 5 square in all but the 'half' sample. This interpretation is consistent with the understood mechanism by which TaqMan probes, which typically have a baseline fluorescence several fold higher than molecular beacons that use a hairpin stem to bring fluorophore and quencher together, enhance their fluorescence upon binding to a target. 131 To verify this interpretation, we used 14-nucleotide 'guard strands' complementary to the label tiles as shown in Figure S3.1(c), such that when added to a sample containing only the matching label tile, they would cause the tile to be double-stranded at low temperatures. For all our label tiles, this addition resulted in higher fluorescence at temperatures lower than the guard strands' ∼ 50 • C predicted 132 melting point, while the label tiles by themselves showed relatively little temperature dependence (Figure S3.1(d)).</p>
        <p>While it was possible to distinguish the formation of structures through the increased and decreased fluorescence during anneals, as in S3.1(b), the complexity and inconsistency of the label tile's behaviour during nucleation motivated a second design, which was used for all subsequent experimental results.While it was possible to distinguish the formation of structures through the increased and decreased fluorescence during anneals, as in S3.1(b), the complexity and inconsistency of the label tile's behaviour during nucleation motivated a second design, which was used for all subsequent experimental results.</p>
        <p>To avoid fluorophore-quencher interactions outside of lattice formation, our second design, shown in Figure S3.2(a), placed the fluorophore and quencher on two separate tiles -a 'label pair' -with no complementary regions. In addition to avoiding the intra-tile interactions of the previous design, the two tiles in a label pair cannot form a dimer that might exhibit quenching, even at relatively low temperature. While normal SSTs in this configuration would not have terminal ends close enough for contact quenching, in this design, one of the two tiles had its orientation reversed, with a crossover in a lattice on the opposite side; as a result, in a well-formed lattice, the fluorophore and quencher are located on facing sides of adjacent helices. The same tile, when used without a fluorophore, does not use the reversed orientation.To avoid fluorophore-quencher interactions outside of lattice formation, our second design, shown in Figure S3.2(a), placed the fluorophore and quencher on two separate tiles -a 'label pair' -with no complementary regions. In addition to avoiding the intra-tile interactions of the previous design, the two tiles in a label pair cannot form a dimer that might exhibit quenching, even at relatively low temperature. While normal SSTs in this configuration would not have terminal ends close enough for contact quenching, in this design, one of the two tiles had its orientation reversed, with a crossover in a lattice on the opposite side; as a result, in a well-formed lattice, the fluorophore and quencher are located on facing sides of adjacent helices. The same tile, when used without a fluorophore, does not use the reversed orientation.</p>
        <p>Using this label design, formation of structures caused a monotonic corresponding decrease from an unquenched, high fluorescence, to a quenched, low fluorescence signal, subject to the temperature dependence of the fluorophore itself (data not shown). This behaviour was seen with reasonable consistency in preliminary multifarious growth experiments, and could be compared with AFM images of the resulting structures.Using this label design, formation of structures caused a monotonic corresponding decrease from an unquenched, high fluorescence, to a quenched, low fluorescence signal, subject to the temperature dependence of the fluorophore itself (data not shown). This behaviour was seen with reasonable consistency in preliminary multifarious growth experiments, and could be compared with AFM images of the resulting structures.</p>
        <p>Given these results, we might hope to construct a formula f linking fluorescence to assembled (as opposed to free) tile concentration, e.g. c assem i = f (c i , F, T ), for a particular fluorescent label tile i, fluorescence reading F , temperature T , and total tile concentration c i . Doing so would be possible using suitable calibration experiments if, for example, free tiles and assembled tiles each had well-defined fluorescence levels at each temperature, independent of other DNA molecules in solution. (Here, we take 'assembled tiles' to mean those within assemblies of at least four strands that adopt the lattice conformation that brings fluorophore and quencher close to each other, while by an abuse of terminology we take 'free tiles' to mean those that are free or bound to others in locally linear or tree-like chains that don't fold the fluorophore and quencher tiles as in the lattice. It would be the concentration of this population that we could infer.)Given these results, we might hope to construct a formula f linking fluorescence to assembled (as opposed to free) tile concentration, e.g. c assem i = f (c i , F, T ), for a particular fluorescent label tile i, fluorescence reading F , temperature T , and total tile concentration c i . Doing so would be possible using suitable calibration experiments if, for example, free tiles and assembled tiles each had well-defined fluorescence levels at each temperature, independent of other DNA molecules in solution. (Here, we take 'assembled tiles' to mean those within assemblies of at least four strands that adopt the lattice conformation that brings fluorophore and quencher close to each other, while by an abuse of terminology we take 'free tiles' to mean those that are free or bound to others in locally linear or tree-like chains that don't fold the fluorophore and quencher tiles as in the lattice. It would be the concentration of this population that we could infer.)</p>
        <p>In initial control experiments (Figure S3.2(c)), we found that strands that bind directly to the fluorophore tile substantially increase its fluorescence, as expected. But more surprisingly, this effect only moderately decreases at higher temperatures where the binding is predicted to be unfavorable, as well as in the presence of unrelated DNA with little expected binding. To characterize this behaviour further, we took melting curves of a 5 × 5 square of 50 nM tiles around the ROX label pair in H, along with a variable amount of an equal mixture of tiles unique to H, outside a 7 × 7 region around the same label pair (Figure S3.2(b)). Owing to the roughly checkerboard pattern of shared and unique tiles, this unique tile mix approximated a collection of unrelated, non-interacting DNA, which was not complementary to either the tiles in the 5 × 5 target assembly, or to other strands within the mixture.In initial control experiments (Figure S3.2(c)), we found that strands that bind directly to the fluorophore tile substantially increase its fluorescence, as expected. But more surprisingly, this effect only moderately decreases at higher temperatures where the binding is predicted to be unfavorable, as well as in the presence of unrelated DNA with little expected binding. To characterize this behaviour further, we took melting curves of a 5 × 5 square of 50 nM tiles around the ROX label pair in H, along with a variable amount of an equal mixture of tiles unique to H, outside a 7 × 7 region around the same label pair (Figure S3.2(b)). Owing to the roughly checkerboard pattern of shared and unique tiles, this unique tile mix approximated a collection of unrelated, non-interacting DNA, which was not complementary to either the tiles in the 5 × 5 target assembly, or to other strands within the mixture.</p>
        <p>As shown in Figure S3.2(d), increasing the total concentration of unrelated, non-specific DNA (i.e., i c i for strand concentrations c i ) in a sample increased fluorescence at all temperatures, in a seemingly consistent way unaffected by the quenching when the 5×5 region grew and melted. The effect was present even when all assemblies were clearly melted, above 65 • C, and appears to be roughly linear between 0 and 20 ţM.As shown in Figure S3.2(d), increasing the total concentration of unrelated, non-specific DNA (i.e., i c i for strand concentrations c i ) in a sample increased fluorescence at all temperatures, in a seemingly consistent way unaffected by the quenching when the 5×5 region grew and melted. The effect was present even when all assemblies were clearly melted, above 65 • C, and appears to be roughly linear between 0 and 20 ţM.</p>
        <p>Knowing that in the absence of Tween, PEG, carrier DNA, or the equivalent (which we did not use in these experiments), low concentrations of DNA can partially absorb onto the walls of test tubes and pipette tips, we explored the hypothesis that this was related to our observations -that somehow the excess strands were playing the role of carrier DNA. While we used coated tubes designed to minimize DNA adsorption (0.5 mL DNA LoBind tubes, Eppendorf), and pipette tips (Eppendorf LoRetention), we considered whether fluorescently-labeled DNA (and assembled structures containing the label pairs) could have been absorbed to the walls when excess strands were not present, thus yielding low fluorescence measurements, but released from the walls when excess strands were available to replace them, thus increasing the measured fluorescence. Quantitatively, the observed effected seemed too large for this mechanism, and further investigation also ruled it out as a significant factor.Knowing that in the absence of Tween, PEG, carrier DNA, or the equivalent (which we did not use in these experiments), low concentrations of DNA can partially absorb onto the walls of test tubes and pipette tips, we explored the hypothesis that this was related to our observations -that somehow the excess strands were playing the role of carrier DNA. While we used coated tubes designed to minimize DNA adsorption (0.5 mL DNA LoBind tubes, Eppendorf), and pipette tips (Eppendorf LoRetention), we considered whether fluorescently-labeled DNA (and assembled structures containing the label pairs) could have been absorbed to the walls when excess strands were not present, thus yielding low fluorescence measurements, but released from the walls when excess strands were available to replace them, thus increasing the measured fluorescence. Quantitatively, the observed effected seemed too large for this mechanism, and further investigation also ruled it out as a significant factor.</p>
        <p>However, strands with regions complementary to regions on the fluorophore tile also appear to affect fluorescence, to a greater extent than non-specific DNA, and this effect also persists even at temperatures where no binding would be expected to be stable.However, strands with regions complementary to regions on the fluorophore tile also appear to affect fluorescence, to a greater extent than non-specific DNA, and this effect also persists even at temperatures where no binding would be expected to be stable.</p>
        <p>To measure this effect, we took melting curves of samples with all tiles for constructing H at 50 nM, along with a 10x concentration checkerboard flag pattern, either surrounding the label pair, or in a position far from the label pair. Both types of samples had the same total DNA concentration, but the sample with the label pair inside the high-concentration pattern, thus having high concentration of tiles adjacent to the fluorophore and quencher, had a significantly higher concentration of strands with regions complementary to regions on the fluorophore and quencher strands. In an anneal and melt, shown in Figure S3.2(e), the sample with high concentration adjacent strands had a significantly higher fluorescence above the melting temperature of the structure, but when structures had formed, had similar fluorescence. The raw fluorescence level at low temperatures, regardless of whether the excess tiles were adjacent or not, was consistent with that of the sample in Figure S3.2(d) that had the most similar ROX FAM ATTO 550 ATTO 647N H H173, H148 H306, H281 H227, H252 H428, H445 A A636, A647 A544, A527 A544, A528 A539, A550 M M863, M855 M830, M836 M761, M754 M759, M767 Table S3.1: Fluorophore/quencher pair locations for each fluorophore and shape. amount of excess DNA, suggesting a lack of sequence-specificity for fluorophores within assembled structures. This also rules out the possibility that the fluorescence increase was just due to unintentional excess of fluorophore tiles over non-labeled tiles (noting that label pair tiles were ordered purified, while all other tiles we used unpurified).To measure this effect, we took melting curves of samples with all tiles for constructing H at 50 nM, along with a 10x concentration checkerboard flag pattern, either surrounding the label pair, or in a position far from the label pair. Both types of samples had the same total DNA concentration, but the sample with the label pair inside the high-concentration pattern, thus having high concentration of tiles adjacent to the fluorophore and quencher, had a significantly higher concentration of strands with regions complementary to regions on the fluorophore and quencher strands. In an anneal and melt, shown in Figure S3.2(e), the sample with high concentration adjacent strands had a significantly higher fluorescence above the melting temperature of the structure, but when structures had formed, had similar fluorescence. The raw fluorescence level at low temperatures, regardless of whether the excess tiles were adjacent or not, was consistent with that of the sample in Figure S3.2(d) that had the most similar ROX FAM ATTO 550 ATTO 647N H H173, H148 H306, H281 H227, H252 H428, H445 A A636, A647 A544, A527 A544, A528 A539, A550 M M863, M855 M830, M836 M761, M754 M759, M767 Table S3.1: Fluorophore/quencher pair locations for each fluorophore and shape. amount of excess DNA, suggesting a lack of sequence-specificity for fluorophores within assembled structures. This also rules out the possibility that the fluorescence increase was just due to unintentional excess of fluorophore tiles over non-labeled tiles (noting that label pair tiles were ordered purified, while all other tiles we used unpurified).</p>
        <p>Our conclusions are as follows:Our conclusions are as follows:</p>
        <p>1. The mostly-quenched fluorescence levels of fluorophores within assembled structures, with a nearby quencher, increase similarly in the presence of unrelated or related single-stranded DNA in solution.1. The mostly-quenched fluorescence levels of fluorophores within assembled structures, with a nearby quencher, increase similarly in the presence of unrelated or related single-stranded DNA in solution.</p>
        <p>2. The mostly-unquenched fluorescence levels of fluorophores on free strands increase in the presence of unrelated single-stranded DNA, with further increases in the presence of related single-stranded DNA that has some sequence complementarity.2. The mostly-unquenched fluorescence levels of fluorophores on free strands increase in the presence of unrelated single-stranded DNA, with further increases in the presence of related single-stranded DNA that has some sequence complementarity.</p>
        <p>3. The free fluorophore tile's temperature dependence is small by comparison.3. The free fluorophore tile's temperature dependence is small by comparison.</p>
        <p>4. Accounting for these effects to obtain quantitative inference of tile and assembly concentrations from fluorescence data would be fraught.4. Accounting for these effects to obtain quantitative inference of tile and assembly concentrations from fluorescence data would be fraught.</p>
        <p>In these experiments, we see that time/temperature of the formation and melting transitions is not very sensitive to the above factors influencing absolute fluorescence.In these experiments, we see that time/temperature of the formation and melting transitions is not very sensitive to the above factors influencing absolute fluorescence.</p>
        <p>It seems likely that using FRET instead of direct fluorescence would alleviate some of these issues, at the cost of having fewer label pair tiles per sample. However, given the surprising influence of excess DNA even for fluorophore/quencher pairs within the assembled lattice context, it is not a foregone conclusion that FRET eliminate these confounds entirely.It seems likely that using FRET instead of direct fluorescence would alleviate some of these issues, at the cost of having fewer label pair tiles per sample. However, given the surprising influence of excess DNA even for fluorophore/quencher pairs within the assembled lattice context, it is not a foregone conclusion that FRET eliminate these confounds entirely.</p>
        <p>As a separate matter, it is well-known that incorporation of fluorophores and quenchers can affect the thermodynamics of DNA hybridization and self-assembly reactions, 129 often being beneficial on the order of up to 2 kcal/mol. This is especially a concern for us, if the modified tiles are part of a critical nucleus that controls nucleation rates. Consistent with this, although also possibly influenced by the reverse orientation 133 of the fluorophore-labeled tile, the AFM-based counts for samples containing label pair tiles near the center of the nucleation region were often notably higher than those that used label tiles elsewhere but otherwise had the same tile concentration pattern (see the table in Section 5.3.1). A: 496 tiles, 926 bonds, 181 unique tiles, 315 shared (168 all, 81 with H, 66 with M)As a separate matter, it is well-known that incorporation of fluorophores and quenchers can affect the thermodynamics of DNA hybridization and self-assembly reactions, 129 often being beneficial on the order of up to 2 kcal/mol. This is especially a concern for us, if the modified tiles are part of a critical nucleus that controls nucleation rates. Consistent with this, although also possibly influenced by the reverse orientation 133 of the fluorophore-labeled tile, the AFM-based counts for samples containing label pair tiles near the center of the nucleation region were often notably higher than those that used label tiles elsewhere but otherwise had the same tile concentration pattern (see the table in Section 5.3.1). A: 496 tiles, 926 bonds, 181 unique tiles, 315 shared (168 all, 81 with H, 66 with M)</p>
        <p>M: 480 tiles, 880 bonds, 190 unique tiles, 290 shared (168 all, 56 with H, 66 with A)M: 480 tiles, 880 bonds, 190 unique tiles, 290 shared (168 all, 56 with H, 66 with A)</p>
        <p>Stable configurations of the structures exhibited significant curvature, forming spirals of roughly 15 to 30 helices, consistent with the tile motif forming tubes of 4 to 20 helices in circumference 108 and being predicted to have relaxed curvature of approximately 30 • per helix. 133 In trajectories, these spirals can change in size, for example, moving between the arrangements seen for H and for A in Figure S3.4b; it is likely that separate simulations would also result in spirals of different sizes. As each structure, from bottom-left to top-right, had 46 parallel helices, in many configurations these corners are at the center of spirals, which likely inhibits growth; numerous structures missing the corners were seen, as shown in Figures 2 andE3. Recall that the curling of the structure that takes place in the simulation, from an initial flat-but-complete configuration, does not reflect the dynamics expected in the experimental system: experimentally, the assembly would begin curling as it nucleates and grows, thus never experiencing the flat-but-complete configuration.Stable configurations of the structures exhibited significant curvature, forming spirals of roughly 15 to 30 helices, consistent with the tile motif forming tubes of 4 to 20 helices in circumference 108 and being predicted to have relaxed curvature of approximately 30 • per helix. 133 In trajectories, these spirals can change in size, for example, moving between the arrangements seen for H and for A in Figure S3.4b; it is likely that separate simulations would also result in spirals of different sizes. As each structure, from bottom-left to top-right, had 46 parallel helices, in many configurations these corners are at the center of spirals, which likely inhibits growth; numerous structures missing the corners were seen, as shown in Figures 2 andE3. Recall that the curling of the structure that takes place in the simulation, from an initial flat-but-complete configuration, does not reflect the dynamics expected in the experimental system: experimentally, the assembly would begin curling as it nucleates and grows, thus never experiencing the flat-but-complete configuration.</p>
        <p>Structures were generated with oxDNA parameters of 1 M NaCl salt concentration, and a temperature of 20 • C (H and A) or 30 • C (M). H and M were run on oxdna.org, 93,94 while A was run on a local server using oxDNA 3.3. Relevant oxDNA files, trajectories, and movies are available at https://www.dna.caltech.edu/ SupplementaryMaterial/MultifariousSST/. 134 We dont envision that flatter structures make any difference for nucleation, as the scale of curvature is considerably larger than that of expected critical nuclei. However, they could affect growth, changing the fluorescence readouts and assemblies visible in AFM images, for example, perhaps affecting the growth of corners of shapes. Our combination of AFM and fluorescence readouts can help investigate these issues in future work.Structures were generated with oxDNA parameters of 1 M NaCl salt concentration, and a temperature of 20 • C (H and A) or 30 • C (M). H and M were run on oxdna.org, 93,94 while A was run on a local server using oxDNA 3.3. Relevant oxDNA files, trajectories, and movies are available at https://www.dna.caltech.edu/ SupplementaryMaterial/MultifariousSST/. 134 We dont envision that flatter structures make any difference for nucleation, as the scale of curvature is considerably larger than that of expected critical nuclei. However, they could affect growth, changing the fluorescence readouts and assemblies visible in AFM images, for example, perhaps affecting the growth of corners of shapes. Our combination of AFM and fluorescence readouts can help investigate these issues in future work.</p>
        <p>Section 4Section 4</p>
        <p>, where the tile number starts from 0 (and does not depend on the letter), and the letter denotes what shapes the tile appears in. S tiles appear in all three shapes, H/A/M tiles appear only in the corresponding shape, and I/B/N tiles appear in two shapes: A+M, H+M, and H+A, respectively, akin to the IUPAC naming for degenerate DNA bases, where the letter is the letter that follows the excluded shape in the alphabet. Spaces in sequences correspond to divisions between glues. Glues are listed from 5' to 3'. They do not indicate complementarity: as we do not consider tile rotations, a glue in position 1 will bind to 3, and position 2 to position 4. Glues with the same names in these two positions (1/3 or 2/4) will be reverse complements of each other: for example, glue 3 of H1 is the reverse complement of glue 1 of H0. The exception to this is for positions that are "null" glues, which are simply 10nt or 11nt all-T regions, on tiles that will form the edges of shapes. On diagrams where the H/A/M shapes are upright, such as in Section 3.3, the glue positions in these sequences will correspond with East, North, West, and South directions, respectively. Null glue names refer to the length in nucleotides (as tiles alternate between having 10/11/11/10 nt domains and 11/10/10/11 nt domains), and, for consistency reasons, a rotated cardinal direction., where the tile number starts from 0 (and does not depend on the letter), and the letter denotes what shapes the tile appears in. S tiles appear in all three shapes, H/A/M tiles appear only in the corresponding shape, and I/B/N tiles appear in two shapes: A+M, H+M, and H+A, respectively, akin to the IUPAC naming for degenerate DNA bases, where the letter is the letter that follows the excluded shape in the alphabet. Spaces in sequences correspond to divisions between glues. Glues are listed from 5' to 3'. They do not indicate complementarity: as we do not consider tile rotations, a glue in position 1 will bind to 3, and position 2 to position 4. Glues with the same names in these two positions (1/3 or 2/4) will be reverse complements of each other: for example, glue 3 of H1 is the reverse complement of glue 1 of H0. The exception to this is for positions that are "null" glues, which are simply 10nt or 11nt all-T regions, on tiles that will form the edges of shapes. On diagrams where the H/A/M shapes are upright, such as in Section 3.3, the glue positions in these sequences will correspond with East, North, West, and South directions, respectively. Null glue names refer to the length in nucleotides (as tiles alternate between having 10/11/11/10 nt domains and 11/10/10/11 nt domains), and, for consistency reasons, a rotated cardinal direction.</p>
        <p>H. https://www.nist.gov/itl/products-and-services/emnist-dataset.H. https://www.nist.gov/itl/products-and-services/emnist-dataset.</p>
        <p>The image of the letter "H" is from the EMNIST database.The image of the letter "H" is from the EMNIST database.</p>
        <p>Avogadro. https://colenda.library.upenn.edu/catalog/81431-p34q7qz9q. Drawing of Lorenzo Romano Amedeo Carlo Avogadro by C. Sentier in 1856, from the Edgar Fahs Smith Image Collection at the University of Pennsylvania. Avogadro "was an Italian scientist, most noted for his contribution to molecular theory now known as Avogadro's law." His law tells us how many parallel pattern recognition events (∼ Aon. http://www.pymvpa.org/datadb/mnist.html. "Aon" is the word for "1" in Scottish Gaelic. The image is from the MNIST database. A. https://www.nist.gov/itl/products-and-services/emnist-dataset. The image of the letter "A" is from the EMNIST database. Mitscherlich. https://library.si.edu/image-gallery/73676. Engraving of Eilhard Mitscherlich by William C. Sharpe, 1860, obtained from the Smithsonian Institute Libraries, which asserts no copyright. Mitscherlich "was a German chemist, who is perhaps best remembered today for his discovery of the phenomenon of crystallographic isomorphism in 1819 [... ] His investigation, also in 1826, of the two crystalline modifications of sulfur threw much light on the fact that the two minerals calcite and aragonite have the same composition but different crystalline forms, a property which Mitscherlich called polymorphism." The multifarious S+H+A+M molecular mix, capable of three distinct geometric ordered assemblies, can be seen as a generalization of the crystal polymorph concept to the limit of a large number of distinct species. Moser. https://www.flickr.com/photos/kavli-ntnu/37030995162. Permission for using this photograph by Bård Ivar Basmo / Kavli Institute for Systems Neuroscience at the Norwegian University of Science and Technology, was obtained directly from May-Britt Moser. Moser, with "her then-husband, Edvard Moser, shared half of the 2014 Nobel Prize in Physiology or Medicine, awardedAvogadro. https://colenda.library.upenn.edu/catalog/81431-p34q7qz9q. Drawing of Lorenzo Romano Amedeo Carlo Avogadro by C. Sentier in 1856, from the Edgar Fahs Smith Image Collection at the University of Pennsylvania. Avogadro "was an Italian scientist, most noted for his contribution to molecular theory now known as Avogadro's law." His law tells us how many parallel pattern recognition events (∼ Aon. http://www.pymvpa.org/datadb/mnist.html. "Aon" is the word for "1" in Scottish Gaelic. The image is from the MNIST database. A. https://www.nist.gov/itl/products-and-services/emnist-dataset. The image of the letter "A" is from the EMNIST database. Mitscherlich. https://library.si.edu/image-gallery/73676. Engraving of Eilhard Mitscherlich by William C. Sharpe, 1860, obtained from the Smithsonian Institute Libraries, which asserts no copyright. Mitscherlich "was a German chemist, who is perhaps best remembered today for his discovery of the phenomenon of crystallographic isomorphism in 1819 [... ] His investigation, also in 1826, of the two crystalline modifications of sulfur threw much light on the fact that the two minerals calcite and aragonite have the same composition but different crystalline forms, a property which Mitscherlich called polymorphism." The multifarious S+H+A+M molecular mix, capable of three distinct geometric ordered assemblies, can be seen as a generalization of the crystal polymorph concept to the limit of a large number of distinct species. Moser. https://www.flickr.com/photos/kavli-ntnu/37030995162. Permission for using this photograph by Bård Ivar Basmo / Kavli Institute for Systems Neuroscience at the Norwegian University of Science and Technology, was obtained directly from May-Britt Moser. Moser, with "her then-husband, Edvard Moser, shared half of the 2014 Nobel Prize in Physiology or Medicine, awarded</p>
        <p>3. 53 • C to 47.2 • C, in steps of -0.2 • C every 12 minutes, taking a measurement at the end of each hold (6 hours in total).3. 53 • C to 47.2 • C, in steps of -0.2 • C every 12 minutes, taking a measurement at the end of each hold (6 hours in total).</p>
        <p>4. A 474. A 47</p>
        <p>• C hold, taking a measurement every 12 minutes (51 hours in total).• C hold, taking a measurement every 12 minutes (51 hours in total).</p>
        <p>M-flagscan-10-3 Kinetic nucleation control experiments, temperature ramp, part 1M-flagscan-10-3 Kinetic nucleation control experiments, temperature ramp, part 1</p>
        <p>5.4 Plate-level fluorescence data used in Section 5.3 0h 30h 60h H-flagscan-01-H 0h 30h 60h H-flagscan-05-H 0h 30h 60h H-flagscan-09-H 0h 30h 60h A-flagscan-01-H 0h 30h 60h A-flagscan-05-H 0h 30h 60h A-flagscan-09-H 0h 30h 60h M-flagscan-01-H 0h 30h 60h M-flagscan-05-H 0h 30h 60h M-flagscan-09-H 0h 30h 60h M-flagscan-13-H 0h 30h 60h H-flagscan-01-A 0h 30h 60h H-flagscan-05-A 0h 30h 60h H-flagscan-09-A 0h 30h 60h A-flagscan-01-A 0h 30h 60h A-flagscan-05-A 0h 30h 60h A-flagscan-09-A 0h 30h 60h M-flagscan-01-A 0h 30h 60h M-flagscan-05-A 0h 30h 60h M-flagscan-09-A 0h 30h 60h M-flagscan-13-A 0h 30h 60h H-flagscan-01-M 0h 30h 60h H-flagscan-05-M 0h 30h 60h H-flagscan-09-M 0h 30h 60h A-flagscan-01-M 0h 30h 60h A-flagscan-05-M 0h 30h 60h A-flagscan-09-M 0h 30h 60h M-flagscan-01-M 0h 30h 60h M-flagscan-05-M 0h 30h 60h M-flagscan-09-M 0h 30h 60h M-flagscan-13-M 0h 30h 60h H-flagscan-01-3 0h 30h 60h H-flagscan-05-3 0h 30h 60h H-flagscan-09-3 0h 30h 60h A-flagscan-01-3 0h 30h 60h A-flagscan-05-3 0h 30h 60h A-flagscan-09-3 0h 30h 60h M-flagscan-01-3 0h 30h 60h M-flagscan-05-3 0h 30h 60h M-flagscan-09-3 M-flagscan-13-3 0h 30h 60h H-flagscan-02-H 0h 30h 60h H-flagscan-06-H 0h 30h 60h H-flagscan-10-H 0h 30h 60h A-flagscan-02-H 0h 30h 60h A-flagscan-06-H 0h 30h 60h A-flagscan-10-H 0h 30h 60h M-flagscan-02-H 0h 30h 60h M-flagscan-06-H 0h 30h 60h M-flagscan-10-H 0h 30h 60h H-flagscan-02-A 0h 30h 60h H-flagscan-06-A 0h 30h 60h H-flagscan-10-A 0h 30h 60h A-flagscan-02-A 0h 30h 60h A-flagscan-06-A 0h 30h 60h A-flagscan-10-A 0h 30h 60h M-flagscan-02-A 0h 30h 60h M-flagscan-06-A 0h 30h 60h M-flagscan-10-A 0h 30h 60h H-flagscan-02-M 0h 30h 60h H-flagscan-06-M 0h 30h 60h H-flagscan-10-M 0h 30h 60h A-flagscan-02-M 0h 30h 60h A-flagscan-06-M 0h 30h 60h A-flagscan-10-M 0h 30h 60h M-flagscan-02-M 0h 30h 60h M-flagscan-06-M 0h 30h 60h M-flagscan-10-M 0h 30h 60h H-flagscan-02-3 0h 30h 60h H-flagscan-06-3 0h 30h 60h H-flagscan-10-3 0h 30h 0h 30h 60h H-flagscan-03-H 0h 30h 60h H-flagscan-07-H 0h 30h 60h H-flagscan-11-H 0h 30h 60h A-flagscan-03-H 0h 30h 60h A-flagscan-07-H 0h 30h 60h A-flagscan-11-H 0h 30h 60h M-flagscan-03-H 0h 30h 60h M-flagscan-07-H 0h 30h 60h M-flagscan-11-H 0h 30h 60h noflag-H 0h 30h 60h H-flagscan-03-A 0h 30h 60h H-flagscan-07-A 0h 30h 60h H-flagscan-11-A 0h 30h 60h A-flagscan-03-A 0h 30h 60h A-flagscan-07-A 0h 30h 60h A-flagscan-11-A 0h 30h 60h M-flagscan-03-A 0h 30h 60h M-flagscan-07-A 0h 30h 60h M-flagscan-11-A 0h 30h 60h noflag-A 0h 30h 60h H-flagscan-03-M 0h 30h 60h H-flagscan-07-M 0h 30h 60h H-flagscan-11-M 0h 30h 60h A-flagscan-03-M 0h 30h 60h A-flagscan-07-M 0h 30h 60h A-flagscan-11-M 0h 30h 60h M-flagscan-03-M 0h 30h 60h M-flagscan-07-M 0h 30h 60h M-flagscan-11-M 0h 30h 60h noflag-M 0h 30h 60h H-flagscan-03-3 0h 30h 60h H-flagscan-07-3 0h 30h 60h H-flagscan-11-3 0h 30h 60h A-flagscan-03-3 0h 30h 60h A-flagscan-07-3 0h 30h 60h A-flagscan-11-3 0h 30h 60h M-flagscan-03-3 0h 30h 60h M-flagscan-07-3 0h 30h 60h M-flagscan-11-3 noflag-3 0h 30h 60h H-flagscan-04-H 0h 30h 60h H-flagscan-08-H 0h 30h 60h H-flagscan-12-H 0h 30h 60h A-flagscan-04-H 0h 30h 60h A-flagscan-08-H 0h 30h 60h A-flagscan-12-H 0h 30h 60h M-flagscan-04-H 0h 30h 60h M-flagscan-08-H 0h 30h 60h M-flagscan-12-H 0h 30h 60h H-flagscan-04-A 0h 30h 60h H-flagscan-08-A 0h 30h 60h H-flagscan-12-A 0h 30h 60h A-flagscan-04-A 0h 30h 60h A-flagscan-08-A 0h 30h 60h A-flagscan-12-A 0h 30h 60h M-flagscan-04-A 0h 30h 60h M-flagscan-08-A 0h 30h 60h M-flagscan-12-A 0h 30h 60h H-flagscan-04-M 0h 30h 60h H-flagscan-08-M 0h 30h 60h H-flagscan-12-M 0h 30h 60h A-flagscan-04-M 0h 30h 60h A-flagscan-08-M 0h 30h 60h A-flagscan-12-M 0h 30h 60h M-flagscan-04-M 0h 30h 60h M-flagscan-08-M 0h 30h 60h M-flagscan-12-M 0h 30h 60h H-flagscan-04-3 0h 30h 60h H-flagscan-08-3 0h 30h 60h H-flagscan-12-3 0h 30h Flag patterns 0h 30h 60h 90h H-flagscan-01-H 0h 30h 60h 90h H-flagscan-05-H 0h 30h 60h 90h H-flagscan-09-H 0h 30h 60h 90h A-flagscan-01-H 0h 30h 60h 90h A-flagscan-05-H 0h 30h 60h 90h A-flagscan-09-H 0h 30h 60h 90h M-flagscan-01-H 0h 30h 60h 90h M-flagscan-05-H 0h 30h 60h 90h M-flagscan-09-H 0h 30h 60h 90h M-flagscan-13-H 0h 30h 60h 90h H-flagscan-01-A 0h 30h 60h 90h H-flagscan-05-A 0h 30h 60h 90h H-flagscan-09-A 0h 30h 60h 90h A-flagscan-01-A 0h 30h 60h 90h A-flagscan-05-A 0h 30h 60h 90h A-flagscan-09-A 0h 30h 60h 90h M-flagscan-01-A 0h 30h 60h 90h M-flagscan-05-A 0h 30h 60h 90h M-flagscan-09-A 0h 30h 60h 90h M-flagscan-13-A 0h 30h 60h 90h H-flagscan-01-M 0h 30h 60h 90h H-flagscan-05-M 0h 30h 60h 90h H-flagscan-09-M 0h 30h 60h 90h A-flagscan-01-M 0h 30h 60h 90h A-flagscan-05-M 0h 30h 60h 90h A-flagscan-09-M 0h 30h 60h 90h M-flagscan-01-M 0h 30h 60h 90h M-flagscan-05-M 0h 30h 60h 90h M-flagscan-09-M 0h 30h 60h 90h M-flagscan-13-M 0h 30h 60h 90h H-flagscan-01-3 0h 30h 60h 90h H-flagscan-05-3 0h 30h 60h 90h H-flagscan-09-3 0h 30h 60h 90h A-flagscan-01-3 0h 30h 60h 90h A-flagscan-05-3 0h 30h 60h 90h A-flagscan-09-3 0h 30h 60h 90h M-flagscan-01-3 0h 30h 60h 90h M-flagscan-05-3 0h 30h 60h 90h M-flagscan-09-3 M-flagscan-13-3 0h 30h 60h 90h H-flagscan-02-H 0h 30h 60h 90h H-flagscan-06-H 0h 30h 60h 90h H-flagscan-10-H 0h 30h 60h 90h A-flagscan-02-H 0h 30h 60h 90h A-flagscan-06-H 0h 30h 60h 90h A-flagscan-10-H 0h 30h 60h 90h M-flagscan-02-H 0h 30h 60h 90h M-flagscan-06-H 0h 30h 60h 90h M-flagscan-10-H 0h 30h 60h 90h H-flagscan-02-A 0h 30h 60h 90h H-flagscan-06-A 0h 30h 60h 90h H-flagscan-10-A 0h 30h 60h 90h A-flagscan-02-A 0h 30h 60h 90h A-flagscan-06-A 0h 30h 60h 90h A-flagscan-10-A 0h 30h 60h 90h M-flagscan-02-A 0h 30h 60h 90h M-flagscan-06-A 0h 30h 60h 90h M-flagscan-10-A 0h 30h 60h 90h H-flagscan-02-M 0h 30h 60h 90h H-flagscan-06-M 0h 30h 60h 90h H-flagscan-10-M 0h 30h 60h 90h A-flagscan-02-M 0h 30h 60h 90h A-flagscan-06-M 0h 30h 60h 90h A-flagscan-10-M 0h 30h 60h 90h M-flagscan-02-M 0h 30h 60h 90h M-flagscan-06-M 0h 30h 60h 90h M-flagscan-10-M 0h 30h 60h 90h H-flagscan-02-3 0h 30h 60h 90h H-flagscan-06-3 0h 30h 60h 90h H-flagscan-10-3 0h 30h 60h 90h A-flagscan-02-3 0h 30h 60h 90h A-flagscan-06-3 0h 30h 60h 90h A-flagscan-10-3 0h 30h 60h 90h M-flagscan-02-3 0h 30h 60h 90h M-flagscan-06-3 0h 30h 60h 90h5.4 Plate-level fluorescence data used in Section 5.3 0h 30h 60h H-flagscan-01-H 0h 30h 60h H-flagscan-05-H 0h 30h 60h H-flagscan-09-H 0h 30h 60h A-flagscan-01-H 0h 30h 60h A-flagscan-05-H 0h 30h 60h A-flagscan-09-H 0h 30h 60h M-flagscan-01-H 0h 30h 60h M-flagscan-05-H 0h 30h 60h M-flagscan-09-H 0h 30h 60h M-flagscan-13-H 0h 30h 60h H-flagscan-01-A 0h 30h 60h H-flagscan-05-A 0h 30h 60h H-flagscan-09-A 0h 30h 60h A-flagscan-01-A 0h 30h 60h A-flagscan-05-A 0h 30h 60h A-flagscan-09-A 0h 30h 60h M-flagscan-01-A 0h 30h 60h M-flagscan-05-A 0h 30h 60h M-flagscan-09-A 0h 30h 60h M-flagscan-13-A 0h 30h 60h H-flagscan-01-M 0h 30h 60h H-flagscan-05-M 0h 30h 60h H-flagscan-09-M 0h 30h 60h A-flagscan-01-M 0h 30h 60h A-flagscan-05-M 0h 30h 60h A-flagscan-09-M 0h 30h 60h M-flagscan-01-M 0h 30h 60h M-flagscan-05-M 0h 30h 60h M-flagscan-09-M 0h 30h 60h M-flagscan-13-M 0h 30h 60h H-flagscan-01-3 0h 30h 60h H-flagscan-05-3 0h 30h 60h H-flagscan-09-3 0h 30h 60h A-flagscan-01-3 0h 30h 60h A-flagscan-05-3 0h 30h 60h A-flagscan-09-3 0h 30h 60h M-flagscan-01-3 0h 30h 60h M-flagscan-05-3 0h 30h 60h M-flagscan-09-3 M-flagscan-13-3 0h 30h 60h H-flagscan-02-H 0h 30h 60h H-flagscan-06-H 0h 30h 60h H-flagscan-10-H 0h 30h 60h A-flagscan-02-H 0h 30h 60h A-flagscan-06-H 0h 30h 60h A-flagscan-10-H 0h 30h 60h M-flagscan-02-H 0h 30h 60h M-flagscan-06-H 0h 30h 60h M-flagscan-10-H 0h 30h 60h H-flagscan-02-A 0h 30h 60h H-flagscan-06-A 0h 30h 60h H-flagscan-10-A 0h 30h 60h A-flagscan-02-A 0h 30h 60h A-flagscan-06-A 0h 30h 60h A-flagscan-10-A 0h 30h 60h M-flagscan-02-A 0h 30h 60h M-flagscan-06-A 0h 30h 60h M-flagscan-10-A 0h 30h 60h H-flagscan-02-M 0h 30h 60h H-flagscan-06-M 0h 30h 60h H-flagscan-10-M 0h 30h 60h A-flagscan-02-M 0h 30h 60h A-flagscan-06-M 0h 30h 60h A-flagscan-10-M 0h 30h 60h M-flagscan-02-M 0h 30h 60h M-flagscan-06-M 0h 30h 60h M-flagscan-10-M 0h 30h 60h H-flagscan-02-3 0h 30h 60h H-flagscan-06-3 0h 30h 60h H-flagscan-10-3 0h 30h 0h 30h 60h H-flagscan-03-H 0h 30h 60h H-flagscan-07-H 0h 30h 60h H-flagscan-11-H 0h 30h 60h A-flagscan-03-H 0h 30h 60h A-flagscan-07-H 0h 30h 60h A-flagscan-11-H 0h 30h 60h M-flagscan-03-H 0h 30h 60h M-flagscan-07-H 0h 30h 60h M-flagscan-11-H 0h 30h 60h noflag-H 0h 30h 60h H-flagscan-03-A 0h 30h 60h H-flagscan-07-A 0h 30h 60h H-flagscan-11-A 0h 30h 60h A-flagscan-03-A 0h 30h 60h A-flagscan-07-A 0h 30h 60h A-flagscan-11-A 0h 30h 60h M-flagscan-03-A 0h 30h 60h M-flagscan-07-A 0h 30h 60h M-flagscan-11-A 0h 30h 60h noflag-A 0h 30h 60h H-flagscan-03-M 0h 30h 60h H-flagscan-07-M 0h 30h 60h H-flagscan-11-M 0h 30h 60h A-flagscan-03-M 0h 30h 60h A-flagscan-07-M 0h 30h 60h A-flagscan-11-M 0h 30h 60h M-flagscan-03-M 0h 30h 60h M-flagscan-07-M 0h 30h 60h M-flagscan-11-M 0h 30h 60h noflag-M 0h 30h 60h H-flagscan-03-3 0h 30h 60h H-flagscan-07-3 0h 30h 60h H-flagscan-11-3 0h 30h 60h A-flagscan-03-3 0h 30h 60h A-flagscan-07-3 0h 30h 60h A-flagscan-11-3 0h 30h 60h M-flagscan-03-3 0h 30h 60h M-flagscan-07-3 0h 30h 60h M-flagscan-11-3 noflag-3 0h 30h 60h H-flagscan-04-H 0h 30h 60h H-flagscan-08-H 0h 30h 60h H-flagscan-12-H 0h 30h 60h A-flagscan-04-H 0h 30h 60h A-flagscan-08-H 0h 30h 60h A-flagscan-12-H 0h 30h 60h M-flagscan-04-H 0h 30h 60h M-flagscan-08-H 0h 30h 60h M-flagscan-12-H 0h 30h 60h H-flagscan-04-A 0h 30h 60h H-flagscan-08-A 0h 30h 60h H-flagscan-12-A 0h 30h 60h A-flagscan-04-A 0h 30h 60h A-flagscan-08-A 0h 30h 60h A-flagscan-12-A 0h 30h 60h M-flagscan-04-A 0h 30h 60h M-flagscan-08-A 0h 30h 60h M-flagscan-12-A 0h 30h 60h H-flagscan-04-M 0h 30h 60h H-flagscan-08-M 0h 30h 60h H-flagscan-12-M 0h 30h 60h A-flagscan-04-M 0h 30h 60h A-flagscan-08-M 0h 30h 60h A-flagscan-12-M 0h 30h 60h M-flagscan-04-M 0h 30h 60h M-flagscan-08-M 0h 30h 60h M-flagscan-12-M 0h 30h 60h H-flagscan-04-3 0h 30h 60h H-flagscan-08-3 0h 30h 60h H-flagscan-12-3 0h 30h Flag patterns 0h 30h 60h 90h H-flagscan-01-H 0h 30h 60h 90h H-flagscan-05-H 0h 30h 60h 90h H-flagscan-09-H 0h 30h 60h 90h A-flagscan-01-H 0h 30h 60h 90h A-flagscan-05-H 0h 30h 60h 90h A-flagscan-09-H 0h 30h 60h 90h M-flagscan-01-H 0h 30h 60h 90h M-flagscan-05-H 0h 30h 60h 90h M-flagscan-09-H 0h 30h 60h 90h M-flagscan-13-H 0h 30h 60h 90h H-flagscan-01-A 0h 30h 60h 90h H-flagscan-05-A 0h 30h 60h 90h H-flagscan-09-A 0h 30h 60h 90h A-flagscan-01-A 0h 30h 60h 90h A-flagscan-05-A 0h 30h 60h 90h A-flagscan-09-A 0h 30h 60h 90h M-flagscan-01-A 0h 30h 60h 90h M-flagscan-05-A 0h 30h 60h 90h M-flagscan-09-A 0h 30h 60h 90h M-flagscan-13-A 0h 30h 60h 90h H-flagscan-01-M 0h 30h 60h 90h H-flagscan-05-M 0h 30h 60h 90h H-flagscan-09-M 0h 30h 60h 90h A-flagscan-01-M 0h 30h 60h 90h A-flagscan-05-M 0h 30h 60h 90h A-flagscan-09-M 0h 30h 60h 90h M-flagscan-01-M 0h 30h 60h 90h M-flagscan-05-M 0h 30h 60h 90h M-flagscan-09-M 0h 30h 60h 90h M-flagscan-13-M 0h 30h 60h 90h H-flagscan-01-3 0h 30h 60h 90h H-flagscan-05-3 0h 30h 60h 90h H-flagscan-09-3 0h 30h 60h 90h A-flagscan-01-3 0h 30h 60h 90h A-flagscan-05-3 0h 30h 60h 90h A-flagscan-09-3 0h 30h 60h 90h M-flagscan-01-3 0h 30h 60h 90h M-flagscan-05-3 0h 30h 60h 90h M-flagscan-09-3 M-flagscan-13-3 0h 30h 60h 90h H-flagscan-02-H 0h 30h 60h 90h H-flagscan-06-H 0h 30h 60h 90h H-flagscan-10-H 0h 30h 60h 90h A-flagscan-02-H 0h 30h 60h 90h A-flagscan-06-H 0h 30h 60h 90h A-flagscan-10-H 0h 30h 60h 90h M-flagscan-02-H 0h 30h 60h 90h M-flagscan-06-H 0h 30h 60h 90h M-flagscan-10-H 0h 30h 60h 90h H-flagscan-02-A 0h 30h 60h 90h H-flagscan-06-A 0h 30h 60h 90h H-flagscan-10-A 0h 30h 60h 90h A-flagscan-02-A 0h 30h 60h 90h A-flagscan-06-A 0h 30h 60h 90h A-flagscan-10-A 0h 30h 60h 90h M-flagscan-02-A 0h 30h 60h 90h M-flagscan-06-A 0h 30h 60h 90h M-flagscan-10-A 0h 30h 60h 90h H-flagscan-02-M 0h 30h 60h 90h H-flagscan-06-M 0h 30h 60h 90h H-flagscan-10-M 0h 30h 60h 90h A-flagscan-02-M 0h 30h 60h 90h A-flagscan-06-M 0h 30h 60h 90h A-flagscan-10-M 0h 30h 60h 90h M-flagscan-02-M 0h 30h 60h 90h M-flagscan-06-M 0h 30h 60h 90h M-flagscan-10-M 0h 30h 60h 90h H-flagscan-02-3 0h 30h 60h 90h H-flagscan-06-3 0h 30h 60h 90h H-flagscan-10-3 0h 30h 60h 90h A-flagscan-02-3 0h 30h 60h 90h A-flagscan-06-3 0h 30h 60h 90h A-flagscan-10-3 0h 30h 60h 90h M-flagscan-02-3 0h 30h 60h 90h M-flagscan-06-3 0h 30h 60h 90h</p>
        <p>Pattern recognitionPattern recognition</p>
        <p>Counts of shapes were done manually by each of the four co-authors. Images were blinded by having a script assign random numbers to each image, and using PNG image files (with no AFM metadata) named using these numbers. CGE had previously seen all images associated with their corresponding samples while performing the AFM imaging. Samples in the tables below are referred to as 'H', 'A', or 'M' for samples with four fluorophores on one shape, and '3' for samples where each shape had one fluorophore (resulting in three total fluorophores). Each imaged sample (with a different fluorophore configuration) of each pattern had three 5 µm × 5 µm images, with exception of the H, A, and M samples for Magnolia, which had two. Owing to the significantly larger numbers of shapes seen in the SHAM mix, only 1/4 of each image was used for counting. Numbers in the tables below are averaged across the counts from each co-author, and normalized to the number of shapes per µm 2 .Counts of shapes were done manually by each of the four co-authors. Images were blinded by having a script assign random numbers to each image, and using PNG image files (with no AFM metadata) named using these numbers. CGE had previously seen all images associated with their corresponding samples while performing the AFM imaging. Samples in the tables below are referred to as 'H', 'A', or 'M' for samples with four fluorophores on one shape, and '3' for samples where each shape had one fluorophore (resulting in three total fluorophores). Each imaged sample (with a different fluorophore configuration) of each pattern had three 5 µm × 5 µm images, with exception of the H, A, and M samples for Magnolia, which had two. Owing to the significantly larger numbers of shapes seen in the SHAM mix, only 1/4 of each image was used for counting. Numbers in the tables below are averaged across the counts from each co-author, and normalized to the number of shapes per µm 2 .</p>
        <p>To examine the potential effect of fluorophore and quencher tile modifications on shapes seen by AFM, several patterns had multiple samples imaged, while the '3' sample was imaged for every pattern. In cases where the concentration pattern did not result in extremely selective nucleation, so that non-target shapes could be counted and smaller effects on nucleation rate could be discerned, shape counts show a bias toward nucleating shapes with more fluorophore-quencher pairs. This effect is particularly evident for the equal-concentration SHAM mix: with one fluorophore-quencher pair on each shape (the '3' sample), counts of each shape are roughly equal, but when four fluorophore-quencher pairs are instead located on a single shape, either 'H' or 'A', that shape rises to over 1/2 of the observed shapes. Both the purification of flourophore-and quencher-modified strands resulting in a higher effective concentration compared to unpurified, unmodified strands, and fluorophore-quencher interactions, may play a role in the effect.To examine the potential effect of fluorophore and quencher tile modifications on shapes seen by AFM, several patterns had multiple samples imaged, while the '3' sample was imaged for every pattern. In cases where the concentration pattern did not result in extremely selective nucleation, so that non-target shapes could be counted and smaller effects on nucleation rate could be discerned, shape counts show a bias toward nucleating shapes with more fluorophore-quencher pairs. This effect is particularly evident for the equal-concentration SHAM mix: with one fluorophore-quencher pair on each shape (the '3' sample), counts of each shape are roughly equal, but when four fluorophore-quencher pairs are instead located on a single shape, either 'H' or 'A', that shape rises to over 1/2 of the observed shapes. Both the purification of flourophore-and quencher-modified strands resulting in a higher effective concentration compared to unpurified, unmodified strands, and fluorophore-quencher interactions, may play a role in the effect.</p>
        <p>Shapes were classified into categories 1 and 2, where 1 were "almost complete" shapes, possibly missing small portions, while 2 were "clearly recognizable" shapes, which were distinctly not complete, but still clearly distinguishable. Detailed criteria distinguishing these categories were not developed: instead, each co-author interpreted the categories independently.Shapes were classified into categories 1 and 2, where 1 were "almost complete" shapes, possibly missing small portions, while 2 were "clearly recognizable" shapes, which were distinctly not complete, but still clearly distinguishable. Detailed criteria distinguishing these categories were not developed: instead, each co-author interpreted the categories independently.</p>
    </text>
</tei>

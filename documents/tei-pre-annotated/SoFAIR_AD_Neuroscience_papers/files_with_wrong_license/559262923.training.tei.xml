<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T16:26+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>User emotional status recognition is becoming a key feature in advanced Human Computer Interfaces (HCI). A key source of emotional information is the spoken expression, which may be part of the interaction between the human and the machine. Speech emotion recognition (SER) is a very active area of research that involves the application of current machine learning and neural networks tools. This ongoing review covers recent and classical approaches to SER reported in the literature.</p>
        <p>Inside the broad field of Automated Emotion Recognition (AER) [83], Speech Emotion Recognition (SER) may be considered a branch of Automatic Speech Recognition (ASR) [30,101,142,152] exploiting the same kind of signal, feature extraction processes, and potential application of diverse machine learning techniques, such as deep learning (DL) architectures, that are also applied in the field of Natural Language Processing (NLP) [61] that shares with SER the sequential nature of the data. Works already developed for ASR on feature extraction, such as the exploitation of Mel Frequency Cepstral Coefficients (MFCC) [27] for classification and pattern recognition, have been seamlessly transferred into SER research. Also, DL approaches developed in ASR or NLP, like several flavors of convolutional neural networks (CNN) and recurrent neural networks (RNN),are evaluated for SER practical applications. In the future, SER is called to be a key technology in the development of innovative Human Computer Interaction (HCI), Human Machine Interaction (HMI), Human Robot Interaction (HRI) systems [129], and affective computing in general [146] which are becoming more important in the upcoming era of the Internet of Things (IoT) when pervasive ambient intelligence will be in a permanent dialog with the human user. Other kind of applications of SER are in the psychological domain, such as the detection of emotional valences in speech for automated depression detection [77].</p>
        <p>Most common data sources for AER are physiological data from wearable sensors, electroencephalographic data (EEG) [20], facial imaging, and speech based data. Though some researchers work on multimodal data fusion [146], most current research is focused on a single data modality. The fusion of imaging and voice data is the multimodal approach most often found in the literature.</p>
        <p>The increasing activity on SER research is reflected in recent reviews [5,4,14,39,65,121,122,134,147], some of them with special focus on DL approaches [1,74]. This paper contributes some updated information relative to these recent reviews, which are usually structured along three axes: the available data repositories, the feature extraction processes, and the classification and pattern recognition approaches. However, DL approaches usually encompass both the feature extraction and classification phases. We follow this typical structure in this paper. The structure of the paper is as follows: Section 2 reviews the most cited data repositories for SER. Section 3 collects conventional machine learning approaches for SER. Section 4 collects recent DL approaches for SER. We provide a table with the spelling of the most used acronyms in Appendix A.</p>
        <p>Some general works on emotion recognition provide an exhaustive enumeration of existing dataset for SER. For instance, Pitterman et al. [105] enumerate and briefly describe over 100 emotion databases, while Swain et al. [134] describes in depth about 60 emotion databases. Additionally, Douglas-Cowie et al. [33] offer some guidelines on how to compile appropriate datasets for SER according to their experience in similar projects. Dataset construction needs to takeinto account factors such as the recording prompts, the speaker selection procedure, the recording setup and the quality control [15]. In this section, we briefly describe the most cited datasets, which are summarized in Table 1. Table 2 summarizes the reference works that use each dataset. DES (Danish Emotional Speech) [36,37] is a Danish emotional database funded by the European Union. It contains recordings from 4 professional actors (2 male and 2 female and comprises about 30 min of speech. The recordings consist of two isolated words (''yes" and ''no"), nine short phrases (four of which are questions) and two passages. Each utterance was spoken emulating each of the following five emotional states: neutral, surprise, happiness, sadness, and anger. It has been one of the most used databases for researchers in this field. Its application range from testing SVM approaches. alone [23] or in combination with hidden markov models [76]. It has been also applied to gender based emotion recognition [67,139], and to novel deep learning approaches [53] and cross-corpus validation [62]. EMODB [18] is a German database with high-quality audio recordings from 10 actors (5 male and 5 female) whom produce 10 German utterances (5 short and 5 longer sentences) with 7 emotions (anger, neutral, anger, boredom, happiness, sadness, disgust). Some emotional expressions have two versions recorded by the same author. Thus, the database provides about 800 sentences. Everyday sentences were used in order to achieve a more natural form. Moreover, actors can speak them from memory without need of memorizing or reading them off a paper. Two examples of sentences are ''Tonight I could tell him" (short) and ''What are the bags standing there under the table?" (long). It is one of the most commonly referred in the technical literature. The works include bioinspired real time speech emotion recognition [81], conventional machine learning classifiers that used features as MFCC (Mel Frequency Cepstral Coefficients) [28,41,51,54,93,100,141,140,158,86,167], LPCC (Linear Prediction Cepstral Coefficient) [40,63,64,67,98,100,116,123,133] and other features [10,11,21,60,62,82,110,114,125,132,144,149,154,161,165,137] as well as deep learning based approaches [9,12,53,59,72,75,91,113,124,130,151,147,45,13,102,119]. eNTERFACE 89 is an audio-visual emotion database. It contains six emotions (happiness, sadness, surprise, anger, disgust and fear) expressed by 42 subjects from 14 different nationalities (81% male and 19% female) as reaction to six successive short stories, each of them eliciting a particular emotion. All the experiments were driven in English. In order to avoid ambiguities, two human experts discarded the samples in which the emotion was not well expressed. It has been used for testing conventional machine learning approaches [62,98,125,25] as well as deep learning models [145]. IEMOCAP (Interactive Emotional Dyadic Motion Capture Database) [19] is a database collected by the Speech Analysis and Interpretation Laboratory at the University of South California. It was recorded from ten actors in dyadic sessions with markers on face, head and hands during scripted and improvised scenarios. It contains approximately 12 h of data. The audio which is split into segments between 3-15 s and labeled by 3-4 human evaluators. Although [18] [ 12,59,60,100,110,40,161,141,140,149,82,132,11,51,63,137,167,86,133,98,123,81,154,165,56,10,54,158,93,125,21,41,116,72,124,144,64,28,62,9,53,91,113,75,130,151,147,102,13,45,119] eNTERFACE [89] [ 62,125,98,63,25,145] IEMOCAP [19] [ 13,164,137,95,56,88,28,26,51,138,8,9,70,91,151,163,50,143,73,45,96,32,166,17,102] SAVEE [47] [ 53,72,66,55,100,25,28,143,92,84] Thai DB [128] [116] INTER1SP [87] [38,64] TESS [34] [107,124,68,43] RAVDESS [79] [ 124,55,40,126,68,157,155,118,90,56,104,52,106,156,92,117,7,6,94,43,131,84] JL-Corpus [57] no Refs. MSP-PODCAST [80] [8] VAM [44] [140]</p>
        <p>the database was initially designed to target anger, sadness, happiness, frustration and neural state, it was expanded to 10 categories (the basic emotions according psychologists [35] anger, sadness, happiness, disgust, fear, and surprise, plus frustration, excited, neutral and other). It is a well-known database that has been studied with several classic machine learning techniques [26,28,51,88,95,137]. Recently it has been also used to experiment with modern convolutional neural networks models [9,91,163,164,96,70,73,17,166,151,45,13,102,50,143] and as part of multimodal studies [32]. 
            <rs type="software">SAVEE</rs> (Survey Audio-Visual Expressed Emotion) [47] contains video and audio recordings of 4 English male actors in 7 emotions (neutral, anger, disgust, fear, happiness, sadness and surprise). Each actor played 120 utterances, which makes 480 sentences in total. For the visual features, 60 markers were painted on the actors' faces. The recordings consist of 15 phonetically-balanced sentences per emotion (3 common sentences, 2 emotion specific sentences and 10 generic sentences). It has been used to test wavelet based feature extraction methods [66,100], deep learning approaches [53,84], gradient boosting classifiers [55], and multisource information fusion [72,25,92,143].
        </p>
        <p>Thai DB (Audiovisual Thai Emotion Database) [128] is composed by recordings of six basic emotions: happiness, sadness, surprise, anger, fear and disgust. The utterances were spoken by six drama-students. They were asked to read 1,000 most commonly used Thai words with one to seven syllables. Recordings where human listeners were not able to recognize the emotion were deleted. The final list has 972 words. Frequently it is referred as one of the easier datasets to achieve good performances upon, because of it only contains isolated words rather than longer passages as other databases. However, the use of isolated words is a common practice when compiling this kind of databases. It has been used to test conventional machine learning techniques, such as SVM, to classify the emotions [116].</p>
        <p>INTER1SP [87] is composed by recordings of one male and one female Spanish professional actors. It contains 184 utterances that include isolated words, digits and sentences. It is about 4 h of data from each speaker and contains 6,040 samples. The emotions anger, disgust, fear, happiness, sadness, surprise and neutral are considered. It was created within the scope of an European project. It is used as dataset for both conventional machine learning [63,64] and deep learning [38] approaches. TESS (Toronto Emotional Speech Set) [34] includes about 2,800 samples of 200 isolated words by two professional actresses. Seven emotions are considered: anger, disgust, fear, happiness, pleasant surprise, sadness and neutral. It is frequently used for testing a kind of deep learning models [124,107,43] although it is also used with conventional machine learning methods [68]. 
            <rs type="software">RAVDESS</rs> (Ryerson Audio-Visual Database of Emotional Speech and Song) [79] is composed by 7356 audio and video clips (roughly 25 GB). It provides samples of speech and songs, thus it also allows to train models for the analysis of musical recordings. It contains 1440 samples of speech audio recordings, which are recorded by 24 professional actors (12 male and 12 female) that read two semantically neutral US English phrases while revealing eight emotions (neutral, calm, happiness, sadness, anger, fear, disgust, surprise).
        </p>
        <p>The phrases are ''Kids are talking by the door" and ''Dogs are sitting by the door" and they were selected according the length in syllables and word frequency and familiarity. Two levels of emotional intensity are considered in each phrase (only one intensity in the neutral emotional state) and there is just one sample for each recording. The audio files are provided as lossless wave format at 48 kHz to avoid the usual artifacts and other alterations of the original signal for the compression. Each one is about 3 s long. As it is one of the most complete databases, it has been extensively used to test conventional machine learning approaches, including SVM, gradient boosting and the hybridization of feature extraction methods base on wavelets and spectral features [40,55,90,118,155,157,68,126] as well as shallow neural networks [124], and deep learning approaches including the classical CNNs and LSTMs [56,104,52,58,106,156,7,6,94,43,131,84,117,92]. [58] is a database specifically adapted for the New Zealand English that include 5 primary and 5 secondary emotions. The latter are important in the Human-Robot Interaction (HRI) domain and they are not usually covered by other databases. Besides the original proposal, we found no references of its exploitation to test machine learning approaches. [87] is an approach to effectively build a large, naturalistic emotional database with balanced emotional content. It relies on existing spontaneous recordings obtained from audio-sharing websites published under public licenses. It provides more than 18,000 natural emotional sentences, over 27 h, from multiple speakers with audio segments between 2.75-11 s. Then, a group of almost 300 evaluators annotated the samples with emotional attributes (arousal, valence, dominance) and an extended list of categorical emotions. Only the labeling with emotional attributes is balanced. It has been used to test a listener dependent approach to apply deep learning architectures to SER [8].</p>
        <p>Table 3 contains the map of the found publications relative to the feature extraction process and the classification method.It is undeniable that SVM are the most cited conventional machine learning approach in the SER domain. Gao et al. [40] use a linear kernel SVM with sequential minimal optimization emotions classification. They compute pitch, intensity, MFCC (mel frequency cepstral coefficients), LSP (line spectral pairs) and ZCR (zero crossing rate) as local features of windows in the range from 20-100 ms on the audio files. They also use a depth first search to obtain values for the duration and overlapping of windows. Then, they apply a smoothing and normalization and obtain some statistics of all speech local features extracted from each utterance, which are used as global features. Dahake et al. [27] compare the use of several kernel functions in SVM to classify emotions utterances in an in-housedatabase. They use MFCC and energy and pitch based features. The study is carried out by classifying each emotion with a different kernel, so the results can not be easily generalized. However, the quadratic kernels appear to outperform other alternatives. Milton et al. [93] apply SVM classifiers with linear and RBF kernels and MFCC features to recognize emotions. They improve the state-of-art with their proposal of using a 3-stage SVM. Sinith et al. [123] use SVM and a combination of MFCC, pitch and energy based features to classify speech emotions. Yang et al. [150] propose the use of Twins SVM and compare its performance against standard SVM for speech emotion recognition. They use a set of featured computed in time-and frequency-domains. MFCC are also considered in a second experiment, where they report an increased performance.</p>
        <p>Zhu-Zhou et al. [167] define a speech-based emotion recognition system that uses MFCCs as features while different realworld scenarios with noise and reverberation are simulated. Mariooryard and Busso [88] propose an emotion recognizing system based on SVM with linear kernel. They put the emphasis on factorizing the speaker characteristics, verbal content and expressive behaviors by applying a metric to quantify the dependency between acoustic features and communication traits. Chen et al. [24] compares two different classifiers such as SVM and MLP to recognize speech emotion. They extract several energy and spectral based instantaneous features from the utterances. Apart from these features, the first and second derivatives are also computed as well as a number of statistics of the instantaneous features. They start with 288 candidate features, which are reduced by applying two different methods (LDA and PCA). Thus, they propose four different models by combining the classifiers and dimensionality reduction methods. Generally, they achieve better global performance with LDA + SVM based methods. Sun et al. [132] propose the use of Hu moments as features to detect emotions in speech and compare the results with other usual features such as MFCC and LPCC as unique set of features or as acombination with others. They use a SVM with polynomial kernel. Liu et al. [78] use an ELM decision tree to classify emotions samples from which a set of features has been extracted and selected by applying LDA and correlation analysis. The experimental results are also compared to other conventional classifiers as SVM, MLP and kNN. Akash et al. [3] use three different classic machine learning methods (SVM, MLP and Bayes networks) to classify emotions. They use a combination of features extracted from the original waveform and its spectrogram. Kishore and Satish [66] compare the use of MFCC and wavelet features using GMM classifiers to classify emotions reporting that the wavelet features outperforms the MFCC. Garg et al. [41] employ a hierarchical decision tree method with SVM, BLG and SVR classifiers to recognize emotions. They use 16 lowlevel descriptors, which cover prosodic, spectral and voice quality features as, for example, MFCC and ZCR, reporting results comparable to the state-of-the-art. Zhang et al. [158] study the influence of several features on the accuracy of speech emotion recognition. The MFCC and ACFC techniques are compared with GMM. Neiberg et al. [97] use GMM to compare three different features sets: standard MFCC, MFCC-low, which are calculated between 20 and 300 HZ to model pitch, and plain pitch features. They use two different databases in English and Swedish languages. They report both MFCC variants are comparable and outperform the pitch features. Seehapoch and Wongthanavasu [116] recognize and classify the speech emotion. They propose energy, F0, ZCR, LPC and MFCC as features and SVM as classifiers. The experimental results offer an better accuracy than state-of-art when a combination of F0, MFCC and energy-based features are used. Daneshfar et al. [28] propose a classifier based on a neural network. The features are extracted from each frame taking into account both spectral (i.e., MFCC) and prosodic (i.e., F0) approaches. Then, the dimensionality is reduced by means of a novel particle swarm optimization (PSO) based method. Li and Akagi [72] present a schema for multilingual speech emotion recognition, in which they emphasize the importance of feature selection. Logistic model trees (LMT) are used to recognize the emotion categories, reporting an improvement over other previously tested techniques such as ANFIS [71]. Kerkeni et al. [64] propose a global approach for speech emotion recognition based on a method to select an optimal combination of features. They use SVM and RNN as classifiers. Wang et al. [144] compare the performance improvement by using wavelet packet coefficient (WPC) over the conventional MFCC to extract features from emotion utterances. They use SVM classifiers with linear and radial basis function kernels. Zhang et al. [161], compare classic classifiers such as kNN and SVM against a kernel isometric mapping-based classifier to recognize emotions. Rieger et al. [111] propose the use of MFCC and other spectral-based features and a ensemble of kNN classifiers to determine the emotion in speech recordings. Abdel-Hamid [2] applies several conventional techniques for feature extraction (prosodic-based, MFCC and wavelet) over a Egyptian Arabic to recognize emotions on speech database. Both SVM and kNN classifiers are used. Chavan and Gohokar [23] propose SVM classifiers with linear, polynomial, RBF and sigmoid kernels to recognize emotion in speech with MFCC and other features (periodicity histogram and fluctuation pattern). Kerkeni et al. [63] also compare several machine learning classifiers for emotion detection and recognition, namely: multivariate linear regression (MLR), SVM and RNN, while Zamil et al. [155] use a logistic model tree (LMT) classifier and MFCCs as features (the related ones to spectral details of excitation and periodicity of the wave). The signal extracted features are the MFCCs and MS computed with and without speaker normalization. Zhao et al. [165] present an approach of robust emotion recognition in noisy environment by using a weighted sparse representation based on the maximum likelihood estimation. They compare results with six conventional machine learning classifiers (kNN, DT, radial basis function MLP, SVM and sparse representation classifier). Matin and Valles [90] propose SVM and a combination of MFCC and ZCR to classify emotions to be applied in the context of autism treatment. The aim of their system is to aid children with ASD to recognize emotions. The model is used to train these children in such a way they are able to identify human emotions in oral conversations. Iliou and Anagnostopoulos [54] compare RF and MLP to classify emotions using MFCC as features. Xiao et al. [149] present a preliminary study on selection of features for speech emotion recognition. The features include statistics of F0, the first 3 formants and energy. Hou et al. [51] propose the use of non-negative matrix factorization (NMF) to reduce the dimensionality in feature space. They use a conventional kNN classifier. They report improvements over other dimensionality techniques.</p>
        <p>Sunitha and Ponnusamy [133] compare the results achieved by applying a SVM classifier and MFCC as features to the EMODB [18] and a Tamil language database created by themselves. The results achieved in both databases are comparable. Rajasekhar and Hota [108] use SVM and a combination of MFCC, pitch and amplitude as features to classify the samples of a database compiled by themselves with a reduced set of emotions. They put the emphasis on the preprocessing of the data in order to improve the recognizing results. Han and Wan [46] propose a speech emotion recognition model based on Proximal SVM to recognize a quite reduced set of emotions in a database recorded by themselves in Chinese language. The method improves the accuracy achieved by common SVM based approaches as well as the response time. Zhang [160] propose a fuzzy SVM to recognize emotions. The reported results improve other SVM classifiers and other classic machine learning methods as GMM and kNN. Atassi and Esposito [11] propose a model based on a combination of classic techniques (GMM and SFFS) to classify emotions. Lugger and Yang [82] propose a model based on GMM to classify emotions. They extract an initial set of 216 suprasegmental and segmental features, which is reduced by applying a SFFS algorithm. They report a detailed study on the relevance of the feature groups for classifying different emotion dimensions. Arias et al. [10] propose low level descriptors such as F0 contours to detect emotions. The model uses PCA for dimension reduction before the emotion classification. Song et al. [125] proposed a speech emotion recognition system based on the non-negative matrix factorization method, which is popular in computer vision and pattern recognition domains. Basically, they consider the discrepancies between source and target data to determine the class of a sample.</p>
        <p>Shegokar and Sircar [118] utilize a type of continuous wavelet transform (Morlet wavelet) as features, and quadratic SVM classifiers to recognize all the eight emotions. Rajisha et al. [109] present MLP and SVM classifiers over a Malayala language database. They use the popular MFCC and other energy based features. They report a good performance although the database contains a limited number of emotions. Mao et al. [86] propose a hybrid approach based on HMM and ANN, combining the dynamic time warping of HMM and the pattern recognition of ANN. The utterances are considered as a series of voiced segments, which are used to extract the feature vectors for the ANN. Lin and Wei [76] use two methods (HMM and SVM) to classify emotions. They find the best subset of instantaneous features by applying SFS and then the results are compared to the results when MFCC are used features. They compute several recognition rates while grouping the samples according to several criteria as for example the gender and they found male samples get a higher recognition rate than female or gender independent cases. Yun and Yoo [154] consider a method for speech emotion recognition that incorporates a loss function based on a the Watson and Tellegen emotion model [136]. Each emotion is modeled by a single HMM that maximizes the minimum separation margin between emotions. The margin is scaled by the loss function. Chenchah and Lachiri [26] present a system that uses HMM as classifier and a combination of classical features, such as MFCC, and other features more robust to noise and reverberation distortions, such as PNCC, to recognize emotions in real-life conditions. Yu [153] reports that results achieved by HMM outperform other classification techniques (kNN and LDA) in a Chinese Mandarin language database with a reduced number of emotions. Several features related to pitch, energy, and spectrum, such as first formants, LPCC and MFCC, are used. Vlasenko et al. [141] use MFCC plus speed and acceleration coefficients as features to classify emotions. They use several classifiers: GMM for basic emotion classification, HMM since multiple states are considered, and finally SVM when other low-level descriptors based on prosodic and articulatory aspects are also employed. Schuller et al. [115] apply GMM and continuous HMM to classify instances from two emotion databases in English and German languages. First they classify the utterances by using global statistics and the help of GMM, then the HMM models are applied over low-level instantaneous features. Schuller et al. [114] compare concepts for robust fusion of prosodic and verbal cues in speech emotion recognition. They use a bag-of-words representation for linguistic content analysis. Then, they proceed to an systematic feature selection by using a SVM-SFFS method. The classification is based on an ensemble approach, where base classifiers are SVM, DT, and NBC. The experiment are carried out over the EMODB [18] database.</p>
        <p>Kadiri et al. [60] present an interesting alternative to the features extraction problem. They use a different approach to the usually employed in the state-of-art by capturing the deviations between the emotional speech and the neutral, non-emotional one. They report that their method based on a hierarchical binary decision tree is comparable or better than the existing prosody and spectral features. Wu et al. [148] apply three types of classifiers (GMM, SVM and MLP) and a meta decision tree (MDT) to fusion the confidence results. They use acoustic-prosodic information and semantic labels as features. Although the number of emotion labels is quite limited they are able to validate the proposal about MDT by improving the results achieved by the classifiers separately. Sreenivasa Rao et al. [127] propose a GMM as classifier and MFCC as features to recognize emotion in speech. They evaluate the proposal for two databases in Telugu language, acted and real emotion speech, respectively. Mao et al. [85] combine the use of conventional machine learning techniques such as SVM and LDA to classify emotions in a Chinese database. The use of LDA improves the results when applied with all tested classifiers. Bozkurt et al. [16] use spectrally weighted MFCC as features to detect emotions by HMM based classifiers. The spectral weighting is derived from the normalized inverse harmonic mean of the line spectral frequency features. The proposal tries to obtain a data fusion of spectral content and formant location information. Kaya and Karpov [62] propose normalization strategies that apply on several corpus emotion recognition. An extreme learning machine (ELM) is used as classifier. They conclude that significant improvements can be achieved by applying these techniques.</p>
        <p>Iqbal and Barua [55] present a real-time recognition system based on Gradient Boosting (GB) and MFCC and other spectral features to classify emotions into the four basic emotions: anger, happiness, sadness and neutral. They report an accuracy equivalent to other works in the state-of-art. Dimitrova-Grekow and Konopko [31] use several machine learning algorithms to detect the emotions and they obtain the best result with random forest (RF). As features they use fundamental frequency series (FFS). Rong et al. [112] also use RF and other decision trees algorithms with MFCC as features to classify emotions in two Chinese databases with a reduced number of examples and emotions. Issa et al. [56] introduce a new architecture based on one-dimensional CNN for identification of emotions using MFCC and other spectral features for training. Caponetti et al. [22] propose the use of LSTM recurrent neural networks to recognize emotions. They use two different features: MFCC and biologically inspired Lyon cochlear model. They report that the latter give better recognition results in comparison to the MFCC features. Ramakrishnan and El Emary [110] present a wide range of features for SER and analyze their performance with the help of HMM and SVM classifiers. Palo and Mohanty [100] develop a reduced set of combinational features for speech emotion recognition. They validate the use of LPCC and MFCC derived from wavelets to train RFB neural networks reporting that their proposal outperforms state-of-art. Cao et al. [21] utilize a SVMbased system to recognize emotions in speech. Their proposal considers a ranking approach which incorporates information about the general expressivity of speakers.</p>
        <p>Lotjidereshgi and Gournay [81] propose a biologically inspired approach to classify the emotion. The method is based on liquid state machines (LSM), a classifier inspired on the spiking neural network (SNN) model giving accuracy rates comparable to the state-of-the-art. Ooi et al. [98] propose an architecture in which prosodic and spectral features are used to recognize the emotions. The prosodic features are used to differentiate emotions. The spectral features, particularly MFCC, are processed by applying PDA and LDA analysis and the results are used to train a RBF neural network. Origlia et al. [99] propose the use of phonetic syllables rather than other approaches based on prosody. They evaluate the model in the valence, activation and dominance space and show that it is competitive with other contemporary works in terms of performance. Vlasenko et al. [140] present a model to classify emotion based on HMM and MFCC features. They use two different databases to check out the proposal: while one is used to train the model, the other is used for testing. Some works reportingcomparisons of classifiers, like [149,46,148,68,126,40,157,155,55], reduce the number of emotions in order to report improved performances. Thirumuru et al. [137] propose a novel feature representation using single frequency filtering and nonlinear energy operator to be used with machine learning classifiers (SVM).</p>
        <p>Multimodal approaches benefit from the fusion of diverse kinds of signals. Chen et al. [25] use a K-means clustering-based method to improve the multimodal emotions classifying in HRI contexts. Zhang et al. [157] propose several models to recognize emotion in a multimodal context. In this case they consider speech but also song and video. They use a combination of low level descriptors that includes energy and spectral features, MFCCs and voicingrelated low level descriptors. Then, they calculate several statistics resulting in 1,170 features. A directed acyclic graph (DAG) SVM is used as classifier. Mower et al. [95] propose a multimodal framework based one emotion profiles and SVM.</p>
        <p>Fig. 1 shows a typical structure combining several deep learning architectures, i.e. CNNs and LSTM, processing as input the MFCC image and providing the emotion identification.Many early approaches, such as Huang et al. [53] used CNN to learn affective salient features for speech emotion recognition. Lim et al. [75] propose a method based on concatenated CNN and RNN without using any traditional hand-crafted features, i.e., a kind of spectrogram images. They report a better accuracy than they achieved using conventional methods at the time of publication. Wani et al. [147] also define a deep neural network with CNN layers for classification. Ando et al. [8] present a speech emotion recognition (SER) scheme to extend models generated from multiple listener data to listener-dependent (LD) emotion recognition models. They propose and compare three procedures to adapt deep neural networks with CNN and LSTM layers, where the inputs are acoustic features, and the output is the emotion class. Meng et al. [91] and Zhao et al. [164] also combine CNN with LSTM to introduce a novel architecture for emotion detection. They compute the spectrograms from audio recordings and pick up the values of the 3D Log-Mel spectrum to feed their model. Pandey et al. [102] propose yet another deep neural network with convolutional layers and LSTM to recognize emotions.</p>
        <p>Fuentes et al. [38] propose the use of a culturally adapted CNN model to recognize speech emotions to be applied as input for a recommendation system. They combine of MFCC and LFE to create an image-based feature dataset. Anvarjon et al. [9] suggest a convolutional neural network (CNN) fed with spectrograms generated from the audio recordings. They propose a model composed by three blocks with several convolutional layers followed by a pooling layer to extract the features plus two additional dense layers and a softmax layer as output. Jiang et al. [59] propose the use of parallelized convolutional recurrent neural networks for emotion recognition. First, they extract features from each utterance, which are learned by using a long short-term memory. Also, they compute the Mel spectrograms and use a CNN to extract features for the images. These two sets of high-level features are learned and normalized and injected over a softmax classifier to determine the emotion in the utterance. Zhang et al. [159] propose a model of multi-CNN to detect emotion in utterances: a 1D CNN for the raw audio recording, a 2D CNN for the Mel-spectrogram generated from the original waveform, and a 3D CNN for temporal-spatial dynamic. The outputs are integrated by an average-pooling layer to produce the classification result. The experiments are performed on Chinese language databases and lightly improved other stateof-art solutions. De Pinto et al. [104] propose a CNN model based on 1D convolutional layers to classify emotions. Shahin et al. [117] compare the results of their model based on CNN and LSTM to the results achieved with conventional classifiers in a wide range of corpora. Nagase et al. [96] apply a deep neural network composed by convolutional and LSTM layers for emotion recognition. They propose label smoothing in order to reduce the overfitting produced by mislabeled information. They use the IEMOCAP [19] and a Japanase dataset. Andayani et al. [7,6] propose a neural architecture based on LSTM and transformers. Mocanu et al. [94] propose a 2D CNN with deep metric learning to emotion recognition. Gokilavani et al. [43] propose a model with five CNN layers.</p>
        <p>They also apply some techniques for data augmentation such as adding noise to the original audio signals. Sultana et al. [131] present a system based on CNN and LSTM networks to classify emotions. They use the RAVDESS database [79] and a second dataset in Bangla language. They use transfer learning between both datasets, i.e. they train on RADVESS and test on the Bangla dataset. Guo et al. [45] use the phase information as well as the usual magnitude information in data to improve the emotion recognizing results.</p>
        <p>Slimi et al. [124] propose the use of a DL network to be used with small-sized databases. The method does not apply conventional data augmentation techniques. Firstly, the spectrogram associated to each recording is generated by using conventional techniques, and resized while preserving the aspect ratio. They report that the bigger the final image dimensions, the higher the accuracy, although it also implies greater computational complexity, and increasing thenumber of neural network parameters. The neural network consists of just one hidden layer and they report better accuracy than the state of the art.</p>
        <p>Praseetha and Vadivel [107] propose a deep learning and recurrent neural networks to classify emotions. They feed the networks with features based on MFCC and spectrograms generated from the audio recordings. The DL networks is composed by two hidden layers apart from the input and output ones. All of them are fully connected. The recurrent model is based in a set of gated recurrent units (GRU). They report better accuracy with their proposal than the state-of-art. Hasan and Islam [48] use a recurrent neural network (RNN) to categorize emotions in Bengali speech. The features are based on MFCC and MS and they try to recognize six emotions: happiness, sadness, anger, surprise, fear and disgust. They report quite discrete accuracy in these preliminary works on a new dataset.</p>
        <p>Many DL approaches to SER rely on transfer learning of well known DL architectures, often applied to spectrum images. Stolar et al. [130] and Badshah et al. [12] present methods based on CNN applied to spectrogram images. Their transfer learning approach uses a pre-trained AlexNet model [69]. Huang and Bao [52] use an AlexNet [69] deep learning architecture with MFCCs as features. They justify the application of 2D convolutional layers with such features because they combine the coefficient information in an axis and its value in the other axis. Zhang et al. [163] consider transfer learning because the limited data. They propose an attention mechanism based on a fully convolutional network that detects which time-frequency region of a speech spectrogram is more relevant for the emotion detection. Gerczuk et al. [42] use a transfer learning approach with a multi-corpus database composed by 26 free available corpora. The final dataset (EmoSet) contains 84,181 multi-lingual audio recordings and it has a duration over 65 h. They use several convolutional neural network architectures and spectrogram generated from the original audio recordings. Popova et al. [106] propose a DL transfer approach based on a VGG-16 [120] and mel spectrograms as features. Wang et al. [143] propose a deep neural network approach based on VGG architectures [120] with bidirectional LSTM.</p>
        <p>Tripathi et al. [138] propose a ResNet [49] based neural network on speech features and trained under focal loss to recognize emotion in speech. Focal loss reshapes the cross-entropy loss function by giving less importance during training to easy examples, usually the majority of the dataset, and focusing more on the hard ones. The method tries to improve the accuracy when there exists a significant class imbalance among various classes. Li et al. [73] also apply ResNet-like deep neural networks [49] to classify emotions in the IEMOCAP [19] database.</p>
        <p>Park et al. [103] present a data augmentation method for speech recognition that is applied directly to the feature inputs of neural networks. It consists of warping the features, masking blocks of frequency channels and masking blocks of time steps. They report the method improves the performance of the tested networks and they are able to obtain state of art results by augmenting the training set using these policies even without the aid of language models. Yi et al. [151] use a DL network approach with a generative adversial network for data augmentation. Shilandari et al. [119] and Latif et al. [70] also propose the use of GAN for data augmentation. Bakhshi et al. [13] generate textured images from the audio recordings to be used by CNN for emotion classifying.</p>
        <p>Zeng et al. [156] propose a deep learning approach based on a ResNet [49] architecture extended with a gated mechanism similar to the used in LSTM. They use spectrograms generated from the audio files as features. Jannat et al. [58] use an Inception-v3 [135] deep learning architecture in a multimodal approach. In this preliminary work they consider only two emotions (happiness and sadness). They report audio-only performance with an accuracy of 66.41% (cross-validation). They use plots of the raw audio signal as features.</p>
        <p>Sanchez-Gutierrez and Gonzalez-Perez [113] apply several discriminative measures (i.e, Anova, Fisher score, etc.) to identify useful neural nodes in deep learning networks in order to prune them and to diminish the resulting error rate. The analysis is performed on several datasets as for example the EMODB [18] speech emotion database. Manohar and Logashanmugam [84] propose a feature selection method to increase the performance in deep neural networks for emotion classification.</p>
        <p>Wang et al. [145] present a multimodal system to recognize emotions from images and audio recordings. Both subsystems are based on deep learning models. In particular the audio subsystem uses CNN and LSTM networks, which are fed with spectrogram images generated from the recordings. Heredia et al. [50] propose a multimodal (video, audio, and text) DL architecture to detect emotions in social robots. Middya et al. [92] propose another multimodal deep learning approach for emotion classification. Dong et al. [32] follow yet another multimodal approach based on deep learning to detect emotions (audio and video). Braunschweiler et al. [17] propose a model to classify emotions in a multimodal context (audio and text). Zhou et al. [166] propose a method based on multi-classifier interactive learning to improve the classification accuracy.</p>
        <p>Nowadays, one of the pillars of science is the availability of data to sustain new developments and the comparison among predictive models.In the field of SER there are many local small datasets. A few datasets are larger, but still not very extensive for the upcoming needs. Some of the most recent datasets have not been exploited, while only two of the older datasets have been exploited reaching conclusions that may become obsolete as new datasets are proposed. In some respects the field of SER is becoming mature regarding the kind of signal features that are more relevant, and the pervasive application of DL architectures is already flourishing in SER literature. However, these approaches need further validation and analysis of their sensitivity to data sampling and the assessment of overfitting to the actual dataset used for validation.</p>
        <p>The most inmediate road for future work is the joint exploitation of available corpora, such as reviewed in [162]. These works would allow to overcome the shortage of data and learm how to build more general recognizers, that are able to recognice emotions across the diversity of idiosyncratic features of the various databases. Additionally, the field of deep learning is bringing new architectures and features to light at high speed, hence it is to be expercted that a number of big jumps in performance will be appearing in the near future, because of the high value of emotional interaction with the users.</p>
        <p>This work has been carried out with the support of project PID2020-116346GB-I00 funded by the Spanish MICIN.</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </text>
</tei>

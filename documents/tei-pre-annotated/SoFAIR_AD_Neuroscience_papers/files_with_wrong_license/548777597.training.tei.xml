<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:07+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>For a brain-computer interface (BCI) system, a calibration procedure is required for each individual user before he/she can use the BCI. This procedure requires approximately 20-30 min to collect enough data to build a reliable decoder. It is, therefore, an interesting topic to build a calibration-free, or subject-independent, BCI. In this article, we construct a large motor imagery (MI)-based electroencephalography (EEG) database and propose a subject-independent framework based on deep convolutional neural networks (CNNs). The database is composed of 54 subjects performing the left-and right-hand MI on two different days, resulting in 21 600 trials for the MI task. In our framework, we formulated the discriminative feature representation as a combination of the spectral-spatial input embedding the diversity of the EEG signals, as well as a feature representation learned from the CNN through a fusion technique that integrates a variety of discriminative brain signal patterns. To generate spectral-spatial inputs, we first consider the discriminative frequency bands in an information-theoretic observation model that measures the power of the features in two classes. From discriminative frequency bands, spectral-spatial inputs that include the unique characteristics of brain signal patterns are generated and then transformed into a covariance matrix as the input to the CNN. In the process of feature representations, spectral-spatial inputs are individually trained through the CNN and then combined by a concatenation fusion technique. In this article, we demonstrate that the classification accuracy of our subject-independent (or calibration-free) model outperforms that of subject-dependent models using various methods [common spatial pattern (CSP), common spatiospectral pattern (CSSP), filter bank CSP (FBCSP), and Bayesian spatiospectral filter optimization (BSSFO)].For a brain-computer interface (BCI) system, a calibration procedure is required for each individual user before he/she can use the BCI. This procedure requires approximately 20-30 min to collect enough data to build a reliable decoder. It is, therefore, an interesting topic to build a calibration-free, or subject-independent, BCI. In this article, we construct a large motor imagery (MI)-based electroencephalography (EEG) database and propose a subject-independent framework based on deep convolutional neural networks (CNNs). The database is composed of 54 subjects performing the left-and right-hand MI on two different days, resulting in 21 600 trials for the MI task. In our framework, we formulated the discriminative feature representation as a combination of the spectral-spatial input embedding the diversity of the EEG signals, as well as a feature representation learned from the CNN through a fusion technique that integrates a variety of discriminative brain signal patterns. To generate spectral-spatial inputs, we first consider the discriminative frequency bands in an information-theoretic observation model that measures the power of the features in two classes. From discriminative frequency bands, spectral-spatial inputs that include the unique characteristics of brain signal patterns are generated and then transformed into a covariance matrix as the input to the CNN. In the process of feature representations, spectral-spatial inputs are individually trained through the CNN and then combined by a concatenation fusion technique. In this article, we demonstrate that the classification accuracy of our subject-independent (or calibration-free) model outperforms that of subject-dependent models using various methods [common spatial pattern (CSP), common spatiospectral pattern (CSSP), filter bank CSP (FBCSP), and Bayesian spatiospectral filter optimization (BSSFO)].</p>
        <p>B RAIN-COMPUTER interface (BCI) is a system that enables a direct communication pathway between a human brain and external devices [1]. BCIs have shown great potential in a variety of clinical applications for communication, control, and rehabilitation [2]- [9]. Furthermore, recent BCI studies have attracted great attention as future technology for the next generation [10]- [15]. Over the course of numerous BCI studies [16]- [21], growing attention has been dedicated to the analysis of electroencephalography (EEG) signals, especially by movement imagination, called motor imagery (MI) [1], [2], [8], [12], [14], [16], [22]. This interest is due to the ability of MI to allow both healthy and disabled people to self-regulate brain signals without an external stimulus.B RAIN-COMPUTER interface (BCI) is a system that enables a direct communication pathway between a human brain and external devices [1]. BCIs have shown great potential in a variety of clinical applications for communication, control, and rehabilitation [2]- [9]. Furthermore, recent BCI studies have attracted great attention as future technology for the next generation [10]- [15]. Over the course of numerous BCI studies [16]- [21], growing attention has been dedicated to the analysis of electroencephalography (EEG) signals, especially by movement imagination, called motor imagery (MI) [1], [2], [8], [12], [14], [16], [22]. This interest is due to the ability of MI to allow both healthy and disabled people to self-regulate brain signals without an external stimulus.</p>
        <p>In BCI systems in general, a user's brain signals can change over minutes, hours, or days due to the differences in the physiological and psychological characteristics of each individual at each time [22]. Moreover, the spatial origin, the amplitude change, and the variability of the brain signals exhibit subjectspecific patterns [23]. If the brain signals are expected to change occasionally, then a decoding method to compensate for the changes in the brain signals is indispensable in the general use of BCIs. However, unfortunately, the process of calibration is an inconvenient and time-consuming task, which requires approximately 20-30 min for a reliable decoder to be built [24], [25]. In the current BCI research, the calibration process is one of the obstacles that prevents the practical use of BCI systems. In conclusion, this problem should be addressed to enable real-world applications of BCIs.In BCI systems in general, a user's brain signals can change over minutes, hours, or days due to the differences in the physiological and psychological characteristics of each individual at each time [22]. Moreover, the spatial origin, the amplitude change, and the variability of the brain signals exhibit subjectspecific patterns [23]. If the brain signals are expected to change occasionally, then a decoding method to compensate for the changes in the brain signals is indispensable in the general use of BCIs. However, unfortunately, the process of calibration is an inconvenient and time-consuming task, which requires approximately 20-30 min for a reliable decoder to be built [24], [25]. In the current BCI research, the calibration process is one of the obstacles that prevents the practical use of BCI systems. In conclusion, this problem should be addressed to enable real-world applications of BCIs.</p>
        <p>To this end, we construct a large-scale MI EEG database and propose a novel subject-independent framework based on spectral-spatial feature representation with deep convolutional neural networks (CNNs). The main contributions of this article can be summarized as follows.To this end, we construct a large-scale MI EEG database and propose a novel subject-independent framework based on spectral-spatial feature representation with deep convolutional neural networks (CNNs). The main contributions of this article can be summarized as follows.</p>
        <p>1) We built a large MI-based EEG database with 54 healthy subjects over two sessions. This database is the largest MI BCI dataset thus far ever reported in the literature, and it provides a sufficient number of training samples for deep learning (DL) architectures (CNNs). 2) We proposed a new discriminative spectral-spatial input to represent a diversity of brain signal patterns across the subjects and sessions. 3) We represent the MI-induced event-related desynchronization (ERD) patterns through the CNN model by applying a spatial fusion technique that combines spectral-spatial inputs from diverse frequency regions. The remainder of this article is organized as follows. Section II discusses the related work. In Section III, we briefly introduce our large-scale database. Section IV elaborates the proposed framework. Details of the experimental results and analysis are discussed in Sections V and VI. Finally, Section VII concludes this article.1) We built a large MI-based EEG database with 54 healthy subjects over two sessions. This database is the largest MI BCI dataset thus far ever reported in the literature, and it provides a sufficient number of training samples for deep learning (DL) architectures (CNNs). 2) We proposed a new discriminative spectral-spatial input to represent a diversity of brain signal patterns across the subjects and sessions. 3) We represent the MI-induced event-related desynchronization (ERD) patterns through the CNN model by applying a spatial fusion technique that combines spectral-spatial inputs from diverse frequency regions. The remainder of this article is organized as follows. Section II discusses the related work. In Section III, we briefly introduce our large-scale database. Section IV elaborates the proposed framework. Details of the experimental results and analysis are discussed in Sections V and VI. Finally, Section VII concludes this article.</p>
        <p>Most conventional MI-based BCI systems were developed based on subject-dependent methods that require calibration time [25]- [28]. Common spatial patterns (CSPs) are one of the most widely used methods in MI-based BCI; CSPs maximize the differences in the variances for the binary classes [23], [25]. Based on CSP methods, advanced algorithms have been proposed, for example, common spatiospectral pattern (CSSP) [29], filter bank CSP (FBCSP) [28], and Bayesian spatio-spectral filter optimization (BSSFO) [30]. The CSSP method is built on an extension of the CSP method with an embedded time delay [29]. The FBCSP method is based on composing a frequency range with nonoverlapping and static frequency bands (e.g., 4-8 Hz, 8-12 Hz,…) [28]. The BSSFO method constructs a data-driven discriminative filter bank and bandwidth selection within a Bayesian framework [30]. However, these methods still require the calibration procedure to train the decoder.Most conventional MI-based BCI systems were developed based on subject-dependent methods that require calibration time [25]- [28]. Common spatial patterns (CSPs) are one of the most widely used methods in MI-based BCI; CSPs maximize the differences in the variances for the binary classes [23], [25]. Based on CSP methods, advanced algorithms have been proposed, for example, common spatiospectral pattern (CSSP) [29], filter bank CSP (FBCSP) [28], and Bayesian spatio-spectral filter optimization (BSSFO) [30]. The CSSP method is built on an extension of the CSP method with an embedded time delay [29]. The FBCSP method is based on composing a frequency range with nonoverlapping and static frequency bands (e.g., 4-8 Hz, 8-12 Hz,…) [28]. The BSSFO method constructs a data-driven discriminative filter bank and bandwidth selection within a Bayesian framework [30]. However, these methods still require the calibration procedure to train the decoder.</p>
        <p>In recent years, various strategies to reduce the calibration time in MI-based BCIs have been proposed. The zero-calibration method aims to immediately use the BCI system without the calibration process for a naive BCI user (e.g., without using training data from new users) and is also called the subject-independent method [31]- [35]. The existing zero-calibration methods are usually built on a transferbased approach that uses preexisting data from other subjects. Ray et al. [31] established an MI database, extracted meaningful brain features based on the FBCSP method from a previously created database, and classified them using a linear support vector machine (SVM). Lotte et al. [32] used a public database that was composed of nine subjects and designed a user-independent model by considering multiresolution frequency decomposition to extract the EEG features, following the idea of the FBCSP method. Fazli et al. [33] proposed a zero-calibration ensemble method that extracted a temporalspatial filter and built an ensemble model on the basis of linear discriminant analysis (LDA). In addition, studies that utilize minimum calibration data have been conducted [34]- [39]. However, these recent zero-training strategies in the literature have primarily concentrated on the linear properties of the given training samples and endeavored to extract EEG features based on conventional subject-dependent methods. At present, in the field of BCI, there is a great interest in investigating machine learning technology and applying it to the analysis of EEG decoding. One prominent example of such advances is the utilization of DL, which relates to other technology trends, such as big data and artificial intelligence.In recent years, various strategies to reduce the calibration time in MI-based BCIs have been proposed. The zero-calibration method aims to immediately use the BCI system without the calibration process for a naive BCI user (e.g., without using training data from new users) and is also called the subject-independent method [31]- [35]. The existing zero-calibration methods are usually built on a transferbased approach that uses preexisting data from other subjects. Ray et al. [31] established an MI database, extracted meaningful brain features based on the FBCSP method from a previously created database, and classified them using a linear support vector machine (SVM). Lotte et al. [32] used a public database that was composed of nine subjects and designed a user-independent model by considering multiresolution frequency decomposition to extract the EEG features, following the idea of the FBCSP method. Fazli et al. [33] proposed a zero-calibration ensemble method that extracted a temporalspatial filter and built an ensemble model on the basis of linear discriminant analysis (LDA). In addition, studies that utilize minimum calibration data have been conducted [34]- [39]. However, these recent zero-training strategies in the literature have primarily concentrated on the linear properties of the given training samples and endeavored to extract EEG features based on conventional subject-dependent methods. At present, in the field of BCI, there is a great interest in investigating machine learning technology and applying it to the analysis of EEG decoding. One prominent example of such advances is the utilization of DL, which relates to other technology trends, such as big data and artificial intelligence.</p>
        <p>DL has been shown to be a great success in computer vision, natural language processing, and many other fields [40]- [46].DL has been shown to be a great success in computer vision, natural language processing, and many other fields [40]- [46].</p>
        <p>As DL methods enabled us to effectively exploit distributed and hierarchical features through multiple layers of nonlinear information processing [40]- [44], it is expected to produce higher performance than traditional classification methods (e.g., SVM) [45]- [47]. To date, DL has been applied to MI-based BCIs and contributed to performance improvements [48]- [51], but they are all still limited to subjectdependent BCI.As DL methods enabled us to effectively exploit distributed and hierarchical features through multiple layers of nonlinear information processing [40]- [44], it is expected to produce higher performance than traditional classification methods (e.g., SVM) [45]- [47]. To date, DL has been applied to MI-based BCIs and contributed to performance improvements [48]- [51], but they are all still limited to subjectdependent BCI.</p>
        <p>There are two key issues to be addressed in utilizing DL approaches to develop a subject-independent BCI. First, there is no large-scale MI database with a large number of subjects and sessions available. This is because establishing a large database for EEG-based BCI requires a tremendous effort. Second, there were scarce studies on subject-independent BCIs based on DL, which requires strategies for extracting discriminative brain features from a large-scale database. Conventional methods worked well in the subject-dependent case but fared poorly for the subject-independent case, because brain signals from different subjects are highly variable, discriminative, and semantic [22], [24].There are two key issues to be addressed in utilizing DL approaches to develop a subject-independent BCI. First, there is no large-scale MI database with a large number of subjects and sessions available. This is because establishing a large database for EEG-based BCI requires a tremendous effort. Second, there were scarce studies on subject-independent BCIs based on DL, which requires strategies for extracting discriminative brain features from a large-scale database. Conventional methods worked well in the subject-dependent case but fared poorly for the subject-independent case, because brain signals from different subjects are highly variable, discriminative, and semantic [22], [24].</p>
        <p>In this article, we propose a novel framework of spectralspatial feature representation using CNNs from a large-scale MI EEG database. To the best of our knowledge, this is the first study to apply a method that has a sufficient number of EEG training samples and a feature extraction method that is fit to subject-independent BCI based on DL.In this article, we propose a novel framework of spectralspatial feature representation using CNNs from a large-scale MI EEG database. To the best of our knowledge, this is the first study to apply a method that has a sufficient number of EEG training samples and a feature extraction method that is fit to subject-independent BCI based on DL.</p>
        <p>The main purpose of our MI database was to capture large variations between the subjects and the sessions. In the experiment, 54 healthy subjects (aged 24-35: 29 male and 25 female) participated in experiments of two sessions. Among them, 38 subjects were naive BCI users, and the others had previous BCI experiment. The interval between the sessions was between one and two weeks. None of the subjects had a history of neurological, psychiatric, or any other pertinent disease.The main purpose of our MI database was to capture large variations between the subjects and the sessions. In the experiment, 54 healthy subjects (aged 24-35: 29 male and 25 female) participated in experiments of two sessions. Among them, 38 subjects were naive BCI users, and the others had previous BCI experiment. The interval between the sessions was between one and two weeks. None of the subjects had a history of neurological, psychiatric, or any other pertinent disease.</p>
        <p>At the beginning of the experiment, the subjects were seated comfortably in a chair with armrests 60 (±5) cm from a 20-in LCD monitor (refresh rate: 60 Hz and resolution: 1600 × 1200). During the experiment, participants were asked to relax and minimize their eye and muscle movements. Additionally, we provided a monetary incentive for the participants, approximately 80 dollars for a single experiment. This study was approved by the Institutional Review Board of Korea University (approval 1040548-KU-IRB-16-159-A-2), and all subjects agreed to participate in this experiment with written consent.At the beginning of the experiment, the subjects were seated comfortably in a chair with armrests 60 (±5) cm from a 20-in LCD monitor (refresh rate: 60 Hz and resolution: 1600 × 1200). During the experiment, participants were asked to relax and minimize their eye and muscle movements. Additionally, we provided a monetary incentive for the participants, approximately 80 dollars for a single experiment. This study was approved by the Institutional Review Board of Korea University (approval 1040548-KU-IRB-16-159-A-2), and all subjects agreed to participate in this experiment with written consent.</p>
        <p>In this experiment, the EEG data were recorded using 62 Ag/AgCl EEG electrodes and four EMG electrodes, with a sampling rate of 1000 Hz. BrainAmp (Brain Products, Munich, Germany) was used as the amplifier in the experiment. The EEG channels were nasion-referenced and grounded to electrode Fpz. The EMG channels were attached at the flexor and extensor of the left and right forearms. Each flexor digitorum Fig. 1. EEG electrode placement of the international 10-20 system and the EMG placement (62 EEG and 4 EMG recording channels), which indicates the locations of the electrodes. The EEG electrodes shown in gray were used for the performance validation.In this experiment, the EEG data were recorded using 62 Ag/AgCl EEG electrodes and four EMG electrodes, with a sampling rate of 1000 Hz. BrainAmp (Brain Products, Munich, Germany) was used as the amplifier in the experiment. The EEG channels were nasion-referenced and grounded to electrode Fpz. The EMG channels were attached at the flexor and extensor of the left and right forearms. Each flexor digitorum Fig. 1. EEG electrode placement of the international 10-20 system and the EMG placement (62 EEG and 4 EMG recording channels), which indicates the locations of the electrodes. The EEG electrodes shown in gray were used for the performance validation.</p>
        <p>profundus muscle with the olecranon was used as an EMG electrode reference. The EEG/EMG electrode configuration and indexing numbers are shown in Fig. 1. The impedances of the EEG electrodes were maintained below 10 k during the entire experiment and checked before the start of each phase. The EMG channels attached to collect the subject's physical movements were excluded because they were out of scope in this article.profundus muscle with the olecranon was used as an EMG electrode reference. The EEG/EMG electrode configuration and indexing numbers are shown in Fig. 1. The impedances of the EEG electrodes were maintained below 10 k during the entire experiment and checked before the start of each phase. The EMG channels attached to collect the subject's physical movements were excluded because they were out of scope in this article.</p>
        <p>Basically, an antialiasing filter was applied before digitizing the EEG signals. Then, recorded EEG signals were downsampled to 100 Hz by the Nyquist theory for our interesting frequency ranges that are below 40 Hz. For the performance validation, 20 electrodes from the motor cortex region were selected (FC-5/3/1/2/4/6, C-5/3/1/z/2/4/6, and CP-5/3/1/z/2/4/6), as shown in gray circles in Fig. 1.Basically, an antialiasing filter was applied before digitizing the EEG signals. Then, recorded EEG signals were downsampled to 100 Hz by the Nyquist theory for our interesting frequency ranges that are below 40 Hz. For the performance validation, 20 electrodes from the motor cortex region were selected (FC-5/3/1/2/4/6, C-5/3/1/z/2/4/6, and CP-5/3/1/z/2/4/6), as shown in gray circles in Fig. 1.</p>
        <p>In the MI paradigm, all subjects were instructed to perform two-class MI tasks (movement imagination of the left or right hand; hand open and close) according to a visual cue (a left or right arrow) on the monitor. In the experiment, each trial started with a black fixation cross for a preparation of the MI task for 3 s at the center of the monitor. Afterward, the subject was instructed to perform the MI task for 4 s when the left or right arrow appeared on the monitor. After each MI task, the screen remained blank for 6 s (±1.5). These entire procedures were based on the general BCI experimental setting [1].In the MI paradigm, all subjects were instructed to perform two-class MI tasks (movement imagination of the left or right hand; hand open and close) according to a visual cue (a left or right arrow) on the monitor. In the experiment, each trial started with a black fixation cross for a preparation of the MI task for 3 s at the center of the monitor. Afterward, the subject was instructed to perform the MI task for 4 s when the left or right arrow appeared on the monitor. After each MI task, the screen remained blank for 6 s (±1.5). These entire procedures were based on the general BCI experimental setting [1].</p>
        <p>The whole experiment is composed of an offline (training) and an online (testing) phase. As the EEG data were acquired in the offline phase, a projection matrix of CSP and a classification model based on CSP and LDA were obtained. Those CSP and LDA parameters were used for the analysis in the online phase. In the online phase, the real-time EEG data for 1.5 s were obtained and filtered with the same frequency band used in the offline phase. Afterward, the projection matrix of CSP from the offline phase was applied to the online EEG data, and then, the logarithm variances were taken as features. Then, LDA parameters were applied to those features, and finally, the output was presented in the form of a left or right arrow to the user as real-time visual neurofeedback to acquire a high-quality EEG signal through user-adaptive training, which has been used in general BCI research [52]. We provided flexible break times for individual participants between the phases.The whole experiment is composed of an offline (training) and an online (testing) phase. As the EEG data were acquired in the offline phase, a projection matrix of CSP and a classification model based on CSP and LDA were obtained. Those CSP and LDA parameters were used for the analysis in the online phase. In the online phase, the real-time EEG data for 1.5 s were obtained and filtered with the same frequency band used in the offline phase. Afterward, the projection matrix of CSP from the offline phase was applied to the online EEG data, and then, the logarithm variances were taken as features. Then, LDA parameters were applied to those features, and finally, the output was presented in the form of a left or right arrow to the user as real-time visual neurofeedback to acquire a high-quality EEG signal through user-adaptive training, which has been used in general BCI research [52]. We provided flexible break times for individual participants between the phases.</p>
        <p>For the experimental environments, we used 
            <rs type="software">Psychophysics Toolbox</rs> Version 
            <rs type="version">3 (PTB-3</rs>) and 
            <rs type="software">OpenBMI toolbox</rs> [53], which can run with 
            <rs type="software">MATLAB</rs> R
            <rs type="version">2012a+</rs> on Windows OS. The 
            <rs type="software">OpenBMI toolbox</rs> is a general-purpose software that we have developed since 2012, and it includes experimental codes as well as the modules for data analysis of BCI. 1
        </p>
        <p>To create a generalized feature representation from a largescale MI dataset, we considered the following questions:To create a generalized feature representation from a largescale MI dataset, we considered the following questions:</p>
        <p>1) how to extract discriminant ERD patterns from continuous EEG data that include a diversity of brain signals in the frequency and spatial domains; 2) how to construct m × m input matrices C from the EEG epoch X (time samples × channels × trials) that would be suitable to be fed into the CNN architecture; 3) how to design a CNN framework that could represent the diversity of spectral-spatial features from the input matrices C;1) how to extract discriminant ERD patterns from continuous EEG data that include a diversity of brain signals in the frequency and spatial domains; 2) how to construct m × m input matrices C from the EEG epoch X (time samples × channels × trials) that would be suitable to be fed into the CNN architecture; 3) how to design a CNN framework that could represent the diversity of spectral-spatial features from the input matrices C;</p>
        <p>Let us denote X n = {x i } n i=1 ∈ R T ×M as a set of single-trial EEGs, and let Y n = {y i } n i=1 be the matching class labels, where n is the number of EEG trials, T is the number of sample points, and M is the number of channels. An individual's EEG signal X n was bandpass-filtered with a fifth-order Butterworth filter based on the predefined frequency bands B. We define B as a set of predefined frequency bands, B = {b k } K k=1 (e.g., [8,30 Hz], [11,20 Hz], and so on), where K is the number of frequency bands. The frequency bands included conventional frequency bands in reference to previous findings [23] and empirically extend the additional frequency bands; the details are shown in Fig. 2. Then, the filtered-EEG signals were segmented between 1000 and 3500 ms after stimulus onset (i.e., 2500 data samples × 20 channels × 200 trials) for an individual subject.Let us denote X n = {x i } n i=1 ∈ R T ×M as a set of single-trial EEGs, and let Y n = {y i } n i=1 be the matching class labels, where n is the number of EEG trials, T is the number of sample points, and M is the number of channels. An individual's EEG signal X n was bandpass-filtered with a fifth-order Butterworth filter based on the predefined frequency bands B. We define B as a set of predefined frequency bands, B = {b k } K k=1 (e.g., [8,30 Hz], [11,20 Hz], and so on), where K is the number of frequency bands. The frequency bands included conventional frequency bands in reference to previous findings [23] and empirically extend the additional frequency bands; the details are shown in Fig. 2. Then, the filtered-EEG signals were segmented between 1000 and 3500 ms after stimulus onset (i.e., 2500 data samples × 20 channels × 200 trials) for an individual subject.</p>
        <p>From a large-scale MI database, all single-trial EEGs deterministically obtained. The bandpass filtering is applied as follows:From a large-scale MI database, all single-trial EEGs deterministically obtained. The bandpass filtering is applied as follows:</p>
        <p>where is the bandpass filtering operation. After the bandpass filtering based on predefined frequency bands, a standard CSP algorithm [25]- [28] is utilized, in which W k ∈R U ×U is analytically obtained from E k by calculating a generalized eigenvector problem, where U is the number of spatial filters to be obtained from the CSP algorithm.where is the bandpass filtering operation. After the bandpass filtering based on predefined frequency bands, a standard CSP algorithm [25]- [28] is utilized, in which W k ∈R U ×U is analytically obtained from E k by calculating a generalized eigenvector problem, where U is the number of spatial filters to be obtained from the CSP algorithm.</p>
        <p>Before measuring the discriminative power between the classes, a feature vector for each frequency band b k is obtained through a matrix multiplication between the filtered EEG signals E k and the spatial filters W k . The variance of the spatially filtered signals is calculated, and then, the logarithm is computed for the featureBefore measuring the discriminative power between the classes, a feature vector for each frequency band b k is obtained through a matrix multiplication between the filtered EEG signals E k and the spatial filters W k . The variance of the spatially filtered signals is calculated, and then, the logarithm is computed for the feature</p>
        <p>Since it is uncertain which frequency bands can compose discriminative brain features, we encode these uncertainties using mutual information over the frequency bands b k and features V k . Mutual information is a measure of the mutual dependence between two random variables and the uncertainty of the random variablesSince it is uncertain which frequency bands can compose discriminative brain features, we encode these uncertainties using mutual information over the frequency bands b k and features V k . Mutual information is a measure of the mutual dependence between two random variables and the uncertainty of the random variables</p>
        <p>where H(•) is the entropy andwhere H(•) is the entropy and</p>
        <p>Since the value of mutual information is obtained for each frequency band, frequency bands b k are rearranged in the descending order (b p ) from largest to smallest in terms of the value of the mutual information in Fig. 2. This method is used to select informative subsets of the original features [54], and on the basis of our belief, it would help us to discover the frequency band that could contribute to composing discriminative ERD among large-scale MI data. On the basis of the rearranged frequency bands b p in Fig. 2, the spectral EEG signals E k and the spatial filters W k were also rearranged to E p and W p according to the new frequency order b p .Since the value of mutual information is obtained for each frequency band, frequency bands b k are rearranged in the descending order (b p ) from largest to smallest in terms of the value of the mutual information in Fig. 2. This method is used to select informative subsets of the original features [54], and on the basis of our belief, it would help us to discover the frequency band that could contribute to composing discriminative ERD among large-scale MI data. On the basis of the rearranged frequency bands b p in Fig. 2, the spectral EEG signals E k and the spatial filters W k were also rearranged to E p and W p according to the new frequency order b p .</p>
        <p>From the spectrally and spatially optimized EEG epoch (W p and E p ), the covariance matrix C p is calculatedFrom the spectrally and spatially optimized EEG epoch (W p and E p ), the covariance matrix C p is calculated</p>
        <p>where the input set C = {C 1 ,C 2 ,…,C P }∈R U ×U × P , U is the number of spatial filters, and P is the number of frequency indices.where the input set C = {C 1 ,C 2 ,…,C P }∈R U ×U × P , U is the number of spatial filters, and P is the number of frequency indices.</p>
        <p>In this article, we utilize the CNN approach to learn representation from a set of spectral-spatial inputs (C). The CNN model consists of several computational blocks [40]- [43]; here, the proposed framework is composed of three convolutional layers, a concatenation layer for fusion, and a fully connected layer. Given the CNN input C p for each frequency band b p , the outcome of the CNN layer at the Lth layer can be obtained as f L (C p ) (see [40]- [43]). To design the CNN, the characteristics of each spectral-spatial input (C p ) for each frequency band (b p ) should be taken into consideration. In the CNN layers, the spectral-spatial inputs are individually passed through the CNN model for each frequency band. Each outcome of the convolutional layer is considered as an input into the subsequent computational block. From the diverse frequency bands (b p ), each convolutional feature is extracted by the CNN, as shown in Fig. 3 GIn this article, we utilize the CNN approach to learn representation from a set of spectral-spatial inputs (C). The CNN model consists of several computational blocks [40]- [43]; here, the proposed framework is composed of three convolutional layers, a concatenation layer for fusion, and a fully connected layer. Given the CNN input C p for each frequency band b p , the outcome of the CNN layer at the Lth layer can be obtained as f L (C p ) (see [40]- [43]). To design the CNN, the characteristics of each spectral-spatial input (C p ) for each frequency band (b p ) should be taken into consideration. In the CNN layers, the spectral-spatial inputs are individually passed through the CNN model for each frequency band. Each outcome of the convolutional layer is considered as an input into the subsequent computational block. From the diverse frequency bands (b p ), each convolutional feature is extracted by the CNN, as shown in Fig. 3 G</p>
        <p>wherewhere</p>
        <p>is the weight matrix of each layer. The whole CNN framework is trained to reduce the loss function by stochastic gradient descent [40]- [43]. Here, the cross-entropy loss function is used, and feature maps are processed by a rectified linear (ReLU) function.is the weight matrix of each layer. The whole CNN framework is trained to reduce the loss function by stochastic gradient descent [40]- [43]. Here, the cross-entropy loss function is used, and feature maps are processed by a rectified linear (ReLU) function.</p>
        <p>In this section, a feature fusion process is presented by integrating discriminant spectral-spatial inputs. Our approach starts by utilizing multiple frequency bands b p , as shown in Fig. 2. Each frequency segment in b p represents the diversity of ERD patterns that contain the inherent information. Our main motivation is to integrate all the segments (b p ) that contain the discriminant ERD patterns in the brain signals.In this section, a feature fusion process is presented by integrating discriminant spectral-spatial inputs. Our approach starts by utilizing multiple frequency bands b p , as shown in Fig. 2. Each frequency segment in b p represents the diversity of ERD patterns that contain the inherent information. Our main motivation is to integrate all the segments (b p ) that contain the discriminant ERD patterns in the brain signals.</p>
        <p>For the integration of multiple convolutional features, we encode the convolutional features and transform them into a linear vector representation as the outcome feature for a concatenation fusion layer, where the high-dimensional convolutional feature vectors are transformed into a low-dimensional vector for the fusion process.For the integration of multiple convolutional features, we encode the convolutional features and transform them into a linear vector representation as the outcome feature for a concatenation fusion layer, where the high-dimensional convolutional feature vectors are transformed into a low-dimensional vector for the fusion process.</p>
        <p>Let us denote C p as the features on the Lth layer, which can be represented as G L p = S= {s 1 , s 2 ,…,s P }∈R D of the set P D-dimensional convolutional features. Thus, a D-dimensional vector was obtained in each frequency band (b p ), and all the individual D-dimensional vectors were concatenated as follows:Let us denote C p as the features on the Lth layer, which can be represented as G L p = S= {s 1 , s 2 ,…,s P }∈R D of the set P D-dimensional convolutional features. Thus, a D-dimensional vector was obtained in each frequency band (b p ), and all the individual D-dimensional vectors were concatenated as follows:</p>
        <p>where S concat ∈R ( P×D) . Here, P is the number of inputs at the Lth layer, D is the dimension of the first fully connected layer of the CNN, and a = s 1 , z = s p . The concatenated fusion layer S concat , which is the output of the algorithm, was connected with a fully connected layer, as shown in Fig. 3. Finally, the value of the two outputs that indicate left-or right-hand MI was obtained from the soft-max classifier.where S concat ∈R ( P×D) . Here, P is the number of inputs at the Lth layer, D is the dimension of the first fully connected layer of the CNN, and a = s 1 , z = s p . The concatenated fusion layer S concat , which is the output of the algorithm, was connected with a fully connected layer, as shown in Fig. 3. Finally, the value of the two outputs that indicate left-or right-hand MI was obtained from the soft-max classifier.</p>
        <p>In the training session, 53 subjects (i.e., N = 1, . . . , 53) over two sessions (a total number of 21 200 trial samples) were used to train the CNN. In the 30 predefined frequency bands (i.e., K = 1, . . . , 30) in Table I, 20 rearranged frequency bands (i.e., P = 1, . . . , 20) were selected in this article. Therefore, 424 000 samples (20 frequency bands × 53 subjects × 200 trials × 2 sessions) were utilized as inputs for the CNN architecture.In the training session, 53 subjects (i.e., N = 1, . . . , 53) over two sessions (a total number of 21 200 trial samples) were used to train the CNN. In the 30 predefined frequency bands (i.e., K = 1, . . . , 30) in Table I, 20 rearranged frequency bands (i.e., P = 1, . . . , 20) were selected in this article. Therefore, 424 000 samples (20 frequency bands × 53 subjects × 200 trials × 2 sessions) were utilized as inputs for the CNN architecture.</p>
        <p>Specifically, all spatial filters of the CSP (U = 10) were used in this article. Based on (4), all EEG epochs were transformed into covariance matrices (M × M; here, M = 20), and zero padding was applied ( M × M; here, M = 28). Thus, the M × M matrix was used for the input of the CNN architecture. The dimension of the first fully connected layer neurons' activations of the CNN D was 256, and the number of layers L was 4. The learning rate of η was 0.00001, and a 50% drop-out rate was applied in the training phase. A filter size of 3 × 3 was used. For the batch size, we used 100 samples for each mini-batch. Adam optimizer was used for the optimization algorithm. All learning parameters were determined by our iterated experiments. The entire experiment was conducted on a desktop computer with a dual-core i7-7700 4.2-GHz processor and 16-GB memory. 
            <rs type="software">TensorFlow</rs> was basically used to implement the algorithms (e.g., concatenate); see Algorithm 1.
        </p>
        <p>For the performance comparison, we evaluated the decoding accuracy of the proposed method by comparing the previous subject-independent approaches [31], [32] as well as subjectdependent approaches [23], [28]- [30].For the performance comparison, we evaluated the decoding accuracy of the proposed method by comparing the previous subject-independent approaches [31], [32] as well as subjectdependent approaches [23], [28]- [30].</p>
        <p>Specifically, a fused model [31], an MR FBCSP (multiresolution FBCSPs) method [32], and a pooled CSP method [32] were implemented as subject-independent approaches. For the fused model, the EEG signals were decomposed using six frequency bands, as in [31]. The MR FBCSP method also used decomposition with three sub-decompositions of different resolutions, which included 20 different features [32]. Additionally, we evaluated the basic ensemble method (Pooled CSP [32]), which concatenates all the given training samples and obtains the CSP filters from the concatenated samples.Specifically, a fused model [31], an MR FBCSP (multiresolution FBCSPs) method [32], and a pooled CSP method [32] were implemented as subject-independent approaches. For the fused model, the EEG signals were decomposed using six frequency bands, as in [31]. The MR FBCSP method also used decomposition with three sub-decompositions of different resolutions, which included 20 different features [32]. Additionally, we evaluated the basic ensemble method (Pooled CSP [32]), which concatenates all the given training samples and obtains the CSP filters from the concatenated samples.</p>
        <p>Additionally, CSP [23], CSSP [29], FBCSP [28], and BSSFO [30] were evaluated as subject-specific methods for comparison. For the analysis of the CSP algorithm, a widely used frequency band (8-30 Hz) was utilized to obtain the spatial patterns [23]. The CSSP method is implemented utilizing time delay embedding from 1 to 15 [29]. The FBCSP method is decomposed using nine frequency bands, as in [28]. In BSSFO, 30 particles were used for the analysis and were iterated 10 times to achieve robust classification results [30]. For all the subject-specific methods, bandpass filtering was applied with a fifth-order Butterworth filter at the frequency band. With respect to spatial filtering, the two highest and two lowest spatial patterns were used for the analysis.Additionally, CSP [23], CSSP [29], FBCSP [28], and BSSFO [30] were evaluated as subject-specific methods for comparison. For the analysis of the CSP algorithm, a widely used frequency band (8-30 Hz) was utilized to obtain the spatial patterns [23]. The CSSP method is implemented utilizing time delay embedding from 1 to 15 [29]. The FBCSP method is decomposed using nine frequency bands, as in [28]. In BSSFO, 30 particles were used for the analysis and were iterated 10 times to achieve robust classification results [30]. For all the subject-specific methods, bandpass filtering was applied with a fifth-order Butterworth filter at the frequency band. With respect to spatial filtering, the two highest and two lowest spatial patterns were used for the analysis.</p>
        <p>For the classifier (except for in the proposed method), we used linear classifiers (LDA) that are specified by the discriminant functions for a binary classification problem. This is because LDA not only achieved the highest accuracy in the subject-independent environment previously [32] but also is widely utilized in the subject-dependent environment [37].For the classifier (except for in the proposed method), we used linear classifiers (LDA) that are specified by the discriminant functions for a binary classification problem. This is because LDA not only achieved the highest accuracy in the subject-independent environment previously [32] but also is widely utilized in the subject-dependent environment [37].</p>
        <p>Fig. 4 shows an example of how we utilized the training and testing samples according to the method of analysis. For the subject-dependent validation, the offline data from session2 (the black box) was used to derive the classifier parameters, and the online data from session2 (the gray box) was used for the performance validation using pretrained parameters. For the subject-independent validation, we utilized a leave-onesubject-out cross-validation (LOSO-CV) procedure [31], [32]. For example, assume that subject1 is the target subject (the test subject). Except for the target subject, all the training samples (the black boxes) were used to train the classifier, and then, the online data from session2 for the target subject (the gray box) were used for the performance evaluation. The same test data were used in both environments.Fig. 4 shows an example of how we utilized the training and testing samples according to the method of analysis. For the subject-dependent validation, the offline data from session2 (the black box) was used to derive the classifier parameters, and the online data from session2 (the gray box) was used for the performance validation using pretrained parameters. For the subject-independent validation, we utilized a leave-onesubject-out cross-validation (LOSO-CV) procedure [31], [32]. For example, assume that subject1 is the target subject (the test subject). Except for the target subject, all the training samples (the black boxes) were used to train the classifier, and then, the online data from session2 for the target subject (the gray box) were used for the performance evaluation. The same test data were used in both environments.</p>
        <p>Input: A set of training samples from a large-scale MI database.Input: A set of training samples from a large-scale MI database.</p>
        <p>• x ∈ R T ×M : EEG data with T sample points and M channels.• x ∈ R T ×M : EEG data with T sample points and M channels.</p>
        <p>• X={x i } n i=1 : an individual's single trial in an EEG epoch, where n is the total number of trials. • Y = {y i } n i=1 : class labels, where y i ∈ {+1, -1}. • K = number of spectral filter bands.• X={x i } n i=1 : an individual's single trial in an EEG epoch, where n is the total number of trials. • Y = {y i } n i=1 : class labels, where y i ∈ {+1, -1}. • K = number of spectral filter bands.</p>
        <p>• P= number of selected spectral filter bands.• P= number of selected spectral filter bands.</p>
        <p>• u= half the number of spatial patterns in a spatial pattern learning algorithm. Output: Spectral-spatial convolutional features S.• u= half the number of spatial patterns in a spatial pattern learning algorithm. Output: Spectral-spatial convolutional features S.</p>
        <p>• S ∈R 1×(D×P) : D and P are the dimension of the first fully-connected layer and the number of inputs, respectively.• S ∈R 1×(D×P) : D and P are the dimension of the first fully-connected layer and the number of inputs, respectively.</p>
        <p>Refer to Section III-Eq.3 end Rearrange the frequency bands b p from the largest MI value to smallest MI value. for p = 1 to P do Rearrange the spectral filters and spatial filters (E p , W p ).Refer to Section III-Eq.3 end Rearrange the frequency bands b p from the largest MI value to smallest MI value. for p = 1 to P do Rearrange the spectral filters and spatial filters (E p , W p ).</p>
        <p>Refer to Section III-Eq.5 end S = concat{G 1 , G 2 , . . . , G P } Feature fusion process: refer to Section III-Eq.6Refer to Section III-Eq.5 end S = concat{G 1 , G 2 , . . . , G P } Feature fusion process: refer to Section III-Eq.6</p>
        <p>Table II shows the averaged decoding accuracies across all subjects for individual methods. The decoding accuracies for the subject-independent methods are 65.65% (±16.11), 67.37% (±16.01), 68.59% (±15.28), and 74.15% (±15.83) for the pooled CSP, fused model, MR-FBCSP, and the proposed method, respectively. In the multiple comparison test of the subject-independent methods and the proposed method, the result of an ANOVA test with Bonferroni was [F (3,212) = 2.9184, p = 0.0351]. The results of a paired t-test between the subject-independent methods and the proposed method were p &lt; 0.001. In case of the subject-dependent methods, the decoding accuracies are 68.57% (±17.57), 69.68% (±18.53), 70.59% (±18.56), and 71.02% (±18.83) for CSP, CSSP, FBCSP, and BSSFO, respectively. In the multiple comparison test of the subject-dependent methods and the proposed method, the result of the ANOVA test with Bonferroni was [F (4,265) = 0.7373, p = 0.5672]. The results of a paired t-test between the subject-dependent methods and the proposed method were p &lt; 0.05.Table II shows the averaged decoding accuracies across all subjects for individual methods. The decoding accuracies for the subject-independent methods are 65.65% (±16.11), 67.37% (±16.01), 68.59% (±15.28), and 74.15% (±15.83) for the pooled CSP, fused model, MR-FBCSP, and the proposed method, respectively. In the multiple comparison test of the subject-independent methods and the proposed method, the result of an ANOVA test with Bonferroni was [F (3,212) = 2.9184, p = 0.0351]. The results of a paired t-test between the subject-independent methods and the proposed method were p &lt; 0.001. In case of the subject-dependent methods, the decoding accuracies are 68.57% (±17.57), 69.68% (±18.53), 70.59% (±18.56), and 71.02% (±18.83) for CSP, CSSP, FBCSP, and BSSFO, respectively. In the multiple comparison test of the subject-dependent methods and the proposed method, the result of the ANOVA test with Bonferroni was [F (4,265) = 0.7373, p = 0.5672]. The results of a paired t-test between the subject-dependent methods and the proposed method were p &lt; 0.05.</p>
        <p>Tables III and IV show the performance changes of the proposed method according to the kernel size and the number of feature maps, as shown in Fig. 3. The decoding accuracies with the change in kernel size are 73.98% (±15.57), 74.15% (±15.83), and 73.72% (±16.06) for 2 × 2, 3 × 3, and 5 × 5 kernels, respectively. The decoding accuracies according to the number of feature maps are 73.51% (±16.33), 73.90% (±16.65), and 74.15% (±15.83) when the first feature map was 5, 8, and 10, respectively. Note that other parameters are based on Section IV-D. Fig. 5 shows the scatter plots that demonstrate the performance comparison for individual subjects. The x-axis is the classification performance of the competing methods, and the y-axis is the performance of the proposed method. The top row depicts the subject-dependent approaches in the order of CSP, FBCSP, and BSSFO, and the bottom row displays the and7 show the performance changes by the number of subjects and the frequency bands. Fig. 6 shows the performance changes according to the number of subjects. To avoid the computational cost, the certain test subject (participant N = 39) and the number of frequency bands (i.e., b p , p = 1, . . . , 20) were determined. The x-axis indicates that the number of subjects (N) increased in steps of one from N = 5 to N = 53. In each increasing step, a certain number of subjects for the training were randomly selected from among all subjects, and the classification performance was validated with the test subject. Note that the composition of a certain number of subjects was randomly changed over five iterations, and the classification performance was obtained from the average of five iteration results. For example, assume that we have ten subjects (i.e., N = 1, . . . , 10). Ten subjects were randomly selected among all subjects for each iteration.Tables III and IV show the performance changes of the proposed method according to the kernel size and the number of feature maps, as shown in Fig. 3. The decoding accuracies with the change in kernel size are 73.98% (±15.57), 74.15% (±15.83), and 73.72% (±16.06) for 2 × 2, 3 × 3, and 5 × 5 kernels, respectively. The decoding accuracies according to the number of feature maps are 73.51% (±16.33), 73.90% (±16.65), and 74.15% (±15.83) when the first feature map was 5, 8, and 10, respectively. Note that other parameters are based on Section IV-D. Fig. 5 shows the scatter plots that demonstrate the performance comparison for individual subjects. The x-axis is the classification performance of the competing methods, and the y-axis is the performance of the proposed method. The top row depicts the subject-dependent approaches in the order of CSP, FBCSP, and BSSFO, and the bottom row displays the and7 show the performance changes by the number of subjects and the frequency bands. Fig. 6 shows the performance changes according to the number of subjects. To avoid the computational cost, the certain test subject (participant N = 39) and the number of frequency bands (i.e., b p , p = 1, . . . , 20) were determined. The x-axis indicates that the number of subjects (N) increased in steps of one from N = 5 to N = 53. In each increasing step, a certain number of subjects for the training were randomly selected from among all subjects, and the classification performance was validated with the test subject. Note that the composition of a certain number of subjects was randomly changed over five iterations, and the classification performance was obtained from the average of five iteration results. For example, assume that we have ten subjects (i.e., N = 1, . . . , 10). Ten subjects were randomly selected among all subjects for each iteration.</p>
        <p>For statistical significance tests of the experimental results on our dataset, the null hypothesis in this article is that the proposed method produces the same mean accuracy as the competing methods. We computed an ANOVA test with Bonferroni for a multiple comparison test. In addition, we computed the p-values using a paired t-test to assess whether the differences in the classification accuracies between two methods are at a significant level. In an ANOVA test of subject-independent methods, the proposed method significantly outperforms the competing methods [F (3,212)For statistical significance tests of the experimental results on our dataset, the null hypothesis in this article is that the proposed method produces the same mean accuracy as the competing methods. We computed an ANOVA test with Bonferroni for a multiple comparison test. In addition, we computed the p-values using a paired t-test to assess whether the differences in the classification accuracies between two methods are at a significant level. In an ANOVA test of subject-independent methods, the proposed method significantly outperforms the competing methods [F (3,212)</p>
        <p>Due to the lack of a large public MI database, the previous studies using DL could work only in subject-dependent environments [48]- [51]. With the use of our large-scale MI database, it is interesting to investigate how many data are truly needed to develop an acceptable subject-independent model with a DL network (i.e., what is the number of training samples required to train the CNN?).Due to the lack of a large public MI database, the previous studies using DL could work only in subject-dependent environments [48]- [51]. With the use of our large-scale MI database, it is interesting to investigate how many data are truly needed to develop an acceptable subject-independent model with a DL network (i.e., what is the number of training samples required to train the CNN?).</p>
        <p>Our MI database includes a large number of subjects over multiple sessions. Therefore, the database includes a variety of brain signals, which represent inherent features for individual subjects and EEG variability over different sessions.Our MI database includes a large number of subjects over multiple sessions. Therefore, the database includes a variety of brain signals, which represent inherent features for individual subjects and EEG variability over different sessions.</p>
        <p>Fig. 6 shows the performance changes according to the number of subjects. Fig. 6 also shows that the classification performance varies with the composition of the subjects used to train the CNN. For the training of the CNN, a particular number of subjects were randomly chosen among all subjects in each increasing step. The arbitrary selection process was repeated five times, and the averaged performance was calculated based on the results of five iterations, as described in Section V. In Fig. 6, it can be seen that there were enormous performance variations over each iteration. For example, when five subjects were used, the results of the first and second iterations showed significant differences in the classification performance. This could be interpreted as the presence of significant differences in the data between different subjects, and therefore, a small number of training samples could either help or harm the actual training. This has been confirmed in the real world by other researchers who considered negative Fig. 6. Classification performance changes with the number of subjects. One subject was chosen for the test data, and 20 frequency indices were used for this experiment. The composition of subjects for training was increased in steps of one from n=5 to n=53 and was randomly changed for each iteration. The box plot shows the classification performance over five iterations.Fig. 6 shows the performance changes according to the number of subjects. Fig. 6 also shows that the classification performance varies with the composition of the subjects used to train the CNN. For the training of the CNN, a particular number of subjects were randomly chosen among all subjects in each increasing step. The arbitrary selection process was repeated five times, and the averaged performance was calculated based on the results of five iterations, as described in Section V. In Fig. 6, it can be seen that there were enormous performance variations over each iteration. For example, when five subjects were used, the results of the first and second iterations showed significant differences in the classification performance. This could be interpreted as the presence of significant differences in the data between different subjects, and therefore, a small number of training samples could either help or harm the actual training. This has been confirmed in the real world by other researchers who considered negative Fig. 6. Classification performance changes with the number of subjects. One subject was chosen for the test data, and 20 frequency indices were used for this experiment. The composition of subjects for training was increased in steps of one from n=5 to n=53 and was randomly changed for each iteration. The box plot shows the classification performance over five iterations.</p>
        <p>transfer (an indication that knowledge transfer has a negative impact on target learning) [55].transfer (an indication that knowledge transfer has a negative impact on target learning) [55].</p>
        <p>The larger the number of training samples that are utilized, the better the classification performance that can be obtained, as shown in Fig. 6. From this perspective, we can explain the differences in the classification performance between the subject-dependent and subject-independent environment, as shown in Table II. The results in Table II show that while the CNN works equally well as other conventional approaches in the subject-dependent condition, it works much better than the conventional approaches in the subject-independent condition. Fig. 6 clearly shows that more training samples contribute to better classification performance. With both the large dataset and DL, our proposed method outperformed the subjectdependent methods.The larger the number of training samples that are utilized, the better the classification performance that can be obtained, as shown in Fig. 6. From this perspective, we can explain the differences in the classification performance between the subject-dependent and subject-independent environment, as shown in Table II. The results in Table II show that while the CNN works equally well as other conventional approaches in the subject-dependent condition, it works much better than the conventional approaches in the subject-independent condition. Fig. 6 clearly shows that more training samples contribute to better classification performance. With both the large dataset and DL, our proposed method outperformed the subjectdependent methods.</p>
        <p>In addition, the investigation of user-specific frequency bands was primarily considered in the subject-dependent BCI study to improve the decoding accuracy. One prominent strategy [30] is to stochastically find multiple frequency ranges that are optimal for individual subjects. However, from the perspective of subject-independent BCI, it is difficult to find or define specific frequency bands due to the unavailability of training data from the target subjects [22]. Therefore, we included standard frequency bands (e.g., mu rhythm and beta rhythm) that have been widely used in MI studies [23] and additional frequency bands that are empirically determined and overlap with each band, as shown in Fig. 2. The reason why we deployed a large number of overlapping frequency bands is that there is no single frequency range that is the most discriminative for all subjects. Therefore, we decided to use a filter bank that consisted of a broad variety of frequency bands, to provide a greater diversity of EEG patterns from all training samples. As the number of frequency bands increased in Fig. 7, the classification performance also improved, which is in line with our previous belief that each frequency segment in b p represents the diversity of EEG patterns that contained the inherent information. Furthermore, as shown in Fig. 7, the performance improvement slows down after 20 frequency indices (b p , p &gt; 20). We determined that 20 frequency indices are the proper size of the filter bank (b p , p = 20, . . . , 30) to show performance superior to that of a single optimal frequency band (b 1 ). Furthermore, an increase in the filter bank leads to an increase in the number of training samples, which is beneficial to the CNN training. As an example, a strategy of changing the configuration of the filter bank to increase the number of training samples is similar to that of data augmentation [22].In addition, the investigation of user-specific frequency bands was primarily considered in the subject-dependent BCI study to improve the decoding accuracy. One prominent strategy [30] is to stochastically find multiple frequency ranges that are optimal for individual subjects. However, from the perspective of subject-independent BCI, it is difficult to find or define specific frequency bands due to the unavailability of training data from the target subjects [22]. Therefore, we included standard frequency bands (e.g., mu rhythm and beta rhythm) that have been widely used in MI studies [23] and additional frequency bands that are empirically determined and overlap with each band, as shown in Fig. 2. The reason why we deployed a large number of overlapping frequency bands is that there is no single frequency range that is the most discriminative for all subjects. Therefore, we decided to use a filter bank that consisted of a broad variety of frequency bands, to provide a greater diversity of EEG patterns from all training samples. As the number of frequency bands increased in Fig. 7, the classification performance also improved, which is in line with our previous belief that each frequency segment in b p represents the diversity of EEG patterns that contained the inherent information. Furthermore, as shown in Fig. 7, the performance improvement slows down after 20 frequency indices (b p , p &gt; 20). We determined that 20 frequency indices are the proper size of the filter bank (b p , p = 20, . . . , 30) to show performance superior to that of a single optimal frequency band (b 1 ). Furthermore, an increase in the filter bank leads to an increase in the number of training samples, which is beneficial to the CNN training. As an example, a strategy of changing the configuration of the filter bank to increase the number of training samples is similar to that of data augmentation [22].</p>
        <p>To summarize, a large-scale MI database is essential for the enhancement of classification performance, and a proper configuration of the filter bank is also an essential factor in profoundly influencing the decoding accuracy of the subjectindependent BCI based on DL.To summarize, a large-scale MI database is essential for the enhancement of classification performance, and a proper configuration of the filter bank is also an essential factor in profoundly influencing the decoding accuracy of the subjectindependent BCI based on DL.</p>
        <p>In this article, one of the main objectives is to construct generalizable brain features. In fact, the signal variability, the amplitude change, the temporal onset variations, and the spatial origin of the EEG segments are different from subject to subject. If we use simple features, such as the logarithm and variance of the EEG signals, it is difficult to extract significant and generalizable features that can include the information for different subjects.In this article, one of the main objectives is to construct generalizable brain features. In fact, the signal variability, the amplitude change, the temporal onset variations, and the spatial origin of the EEG segments are different from subject to subject. If we use simple features, such as the logarithm and variance of the EEG signals, it is difficult to extract significant and generalizable features that can include the information for different subjects.</p>
        <p>As it is known, CSP maximizes the variance differences between binary class signals. The principle of CSP is to perform a linear spatial transformation to project the highdimensional EEG data into a low-dimensional spatial subspace with such a projection matrix, where each row represents a spatial filter that consists of the weights for each channel. Specifically, the first and last columns of the spatial filters are the most important spatial filters, which results in the largest variance of one task and the smallest variance of the other. Overall, CSP-based methods were used to form a subset of the most discriminant spatial filters (i.e., one or more spatial filters).As it is known, CSP maximizes the variance differences between binary class signals. The principle of CSP is to perform a linear spatial transformation to project the highdimensional EEG data into a low-dimensional spatial subspace with such a projection matrix, where each row represents a spatial filter that consists of the weights for each channel. Specifically, the first and last columns of the spatial filters are the most important spatial filters, which results in the largest variance of one task and the smallest variance of the other. Overall, CSP-based methods were used to form a subset of the most discriminant spatial filters (i.e., one or more spatial filters).</p>
        <p>With the feature representation and CNN architecture presented in this article, we believe that the learning process should be based on the most important spatial filters as well as the relations between each column of spatial filters that would be helpful to construct generalizable brain features from a large database. Therefore, we started with the idea of combining CSP from various frequency bands which, in turn, could capture relations between the columns of spatial filters. To support our idea, there were many studies in the past that were relevant to our idea and that utilize the relationships between the features, such as the connectivity or correlation between other feature dimensions, to improve the classification performance or the predictions in research on traditional neuroscience, MI, and other subjects [56]- [59].With the feature representation and CNN architecture presented in this article, we believe that the learning process should be based on the most important spatial filters as well as the relations between each column of spatial filters that would be helpful to construct generalizable brain features from a large database. Therefore, we started with the idea of combining CSP from various frequency bands which, in turn, could capture relations between the columns of spatial filters. To support our idea, there were many studies in the past that were relevant to our idea and that utilize the relationships between the features, such as the connectivity or correlation between other feature dimensions, to improve the classification performance or the predictions in research on traditional neuroscience, MI, and other subjects [56]- [59].</p>
        <p>In addition, to understand how much the given input is related to the MI signals, let us study the first and last columns of spatial filters that are the most significant information in CSP, which demonstrates the largest variance of one class and the smallest variance of the other class. From these insights, if we apply CSP to the EEG signals and examine the covariance matrix of the given input, one side of the covariance matrix would have relatively high values compared with the other side due to the largest and smallest spatial filters. For example, the first spatial filter was meant to maximize the left class of MI, and therefore, the left diagonal components in the left MI input would have comparatively high values.In addition, to understand how much the given input is related to the MI signals, let us study the first and last columns of spatial filters that are the most significant information in CSP, which demonstrates the largest variance of one class and the smallest variance of the other class. From these insights, if we apply CSP to the EEG signals and examine the covariance matrix of the given input, one side of the covariance matrix would have relatively high values compared with the other side due to the largest and smallest spatial filters. For example, the first spatial filter was meant to maximize the left class of MI, and therefore, the left diagonal components in the left MI input would have comparatively high values.</p>
        <p>To understand this aspect in more detail, let us investigate the comprehension about what features are learned from the networks and how those features are related to MI signals.To understand this aspect in more detail, let us investigate the comprehension about what features are learned from the networks and how those features are related to MI signals.</p>
        <p>The inputs (covariance matrices) are shown at the top part of Fig. 8, and it can be seen that one side of the diagonal components in the covariance matrix has relatively high values (red) due to the discriminating effect of the spatial filters. This can be interpreted as follows: the given input is a representation of the MI signals (left or right motor imagination). To see what features have been primarily learned and to understand how CNNs work due to the large number of interacting and nonlinear operations, we visualized the activation of each convolutional network layer in Fig. 8 of the CNN [60]. In Fig. 8, the relative value of the activation in each convolutional layer is displayed in color. A larger amount of red tells us that there is more activation in each convolutional network layer. Through more active regions in the activation map, it is possible to see what features the network learned.The inputs (covariance matrices) are shown at the top part of Fig. 8, and it can be seen that one side of the diagonal components in the covariance matrix has relatively high values (red) due to the discriminating effect of the spatial filters. This can be interpreted as follows: the given input is a representation of the MI signals (left or right motor imagination). To see what features have been primarily learned and to understand how CNNs work due to the large number of interacting and nonlinear operations, we visualized the activation of each convolutional network layer in Fig. 8 of the CNN [60]. In Fig. 8, the relative value of the activation in each convolutional layer is displayed in color. A larger amount of red tells us that there is more activation in each convolutional network layer. Through more active regions in the activation map, it is possible to see what features the network learned.</p>
        <p>From each activation map, it can be seen that the proposed method effectively exploited the distributed and hierarchical features through multiple layers. On the first activation mapped in the first layer, the network focused on the simplistic features, such as straight edges and simple colors in the given input. As the layers become deeper, the learning process is based on the values according to the MI class in the covariance input. On the last activation map of the last layer, the network focused not only on the most important spatial filters (for the left class, more red appears on the upper left corner, while for the right class, more red appears on the lower right corner) but also on correlations (more red dots appear on the lower left and upper right part) between different feature dimensions. From this observation, we can conclude that the most important spatial filters are significant for the classification in addition to the relations between the other feature dimensions that are critical for the classification.From each activation map, it can be seen that the proposed method effectively exploited the distributed and hierarchical features through multiple layers. On the first activation mapped in the first layer, the network focused on the simplistic features, such as straight edges and simple colors in the given input. As the layers become deeper, the learning process is based on the values according to the MI class in the covariance input. On the last activation map of the last layer, the network focused not only on the most important spatial filters (for the left class, more red appears on the upper left corner, while for the right class, more red appears on the lower right corner) but also on correlations (more red dots appear on the lower left and upper right part) between different feature dimensions. From this observation, we can conclude that the most important spatial filters are significant for the classification in addition to the relations between the other feature dimensions that are critical for the classification.</p>
        <p>This result could shed light on why the proposed method outperformed other CSP-based methods that use few spatial filters and linear characteristics to classify brain signals. When only a few spatial filters of CSP are used, the correlations between different feature dimensions in the inputs are lost, which leads to degraded classification performance. In our strategy, as we consider all input information as well as the correlations between all the dimensions, our approach can outperform other methods.This result could shed light on why the proposed method outperformed other CSP-based methods that use few spatial filters and linear characteristics to classify brain signals. When only a few spatial filters of CSP are used, the correlations between different feature dimensions in the inputs are lost, which leads to degraded classification performance. In our strategy, as we consider all input information as well as the correlations between all the dimensions, our approach can outperform other methods.</p>
        <p>To summarize, the proposed feature shows a high level of performance through learning the most significant spatial filters as well as the relationships between different feature dimensions that are related to MI. This study is the first attempt to exploit generalizable brain features in the CNN pipeline for subject-independent BCI.To summarize, the proposed feature shows a high level of performance through learning the most significant spatial filters as well as the relationships between different feature dimensions that are related to MI. This study is the first attempt to exploit generalizable brain features in the CNN pipeline for subject-independent BCI.</p>
        <p>In this article, the proposed framework is composed of three convolutional layers, a concatenation layer for fusion, and a fully layer. The input of our framework is the covariance matrix, which has a dimension of 28 × 28. Since the input dimension is relatively small, we hypothesize that significant features could be extracted through a few convolutional layers rather than many convolutional layers. Based on these considerations, we used three layers in the CNN. Similarly, we think that similar features could be extracted regardless of whether the number of feature maps in the convolutional layer is large or small. Based on these considerations, we decided to start with ten feature maps. Because of the small input size, we also used a small kernel size. The above-mentioned considerations might also be beneficial to overcome a possible overfitting problem due to the relatively small number of free parameters in the proposed model.In this article, the proposed framework is composed of three convolutional layers, a concatenation layer for fusion, and a fully layer. The input of our framework is the covariance matrix, which has a dimension of 28 × 28. Since the input dimension is relatively small, we hypothesize that significant features could be extracted through a few convolutional layers rather than many convolutional layers. Based on these considerations, we used three layers in the CNN. Similarly, we think that similar features could be extracted regardless of whether the number of feature maps in the convolutional layer is large or small. Based on these considerations, we decided to start with ten feature maps. Because of the small input size, we also used a small kernel size. The above-mentioned considerations might also be beneficial to overcome a possible overfitting problem due to the relatively small number of free parameters in the proposed model.</p>
        <p>To determine the convergence process of the proposed framework, we investigated the evolution of the classification results during the training phase. The training process converged within approximately 20 epochs for all subjects. Thus, we chose 20 epochs for early stopping, as shown in Fig. 9. The total number of free parameters is 72 264 076.To determine the convergence process of the proposed framework, we investigated the evolution of the classification results during the training phase. The training process converged within approximately 20 epochs for all subjects. Thus, we chose 20 epochs for early stopping, as shown in Fig. 9. The total number of free parameters is 72 264 076.</p>
        <p>Our approach starts by utilizing multiple frequency bands. We believe that those frequency bands have discriminant brain features. Thus, we trained all frequency bands individually to obtain frequency-wise features through the convolutional layers. Then, we used the concatenation fusion technique and integrated all frequency-wise features, which transformed high-dimensional convolutional features into a vector with low dimensionality for parameter and dimension reduction [61], [62].Our approach starts by utilizing multiple frequency bands. We believe that those frequency bands have discriminant brain features. Thus, we trained all frequency bands individually to obtain frequency-wise features through the convolutional layers. Then, we used the concatenation fusion technique and integrated all frequency-wise features, which transformed high-dimensional convolutional features into a vector with low dimensionality for parameter and dimension reduction [61], [62].</p>
        <p>In the training environment (i.e., 53 subjects and 20 frequency bands), the time duration was 12 min for each individual subject. The testing session of the proposed method requires a duration of approximately 0.15 ms to classify a single trial. With the subject-independent approach, the BCI system is pretrained before being employed by a new subject. Therefore, the testing time is of more concern than the training time in the BCI system. The test time duration of other methods in the training (testing) phase was 48.18 s (0.051 s), 853.418 s (0.036 s), and 1049.6679 s (0.022 s) for pooled CSP, fused model, and MR FBCSP, respectively.In the training environment (i.e., 53 subjects and 20 frequency bands), the time duration was 12 min for each individual subject. The testing session of the proposed method requires a duration of approximately 0.15 ms to classify a single trial. With the subject-independent approach, the BCI system is pretrained before being employed by a new subject. Therefore, the testing time is of more concern than the training time in the BCI system. The test time duration of other methods in the training (testing) phase was 48.18 s (0.051 s), 853.418 s (0.036 s), and 1049.6679 s (0.022 s) for pooled CSP, fused model, and MR FBCSP, respectively.</p>
        <p>From the individual classification performance shown in Fig. 5, it can be seen that the proposed method would sacrifice a certain percentage of the excellent subjects' performance while significantly improving some moderate subjects' performance. We suggest that the reason is that the proposed method could construct generalizable features. Generally, those who showed a low classification performance should require a number of training samples to extract discriminant brain features. However, the experimental process for moderate users in a subject-dependent environment would end before acquiring many samples that can help them construct their own brain features. In this article, we provide a large number of training samples from a variety of subjects and various frequencies' information. This could be helpful in constructing generalizable features for moderate performers and improving their classification performance.From the individual classification performance shown in Fig. 5, it can be seen that the proposed method would sacrifice a certain percentage of the excellent subjects' performance while significantly improving some moderate subjects' performance. We suggest that the reason is that the proposed method could construct generalizable features. Generally, those who showed a low classification performance should require a number of training samples to extract discriminant brain features. However, the experimental process for moderate users in a subject-dependent environment would end before acquiring many samples that can help them construct their own brain features. In this article, we provide a large number of training samples from a variety of subjects and various frequencies' information. This could be helpful in constructing generalizable features for moderate performers and improving their classification performance.</p>
        <p>However, excellent subjects might have their own inherent brain features and thus might not need many training samples. In other words, excellent subjects might already have discriminative features. This means that generalizable brain features could deteriorate the classification performance of excellent subjects. In our opinion, this aspect is not a major concern, as we know that it is very difficult to improve the performance for moderate and poor subjects while it is much easier to achieve high performance for excellent subjects. For example, one can use the resting state of the brain signal to predict whether this subject is an excellent BCI user [63]. Once we know that a subject is an excellent BCI user, we can apply user-specific or adaptation methods with very few trials to refine the classifier, to achieve good classification performance [37], [39]. Furthermore, even for moderate BCI users, we can also apply adaptation methods based on a pretrained model. If initial value of the classifier parameters is stable and generalized, adaptation methods will be effective for those who are naive BCI users or experienced to create their own decoder for the long-term use of BCIs.However, excellent subjects might have their own inherent brain features and thus might not need many training samples. In other words, excellent subjects might already have discriminative features. This means that generalizable brain features could deteriorate the classification performance of excellent subjects. In our opinion, this aspect is not a major concern, as we know that it is very difficult to improve the performance for moderate and poor subjects while it is much easier to achieve high performance for excellent subjects. For example, one can use the resting state of the brain signal to predict whether this subject is an excellent BCI user [63]. Once we know that a subject is an excellent BCI user, we can apply user-specific or adaptation methods with very few trials to refine the classifier, to achieve good classification performance [37], [39]. Furthermore, even for moderate BCI users, we can also apply adaptation methods based on a pretrained model. If initial value of the classifier parameters is stable and generalized, adaptation methods will be effective for those who are naive BCI users or experienced to create their own decoder for the long-term use of BCIs.</p>
        <p>In current MI-based BCI, the intrasubject and intersubject variability of brain signal is one of the fundamental issues [23]- [27]. In a recent interesting study, they showed that a large number of BCI users have performance variations over both subjects and sessions [64]. In fact, many researchers have also raised similar questions in that there would be a high level of subject-wise signal variability on EEG signals when the participants perform the MI tasks [22]. Here, we believed that the variability of brain signals in individual subjects is inevitable; therefore, we should find out solutions to prevent performance degradation by the signal variability. To do this, we examined this issue as to how subject-wise signal variability affects performance degradation and how we can obtain better classification performance despite these signal variability.In current MI-based BCI, the intrasubject and intersubject variability of brain signal is one of the fundamental issues [23]- [27]. In a recent interesting study, they showed that a large number of BCI users have performance variations over both subjects and sessions [64]. In fact, many researchers have also raised similar questions in that there would be a high level of subject-wise signal variability on EEG signals when the participants perform the MI tasks [22]. Here, we believed that the variability of brain signals in individual subjects is inevitable; therefore, we should find out solutions to prevent performance degradation by the signal variability. To do this, we examined this issue as to how subject-wise signal variability affects performance degradation and how we can obtain better classification performance despite these signal variability.</p>
        <p>In Fig. 6, as more subjects were used for training, we could see that performance degradation by subject-wise signal variability was reduced (high mean accuracy and low variation). In Fig. 7, utilizing multiple frequency indices allows the model to achieve a robust result while preventing performance deterioration. Additionally, in Fig. 2, we can observe how the range of frequency bands can be key to obtaining robust performance despite the signal variability. Here, the rearranged frequency indices were chosen according to the value of the mutual information, and most rearranged indices were located on narrow frequency bands. This finding appears to indicate that narrow frequency bands are regions that can extract discriminate features and thus provide solid classification performance and further could even reduce subject-wise signal variability.In Fig. 6, as more subjects were used for training, we could see that performance degradation by subject-wise signal variability was reduced (high mean accuracy and low variation). In Fig. 7, utilizing multiple frequency indices allows the model to achieve a robust result while preventing performance deterioration. Additionally, in Fig. 2, we can observe how the range of frequency bands can be key to obtaining robust performance despite the signal variability. Here, the rearranged frequency indices were chosen according to the value of the mutual information, and most rearranged indices were located on narrow frequency bands. This finding appears to indicate that narrow frequency bands are regions that can extract discriminate features and thus provide solid classification performance and further could even reduce subject-wise signal variability.</p>
        <p>Beyond the MI area, other EEG-based studies have also focused on these signal variability in individual subjects.Beyond the MI area, other EEG-based studies have also focused on these signal variability in individual subjects.</p>
        <p>Reference [7] showed individual difference and subject variability of brain signal between the control and schizophrenia participants in the ERP task. Reference [65] showed clear individual difference of brain signal through visual and auditory ERP tasks. In summary, variability of brain signal in individual subjects is unavoidable and it is crucial to develop new algorithms to mitigate the performance deterioration and obtain robust performance.Reference [7] showed individual difference and subject variability of brain signal between the control and schizophrenia participants in the ERP task. Reference [65] showed clear individual difference of brain signal through visual and auditory ERP tasks. In summary, variability of brain signal in individual subjects is unavoidable and it is crucial to develop new algorithms to mitigate the performance deterioration and obtain robust performance.</p>
        <p>This article has several limitations that call for future investigations. First, the proposed method is based on the CNN structure, and there are several new architectures that are worthwhile to explore [61], [62]. These new architectures might be useful to further boost the performance. Second, we could also use other types of inputs, such as a frequencyby-time matrix or channel-by-time matrix, to allow the deep neural nets to learn the most discriminant features for classification. Third, we need a more informative visualization tool to interpret the outcome of a DL model, such as in [60] and [66], for a better understanding of the neurophysiological patterns of the MI. Finally, a fast adaptation method based on a pretrained model can be developed for the long-term use of BCI [37], [39].This article has several limitations that call for future investigations. First, the proposed method is based on the CNN structure, and there are several new architectures that are worthwhile to explore [61], [62]. These new architectures might be useful to further boost the performance. Second, we could also use other types of inputs, such as a frequencyby-time matrix or channel-by-time matrix, to allow the deep neural nets to learn the most discriminant features for classification. Third, we need a more informative visualization tool to interpret the outcome of a DL model, such as in [60] and [66], for a better understanding of the neurophysiological patterns of the MI. Finally, a fast adaptation method based on a pretrained model can be developed for the long-term use of BCI [37], [39].</p>
        <p>In this article, we proposed a subject-independent CNN framework for an MI-based BCI system. Spectral-spatial input generation is used to represent the general brain signal patterns from a large-scale MI database. The experimental results indicate that the proposed method significantly outperformed previous subject-independent as well as conventional subjectdependent approaches. In conclusion, this article demonstrates the superior performance and promising potential of the proposed feature representation coupled with a deep neural network method. This article could pave the way for a practical implementation of a subject-independent BCI.In this article, we proposed a subject-independent CNN framework for an MI-based BCI system. Spectral-spatial input generation is used to represent the general brain signal patterns from a large-scale MI database. The experimental results indicate that the proposed method significantly outperformed previous subject-independent as well as conventional subjectdependent approaches. In conclusion, this article demonstrates the superior performance and promising potential of the proposed feature representation coupled with a deep neural network method. This article could pave the way for a practical implementation of a subject-independent BCI.</p>
        <p>subject-independent approaches in the order of pooled-CSP, fused model, and MR FBCSP. The percentages of subjects who showed higher performance in the proposed method than in the other methods are 74.1% (40 of 54, p ≤ 0.001), 79.6% (43 of 54, p ≤ 0.001), 77.8% (42 of 54, p ≤ 0.001), 64.8% (35 of 54, p ≤ 0.001), 57.4% (31 of 54, p ≤ 0.001), and 51.9% (28 of 54, p ≤ 0.001) for pooled-CSP, fused model, MR FBCSP, CSP, FBCSP, and BSSFO, respectively. A paired t-test is used for the statistical analysis. Additionally, the percentages of subjects who achieved 80% classification performance in each method are 22.2% (12 of 54), 29.6% (16 of 54), 25.9% (14 of 54), 33.3% (18 of 54), 37.0% (20 of 54), 37.0% (20 of 54), and 44.0% (24 of 54) for pooled-CSP, fused model, MR FBCSP, CSP, FBCSP, BSSFO, and the proposed method, respectively.subject-independent approaches in the order of pooled-CSP, fused model, and MR FBCSP. The percentages of subjects who showed higher performance in the proposed method than in the other methods are 74.1% (40 of 54, p ≤ 0.001), 79.6% (43 of 54, p ≤ 0.001), 77.8% (42 of 54, p ≤ 0.001), 64.8% (35 of 54, p ≤ 0.001), 57.4% (31 of 54, p ≤ 0.001), and 51.9% (28 of 54, p ≤ 0.001) for pooled-CSP, fused model, MR FBCSP, CSP, FBCSP, and BSSFO, respectively. A paired t-test is used for the statistical analysis. Additionally, the percentages of subjects who achieved 80% classification performance in each method are 22.2% (12 of 54), 29.6% (16 of 54), 25.9% (14 of 54), 33.3% (18 of 54), 37.0% (20 of 54), 37.0% (20 of 54), and 44.0% (24 of 54) for pooled-CSP, fused model, MR FBCSP, CSP, FBCSP, BSSFO, and the proposed method, respectively.</p>
        <p>Figs.Figs.</p>
        <p>66</p>
        <p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/</p>
        <p>https://github.com/PatternRecognition/OpenBMIhttps://github.com/PatternRecognition/OpenBMI</p>
        <p>This work was supported in part by Institute for Information and Communications Technology Planning and Evaluation (IITP) Grant funded by the Korea Government (MSIT) (Development of Intelligent Pattern Recognition Softwares for Ambulatory Brain-Computer 
            <rs type="software">Interface</rs>) under Grant 2015-0-00185 and (Development of BCI based Brain and Cognitive Computing Technology for Recognizing User's Intentions using Deep Learning) under Grant 2017-0-00451 and in part by the Samsung Research Funding Center of Samsung Electronics under Project SRFC-TC1603-02.
        </p>
    </text>
</tei>

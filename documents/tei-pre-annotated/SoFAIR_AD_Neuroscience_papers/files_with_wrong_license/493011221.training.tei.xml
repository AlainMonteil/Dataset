<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:09+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>For many years, psychiatrists have tried to understand factors involved in response to medications or psychotherapies, in order to personalize their treatment choices. There is now a broad and growing interest in the idea that we can develop models to personalize treatment decisions using new statistical approaches from the field of machine learning and applying them to larger volumes of data. In this pursuit, there has been a paradigm shift away from experimental studies to confirm or refute specific hypotheses towards a focus on the overall explanatory power of a predictive model when tested on new, unseen datasets. In this paper, we review key studies using machine learning to predict treatment outcomes in psychiatry, ranging from medications and psychotherapies to digital interventions and neurobiological treatments. Next, we focus on some new sources of data that are being used for the development of predictive models based on machine learning, such as electronic health records, smartphone and social media data, and on the potential utility of data from genetics, electrophysiology, neuroimaging and cognitive testing. Finally, we discuss how far the field has come towards implementing prediction tools in real-world clinical practice. Relatively few retrospective studies to-date include appropriate external validation procedures, and there are even fewer prospective studies testing the clinical feasibility and effectiveness of predictive models. Applications of machine learning in psychiatry face some of the same ethical challenges posed by these techniques in other areas of medicine or computer science, which we discuss here. In short, machine learning is a nascent but important approach to improve the effectiveness of mental health care, and several prospective clinical studies suggest that it may be working already.For many years, psychiatrists have tried to understand factors involved in response to medications or psychotherapies, in order to personalize their treatment choices. There is now a broad and growing interest in the idea that we can develop models to personalize treatment decisions using new statistical approaches from the field of machine learning and applying them to larger volumes of data. In this pursuit, there has been a paradigm shift away from experimental studies to confirm or refute specific hypotheses towards a focus on the overall explanatory power of a predictive model when tested on new, unseen datasets. In this paper, we review key studies using machine learning to predict treatment outcomes in psychiatry, ranging from medications and psychotherapies to digital interventions and neurobiological treatments. Next, we focus on some new sources of data that are being used for the development of predictive models based on machine learning, such as electronic health records, smartphone and social media data, and on the potential utility of data from genetics, electrophysiology, neuroimaging and cognitive testing. Finally, we discuss how far the field has come towards implementing prediction tools in real-world clinical practice. Relatively few retrospective studies to-date include appropriate external validation procedures, and there are even fewer prospective studies testing the clinical feasibility and effectiveness of predictive models. Applications of machine learning in psychiatry face some of the same ethical challenges posed by these techniques in other areas of medicine or computer science, which we discuss here. In short, machine learning is a nascent but important approach to improve the effectiveness of mental health care, and several prospective clinical studies suggest that it may be working already.</p>
        <p>Treatment interventions in psychiatry are far from being ef fective in all cases in which they are indicated. In depression, for example, only 3050% of individuals achieve remission after what ever initial treatment they receive, even in the context of a well conducted clinical trial 1 . Eventually, after trying some number or combination of treatments, most patients do attain remission. What if, rather than iterating through the available treatments that a patient might benefit from, we could predict the right treatment for each individual from the start?Treatment interventions in psychiatry are far from being ef fective in all cases in which they are indicated. In depression, for example, only 3050% of individuals achieve remission after what ever initial treatment they receive, even in the context of a well conducted clinical trial 1 . Eventually, after trying some number or combination of treatments, most patients do attain remission. What if, rather than iterating through the available treatments that a patient might benefit from, we could predict the right treatment for each individual from the start?</p>
        <p>Researchers have wanted this for decades. Historically, they have tried to understand specific factors involved in treatment response based on theoretical groundings, leading to many stud ies focusing on single variables such as early childhood stress, suicidality, major life events, or comorbid diagnoses. Since then, the ongoing search for one (or a few) true explanatory variables has included many levels of analysis, including: the patient (clini cal characteristics, blood marker levels), his/her brain (structural and functional neuroimaging, cerebral blood flow, scalp electri cal recordings), his/her genes (single nucleotide polymorphisms, mutations/rare genetic variants, copy number variations, gene expression), and intervention characteristics (the medication or psychotherapy selected, the way it was delivered, the provider, the therapeutic alliance). If one variable alone could accurately predict treatment response, our field would probably have found it by now. Instead, most characteristics identified so far have shown small explanatory power over treatment outcomes, and researchers' attention naturally turned towards multivariable models that can incorporate many smaller effects.Researchers have wanted this for decades. Historically, they have tried to understand specific factors involved in treatment response based on theoretical groundings, leading to many stud ies focusing on single variables such as early childhood stress, suicidality, major life events, or comorbid diagnoses. Since then, the ongoing search for one (or a few) true explanatory variables has included many levels of analysis, including: the patient (clini cal characteristics, blood marker levels), his/her brain (structural and functional neuroimaging, cerebral blood flow, scalp electri cal recordings), his/her genes (single nucleotide polymorphisms, mutations/rare genetic variants, copy number variations, gene expression), and intervention characteristics (the medication or psychotherapy selected, the way it was delivered, the provider, the therapeutic alliance). If one variable alone could accurately predict treatment response, our field would probably have found it by now. Instead, most characteristics identified so far have shown small explanatory power over treatment outcomes, and researchers' attention naturally turned towards multivariable models that can incorporate many smaller effects.</p>
        <p>Machine learning is a collection of statistical tools and ap proaches that are extremely well suited to this goal of detecting and aggregating small effects in order to predict an outcome of interest 2 . It allows researchers to go from evaluating a small num ber (~10) of predictor variables to many hundreds or thousands of variables or variable combinations. There are many potential pitfalls when applying these techniques, but, when implemented well, they afford many opportunities for psychiatric research 3,4 . They allow us to examine many variables, even correlated ones, simultaneously. They move away from exclusively additive mod els and allow us to identify more complex nonlinear patterns in data. They more naturally bridge disparate data types, potentially incorporating clinical assessments, geospatial information, and biological findings into a single analysis. By unlocking powerful hypothesisfree approaches, they enable us to discover factors that are less intuitive but nonetheless predictive of outcomes.Machine learning is a collection of statistical tools and ap proaches that are extremely well suited to this goal of detecting and aggregating small effects in order to predict an outcome of interest 2 . It allows researchers to go from evaluating a small num ber (~10) of predictor variables to many hundreds or thousands of variables or variable combinations. There are many potential pitfalls when applying these techniques, but, when implemented well, they afford many opportunities for psychiatric research 3,4 . They allow us to examine many variables, even correlated ones, simultaneously. They move away from exclusively additive mod els and allow us to identify more complex nonlinear patterns in data. They more naturally bridge disparate data types, potentially incorporating clinical assessments, geospatial information, and biological findings into a single analysis. By unlocking powerful hypothesisfree approaches, they enable us to discover factors that are less intuitive but nonetheless predictive of outcomes.</p>
        <p>The introduction of machine learning in psychiatry is more than just adding an analysis tool for combining and exploring big ger data sets -it marks a paradigm shift 5 . For years, we used classi cal statistical approaches to confirm or refute specific hypotheses. Now, machine learning studies shift the focus toward the overall predictive power of a model, particularly how accurately it predicts the desired outcome in a new, unseen dataset. Studies in this field are evaluated primarily by their potential clinical impact: what our model can reliably tell us about the prognosis of new patients in the future, and what we can do with that information to improve clinical practice.The introduction of machine learning in psychiatry is more than just adding an analysis tool for combining and exploring big ger data sets -it marks a paradigm shift 5 . For years, we used classi cal statistical approaches to confirm or refute specific hypotheses. Now, machine learning studies shift the focus toward the overall predictive power of a model, particularly how accurately it predicts the desired outcome in a new, unseen dataset. Studies in this field are evaluated primarily by their potential clinical impact: what our model can reliably tell us about the prognosis of new patients in the future, and what we can do with that information to improve clinical practice.</p>
        <p>With this in mind, this paper explores the promise of machine learning in predicting treatment outcomes in psychiatry. There are many things that we do not focus on. This is not a primer on ma chine learning 6 , an explanation of how it works 2 , or a debate about what counts as machine learning versus traditional statistics or "nonmachinelearning". We do not explain how to build predic tive models 7 or how to validate them. We are not formally com paring different algorithmic approaches, how each one works, or circumstances where one may be more appropriate than another. We also avoid a distinction between moderators versus mediators of treatment outcomes, or whether a model predicts outcomes specifically for a treatment versus others or predicts outcomes more generically for multiple treatments 8 . Finally, we do not aim to review the many sociodemographic and clinical variables that have been or can be used for prediction of treatment response in psychiatry, which generally have the most predictive power and are cheapest to collect 9,10 .With this in mind, this paper explores the promise of machine learning in predicting treatment outcomes in psychiatry. There are many things that we do not focus on. This is not a primer on ma chine learning 6 , an explanation of how it works 2 , or a debate about what counts as machine learning versus traditional statistics or "nonmachinelearning". We do not explain how to build predic tive models 7 or how to validate them. We are not formally com paring different algorithmic approaches, how each one works, or circumstances where one may be more appropriate than another. We also avoid a distinction between moderators versus mediators of treatment outcomes, or whether a model predicts outcomes specifically for a treatment versus others or predicts outcomes more generically for multiple treatments 8 . Finally, we do not aim to review the many sociodemographic and clinical variables that have been or can be used for prediction of treatment response in psychiatry, which generally have the most predictive power and are cheapest to collect 9,10 .</p>
        <p>We begin by discussing machine learning methods, how they compare to traditional statistical approaches, and to what extent is machine learning specifically adding value. Next, we provide an overview of the interventions for which researchers have tried to use machine learning methods to predict outcomes, ranging from medications and psychotherapy to digital interventions and neurobiological treatments. In doing so, we highlight characteris tics that made them gold standard examples, and discuss the dif ferent goals that can be achieved in each context. Next, we focus on the potential utility of electronic health records, smartphone and social media data, and of data from genetics, electrophysiol ogy, neuroimaging and cognitive testing for the development of predictive models based on machine learning. Finally, we help the reader understand the broader context: how close have we come to implementing these prediction tools in realworld clini cal practice; and what are the ethical challenges that these tools carry. The intent of this paper is to review studies throughout psy chiatry; any emphasis on depression is not intentional, but it does reflect the fact that the majority of research in this field has been conducted in people with that mental disorder.We begin by discussing machine learning methods, how they compare to traditional statistical approaches, and to what extent is machine learning specifically adding value. Next, we provide an overview of the interventions for which researchers have tried to use machine learning methods to predict outcomes, ranging from medications and psychotherapy to digital interventions and neurobiological treatments. In doing so, we highlight characteris tics that made them gold standard examples, and discuss the dif ferent goals that can be achieved in each context. Next, we focus on the potential utility of electronic health records, smartphone and social media data, and of data from genetics, electrophysiol ogy, neuroimaging and cognitive testing for the development of predictive models based on machine learning. Finally, we help the reader understand the broader context: how close have we come to implementing these prediction tools in realworld clini cal practice; and what are the ethical challenges that these tools carry. The intent of this paper is to review studies throughout psy chiatry; any emphasis on depression is not intentional, but it does reflect the fact that the majority of research in this field has been conducted in people with that mental disorder.</p>
        <p>Machine learning studies generally differ from traditional re search in two ways. The first is a focus on prediction (explanatory power of the model) rather than inference (hypothesis testing). The second is a shift towards model flexibility, with the ability to handle large numbers of predictors simultaneously.Machine learning studies generally differ from traditional re search in two ways. The first is a focus on prediction (explanatory power of the model) rather than inference (hypothesis testing). The second is a shift towards model flexibility, with the ability to handle large numbers of predictors simultaneously.</p>
        <p>Prediction can be performed without machine learning al gorithms, and many studies still use traditional statistical tech niques such as logistic regression. In fact, when assumptions and sample size requirements are reasonably met, the number of predictors is small (≤25), and nonlinear effects are relative ly weak, traditional parametric models will likely predict well. Several studies found no benefit of machine learning over tra ditional logistic regression, for example in predicting treatment resistance in major depression 11 , brain injury outcomes 12 , or ma jor chronic diseases 13 .Prediction can be performed without machine learning al gorithms, and many studies still use traditional statistical tech niques such as logistic regression. In fact, when assumptions and sample size requirements are reasonably met, the number of predictors is small (≤25), and nonlinear effects are relative ly weak, traditional parametric models will likely predict well. Several studies found no benefit of machine learning over tra ditional logistic regression, for example in predicting treatment resistance in major depression 11 , brain injury outcomes 12 , or ma jor chronic diseases 13 .</p>
        <p>One recent systematic review of clinical prediction models found no difference in performance between machine learning and logistic regression 14 , although the authors considered in the category of logistic regression some advanced frameworks that could be included within machine learning, such as penalization (e.g., lasso, ridge or elastic net) and splines (which capture non linearities). In areas of medicine such as diabetes and heart fail ure, simple logistic models have performed well and have been externally validated more than machine learning models 15,16 .One recent systematic review of clinical prediction models found no difference in performance between machine learning and logistic regression 14 , although the authors considered in the category of logistic regression some advanced frameworks that could be included within machine learning, such as penalization (e.g., lasso, ridge or elastic net) and splines (which capture non linearities). In areas of medicine such as diabetes and heart fail ure, simple logistic models have performed well and have been externally validated more than machine learning models 15,16 .</p>
        <p>The added value of machine learning approaches emerges when the number of potential predictors is large and/or their effects are nonlinear. Many machine learning algorithms are capable of handling large numbers of predictors, even in cases where there are more predictor variables than observations, due to builtin overfitting control. For example, ridge, lasso and elas tic net regression 17 include penalization, which forces the regres sion coefficients to be closer to zero than in the traditional linear or logistic regression models. Machine learning approaches are also good at capturing complex, interactive, or nonlinear effects. For example, treebased models are able to evaluate many pos sible variables and variable combinations to identify subgroups that could not be captured by traditional linear models. Another common technique adopted by machine learning approaches is "ensembling". Here, several models are fitted on random sam ples of the original dataset, and then an average is taken amongst the predictions from each model. This approach is a key element of many popular machine learning techniques today, especially gradient boosting machines and random forests 1820 .The added value of machine learning approaches emerges when the number of potential predictors is large and/or their effects are nonlinear. Many machine learning algorithms are capable of handling large numbers of predictors, even in cases where there are more predictor variables than observations, due to builtin overfitting control. For example, ridge, lasso and elas tic net regression 17 include penalization, which forces the regres sion coefficients to be closer to zero than in the traditional linear or logistic regression models. Machine learning approaches are also good at capturing complex, interactive, or nonlinear effects. For example, treebased models are able to evaluate many pos sible variables and variable combinations to identify subgroups that could not be captured by traditional linear models. Another common technique adopted by machine learning approaches is "ensembling". Here, several models are fitted on random sam ples of the original dataset, and then an average is taken amongst the predictions from each model. This approach is a key element of many popular machine learning techniques today, especially gradient boosting machines and random forests 1820 .</p>
        <p>Several recent treatment outcome prediction studies in psy chiatry demonstrated the added value of machine learning. Ran dom forests and/or elastic net regression 2124 , as well as support vector machines 25 , were found to outperform traditional regres sion methods. Largescale comparisons on benchmark datasets consistently found machine learning to outperform traditional methods 2629 . Overall, boosted trees (random forests and gradient boosting machines), regularized regression, support vector ma chines, and artificial neural networks can all perform well, but no one method will have the best performance across all situations.Several recent treatment outcome prediction studies in psy chiatry demonstrated the added value of machine learning. Ran dom forests and/or elastic net regression 2124 , as well as support vector machines 25 , were found to outperform traditional regres sion methods. Largescale comparisons on benchmark datasets consistently found machine learning to outperform traditional methods 2629 . Overall, boosted trees (random forests and gradient boosting machines), regularized regression, support vector ma chines, and artificial neural networks can all perform well, but no one method will have the best performance across all situations.</p>
        <p>While researchers typically aim to maximize predictive per formance, practical aspects such as explainability or the cost of including more variables should also be considered. In some cases, simpler models with slightly lower predictive accuracy or higher generalizability might be preferred, because they already capture most of the effects 30,31 . There is no silver bullet in statis tics, and all prediction algorithms face the socalled biasvar iance tradeoff 2,32,33 , where flexibility needs to be balanced with the risk of overfitting. For machine learning methods to capture increasingly complex effects, much larger sample sizes are still needed. Although these methods can deal with large numbers of potential predictor variables, careful preselection of variables likely improves predictive accuracy.While researchers typically aim to maximize predictive per formance, practical aspects such as explainability or the cost of including more variables should also be considered. In some cases, simpler models with slightly lower predictive accuracy or higher generalizability might be preferred, because they already capture most of the effects 30,31 . There is no silver bullet in statis tics, and all prediction algorithms face the socalled biasvar iance tradeoff 2,32,33 , where flexibility needs to be balanced with the risk of overfitting. For machine learning methods to capture increasingly complex effects, much larger sample sizes are still needed. Although these methods can deal with large numbers of potential predictor variables, careful preselection of variables likely improves predictive accuracy.</p>
        <p>While traditional research approaches focused on p values for specific coefficients in a model, prediction studies focus on the overall explanatory power of the model, often in terms of R 2 , bal anced accuracy, or area under the receiver operating characteristic curve (AUC). Predictive studies require a keen focus on validation approaches, to examine whether the model is learning patterns that are substantive and consistent from one dataset to another, or whether the model has simply learned idiosyncrasies of the initial training data. Table 1 discusses various kinds of validation that are conducted in predictive studies, from internal approaches that use just one dataset, to external validation approaches that use data from independent sites, studies, trials, countries, or consortia to test model generalizability. Validation frameworks, especially ex ternal validation, are critical for developing models that are reli able and useful, and understanding whether the fitted model is likely to generalize to unseen data in the future 3436 .While traditional research approaches focused on p values for specific coefficients in a model, prediction studies focus on the overall explanatory power of the model, often in terms of R 2 , bal anced accuracy, or area under the receiver operating characteristic curve (AUC). Predictive studies require a keen focus on validation approaches, to examine whether the model is learning patterns that are substantive and consistent from one dataset to another, or whether the model has simply learned idiosyncrasies of the initial training data. Table 1 discusses various kinds of validation that are conducted in predictive studies, from internal approaches that use just one dataset, to external validation approaches that use data from independent sites, studies, trials, countries, or consortia to test model generalizability. Validation frameworks, especially ex ternal validation, are critical for developing models that are reli able and useful, and understanding whether the fitted model is likely to generalize to unseen data in the future 3436 .</p>
        <p>Predicting treatment outcomes for psychiatric medications is the most active area of research in the field, primarily because they were the easiest place to start. Machine learning studies require large volumes of data to build predictive models, ideally with clearly labelled outcomes, control over the intervention, and relevant data about the patients before treatment. Since this describes most large clinical trials, and most large clinical trials in psychiatry are con ducted to evaluate efficacy of a medication, most machine learning efforts began by investigating treatment responses to medications treating depression, schizophrenia or bipolar disorder.Predicting treatment outcomes for psychiatric medications is the most active area of research in the field, primarily because they were the easiest place to start. Machine learning studies require large volumes of data to build predictive models, ideally with clearly labelled outcomes, control over the intervention, and relevant data about the patients before treatment. Since this describes most large clinical trials, and most large clinical trials in psychiatry are con ducted to evaluate efficacy of a medication, most machine learning efforts began by investigating treatment responses to medications treating depression, schizophrenia or bipolar disorder.</p>
        <p>These studies mostly used information from demographic in take forms and clinical symptom scales common in clinical trials, although more recently genetic and neuroimaging data have also been incorporated (discussed later in this paper). Despite being the most active area of research, most resulting models have not yet been validated in external samples. Relatively few prediction tools generated by mental health researchers so far have advanced through implementation studies and into clinical practice 3739 . Here we focus on examples of studies that were adequately pow ered, underwent external validation, or are notable for other rea sons.These studies mostly used information from demographic in take forms and clinical symptom scales common in clinical trials, although more recently genetic and neuroimaging data have also been incorporated (discussed later in this paper). Despite being the most active area of research, most resulting models have not yet been validated in external samples. Relatively few prediction tools generated by mental health researchers so far have advanced through implementation studies and into clinical practice 3739 . Here we focus on examples of studies that were adequately pow ered, underwent external validation, or are notable for other rea sons.</p>
        <p>Most treatment prediction studies have focused on antide pressants commonly used in the acute phase of depression. For example, Chekroud et al 40 determined a small group of 25 pre treatment variables that were most predictive of remission with citalopram in the Sequenced Alternative Treatments for Depres sion (STAR*D) trial. This model achieved an accuracy of 64.6%. The model was then applied to data from another clinical trial to examine whether it can generalize to patients from an entirely in dependent population. The model was able to predict response to two similar antidepressant regimens (escitalopram plus pla cebo, and escitalopram plus bupropion, each with an accuracy of around 60%), but the model did not predict remission better than chance for patients who took venlafaxine plus mirtazapine (51%).Most treatment prediction studies have focused on antide pressants commonly used in the acute phase of depression. For example, Chekroud et al 40 determined a small group of 25 pre treatment variables that were most predictive of remission with citalopram in the Sequenced Alternative Treatments for Depres sion (STAR*D) trial. This model achieved an accuracy of 64.6%. The model was then applied to data from another clinical trial to examine whether it can generalize to patients from an entirely in dependent population. The model was able to predict response to two similar antidepressant regimens (escitalopram plus pla cebo, and escitalopram plus bupropion, each with an accuracy of around 60%), but the model did not predict remission better than chance for patients who took venlafaxine plus mirtazapine (51%).</p>
        <p>The five most important variables identified by the model in predicting remission were baseline depression severity, employThe five most important variables identified by the model in predicting remission were baseline depression severity, employ</p>
        <p>None, p value testing The entire sample is used to predict an outcome, and a p value indicates the probability of obtaining the result in the absence of a true effect. The study cannot make any claims concerning translation or generalizability because they have not been tested.None, p value testing The entire sample is used to predict an outcome, and a p value indicates the probability of obtaining the result in the absence of a true effect. The study cannot make any claims concerning translation or generalizability because they have not been tested.</p>
        <p>One subject is randomly chosen and left out. A model is trained on the remaining subjects and applied to the left-out subject to assess generalizability. This procedure is repeated for every subject in the dataset. This is the simplest form of cross-validation. It produces optimistic biased results.One subject is randomly chosen and left out. A model is trained on the remaining subjects and applied to the left-out subject to assess generalizability. This procedure is repeated for every subject in the dataset. This is the simplest form of cross-validation. It produces optimistic biased results.</p>
        <p>The sample is randomly divided into subsamples (called "folds"). One fold is left out and statistical models are trained on the remaining subjects. The models are applied to the subjects in the left-out fold to assess generalizability. This is a common technique to reduce overfitting. However, when the data are from one sample (even if collected at multiple sites), generalizability claims need to be tempered.The sample is randomly divided into subsamples (called "folds"). One fold is left out and statistical models are trained on the remaining subjects. The models are applied to the subjects in the left-out fold to assess generalizability. This is a common technique to reduce overfitting. However, when the data are from one sample (even if collected at multiple sites), generalizability claims need to be tempered.</p>
        <p>Leave-one-site-out cross-validation Instead of randomly leaving out subjects, sites are now randomly left out. Models are fitted on the remaining sites, and applied to the left-out site. This assesses cross-site generalizability, and the same technique can be extended to any other group definition, such as blocks of time, gender or ethnicity. Generalizability and translational claims still need to be tempered.Leave-one-site-out cross-validation Instead of randomly leaving out subjects, sites are now randomly left out. Models are fitted on the remaining sites, and applied to the left-out site. This assesses cross-site generalizability, and the same technique can be extended to any other group definition, such as blocks of time, gender or ethnicity. Generalizability and translational claims still need to be tempered.</p>
        <p>A model is created in one study and applied to a completely separate sample. This approach reflects a high degree of generalizability capacity. Demonstrations can be increasingly close to real-life circumstances, which strengthens the evidence of generalizability and translational potential (but does not guarantee it). The approach may still be subject to poor sociodemographic representation, sampling biases, or study designs that do not reflect clinical reality.A model is created in one study and applied to a completely separate sample. This approach reflects a high degree of generalizability capacity. Demonstrations can be increasingly close to real-life circumstances, which strengthens the evidence of generalizability and translational potential (but does not guarantee it). The approach may still be subject to poor sociodemographic representation, sampling biases, or study designs that do not reflect clinical reality.</p>
        <p>A previously-created model is evaluated in a prospective study that is ideally randomized and in conditions as close to clinical reality as possible, in order to test whether the tool is safe and effective in practice. Prospective validation studies are still susceptible to the same concerns around external validity as all other clinical trials (e.g., participant compensation and meaningful endpoints), and require large sample sizes, a broad and unbiased recruitment process, and good clinical practices. As with other clinical trials, a phased process may be necessary to first evaluate feasibility and safety in a smaller sample before proceeding to broad evaluation of effectiveness.A previously-created model is evaluated in a prospective study that is ideally randomized and in conditions as close to clinical reality as possible, in order to test whether the tool is safe and effective in practice. Prospective validation studies are still susceptible to the same concerns around external validity as all other clinical trials (e.g., participant compensation and meaningful endpoints), and require large sample sizes, a broad and unbiased recruitment process, and good clinical practices. As with other clinical trials, a phased process may be necessary to first evaluate feasibility and safety in a smaller sample before proceeding to broad evaluation of effectiveness.</p>
        <p>ment status, feeling restless during the past seven days (psycho motor agitation), reduced energy level during the past seven days, and Black or African American ethnicity. The study was later replicated by Nie et al 41 , who similarly trained a model to predict citalopram treatment outcomes using information easily obtainable at baseline. The team trained and tested the model in the STAR*D dataset and validated it in data from a different openlabel citalopram trial, using 22 predictor variables that overlapped between the two trials. Despite minor differences de pending on the specific algorithm used, the balanced accuracy of the models was roughly 6467%. An earlier study by Perlis 11 showed that eventual treatment resistance might also be predictable from the outset. The author developed a model using STAR*D data that was able to predict at baseline whether an individual would not reach remission after two antidepressant treatment trials, with an AUC of 0.71. Early proofs of concept like the Perlis study did not include external validation, at least partly due to the lack of independent datasets with similar trial designs that could be used for that validation.ment status, feeling restless during the past seven days (psycho motor agitation), reduced energy level during the past seven days, and Black or African American ethnicity. The study was later replicated by Nie et al 41 , who similarly trained a model to predict citalopram treatment outcomes using information easily obtainable at baseline. The team trained and tested the model in the STAR*D dataset and validated it in data from a different openlabel citalopram trial, using 22 predictor variables that overlapped between the two trials. Despite minor differences de pending on the specific algorithm used, the balanced accuracy of the models was roughly 6467%. An earlier study by Perlis 11 showed that eventual treatment resistance might also be predictable from the outset. The author developed a model using STAR*D data that was able to predict at baseline whether an individual would not reach remission after two antidepressant treatment trials, with an AUC of 0.71. Early proofs of concept like the Perlis study did not include external validation, at least partly due to the lack of independent datasets with similar trial designs that could be used for that validation.</p>
        <p>The above antidepressant studies selected predictors in a purely datadriven way, including all data that could be extracted at baseline and then using machine learning methods that dis card irrelevant information or are amenable to including many variables at once. However, the choice of predictors is not always hypothesisfree, and a priori knowledge from scientific literature can also guide the choice of variables and yield useful results. In iesta et al 42 aimed to predict remission of depression in patients treated with escitalopram or nortriptyline using only variables that had previously been confirmed as individual predictors or moderators of response to treatment. Their models predicted overall response to medication with an AUC of 0.74 and response to escitalopram with an AUC of 0.75, but prediction of nortrip tyline outcomes was not statistically significant. In subsequent work incorporating genetic data to the models 43 , these authors predicted response to escitalopram and nortriptyline with an AUC of 0.77.The above antidepressant studies selected predictors in a purely datadriven way, including all data that could be extracted at baseline and then using machine learning methods that dis card irrelevant information or are amenable to including many variables at once. However, the choice of predictors is not always hypothesisfree, and a priori knowledge from scientific literature can also guide the choice of variables and yield useful results. In iesta et al 42 aimed to predict remission of depression in patients treated with escitalopram or nortriptyline using only variables that had previously been confirmed as individual predictors or moderators of response to treatment. Their models predicted overall response to medication with an AUC of 0.74 and response to escitalopram with an AUC of 0.75, but prediction of nortrip tyline outcomes was not statistically significant. In subsequent work incorporating genetic data to the models 43 , these authors predicted response to escitalopram and nortriptyline with an AUC of 0.77.</p>
        <p>A second use of machine learning to predict medication out comes is to better define subgroups of patients, symptoms, or symptom trajectories, and then use these subgroups to make more nuanced predictions. Drysdale et al 44 used clustering to identify four "subtypes", or groups, amongst 1,188 depressed patients based on patterns of dysfunctional connectivity in lim bic and frontostriatal networks. They developed classifiers for each depressive subtype using support vector machines and later tested these models on an independent dataset, accurately classifying 86.2% of the testing sample. As a next step, the team used the subtypes to predict response to transcranial magnetic stimulation, but did not validate these predictions in any inde pendent sample. Although the biotypes approach is interesting, subsequent methodological research has highlighted concerns and limitations 45 .A second use of machine learning to predict medication out comes is to better define subgroups of patients, symptoms, or symptom trajectories, and then use these subgroups to make more nuanced predictions. Drysdale et al 44 used clustering to identify four "subtypes", or groups, amongst 1,188 depressed patients based on patterns of dysfunctional connectivity in lim bic and frontostriatal networks. They developed classifiers for each depressive subtype using support vector machines and later tested these models on an independent dataset, accurately classifying 86.2% of the testing sample. As a next step, the team used the subtypes to predict response to transcranial magnetic stimulation, but did not validate these predictions in any inde pendent sample. Although the biotypes approach is interesting, subsequent methodological research has highlighted concerns and limitations 45 .</p>
        <p>Chekroud et al 46 used clustering to identify groups of symp toms and mixedeffects regression to determine if they had different response trajectories. Three symptom clusters (core emotional, sleep and atypical) emerged consistently from two in dependent medication trials -STAR*D and Combining Medica tions to Enhance Depression Outcomes (COMED) -across two commonly used symptom scales. The authors subsequently used data from STAR*D to train gradient boosting machines (one for each combination of cluster and medication arm), finding mod est improvements in the ability of clusters of symptoms to predict total severity outcomes. The same symptom clustering approach was also effective in a study of treatments for adolescents 47 .Chekroud et al 46 used clustering to identify groups of symp toms and mixedeffects regression to determine if they had different response trajectories. Three symptom clusters (core emotional, sleep and atypical) emerged consistently from two in dependent medication trials -STAR*D and Combining Medica tions to Enhance Depression Outcomes (COMED) -across two commonly used symptom scales. The authors subsequently used data from STAR*D to train gradient boosting machines (one for each combination of cluster and medication arm), finding mod est improvements in the ability of clusters of symptoms to predict total severity outcomes. The same symptom clustering approach was also effective in a study of treatments for adolescents 47 .</p>
        <p>Other researchers first used techniques like growth mixture modeling 48 or finite mixture modeling 49 to identify trajectories of symptom response such as "fast and stable remitter", "sus tained response", or "late relapse". Machine learning models were then developed to try and predict the specific response tra jectory a patient will have for a given treatment. This approach is potentially more robust to the noise that is naturally present amongst individual patient trajectories and less affected by the way that outcomes are defined in trials -e.g., whether remission is defined as a score of 5 on the Patient Health Questionnaire9 (PHQ9) or a score of 5 or 6 on the Quick Inventory of Depressive Symptomatology (QIDS) 48,49 . However, the approach relies on the availability of repeated measures.Other researchers first used techniques like growth mixture modeling 48 or finite mixture modeling 49 to identify trajectories of symptom response such as "fast and stable remitter", "sus tained response", or "late relapse". Machine learning models were then developed to try and predict the specific response tra jectory a patient will have for a given treatment. This approach is potentially more robust to the noise that is naturally present amongst individual patient trajectories and less affected by the way that outcomes are defined in trials -e.g., whether remission is defined as a score of 5 on the Patient Health Questionnaire9 (PHQ9) or a score of 5 or 6 on the Quick Inventory of Depressive Symptomatology (QIDS) 48,49 . However, the approach relies on the availability of repeated measures.</p>
        <p>Medication treatment outcomes have been most widely stud ied in depression, due to the prevalence of the condition and extant available data, but the approach has also been proven in other psychiatric conditions. For schizophrenia, Koutsouleris et al 25 used data from the European First Episode Schizophrenia Trial (EUFEST, N=344) to predict good and bad outcomes based on global functioning scores over time using a support vector machine, and validated the ten most predictive features on an unseen sample of 108 patients with a balanced accuracy of 71.7%. The most valuable predictors identified were largely psychosocial variables, rather than symptom data: unemployment, poor edu cation, functional deficits, and unmet psychosocial needs.Medication treatment outcomes have been most widely stud ied in depression, due to the prevalence of the condition and extant available data, but the approach has also been proven in other psychiatric conditions. For schizophrenia, Koutsouleris et al 25 used data from the European First Episode Schizophrenia Trial (EUFEST, N=344) to predict good and bad outcomes based on global functioning scores over time using a support vector machine, and validated the ten most predictive features on an unseen sample of 108 patients with a balanced accuracy of 71.7%. The most valuable predictors identified were largely psychosocial variables, rather than symptom data: unemployment, poor edu cation, functional deficits, and unmet psychosocial needs.</p>
        <p>Again in schizophrenia, Leighton et al 50 were not only success ful in predicting response to medication treatment in first episode psychosis, but also in validating findings in two independent samples. They first identified predictors that were available across three studies -the Evaluating the Development and Impact of Early Intervention Services (EDEN) study in England, two cohorts recruited from the National Health Service (NHS) in Scotland, and the Danish clinical trial called OPUS. This allowed them to build and test harmonized models across the three studies to predict four outcomes capturing different aspects of recovery: symptom remission, social recovery, vocational recovery and quality of life. Next, they used logistic regression with elastic net regularization to identify the most relevant predictors in the EDEN study (N=1027) -much like Chekroud et al 40 -to determine a smaller subset of variables that could still predict outcomes but require less effort for future data collection and improve clinical applicability. These regularized models trained in the EDEN sample reached internal validation AUCs of 0.70 to 0.74 (depending on the outcome meas ure). When tested in the second Scottish cohort, the AUC ranged from 0.68 to 0.87. In the OPUS trial, it ranged from 0.57 to 0.68.Again in schizophrenia, Leighton et al 50 were not only success ful in predicting response to medication treatment in first episode psychosis, but also in validating findings in two independent samples. They first identified predictors that were available across three studies -the Evaluating the Development and Impact of Early Intervention Services (EDEN) study in England, two cohorts recruited from the National Health Service (NHS) in Scotland, and the Danish clinical trial called OPUS. This allowed them to build and test harmonized models across the three studies to predict four outcomes capturing different aspects of recovery: symptom remission, social recovery, vocational recovery and quality of life. Next, they used logistic regression with elastic net regularization to identify the most relevant predictors in the EDEN study (N=1027) -much like Chekroud et al 40 -to determine a smaller subset of variables that could still predict outcomes but require less effort for future data collection and improve clinical applicability. These regularized models trained in the EDEN sample reached internal validation AUCs of 0.70 to 0.74 (depending on the outcome meas ure). When tested in the second Scottish cohort, the AUC ranged from 0.68 to 0.87. In the OPUS trial, it ranged from 0.57 to 0.68.</p>
        <p>Predicting medication response in other mental disorders is still in early stages. Two studies 51,52 used baseline sociode mographic, clinical and family history information to predict response to medications commonly used in bipolar disorder: lithium and quetiapine. Although both obtained models with performance above chance, neither was validated in independ ent samples, and one used 180 variables for prediction 51 , which limits its clinical applicability.Predicting medication response in other mental disorders is still in early stages. Two studies 51,52 used baseline sociode mographic, clinical and family history information to predict response to medications commonly used in bipolar disorder: lithium and quetiapine. Although both obtained models with performance above chance, neither was validated in independ ent samples, and one used 180 variables for prediction 51 , which limits its clinical applicability.</p>
        <p>Historically, efforts to predict treatment outcomes in psycho therapies have focused on theoreticallymotivated single vari ables that might moderate treatment outcomes. Only relatively recently have psychotherapy researchers applied machine learn ing approaches to predict treatment outcomes 53 . Even amongst these studies, the historical focus on moderators of psychothera peutic effects has persisted, leading researchers to distinguish between "prognostic" and "prescriptive" models. Prognostic models are those that predict whether a patient will recover with a given treatment. Prescriptive models instead predict which of two (or more) treatments is best suited for a particular patient 54 . Both kinds of model can clearly have clinical utility, even if they answer slightly different questions. The differences continue to blur further with more recent attempts to build prescriptive models by developing multiple prognostic models for different treatments and then comparing their outputs 55 .Historically, efforts to predict treatment outcomes in psycho therapies have focused on theoreticallymotivated single vari ables that might moderate treatment outcomes. Only relatively recently have psychotherapy researchers applied machine learn ing approaches to predict treatment outcomes 53 . Even amongst these studies, the historical focus on moderators of psychothera peutic effects has persisted, leading researchers to distinguish between "prognostic" and "prescriptive" models. Prognostic models are those that predict whether a patient will recover with a given treatment. Prescriptive models instead predict which of two (or more) treatments is best suited for a particular patient 54 . Both kinds of model can clearly have clinical utility, even if they answer slightly different questions. The differences continue to blur further with more recent attempts to build prescriptive models by developing multiple prognostic models for different treatments and then comparing their outputs 55 .</p>
        <p>In an early effort, Lutz et al 53 used nearest neighbor modeling to predict rate of symptom change and sessionbysession varia bility. Models were based on age, gender and baseline symptom scores. Compared to nonmachine learning models, the nearest neighbor predictions were more highly correlated with actual values of rate of change, but not sessionbysession variability.In an early effort, Lutz et al 53 used nearest neighbor modeling to predict rate of symptom change and sessionbysession varia bility. Models were based on age, gender and baseline symptom scores. Compared to nonmachine learning models, the nearest neighbor predictions were more highly correlated with actual values of rate of change, but not sessionbysession variability.</p>
        <p>Since then, other approaches to prediction in psychotherapy proliferated. DeRubeis et al 56 developed a multivariable mod eling method, known as the "personalized advantage index" (PAI), that uses interaction effects between baseline variables and treatment condition, to predict whether a patient will re spond better to antidepressants versus cognitive behavioral therapy (CBT). Amongst their small sample of 154 individuals, a clinically meaningful advantage (PAI ≥3), favoring one of the treatments relative to the other, was predicted for 60% of the pa tients. When these patients were divided into those randomly as signed to their "optimal" treatment versus those assigned to their "nonoptimal" treatment, outcomes in the former group were better (d = 0.58, 95% CI: 0.171.01). Similar approaches have been developed by other groups 55,58 , and more recently improved fur ther by the use of machine learning approaches 59 to generate better predictions and incorporate more variables.Since then, other approaches to prediction in psychotherapy proliferated. DeRubeis et al 56 developed a multivariable mod eling method, known as the "personalized advantage index" (PAI), that uses interaction effects between baseline variables and treatment condition, to predict whether a patient will re spond better to antidepressants versus cognitive behavioral therapy (CBT). Amongst their small sample of 154 individuals, a clinically meaningful advantage (PAI ≥3), favoring one of the treatments relative to the other, was predicted for 60% of the pa tients. When these patients were divided into those randomly as signed to their "optimal" treatment versus those assigned to their "nonoptimal" treatment, outcomes in the former group were better (d = 0.58, 95% CI: 0.171.01). Similar approaches have been developed by other groups 55,58 , and more recently improved fur ther by the use of machine learning approaches 59 to generate better predictions and incorporate more variables.</p>
        <p>Several studies since then have tried to predict which evi dencebased psychotherapy is most likely to benefit a specific patient 55,59 , including efforts to identify which of two (or more) psychotherapies may be most effective 60,61 , and whether a given patient is predicted to respond better to psychotherapy or medi cations 56 . A recent scoping review 62 identified a total of 44 studies that developed and tested a machine learning model in psycho therapy, but only seven of them reported on the feasibility of the tool. Since psychotherapy trials are often expensive and rarely have large sample sizes, some have argued that predictive mod els may need to be developed initially with large observational datasets 63 .Several studies since then have tried to predict which evi dencebased psychotherapy is most likely to benefit a specific patient 55,59 , including efforts to identify which of two (or more) psychotherapies may be most effective 60,61 , and whether a given patient is predicted to respond better to psychotherapy or medi cations 56 . A recent scoping review 62 identified a total of 44 studies that developed and tested a machine learning model in psycho therapy, but only seven of them reported on the feasibility of the tool. Since psychotherapy trials are often expensive and rarely have large sample sizes, some have argued that predictive mod els may need to be developed initially with large observational datasets 63 .</p>
        <p>PAIstyle approaches that calculate treatment by variable in teractions quickly lead to highdimensionality prediction analy ses that are prone to overfitting (or require very large sample sizes). Using data from two Dutch randomized trials, van Bron swijk et al 60 examined whether PAI models developed in one clinical trial dataset were able to successfully generalize to an independent dataset. Although the models performed statisti cally above chance in the trial used to train them, they did not generalize to the other clinical trial when predicting benefit for CBT versus interpersonal therapy (IPT) for depression.PAIstyle approaches that calculate treatment by variable in teractions quickly lead to highdimensionality prediction analy ses that are prone to overfitting (or require very large sample sizes). Using data from two Dutch randomized trials, van Bron swijk et al 60 examined whether PAI models developed in one clinical trial dataset were able to successfully generalize to an independent dataset. Although the models performed statisti cally above chance in the trial used to train them, they did not generalize to the other clinical trial when predicting benefit for CBT versus interpersonal therapy (IPT) for depression.</p>
        <p>The psychotherapy literature has generated several other pre diction models, potentially optimizing significant aspects of patient care. For example, models have been developed 64,65 that would enable mental health providers to select low or high intensity treatments for patients on the basis of their expected prognosis. Other studies have tried to deconstruct the content that is traditionally combined to form a course of psychothera py treatment, in order to predict which treatment components should be delivered within a given intervention, as well as the order in which the components should be implemented 6668 . Other novel directions include using machine learning to match patients to specific therapists 69 , replicating human ratings and judgements 70,71 , and using natural language processing tech niques to discover patterns of therapistpatient interactions that predict treatment response 72,73 .The psychotherapy literature has generated several other pre diction models, potentially optimizing significant aspects of patient care. For example, models have been developed 64,65 that would enable mental health providers to select low or high intensity treatments for patients on the basis of their expected prognosis. Other studies have tried to deconstruct the content that is traditionally combined to form a course of psychothera py treatment, in order to predict which treatment components should be delivered within a given intervention, as well as the order in which the components should be implemented 6668 . Other novel directions include using machine learning to match patients to specific therapists 69 , replicating human ratings and judgements 70,71 , and using natural language processing tech niques to discover patterns of therapistpatient interactions that predict treatment response 72,73 .</p>
        <p>In general, many machine learning approaches to predict responses to psychotherapies are in the early stages of develop ment 62 . However, a notable exception is found in the welldevel oped literature on routine outcome monitoring and "progress feedback". This involves tracking a patient's response to treat ment in real time by entering his/her selfreported outcome/ symptom measures into a computerized system that compares his/her response to predicted trajectories of improvement de rived from clinical data using conventional statistical analyses (e.g., longitudinal multilevel/mixed models and growth curve modelling). There are now over 20 randomized controlled trials and several metaanalyses indicating that such clinical predic tion models can help to improve treatment outcomes 74 .In general, many machine learning approaches to predict responses to psychotherapies are in the early stages of develop ment 62 . However, a notable exception is found in the welldevel oped literature on routine outcome monitoring and "progress feedback". This involves tracking a patient's response to treat ment in real time by entering his/her selfreported outcome/ symptom measures into a computerized system that compares his/her response to predicted trajectories of improvement de rived from clinical data using conventional statistical analyses (e.g., longitudinal multilevel/mixed models and growth curve modelling). There are now over 20 randomized controlled trials and several metaanalyses indicating that such clinical predic tion models can help to improve treatment outcomes 74 .</p>
        <p>In addition to models investigating differential response to treatment and treatment optimization, the psychotherapy litera ture also includes adequately powered studies predicting overall response to treatment based on sociodemographic and clinical variables, much like the literature on response to medication. Buckman et al 75 built nine different models, using depression and anxiety symptoms, social support, alcohol use, and life events to predict depressive symptom response after 34 months of treatment in primary care settings. Models were trained on data from three clinical trials (N=1,722) and tested on three in dependent trials (N=1,136). All models predicted remission bet ter than a null model using only one postbaseline depression severity measurement. Green et al 76 also predicted depressive symptom response to psychotherapy in 4,393 patients from com munity health services. They found that a model with only five pretreatment variables (initial severity of anxiety and depres sion, ethnicity, deprivation and gender) predicted reduction of anxiety and depression symptoms with an accuracy of 74.9%. The number of sessions attended/missed was also an important factor affecting treatment response.In addition to models investigating differential response to treatment and treatment optimization, the psychotherapy litera ture also includes adequately powered studies predicting overall response to treatment based on sociodemographic and clinical variables, much like the literature on response to medication. Buckman et al 75 built nine different models, using depression and anxiety symptoms, social support, alcohol use, and life events to predict depressive symptom response after 34 months of treatment in primary care settings. Models were trained on data from three clinical trials (N=1,722) and tested on three in dependent trials (N=1,136). All models predicted remission bet ter than a null model using only one postbaseline depression severity measurement. Green et al 76 also predicted depressive symptom response to psychotherapy in 4,393 patients from com munity health services. They found that a model with only five pretreatment variables (initial severity of anxiety and depres sion, ethnicity, deprivation and gender) predicted reduction of anxiety and depression symptoms with an accuracy of 74.9%. The number of sessions attended/missed was also an important factor affecting treatment response.</p>
        <p>In recent years, online delivery of mental health interventions has been seen as a promising approach to reducing barriers to care, with growing evidence for the effectiveness of both guided and unguided delivery 77,78 . Interventions such as internetbased CBT (iCBT) may be particularly amenable to the use of machine learning techniques, due to the possibility of longitudinal stand ardized collection of outcome data at scale, and the potential to directly incorporate machine learning outputs into online or app based interventions. For example, in guided treatments, machine learning tools could provide feedback to therapists or alerts re garding risk. They could also be used to drive justintime adaptive interventions 79 . Smartphone delivery also opens up the possibil ity of automated collection of sensor data to derive behavioral markers 80 , which would open up many possibilities for tailored interventions, while also raising a number of privacy and ethical concerns.In recent years, online delivery of mental health interventions has been seen as a promising approach to reducing barriers to care, with growing evidence for the effectiveness of both guided and unguided delivery 77,78 . Interventions such as internetbased CBT (iCBT) may be particularly amenable to the use of machine learning techniques, due to the possibility of longitudinal stand ardized collection of outcome data at scale, and the potential to directly incorporate machine learning outputs into online or app based interventions. For example, in guided treatments, machine learning tools could provide feedback to therapists or alerts re garding risk. They could also be used to drive justintime adaptive interventions 79 . Smartphone delivery also opens up the possibil ity of automated collection of sensor data to derive behavioral markers 80 , which would open up many possibilities for tailored interventions, while also raising a number of privacy and ethical concerns.</p>
        <p>Machine learningderived outcome predictions for iCBT may have advantages with regard to ease of deployment, for example by providing integrated decision support for case management. However, most existing work focused on predicting outcomes has been exploratory in nature and based on modest sample siz es. A key distinction is between approaches that use only base line pretreatment data, and hence may be applied to direct the choice of treatment, and approaches which use data gathered during the course of treatment, such as regular outcome meas ures or ecological momentary assessment (EMA).Machine learningderived outcome predictions for iCBT may have advantages with regard to ease of deployment, for example by providing integrated decision support for case management. However, most existing work focused on predicting outcomes has been exploratory in nature and based on modest sample siz es. A key distinction is between approaches that use only base line pretreatment data, and hence may be applied to direct the choice of treatment, and approaches which use data gathered during the course of treatment, such as regular outcome meas ures or ecological momentary assessment (EMA).</p>
        <p>As an example of the former, Lenhard et al 81 examined how clinical baseline variables can be used to predict posttreatment outcomes for 61 adolescents in a trial of iCBT for obsessive compulsive disorder. Whereas multivariable logistic regression detected no significant predictors, the four machine learning algorithms investigated were able to predict treatment response with a 75 to 83% accuracy.As an example of the former, Lenhard et al 81 examined how clinical baseline variables can be used to predict posttreatment outcomes for 61 adolescents in a trial of iCBT for obsessive compulsive disorder. Whereas multivariable logistic regression detected no significant predictors, the four machine learning algorithms investigated were able to predict treatment response with a 75 to 83% accuracy.</p>
        <p>In a study which included, in addition to demographic and clinical data, therapyrelated predictors of treatment credibility and working alliance, assessed at week 2 of treatment, Flygare et al 82 used a random forest algorithm to predict remission from body dysmorphic disorder after iCBT in a sample of 88 patients, comparing the results to logistic regression. Random forests achieved a prediction accuracy of 78% at posttreatment, with lower accuracy in subsequent followups. The most important predictors were depressive symptoms, treatment credibility, working alliance, and initial severity of the disorder.In a study which included, in addition to demographic and clinical data, therapyrelated predictors of treatment credibility and working alliance, assessed at week 2 of treatment, Flygare et al 82 used a random forest algorithm to predict remission from body dysmorphic disorder after iCBT in a sample of 88 patients, comparing the results to logistic regression. Random forests achieved a prediction accuracy of 78% at posttreatment, with lower accuracy in subsequent followups. The most important predictors were depressive symptoms, treatment credibility, working alliance, and initial severity of the disorder.</p>
        <p>van Breda et al 83 added EMA data to models using baseline mea sures in a study predicting outcomes for patients who were ran domized to blended therapy (facetoface CBT and iCBT) or treat ment as usual. This approach did not improve prediction accuracy.van Breda et al 83 added EMA data to models using baseline mea sures in a study predicting outcomes for patients who were ran domized to blended therapy (facetoface CBT and iCBT) or treat ment as usual. This approach did not improve prediction accuracy.</p>
        <p>The effectiveness of digital CBT interventions is mediated by patient engagement 84 . Detailed patient engagement data can be gathered automatically in online or appbased interventions; this may include data such as content views, completion of exercises, and interactions with clinical supporters 85 . Engagement data may be used within predictive models, providing interpretable and ac tionable outputs (e.g., the need for more frequent therapist contact in order to motivate greater engagement). Chien et al 86 analyzed engagement data from 54,604 patients using a supported online intervention for depression and anxiety. A hidden Markov model was used to identify five engagement subtypes, based on patient interactions with sections of the intervention. Interestingly, while in general patients who engaged more achieved better outcomes, the best outcomes were found in those who were more likely to complete content belonging to key components of CBT (i.e., cog nitive restructuring and behavioral activation) within the first two weeks on the program, despite not spending the highest amount of time using the intervention. This work demonstrates the feasi bility of gathering detailed engagement and outcome data at scale.The effectiveness of digital CBT interventions is mediated by patient engagement 84 . Detailed patient engagement data can be gathered automatically in online or appbased interventions; this may include data such as content views, completion of exercises, and interactions with clinical supporters 85 . Engagement data may be used within predictive models, providing interpretable and ac tionable outputs (e.g., the need for more frequent therapist contact in order to motivate greater engagement). Chien et al 86 analyzed engagement data from 54,604 patients using a supported online intervention for depression and anxiety. A hidden Markov model was used to identify five engagement subtypes, based on patient interactions with sections of the intervention. Interestingly, while in general patients who engaged more achieved better outcomes, the best outcomes were found in those who were more likely to complete content belonging to key components of CBT (i.e., cog nitive restructuring and behavioral activation) within the first two weeks on the program, despite not spending the highest amount of time using the intervention. This work demonstrates the feasi bility of gathering detailed engagement and outcome data at scale.</p>
        <p>Interactions between patient and therapist, and the content of text in patient exercises, may also be analyzed using sentiment analysis techniques 87 . Analysis of patient texts might be embed ded in therapist feedback tools for guided interventions, or as features within predictive models. Ewbank et al 73 conducted an analysis of 90,934 session transcripts (specifically, CBT via real time text messages). Deep learning was used to automatically categorize utterances from the transcripts into feature categories related to CBT competences, and then multivariable logistic re gression was applied to assess the association with treatment out comes. A number of session features, such as "therapeutic praise", were associated with greater odds of improvement.Interactions between patient and therapist, and the content of text in patient exercises, may also be analyzed using sentiment analysis techniques 87 . Analysis of patient texts might be embed ded in therapist feedback tools for guided interventions, or as features within predictive models. Ewbank et al 73 conducted an analysis of 90,934 session transcripts (specifically, CBT via real time text messages). Deep learning was used to automatically categorize utterances from the transcripts into feature categories related to CBT competences, and then multivariable logistic re gression was applied to assess the association with treatment out comes. A number of session features, such as "therapeutic praise", were associated with greater odds of improvement.</p>
        <p>Chikersal et al 88 analyzed 234,735 messages sent from clini cal supporters to clients within an iCBT platform, examining how support strategies correlate with clinical outcomes. They used kmeans clustering to identify supporters whose messages were linked with "high", "medium" or "low" improvements in cli ent outcomes, as measured by PHQ9 and Generalized Anxiety Disorder7 (GAD7). The messages of more successful support ers were more positively phrased, more encouraging, more often used first person plural pronouns, were less abstract, and refer enced more social behaviors. Association rule mining was then applied to linguistic features in the messages in order to identify contexts in which particular support strategies were more effec tive. For less engaged patients, longer, more positive and more supportive messages were linked with better outcomes. For more engaged clients, messages with less negative words, less abstraction, and more references to social behaviors were asso ciated to better outcomes. Such results could ultimately be used in the design of supporter training materials.Chikersal et al 88 analyzed 234,735 messages sent from clini cal supporters to clients within an iCBT platform, examining how support strategies correlate with clinical outcomes. They used kmeans clustering to identify supporters whose messages were linked with "high", "medium" or "low" improvements in cli ent outcomes, as measured by PHQ9 and Generalized Anxiety Disorder7 (GAD7). The messages of more successful support ers were more positively phrased, more encouraging, more often used first person plural pronouns, were less abstract, and refer enced more social behaviors. Association rule mining was then applied to linguistic features in the messages in order to identify contexts in which particular support strategies were more effec tive. For less engaged patients, longer, more positive and more supportive messages were linked with better outcomes. For more engaged clients, messages with less negative words, less abstraction, and more references to social behaviors were asso ciated to better outcomes. Such results could ultimately be used in the design of supporter training materials.</p>
        <p>One could also try to predict whether a patient engages or drops out of care. Wallert et al 89 aimed to predict adherence to an online intervention targeting symptoms of depression and anxiety in people who had experienced a myocardial infarction. The analysis included linguistic features of the homework texts as well as demographic and clinical characteristics. The strongest predictors of adherence were cardiacrelated fear, gender, and the number of words in the first homework assignment.One could also try to predict whether a patient engages or drops out of care. Wallert et al 89 aimed to predict adherence to an online intervention targeting symptoms of depression and anxiety in people who had experienced a myocardial infarction. The analysis included linguistic features of the homework texts as well as demographic and clinical characteristics. The strongest predictors of adherence were cardiacrelated fear, gender, and the number of words in the first homework assignment.</p>
        <p>Numerous neurobiological options have emerged as potential treatments for severe and treatmentresistant depression, such as transcranial magnetic stimulation (TMS) and electroconvul sive therapy (ECT). Given the potential risks and side effects of these treatments, as well as their higher financial costs, there is an especially strong interest in identifying for whom they are safe and effective 9092 .Numerous neurobiological options have emerged as potential treatments for severe and treatmentresistant depression, such as transcranial magnetic stimulation (TMS) and electroconvul sive therapy (ECT). Given the potential risks and side effects of these treatments, as well as their higher financial costs, there is an especially strong interest in identifying for whom they are safe and effective 9092 .</p>
        <p>Recent reviews have examined predictors of treatment re sponse and relapse among depressed patients receiving TMS 9294 . TMS studies with more female patients tend to have higher ef fect sizes, suggesting that gender may be a predictor of TMS out comes 95 . Although several studies have attempted to examine neurobiological predictors of response to TMS, the findings are currently inconsistent 92 . Small sample size generally means that machine learning study designs are likely to overfit and produce results that will not replicate later.Recent reviews have examined predictors of treatment re sponse and relapse among depressed patients receiving TMS 9294 . TMS studies with more female patients tend to have higher ef fect sizes, suggesting that gender may be a predictor of TMS out comes 95 . Although several studies have attempted to examine neurobiological predictors of response to TMS, the findings are currently inconsistent 92 . Small sample size generally means that machine learning study designs are likely to overfit and produce results that will not replicate later.</p>
        <p>Efforts to predict treatment outcomes for ECT are still primar ily traditional association studies. Some of them identified a few variables that appear to replicate across studies. Better outcomes have been found for older patients, those with psychotic depres sion, those with high suicidal intent, and those who exhibit early symptom changes 90,96 . However, due to the small sample size in most ECT trials, and the typically nonrandomized study de signs, this area has not seen much progress. These are also obsta cles to the application of machine learning techniques.Efforts to predict treatment outcomes for ECT are still primar ily traditional association studies. Some of them identified a few variables that appear to replicate across studies. Better outcomes have been found for older patients, those with psychotic depres sion, those with high suicidal intent, and those who exhibit early symptom changes 90,96 . However, due to the small sample size in most ECT trials, and the typically nonrandomized study de signs, this area has not seen much progress. These are also obsta cles to the application of machine learning techniques.</p>
        <p>Electronic health records (EHR) are increasingly widely adopt ed in health care systems. They comprise data routinely collected and maintained for individual patients over the course of their clinical care. As such, these data may be particularly useful for building predictive models in psychiatry that could be read ily integrated into points of care within clinical settings 97 . EHR data can be divided into two major types: coded structured data, including diagnostic codes, procedure codes, laboratory and medication prescription codes; and unstructured data, including clinical notes and other textbased documentation, which can be mined using natural language processing.Electronic health records (EHR) are increasingly widely adopt ed in health care systems. They comprise data routinely collected and maintained for individual patients over the course of their clinical care. As such, these data may be particularly useful for building predictive models in psychiatry that could be read ily integrated into points of care within clinical settings 97 . EHR data can be divided into two major types: coded structured data, including diagnostic codes, procedure codes, laboratory and medication prescription codes; and unstructured data, including clinical notes and other textbased documentation, which can be mined using natural language processing.</p>
        <p>Recent studies have tested the potential of EHR data to predict treatment outcomes in psychiatry, with the bulk of efforts to date focused on depression, though examples exist for bipolar disor der 98 and schizophrenia 99 . Machine learningbased efforts using EHR data have sought to identify those individuals who are likely to drop out after initiating antidepressants 100 , those who will show a stable treatment response to antidepressants 101 , and those who may transition to a bipolar diagnosis after starting antidepres sants for depression 102 . Such applications have shown promising, though still modest and not yet clinically actionable, results.Recent studies have tested the potential of EHR data to predict treatment outcomes in psychiatry, with the bulk of efforts to date focused on depression, though examples exist for bipolar disor der 98 and schizophrenia 99 . Machine learningbased efforts using EHR data have sought to identify those individuals who are likely to drop out after initiating antidepressants 100 , those who will show a stable treatment response to antidepressants 101 , and those who may transition to a bipolar diagnosis after starting antidepres sants for depression 102 . Such applications have shown promising, though still modest and not yet clinically actionable, results.</p>
        <p>Applying logistic regression and random forest approaches, Pradier et al 102 used demographic and structured EHR data (i.e., diagnostic, medication and procedure codes) available at the time of initial prescription to predict treatment dropout after initiating one of nine most common antidepressants. Although mean AUC was below 0.70, they found that incorporating EHR data significantly improved prediction of treatment dropout compared to demographic information alone, and that predic tive performance varied by type of antidepressant (AUC as high as 0.80 for escitalopram) and provider type (higher accuracy among psychiatristtreated individuals).Applying logistic regression and random forest approaches, Pradier et al 102 used demographic and structured EHR data (i.e., diagnostic, medication and procedure codes) available at the time of initial prescription to predict treatment dropout after initiating one of nine most common antidepressants. Although mean AUC was below 0.70, they found that incorporating EHR data significantly improved prediction of treatment dropout compared to demographic information alone, and that predic tive performance varied by type of antidepressant (AUC as high as 0.80 for escitalopram) and provider type (higher accuracy among psychiatristtreated individuals).</p>
        <p>Hughes et al 101 applied logistic regression and extremely randomized trees with demographic and structured EHR data to predict general and drugspecific treatment continuity in patients receiving any of 11 antidepressants, observing a mean AUC of 0.630.66 and similar performance when evaluated at a separate site.Hughes et al 101 applied logistic regression and extremely randomized trees with demographic and structured EHR data to predict general and drugspecific treatment continuity in patients receiving any of 11 antidepressants, observing a mean AUC of 0.630.66 and similar performance when evaluated at a separate site.</p>
        <p>Where symptom score (e.g., PHQ9) data have been avail able for smaller EHR cohorts (e.g., N&lt;2,500) 103 , LASSO models incorporating demographic information, structured and un structured EHR data, and baseline symptom scores have shown modesttoadequate performance in predicting improvements in depressive symptom severity, for both medication treatment (AUC=0.66) and psychotherapy (AUC=0.75). However, the most important predictor in these models was baseline symptom scores. Only when symptom scores are routinely integrated into EHR treatment workflows will such models be relevant for out come prediction in largescale health systems.Where symptom score (e.g., PHQ9) data have been avail able for smaller EHR cohorts (e.g., N&lt;2,500) 103 , LASSO models incorporating demographic information, structured and un structured EHR data, and baseline symptom scores have shown modesttoadequate performance in predicting improvements in depressive symptom severity, for both medication treatment (AUC=0.66) and psychotherapy (AUC=0.75). However, the most important predictor in these models was baseline symptom scores. Only when symptom scores are routinely integrated into EHR treatment workflows will such models be relevant for out come prediction in largescale health systems.</p>
        <p>When using EHR data for predicting treatment outcomes in psychiatry, a key challenge is how to operationalize the outcome of interest using available clinical information. This usually in volves establishing a set of rules around which relevant EHR fea tures are observed, or not observed, in a cohort of patients over a defined period. For example, treatment dropout was defined by Pradier et al 100 as less than 90 days of prescription availability after index antidepressant initiation, with no evidence of alterna tive psychiatric treatment procedures. Antidepressant treatment stability, on the other hand, has been defined as two or more an tidepressant medication prescription codes at least 30 days apart over a period of at least 90 days, with additional rules about the maximum time gap between adjacent prescription codes, and other medication possession indicators 101 .When using EHR data for predicting treatment outcomes in psychiatry, a key challenge is how to operationalize the outcome of interest using available clinical information. This usually in volves establishing a set of rules around which relevant EHR fea tures are observed, or not observed, in a cohort of patients over a defined period. For example, treatment dropout was defined by Pradier et al 100 as less than 90 days of prescription availability after index antidepressant initiation, with no evidence of alterna tive psychiatric treatment procedures. Antidepressant treatment stability, on the other hand, has been defined as two or more an tidepressant medication prescription codes at least 30 days apart over a period of at least 90 days, with additional rules about the maximum time gap between adjacent prescription codes, and other medication possession indicators 101 .</p>
        <p>EHR data are also highly dimensional, with tens of thousands of possible diagnostic codes in addition to possible medication and procedure codes. Machine learning methods may be particu larly suitable for modeling complex signals across a diverse set of EHRbased predictors, but also for reducing their dimensions pri or to modeling. In their study of antidepressant treatment stabil ity, Hughes et al 101 applied supervised topic modeling using latent Dirichlet allocation to reduce 9,256 coded EHR features into 10 interpretable empirically derived topics, finding that a classifier for continuous treatment based on this lowerdimensional set of predictors showed comparable performance to a logistic regres sion based on a higherdimensional set of features. Simpler meth ods, such as selecting only diagnostic codes that meet a frequency threshold in the patient population, have also been used 100 .EHR data are also highly dimensional, with tens of thousands of possible diagnostic codes in addition to possible medication and procedure codes. Machine learning methods may be particu larly suitable for modeling complex signals across a diverse set of EHRbased predictors, but also for reducing their dimensions pri or to modeling. In their study of antidepressant treatment stabil ity, Hughes et al 101 applied supervised topic modeling using latent Dirichlet allocation to reduce 9,256 coded EHR features into 10 interpretable empirically derived topics, finding that a classifier for continuous treatment based on this lowerdimensional set of predictors showed comparable performance to a logistic regres sion based on a higherdimensional set of features. Simpler meth ods, such as selecting only diagnostic codes that meet a frequency threshold in the patient population, have also been used 100 .</p>
        <p>Smartphones can provide various kinds of data that are diffi cult to acquire through other means. Their first and biggest fea ture is that they contain many sensors that can passively collect data across a variety of domains. Passive smartphone data include dynamic measures of sleep quality, exercise, heart rate, geospatial locations, language use, and communication patterns 80,104 . Ma chine learning methods are indispensable for dealing with com plex patterns in these sensor data 105 . Currently available studies applying machine learning to predict mental health outcomes us ing sensor data have generally employed modest samples of 7 to 70 participants, yielding proofsofprinciple more than generaliz able results 80,106108 . Mobile phones also facilitate the collection of EMA data, allowing investigators to perform measurements at frequent intervals (e.g., several times a day). Furthermore, smart phonebased neurocognitive assessments appear to be a promis ing way to scalably collect cognitive data 109,110 .Smartphones can provide various kinds of data that are diffi cult to acquire through other means. Their first and biggest fea ture is that they contain many sensors that can passively collect data across a variety of domains. Passive smartphone data include dynamic measures of sleep quality, exercise, heart rate, geospatial locations, language use, and communication patterns 80,104 . Ma chine learning methods are indispensable for dealing with com plex patterns in these sensor data 105 . Currently available studies applying machine learning to predict mental health outcomes us ing sensor data have generally employed modest samples of 7 to 70 participants, yielding proofsofprinciple more than generaliz able results 80,106108 . Mobile phones also facilitate the collection of EMA data, allowing investigators to perform measurements at frequent intervals (e.g., several times a day). Furthermore, smart phonebased neurocognitive assessments appear to be a promis ing way to scalably collect cognitive data 109,110 .</p>
        <p>Few studies have used smartphone data to predict treatment outcomes. These include studies using text data from emails to predict treatment response in patients with social anxiety 111 , EMA data to predict changes in selfesteem from an online in tervention 112 , and EMA data to predict treatment response in patients with depression 83 . In the study predicting depression outcomes, a model including EMA data did not outperform a model using baseline characteristics 83 , showing that the former data do not always provide incremental value.Few studies have used smartphone data to predict treatment outcomes. These include studies using text data from emails to predict treatment response in patients with social anxiety 111 , EMA data to predict changes in selfesteem from an online in tervention 112 , and EMA data to predict treatment response in patients with depression 83 . In the study predicting depression outcomes, a model including EMA data did not outperform a model using baseline characteristics 83 , showing that the former data do not always provide incremental value.</p>
        <p>Social media allow investigators to access large amounts of data relating to language use and online activity. However, to our knowledge, these data have not yet been used to predict treat ment responses. One of the tradeoffs between incorporating dif ferent types of data is the cost and quantity versus quality of data: very often these data present with noise which may hinder the ability to identify meaningful patterns and signals. Novel meth ods of topological machine learning are robust to noise, and al low to extract descriptors of the shape and structure of data that can augment performance for the analysis of intensive time point measurements 113 . Such data with repeated measures may be useful for testing hypotheses, since sample size may compen sate for the increased noise of data 114 .Social media allow investigators to access large amounts of data relating to language use and online activity. However, to our knowledge, these data have not yet been used to predict treat ment responses. One of the tradeoffs between incorporating dif ferent types of data is the cost and quantity versus quality of data: very often these data present with noise which may hinder the ability to identify meaningful patterns and signals. Novel meth ods of topological machine learning are robust to noise, and al low to extract descriptors of the shape and structure of data that can augment performance for the analysis of intensive time point measurements 113 . Such data with repeated measures may be useful for testing hypotheses, since sample size may compen sate for the increased noise of data 114 .</p>
        <p>Machine learning methods are an appealing analytical ap proach for bridging genetic data with the prediction of treatment response in psychiatry. They put the focus on prediction rather than association, are able to detect interactions between loci, wisely handle correlation, and do not assume a predefined sta tistical model or additivity 115 .Machine learning methods are an appealing analytical ap proach for bridging genetic data with the prediction of treatment response in psychiatry. They put the focus on prediction rather than association, are able to detect interactions between loci, wisely handle correlation, and do not assume a predefined sta tistical model or additivity 115 .</p>
        <p>Machine learning has been used with the objective to im prove prediction of treatment outcomes from genetics alone in many diseases, including cancer 116,117 and hypertension 118 .Machine learning has been used with the objective to im prove prediction of treatment outcomes from genetics alone in many diseases, including cancer 116,117 and hypertension 118 .</p>
        <p>The question of whether an individual's genetic background could affect how he/she responds to medication treatment has been investigated in pharmacogenomics. An earlier study apply ing genomewide complex trait analysis in a sample of roughly 3,000 depressed patients suggested that common genetic varia tion could explain up to 42% of observed individual differences in antidepressant treatment response 119 , suggesting that mod eling common genetic variation could be useful for prediction. However, results of pharmacogenomic studies have so far, in general, been underwhelming 120 .The question of whether an individual's genetic background could affect how he/she responds to medication treatment has been investigated in pharmacogenomics. An earlier study apply ing genomewide complex trait analysis in a sample of roughly 3,000 depressed patients suggested that common genetic varia tion could explain up to 42% of observed individual differences in antidepressant treatment response 119 , suggesting that mod eling common genetic variation could be useful for prediction. However, results of pharmacogenomic studies have so far, in general, been underwhelming 120 .</p>
        <p>Polygenic scores are a common method for quantifying the overall contribution of common genetic variation to particular traits 121 . Polygenic associations with treatment response have been investigated in relatively small patient cohorts (most N&lt;1000) to date, with mixed findings 122125 . For example, polygenic scores for major depression and schizophrenia did not significantly predict antidepressant efficacy (based on symptom improvement) in clas sic treatment studies such as GenomeBased Therapeutic Drugs for Depression (GENDEP) and STAR*D 123 . However, these scores were built on earlier genome wide association studies (GWAS) and were likely underpowered. Wellpowered GWAS of antidepressant response have produced mixed results, with one study identify ing gene sets of relevance for bupropion response 126 and another observing no significant findings for antidepressant resistance 127 . Largerscale GWAS metaanalysis efforts are needed and ongoing. Even fewer studies have examined common genetic variation as sociated with responses to other treatment modalities such as psy chotherapy 125 or ECT 128 .Polygenic scores are a common method for quantifying the overall contribution of common genetic variation to particular traits 121 . Polygenic associations with treatment response have been investigated in relatively small patient cohorts (most N&lt;1000) to date, with mixed findings 122125 . For example, polygenic scores for major depression and schizophrenia did not significantly predict antidepressant efficacy (based on symptom improvement) in clas sic treatment studies such as GenomeBased Therapeutic Drugs for Depression (GENDEP) and STAR*D 123 . However, these scores were built on earlier genome wide association studies (GWAS) and were likely underpowered. Wellpowered GWAS of antidepressant response have produced mixed results, with one study identify ing gene sets of relevance for bupropion response 126 and another observing no significant findings for antidepressant resistance 127 . Largerscale GWAS metaanalysis efforts are needed and ongoing. Even fewer studies have examined common genetic variation as sociated with responses to other treatment modalities such as psy chotherapy 125 or ECT 128 .</p>
        <p>DNA methylation and gene expression data have been ex plored in combination with phenotypic datasets of demograph ic and clinical variables on their ability to predict response to multiple medications. A recent review 129 pointed out genetic prediction of therapeutic outcomes in depression as the most promising 43,130133 , with an overall accuracy of 0.82 (95% CI: 0.77 0.87) 134 . Models combining multiple data types, such as periph eral gene expression data, neuroimaging and clinical variables, achieved significantly higher accuracy 134 .DNA methylation and gene expression data have been ex plored in combination with phenotypic datasets of demograph ic and clinical variables on their ability to predict response to multiple medications. A recent review 129 pointed out genetic prediction of therapeutic outcomes in depression as the most promising 43,130133 , with an overall accuracy of 0.82 (95% CI: 0.77 0.87) 134 . Models combining multiple data types, such as periph eral gene expression data, neuroimaging and clinical variables, achieved significantly higher accuracy 134 .</p>
        <p>Treebased approaches were the most popular machine learn ing methods, followed by penalized regression, support vector machines and deep learning 129 . Studies were quite heterogene ous in design, methods, implementation and validation, limiting our capacity to elucidate the extent to which machine learning in tegrated with genetics can predict antidepressant drug response.Treebased approaches were the most popular machine learn ing methods, followed by penalized regression, support vector machines and deep learning 129 . Studies were quite heterogene ous in design, methods, implementation and validation, limiting our capacity to elucidate the extent to which machine learning in tegrated with genetics can predict antidepressant drug response.</p>
        <p>Evidence for polygenic risk scores versus support vector ma chines for the prediction of treatmentresistant schizophrenia from GWAS data have been reviewed 135 . Although support vector machines might be more suitable to take into account complex genetic interactions, the traditional polygenic risk score approach showed higher accuracy for classifying treatmentresistant indi viduals 115 .Evidence for polygenic risk scores versus support vector ma chines for the prediction of treatmentresistant schizophrenia from GWAS data have been reviewed 135 . Although support vector machines might be more suitable to take into account complex genetic interactions, the traditional polygenic risk score approach showed higher accuracy for classifying treatmentresistant indi viduals 115 .</p>
        <p>Despite many efforts to use many kinds of genetic information in many different ways, results so far have not been sufficiently compelling or accurate to support the use of these approaches to guide clinical care 136,137 . In the future, until novel analytic tech niques become available to extract signal from the genome, or a better understanding of the genetic basis for mental illness emerges, the most promising avenue in this context is to inte grate genetic information into multivariable analyses to poten tially improve broader model performance 133,137 .Despite many efforts to use many kinds of genetic information in many different ways, results so far have not been sufficiently compelling or accurate to support the use of these approaches to guide clinical care 136,137 . In the future, until novel analytic tech niques become available to extract signal from the genome, or a better understanding of the genetic basis for mental illness emerges, the most promising avenue in this context is to inte grate genetic information into multivariable analyses to poten tially improve broader model performance 133,137 .</p>
        <p>Tailoring treatment decisions based on brain measures is intuitively appealing and empirically welljustified. Systematic reviews and metaanalyses indicate that therapeutic outcomes are often related to pretreatment brain differences and that the brain changes as a result of therapy 138145 . However, in previous research using traditional statistical methods, effect sizes were too low to make the jump from statistical significance to clini cal relevance, external validation was rare, sample sizes were small, methodological and siterelated variance was high, and in many cases the techniques were not suited to an integration into clinical routine due to their costbenefit ratio (e.g., positron emission tomography) or reliance on experimental protocols that are unavailable in most clinical settings 138,139,143,145,146 . Ma chine learning approaches offer hope in overcoming these bar riers to clinical implementation. Preliminary reviews comparing accuracies support this optimism by suggesting superiority for treatment prediction with respect to traditional statistical meth ods 134 .Tailoring treatment decisions based on brain measures is intuitively appealing and empirically welljustified. Systematic reviews and metaanalyses indicate that therapeutic outcomes are often related to pretreatment brain differences and that the brain changes as a result of therapy 138145 . However, in previous research using traditional statistical methods, effect sizes were too low to make the jump from statistical significance to clini cal relevance, external validation was rare, sample sizes were small, methodological and siterelated variance was high, and in many cases the techniques were not suited to an integration into clinical routine due to their costbenefit ratio (e.g., positron emission tomography) or reliance on experimental protocols that are unavailable in most clinical settings 138,139,143,145,146 . Ma chine learning approaches offer hope in overcoming these bar riers to clinical implementation. Preliminary reviews comparing accuracies support this optimism by suggesting superiority for treatment prediction with respect to traditional statistical meth ods 134 .</p>
        <p>Early studies in this area applied machine learning to detect outcomes such as response to clozapine in psychosis 147 and to selective serotonin reuptake inhibitors (SSRIs) in depres sion 148150 , but the majority of research has focused on predicting brain stimulation outcomes for depression 148,151155 . For example, Corlier et al 156 found that alpha spectral correlation could be used to measure EEG connectivity, which then predicted response to repetitive TMS (rTMS), using crossvalidated logistic regression, with an accuracy of 77% in a subgroup of depressed individuals. This increased to 81% when adding clinical symptoms of depres sion. Most studies report predictive accuracies of &gt;80% on the basis of pilot samples consisting of approximately 50 cases or less 155 , reflecting the strong likelihood of bias and overfitting that is also seen with magnetic resonance imaging (MRI) 157 .Early studies in this area applied machine learning to detect outcomes such as response to clozapine in psychosis 147 and to selective serotonin reuptake inhibitors (SSRIs) in depres sion 148150 , but the majority of research has focused on predicting brain stimulation outcomes for depression 148,151155 . For example, Corlier et al 156 found that alpha spectral correlation could be used to measure EEG connectivity, which then predicted response to repetitive TMS (rTMS), using crossvalidated logistic regression, with an accuracy of 77% in a subgroup of depressed individuals. This increased to 81% when adding clinical symptoms of depres sion. Most studies report predictive accuracies of &gt;80% on the basis of pilot samples consisting of approximately 50 cases or less 155 , reflecting the strong likelihood of bias and overfitting that is also seen with magnetic resonance imaging (MRI) 157 .</p>
        <p>Taskrelated functional MRI (fMRI) has been used for treat ment prediction 158 : for example, by modelling amygdala engage ment interactions with early life stress during an experimental task to predict antidepressant outcome 159 or by using fear con ditioning responses to predict panic disorder treatment out come 160,161 . Similar taskrelated predictive models have been built in a number of studies of CBT 162 or antidepressant re sponses 162164 . In taskbased fMRI, however, the translational po tential is limited due to the use of lengthy and methodologically complicated experimental paradigms. Restingstate fMRI is a popular alternative, because it measures behaviourallyrelevant, synchronized brain network activity at rest, and the imaging pro tocols can be more easily harmonized across scanners 165 . Stud ies in this field have demonstrated similar accuracies for CBT 166 , traumafocused psychotherapy 167 , antidepressant treatment 168 , and antipsychotic therapy 169 , while also showing predictive ac curacy for ECT 165,170 .Taskrelated functional MRI (fMRI) has been used for treat ment prediction 158 : for example, by modelling amygdala engage ment interactions with early life stress during an experimental task to predict antidepressant outcome 159 or by using fear con ditioning responses to predict panic disorder treatment out come 160,161 . Similar taskrelated predictive models have been built in a number of studies of CBT 162 or antidepressant re sponses 162164 . In taskbased fMRI, however, the translational po tential is limited due to the use of lengthy and methodologically complicated experimental paradigms. Restingstate fMRI is a popular alternative, because it measures behaviourallyrelevant, synchronized brain network activity at rest, and the imaging pro tocols can be more easily harmonized across scanners 165 . Stud ies in this field have demonstrated similar accuracies for CBT 166 , traumafocused psychotherapy 167 , antidepressant treatment 168 , and antipsychotic therapy 169 , while also showing predictive ac curacy for ECT 165,170 .</p>
        <p>A challenge of functional imaging is reliability across scan ners, especially in nonexperimental clinical settings. Structural neuroimaging may provide an opportunity for faster implemen tation into existing clinical routines. Most studies have involved grey matter measurements, and ECT treatment prediction has been a frequent focus, with studies using wholebrain approach es 171 , regional measurements 172 , and combinations of neuro imaging modalities 173 . White matter measurements (e.g., with diffusion tensor imaging) have been relatively less commonly considered.A challenge of functional imaging is reliability across scan ners, especially in nonexperimental clinical settings. Structural neuroimaging may provide an opportunity for faster implemen tation into existing clinical routines. Most studies have involved grey matter measurements, and ECT treatment prediction has been a frequent focus, with studies using wholebrain approach es 171 , regional measurements 172 , and combinations of neuro imaging modalities 173 . White matter measurements (e.g., with diffusion tensor imaging) have been relatively less commonly considered.</p>
        <p>Overall, the lack of multisite studies and external validation reflects the pilotstudy stage of research in this area, where re sults can be interpreted as promising but highly experimental. Whether the machine learning results will ultimately agree with the low effect sizes found with classical statistical approaches re mains an open question 143,145 .Overall, the lack of multisite studies and external validation reflects the pilotstudy stage of research in this area, where re sults can be interpreted as promising but highly experimental. Whether the machine learning results will ultimately agree with the low effect sizes found with classical statistical approaches re mains an open question 143,145 .</p>
        <p>Cognitive testing is a straightforward method to indirectly as sess brain functioning that has been historically linked to treat ment outcomes. Although such testing can be timeconsuming and costly when performed by a trained neuropsychologist, more recent computerized methods can facilitate efficient digi tal assessments that lend themselves especially well to machine learning, including from passively collecting smartphone mea surements as described above 80,114,174 .Cognitive testing is a straightforward method to indirectly as sess brain functioning that has been historically linked to treat ment outcomes. Although such testing can be timeconsuming and costly when performed by a trained neuropsychologist, more recent computerized methods can facilitate efficient digi tal assessments that lend themselves especially well to machine learning, including from passively collecting smartphone mea surements as described above 80,114,174 .</p>
        <p>Etkin et al 175 conducted an early study in this area, as part of the international Study to Predict Optimized Treatment in De pression (iSPOTD), aimed to predict response to antidepressant treatment using a battery of computerized cognitive tasks as sessing attention, processing speed, memory, and executive and emotional functions. In order to obtain accurate predictive esti mates, they first classified depressed individuals into a subgroup with particularly poor cognition before training a supervised dis criminant function to predict remission. Results demonstrated that remission following escitalopram could be predicted with 72% accuracy, but this was not confirmed with sertraline or ven lafaxine.Etkin et al 175 conducted an early study in this area, as part of the international Study to Predict Optimized Treatment in De pression (iSPOTD), aimed to predict response to antidepressant treatment using a battery of computerized cognitive tasks as sessing attention, processing speed, memory, and executive and emotional functions. In order to obtain accurate predictive esti mates, they first classified depressed individuals into a subgroup with particularly poor cognition before training a supervised dis criminant function to predict remission. Results demonstrated that remission following escitalopram could be predicted with 72% accuracy, but this was not confirmed with sertraline or ven lafaxine.</p>
        <p>Subtyping or unsupervised learning approaches have also been helpful to identify response trajectories to cognitive train ing. A recent study found that selforganizing maps detecting multivariate relationships between cognitive functions associ ated with working memory task performance could identify in dividuals who differentially responded to the training 176 .Subtyping or unsupervised learning approaches have also been helpful to identify response trajectories to cognitive train ing. A recent study found that selforganizing maps detecting multivariate relationships between cognitive functions associ ated with working memory task performance could identify in dividuals who differentially responded to the training 176 .</p>
        <p>Not all prediction models will translate readily for use in clini cal or other realworld settings. In evaluating the readiness of predictive models for realworld implementation, key criteria include external validation, empirical support from implemen tation trials, and acceptability to users (e.g., clinicians).Not all prediction models will translate readily for use in clini cal or other realworld settings. In evaluating the readiness of predictive models for realworld implementation, key criteria include external validation, empirical support from implemen tation trials, and acceptability to users (e.g., clinicians).</p>
        <p>External crossvalidation remains the gold standard for evalu ating realworld performance, as it quantifies performance loss when a trained model is applied to a completely independ ent sample. In addition, it guards against increased researcher degreesoffreedom that may result from the many tuning pa rameters of more complex machine learning methods. A review focusing on machine learning in psychotherapy research report ed that only 3 of 51 studies had performed external validation 62 .External crossvalidation remains the gold standard for evalu ating realworld performance, as it quantifies performance loss when a trained model is applied to a completely independ ent sample. In addition, it guards against increased researcher degreesoffreedom that may result from the many tuning pa rameters of more complex machine learning methods. A review focusing on machine learning in psychotherapy research report ed that only 3 of 51 studies had performed external validation 62 .</p>
        <p>Studies without external validation are at high risk of over confidence, as demonstrated by Van Bronswijk et al 60 , who de veloped and then tested a treatment selection model across two randomized controlled trials comparing CBT and IPT. They found that the estimated effect size for the benefit of receiving the modelrecommended treatment (generated through internal crossvalidation) shrunk by 77% when the model was tested us ing the second study's data (external validation). Some prediction efforts using large naturalistic samples have reported positive results following external validation 65,177,178 .Studies without external validation are at high risk of over confidence, as demonstrated by Van Bronswijk et al 60 , who de veloped and then tested a treatment selection model across two randomized controlled trials comparing CBT and IPT. They found that the estimated effect size for the benefit of receiving the modelrecommended treatment (generated through internal crossvalidation) shrunk by 77% when the model was tested us ing the second study's data (external validation). Some prediction efforts using large naturalistic samples have reported positive results following external validation 65,177,178 .</p>
        <p>When a model undergoes external validation and successfully predicts outcomes, the next step towards realworld use is an im plementation trial. These trials provide the most compelling evi dence for the value of a decision support tool. Here, patients are usually allocated to algorithmguided treatment (generally with in a shared decisionmaking framework) or treatment as usual.When a model undergoes external validation and successfully predicts outcomes, the next step towards realworld use is an im plementation trial. These trials provide the most compelling evi dence for the value of a decision support tool. Here, patients are usually allocated to algorithmguided treatment (generally with in a shared decisionmaking framework) or treatment as usual.</p>
        <p>Trialbased efforts to evaluate the efficacy of treatment per sonalization tools have begun to emerge. One example is a mul tiservice cluster randomized trial 179 , in which patients (N=951) were referred to either high or lowintensity psychotherapy. In one arm, the choice of intensity was informed by an algorithm previously developed in a naturalistic dataset. In the other arm, most patients started on lowintensity psychotherapy and were later referred to highintensity treatment in the case of nonre sponse, as per usual stepped care. The study found higher de pression remission rates in patients whose initial treatment was recommended by the algorithm compared to usual stepped care (52.3% vs. 45.1%, odds ratio, OR=1.40, p=0.025).Trialbased efforts to evaluate the efficacy of treatment per sonalization tools have begun to emerge. One example is a mul tiservice cluster randomized trial 179 , in which patients (N=951) were referred to either high or lowintensity psychotherapy. In one arm, the choice of intensity was informed by an algorithm previously developed in a naturalistic dataset. In the other arm, most patients started on lowintensity psychotherapy and were later referred to highintensity treatment in the case of nonre sponse, as per usual stepped care. The study found higher de pression remission rates in patients whose initial treatment was recommended by the algorithm compared to usual stepped care (52.3% vs. 45.1%, odds ratio, OR=1.40, p=0.025).</p>
        <p>Another recent example comes from Lutz et al 180 , who used archival data from an outpatient CBT clinic to develop a pre dictive decision support system providing therapists with treat ment strategy recommendations and psychometric feedback enhanced with clinical problemsolving tools. They randomized therapistpatient dyads (N=538) to treatment as usual or to algo rithminformed treatment. They reported that, overall, outcomes for those who were randomized to the intervention did not differ from those who received usual care. However, there was signifi cant variability in the extent to which therapists in the interven tion condition followed the recommendations provided by the decision support tool. When the authors analyzed outcomes for patients whose therapists had followed the recommendations, significant benefits emerged.Another recent example comes from Lutz et al 180 , who used archival data from an outpatient CBT clinic to develop a pre dictive decision support system providing therapists with treat ment strategy recommendations and psychometric feedback enhanced with clinical problemsolving tools. They randomized therapistpatient dyads (N=538) to treatment as usual or to algo rithminformed treatment. They reported that, overall, outcomes for those who were randomized to the intervention did not differ from those who received usual care. However, there was signifi cant variability in the extent to which therapists in the interven tion condition followed the recommendations provided by the decision support tool. When the authors analyzed outcomes for patients whose therapists had followed the recommendations, significant benefits emerged.</p>
        <p>Browning et al 181 conducted another trial randomizing de pressed patients to either algorithminformed care or usual care for depression. Their algorithm, called PReDicT, used information from symptom scales and behavioral tests of affective cognition to predict nonresponse to treatment with citalopram. After eight weeks of treatment, the rate of depressive symptom response in the PReDicT arm was 55.9%, versus 51.8% in the usual care arm (not significant, OR=1. 18, p=0.25). Of all instances where the algorithm predicted nonresponse, only 65% prompted a change in treat ment regimen, and most consisted of an increase in dosage only.Browning et al 181 conducted another trial randomizing de pressed patients to either algorithminformed care or usual care for depression. Their algorithm, called PReDicT, used information from symptom scales and behavioral tests of affective cognition to predict nonresponse to treatment with citalopram. After eight weeks of treatment, the rate of depressive symptom response in the PReDicT arm was 55.9%, versus 51.8% in the usual care arm (not significant, OR=1. 18, p=0.25). Of all instances where the algorithm predicted nonresponse, only 65% prompted a change in treat ment regimen, and most consisted of an increase in dosage only.</p>
        <p>In combination, the above findings highlight that accurate algorithms are not enough to ensure the success of a decision support system for precision treatment 39 . When randomizing patients to algorithminformed care or usual care, clinicians may override algorithm recommendations and choose alternative treatments. Patients may refuse the algorithmrecommended treatment, or have restrictions to its use that were not contem plated by the decision support tool (e.g., prohibitive cost of ther apy). In light of this, effect sizes for these interventions will often vary when applied in different settings.In combination, the above findings highlight that accurate algorithms are not enough to ensure the success of a decision support system for precision treatment 39 . When randomizing patients to algorithminformed care or usual care, clinicians may override algorithm recommendations and choose alternative treatments. Patients may refuse the algorithmrecommended treatment, or have restrictions to its use that were not contem plated by the decision support tool (e.g., prohibitive cost of ther apy). In light of this, effect sizes for these interventions will often vary when applied in different settings.</p>
        <p>The use of predictive models may be uniquely challenging in psychotherapy research and practice. One challenge is that a giv en therapist is only trained to provide a limited subset of psycho therapies. Whereas a psychiatrist may be qualified to prescribe a large number of different medications or medication combina tions, a psychotherapist is less likely to be able to competently provide many different psychotherapies. Another consideration is that predictions from a model may lead to selffulfilling proph ecies, in which clinicians treat "easy" patients (those with good prognoses) differently than "difficult" patients 182 .The use of predictive models may be uniquely challenging in psychotherapy research and practice. One challenge is that a giv en therapist is only trained to provide a limited subset of psycho therapies. Whereas a psychiatrist may be qualified to prescribe a large number of different medications or medication combina tions, a psychotherapist is less likely to be able to competently provide many different psychotherapies. Another consideration is that predictions from a model may lead to selffulfilling proph ecies, in which clinicians treat "easy" patients (those with good prognoses) differently than "difficult" patients 182 .</p>
        <p>For both medications and psychotherapies, in realworld, treatment decisions are rarely going to be made solely based on model recommendations. Rather, these decisions will involve the preferences of patients, the recommendations of clinicians, the availability and costs of treatments, and several other con siderations 183 . As such, the development of datadriven decision tools should be informed by extensive consultation and copro duction with the intended users, in order to implement models that maximize acceptability and compatibility with other clinical guidelines (i.e., risk management procedures, norms about safe dosage or titration of medications).For both medications and psychotherapies, in realworld, treatment decisions are rarely going to be made solely based on model recommendations. Rather, these decisions will involve the preferences of patients, the recommendations of clinicians, the availability and costs of treatments, and several other con siderations 183 . As such, the development of datadriven decision tools should be informed by extensive consultation and copro duction with the intended users, in order to implement models that maximize acceptability and compatibility with other clinical guidelines (i.e., risk management procedures, norms about safe dosage or titration of medications).</p>
        <p>Another crucial barrier to implementation is the interpretabil ity of machine learning models. As algorithms become increas ingly complex, sometimes called "black box" algorithms, they can become very difficult to interpret, and therefore unlikely to be acceptable to clinical users. Methods for explaining predic tions of complex models have therefore been developed 184,185 , but there is currently no agreedupon measure for assessing the quality or accuracy of these explanations. In addition, blackbox predictive models combined with (similarly complex) explana tory methods may yield complicated decision pathways that in crease the likelihood of human error 186 .Another crucial barrier to implementation is the interpretabil ity of machine learning models. As algorithms become increas ingly complex, sometimes called "black box" algorithms, they can become very difficult to interpret, and therefore unlikely to be acceptable to clinical users. Methods for explaining predic tions of complex models have therefore been developed 184,185 , but there is currently no agreedupon measure for assessing the quality or accuracy of these explanations. In addition, blackbox predictive models combined with (similarly complex) explana tory methods may yield complicated decision pathways that in crease the likelihood of human error 186 .</p>
        <p>In order to ensure that algorithm recommendations are used in trials, additional thought and effort must be devoted to issues of dissemination and implementation, with the goal of making the recommendations simple to generate, easy to understand, trustworthy, ethical, costeffective, and compelling enough to influence the decisionmaker(s) 187 .In order to ensure that algorithm recommendations are used in trials, additional thought and effort must be devoted to issues of dissemination and implementation, with the goal of making the recommendations simple to generate, easy to understand, trustworthy, ethical, costeffective, and compelling enough to influence the decisionmaker(s) 187 .</p>
        <p>A recent experiment was conducted with 220 antidepressant prescribing clinicians to assess the impact of providing machine learning recommendations and accompanying explanations 188 . It was found that recommendations did not improve accurate se lection of antidepressants in hypothetical patient scenarios, and that accuracy was even lower when incorrect recommendations were presented than when standard information was available. Prospective fieldtests 181,189 are one method for identifying the myriad institutional, cultural and contextual factors that could af fect the uptake and sustained use of a precision psychiatry tool, aiming to coproduce acceptable and interpretable decision tools with the intended users.A recent experiment was conducted with 220 antidepressant prescribing clinicians to assess the impact of providing machine learning recommendations and accompanying explanations 188 . It was found that recommendations did not improve accurate se lection of antidepressants in hypothetical patient scenarios, and that accuracy was even lower when incorrect recommendations were presented than when standard information was available. Prospective fieldtests 181,189 are one method for identifying the myriad institutional, cultural and contextual factors that could af fect the uptake and sustained use of a precision psychiatry tool, aiming to coproduce acceptable and interpretable decision tools with the intended users.</p>
        <p>From the development of machine learning tools to their po tential deployment into clinical care, we can identify several ethi cal challenges 190193 .From the development of machine learning tools to their po tential deployment into clinical care, we can identify several ethi cal challenges 190193 .</p>
        <p>The first challenge concerns responsibility. With the imple mentation of machine learning programs into clinical practice, physicians and machine learningbased tools would become "teammates" that collaborate in selecting an optimal treat ment 194,195 . In such a scenario, who will hold authority and ethical responsibility over the decision made? We believe that a competent human agent should check and take final responsi bility on the machine learningbased suggestions 196 , as only he/ she is equipped with empathy, a good understanding of the con textual environment and, most uniquely, consciousness.The first challenge concerns responsibility. With the imple mentation of machine learning programs into clinical practice, physicians and machine learningbased tools would become "teammates" that collaborate in selecting an optimal treat ment 194,195 . In such a scenario, who will hold authority and ethical responsibility over the decision made? We believe that a competent human agent should check and take final responsi bility on the machine learningbased suggestions 196 , as only he/ she is equipped with empathy, a good understanding of the con textual environment and, most uniquely, consciousness.</p>
        <p>The second challenge is to avoid dehumanization 197 . Ma chine learning can incorporate a great variety of psychological, environmental and social variables, and there is some progress towards including subjective patient experience into machine learning models 198 . However, giving a patient the space to articu late his/her concerns is essential to ensure accurate diagnosis, health outcomes, and humane care 199 .The second challenge is to avoid dehumanization 197 . Ma chine learning can incorporate a great variety of psychological, environmental and social variables, and there is some progress towards including subjective patient experience into machine learning models 198 . However, giving a patient the space to articu late his/her concerns is essential to ensure accurate diagnosis, health outcomes, and humane care 199 .</p>
        <p>Third, making decisions is an intricate part of physicians' ac tivity. The nonexpert tends to act as a "technician" and more likely relies on protocols, whilst the expert, after the observation of many cases, is more prone to making decisions based on tacit knowledge 200202 . The ethical mandate is that practitioners use all of their capabilities, including those based on selfexperience and observation, even if this is in discordance with a statistical model. Disagreements between physicians and machine learn ingbased decisions may lead to consultations with other clini cians 193 . However, in the context of modern health care systems, respecting clinicians' judgement is vital 193,203 , and they should not be forced to act against their own criteria (freedom of ac tion) 204 .Third, making decisions is an intricate part of physicians' ac tivity. The nonexpert tends to act as a "technician" and more likely relies on protocols, whilst the expert, after the observation of many cases, is more prone to making decisions based on tacit knowledge 200202 . The ethical mandate is that practitioners use all of their capabilities, including those based on selfexperience and observation, even if this is in discordance with a statistical model. Disagreements between physicians and machine learn ingbased decisions may lead to consultations with other clini cians 193 . However, in the context of modern health care systems, respecting clinicians' judgement is vital 193,203 , and they should not be forced to act against their own criteria (freedom of ac tion) 204 .</p>
        <p>Practitioners (especially those with less expertise) might be in danger of not developing/losing their own clinical judgement and become dependent on automatically deployed machine learning outcomes 205 , particularly for those complex cases that they fear they are not competent enough to solve. This would risk disem powerment of clinicians. On the other hand, it is a physicians' duty to train themselves in the use, understanding and interpre tation of machine learning applications, so that they can trust the system and its outputs, and contribute to patients' acceptance 206 .Practitioners (especially those with less expertise) might be in danger of not developing/losing their own clinical judgement and become dependent on automatically deployed machine learning outcomes 205 , particularly for those complex cases that they fear they are not competent enough to solve. This would risk disem powerment of clinicians. On the other hand, it is a physicians' duty to train themselves in the use, understanding and interpre tation of machine learning applications, so that they can trust the system and its outputs, and contribute to patients' acceptance 206 .</p>
        <p>Machine learning tools need to be transparent to the human teammates to facilitate understanding 194,207 . The idea of trans parency is opposite to that of "blackbox" machine learning al gorithms, in which the patterns the algorithm follows to make a decision for a given patient are opaque to the person and even to the developer, making very challenging (if not impossible) for the affected person to understand how the system worked out an output for him/her 190 . This risks not only increasing clinicians' resistance to use the tool, but also disempowering patients and disrespecting their autonomy. Developers should consider sim pler algorithms that balance interpretability with accuracy 191 .Machine learning tools need to be transparent to the human teammates to facilitate understanding 194,207 . The idea of trans parency is opposite to that of "blackbox" machine learning al gorithms, in which the patterns the algorithm follows to make a decision for a given patient are opaque to the person and even to the developer, making very challenging (if not impossible) for the affected person to understand how the system worked out an output for him/her 190 . This risks not only increasing clinicians' resistance to use the tool, but also disempowering patients and disrespecting their autonomy. Developers should consider sim pler algorithms that balance interpretability with accuracy 191 .</p>
        <p>Furthermore, a central issue in fair machine learning develop ment arises when the training dataset is not a good representa tion of the phenomenon being studied 192,208 . A model trained in such data will predict erroneous outcomes for groups that were underrepresented 209 . For example, a widely used machine learn ing algorithm assigned the same level of disease risk to Black and White patients, even if Black patients were sicker than White patients 210 . As a consequence, the system was actively causing harm to Black patients by leading to allocation of fewer resourc es to them. Potentially discriminatory predictors should be left out of the model, but developers should be aware that surrogate variables correlated with the excluded set might still become rel evant for prediction. Objective unbiased applications might help reduce discrimination in machine learning 211,212 .Furthermore, a central issue in fair machine learning develop ment arises when the training dataset is not a good representa tion of the phenomenon being studied 192,208 . A model trained in such data will predict erroneous outcomes for groups that were underrepresented 209 . For example, a widely used machine learn ing algorithm assigned the same level of disease risk to Black and White patients, even if Black patients were sicker than White patients 210 . As a consequence, the system was actively causing harm to Black patients by leading to allocation of fewer resourc es to them. Potentially discriminatory predictors should be left out of the model, but developers should be aware that surrogate variables correlated with the excluded set might still become rel evant for prediction. Objective unbiased applications might help reduce discrimination in machine learning 211,212 .</p>
        <p>Finally, the risk of misuse of personal and sensitive data ex changed in machine learning is high 213 . For this reason, machine learning tools can be only used when data security and privacy are guaranteed.Finally, the risk of misuse of personal and sensitive data ex changed in machine learning is high 213 . For this reason, machine learning tools can be only used when data security and privacy are guaranteed.</p>
        <p>This paper reviews several studies suggesting that it is possible to predict outcomes and personalize psychiatric treatment by using machine learning. Several gold standard prediction studies have shown that we can predict whether a depressed patient will respond to specific antidepressants 40,41 , to specific psychothera peutic techniques 177 , and whether patients with first episode psychosis will have good prognosis after one year with certain antipsychotic medications 25,50 . At least three predictive models have even been tested in prospective clinical trials.This paper reviews several studies suggesting that it is possible to predict outcomes and personalize psychiatric treatment by using machine learning. Several gold standard prediction studies have shown that we can predict whether a depressed patient will respond to specific antidepressants 40,41 , to specific psychothera peutic techniques 177 , and whether patients with first episode psychosis will have good prognosis after one year with certain antipsychotic medications 25,50 . At least three predictive models have even been tested in prospective clinical trials.</p>
        <p>Despite this progress, the potential for machine learning in psychiatry has just begun to be explored. Predicting treatment response is just one relatively narrow use case where machine learning can add value and improve mental health care. Predic tion can help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or nonadherence or dropout from care after initiation. We could streamline patients to the appro priate level of care, such as selfguided programs vs. outpatient care, or intensive outpatient versus inpatient care, to maximize scarce health care resources. In selecting a specific treatment ap proach, we could optimize dosing or predict side effect profiles in order to improve symptoms but minimize impact on patient quality of life. Some psychiatric treatments carry high cost (e.g., ketamine, ECT) or unwanted side effects (e.g., metabolic disrup tion and weight gain for antipsychotics). Doing no harm is argu ably more important than improving the probability of recovery, and so precision mental health efforts could be especially impor tant in identifying which treatments are safest and most tolerable.Despite this progress, the potential for machine learning in psychiatry has just begun to be explored. Predicting treatment response is just one relatively narrow use case where machine learning can add value and improve mental health care. Predic tion can help with so many more clinical decisions and clinical processes. We could predict barriers that prevent an individual from engaging in care initially, or nonadherence or dropout from care after initiation. We could streamline patients to the appro priate level of care, such as selfguided programs vs. outpatient care, or intensive outpatient versus inpatient care, to maximize scarce health care resources. In selecting a specific treatment ap proach, we could optimize dosing or predict side effect profiles in order to improve symptoms but minimize impact on patient quality of life. Some psychiatric treatments carry high cost (e.g., ketamine, ECT) or unwanted side effects (e.g., metabolic disrup tion and weight gain for antipsychotics). Doing no harm is argu ably more important than improving the probability of recovery, and so precision mental health efforts could be especially impor tant in identifying which treatments are safest and most tolerable.</p>
        <p>Machine learning could even help sequence treatments over time, or design specific treatment protocols for an individual. For example, modular psychological interventions can be personal ized 66,68 , or tailored health behavior change interventions can be customized for an individual. This form of personalization and customization has proven effective in contexts like smoking ces sation, breast cancer screening, and physical activity 214,215 .Machine learning could even help sequence treatments over time, or design specific treatment protocols for an individual. For example, modular psychological interventions can be personal ized 66,68 , or tailored health behavior change interventions can be customized for an individual. This form of personalization and customization has proven effective in contexts like smoking ces sation, breast cancer screening, and physical activity 214,215 .</p>
        <p>Techniques like natural language processing, often using ma chine learning algorithms, give us the ability to draw insights from textbased data -e.g., social media posts, peersupport conversations, or conversation transcriptions -that might in form the content that is offered to an individual as part of his/her treatment to maximize future outcomes. In addition, the same analytic techniques may form the basis of interventions, such as chatbots, that could provide scalable support for loneliness, stress, or other subclinical psychological issues when human support is unavailable or not clinically warranted. This person alization of iCBT treatment may be particularly necessary for un guided interventions, where nonadherence is widespread and undermines the potential for symptom relief.Techniques like natural language processing, often using ma chine learning algorithms, give us the ability to draw insights from textbased data -e.g., social media posts, peersupport conversations, or conversation transcriptions -that might in form the content that is offered to an individual as part of his/her treatment to maximize future outcomes. In addition, the same analytic techniques may form the basis of interventions, such as chatbots, that could provide scalable support for loneliness, stress, or other subclinical psychological issues when human support is unavailable or not clinically warranted. This person alization of iCBT treatment may be particularly necessary for un guided interventions, where nonadherence is widespread and undermines the potential for symptom relief.</p>
        <p>Machine learning is a powerful tool that can help sift through multimodal predictors and model their complex/nonlinear contributions. And it can identify specific subtypes of patients, e.g., through clustering, for more nuanced prediction of treat ment outcomes. Machine learning techniques are allowing us to extract more knowledge from bigger datasets in a more efficient way -which is a good and promising thing.Machine learning is a powerful tool that can help sift through multimodal predictors and model their complex/nonlinear contributions. And it can identify specific subtypes of patients, e.g., through clustering, for more nuanced prediction of treat ment outcomes. Machine learning techniques are allowing us to extract more knowledge from bigger datasets in a more efficient way -which is a good and promising thing.</p>
        <p>However, the ultimate goal of psychiatry is to better treat men tal illness. The path toward machine learning improving psychi atric care in reallife settings is not only governed by statistical, but also by implementation considerations. Recent seminal findings 180,181 highlight that accurate algorithms alone are not enough to ensure the success of a decision support system for precision treatment. This is because many things change in the transition from a research setting into real patient care 39 . In practice, clinicians may override algorithm recommendations and choose alternative treatments. Patients may refuse the algorithmrecommended treatment, or have restrictions to its use that were not contemplated by the decision support tool. Recommendations may be provided in a poorlydesigned user interface, and thus may go unseen or be actively ignored. All of these factors contribute to a general phenomenon of reduced ef fect sizes when an algorithm is implemented in clinical practice.However, the ultimate goal of psychiatry is to better treat men tal illness. The path toward machine learning improving psychi atric care in reallife settings is not only governed by statistical, but also by implementation considerations. Recent seminal findings 180,181 highlight that accurate algorithms alone are not enough to ensure the success of a decision support system for precision treatment. This is because many things change in the transition from a research setting into real patient care 39 . In practice, clinicians may override algorithm recommendations and choose alternative treatments. Patients may refuse the algorithmrecommended treatment, or have restrictions to its use that were not contemplated by the decision support tool. Recommendations may be provided in a poorlydesigned user interface, and thus may go unseen or be actively ignored. All of these factors contribute to a general phenomenon of reduced ef fect sizes when an algorithm is implemented in clinical practice.</p>
        <p>In our own personal experience, patient concerns around privacy are a very real problem. Because mental health is par ticularly sensitive, capturing personal data can be challenging and we need to innovate ways of collecting these data so that we do not have a biased perspective of the landscape due to a poor sampling within certain groups. Data needs to be collected in such a way that participants are aware of how and for what pur poses those data will be used 216 .In our own personal experience, patient concerns around privacy are a very real problem. Because mental health is par ticularly sensitive, capturing personal data can be challenging and we need to innovate ways of collecting these data so that we do not have a biased perspective of the landscape due to a poor sampling within certain groups. Data needs to be collected in such a way that participants are aware of how and for what pur poses those data will be used 216 .</p>
        <p>Technology systems must implement careful logging process es to examine concept or data drift, where the underlying distri bution of a predictor or an outcome changes over time, and to ensure that the inputs and outputs of the system are auditable. This is a collective exercise of building trust in predictive mod els and how these will be potentially used to enhance patient outcomes, and can avoid the introduction of harm or biases in decisionmaking processes.Technology systems must implement careful logging process es to examine concept or data drift, where the underlying distri bution of a predictor or an outcome changes over time, and to ensure that the inputs and outputs of the system are auditable. This is a collective exercise of building trust in predictive mod els and how these will be potentially used to enhance patient outcomes, and can avoid the introduction of harm or biases in decisionmaking processes.</p>
        <p>This paper reviews many kinds of data that have been used to predict treatment outcomes in psychiatry. Ultimately, treatment responses emerge from multiple interacting biological, psycho logical and social factors. Therefore, in theory, multimodal ap proaches using demographic, clinical and brain variables should result in the most accurate predictions 217 . However, to this date, it is clear that certain kinds of data -specifically sociodemograph ic, selfreport, psychosocial and clinical data -consistently offer more meaningful and generalizable predictions. Other types of data that might be more scientifically appealing -such as neuro imaging and genetic data -have not yet shown compelling results in a large external sample, let alone in prospective implementa tion studies.This paper reviews many kinds of data that have been used to predict treatment outcomes in psychiatry. Ultimately, treatment responses emerge from multiple interacting biological, psycho logical and social factors. Therefore, in theory, multimodal ap proaches using demographic, clinical and brain variables should result in the most accurate predictions 217 . However, to this date, it is clear that certain kinds of data -specifically sociodemograph ic, selfreport, psychosocial and clinical data -consistently offer more meaningful and generalizable predictions. Other types of data that might be more scientifically appealing -such as neuro imaging and genetic data -have not yet shown compelling results in a large external sample, let alone in prospective implementa tion studies.</p>
        <p>Ultimately, data types that can be easily integrated into clini cal care in a costeffective and ethical way, which is appropriate for the prevalence and invasiveness of the therapy, are most like ly to show favorable return on investment for ultimate decision makers in health systems and health payers.Ultimately, data types that can be easily integrated into clini cal care in a costeffective and ethical way, which is appropriate for the prevalence and invasiveness of the therapy, are most like ly to show favorable return on investment for ultimate decision makers in health systems and health payers.</p>
        <p>World Psychiatry 20:2 -June 2021World Psychiatry 20:2 -June 2021</p>
        <p>ACKNOWLEDGEMENTS A.M. Chekroud holds equity in Spring Care Inc., Carbon Health Technologies Inc., and UnitedHealthcare. J. Bondar is an employee of Spring Care Inc. G. Do herty is cofounder of SilverCloud Health. R. Iniesta is supported by the Brain &amp; Behavior Research Foundation and the National Institute for Health Research (NIHR) Maudsley Biomedical Research Centre.ACKNOWLEDGEMENTS A.M. Chekroud holds equity in Spring Care Inc., Carbon Health Technologies Inc., and UnitedHealthcare. J. Bondar is an employee of Spring Care Inc. G. Do herty is cofounder of SilverCloud Health. R. Iniesta is supported by the Brain &amp; Behavior Research Foundation and the National Institute for Health Research (NIHR) Maudsley Biomedical Research Centre.</p>
    </text>
</tei>

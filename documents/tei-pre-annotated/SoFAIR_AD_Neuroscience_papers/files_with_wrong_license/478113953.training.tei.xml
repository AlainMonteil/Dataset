<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:14+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Spiking Neural Networks (SNNs) use spatiotemporal spike patterns to represent and transmit information, which are not only biologically realistic but also suitable for ultralow-power event-driven neuromorphic implementation. Just like other deep learning techniques, Deep Spiking Neural Networks (DeepSNNs) benefit from the deep architecture. However, the training of DeepSNNs is not straightforward because the wellstudied error back-propagation (BP) algorithm is not directly applicable. In this paper, we first establish an understanding as to why error back-propagation does not work well in DeepSNNs. We then propose a simple yet efficient Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons and a Spike-Timing-Dependent Back-Propagation (STDBP) learning algorithm for DeepSNNs where the timing of individual spikes is used to convey information (temporal coding), and learning (back-propagation) is performed based on spike timing in an event-driven manner. We show that DeepSNNs trained with the proposed single spike time-based learning algorithm can achieve state-of-the-art classification accuracy. Furthermore, by utilizing the trained model parameters obtained from the proposed STDBP learning algorithm, we demonstrate ultra-lowpower inference operations on a recently proposed neuromorphic inference accelerator. The experimental results also show that the neuromorphic hardware consumes 0.751 mW of the total power consumption and achieves a low latency of 47.71 ms to classify an image from the MNIST dataset. Overall, this work investigates the contribution of spike timing dynamics for information encoding, synaptic plasticity and decision making, providing a new perspective to the design of future DeepSNNs and neuromorphic hardware.Spiking Neural Networks (SNNs) use spatiotemporal spike patterns to represent and transmit information, which are not only biologically realistic but also suitable for ultralow-power event-driven neuromorphic implementation. Just like other deep learning techniques, Deep Spiking Neural Networks (DeepSNNs) benefit from the deep architecture. However, the training of DeepSNNs is not straightforward because the wellstudied error back-propagation (BP) algorithm is not directly applicable. In this paper, we first establish an understanding as to why error back-propagation does not work well in DeepSNNs. We then propose a simple yet efficient Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons and a Spike-Timing-Dependent Back-Propagation (STDBP) learning algorithm for DeepSNNs where the timing of individual spikes is used to convey information (temporal coding), and learning (back-propagation) is performed based on spike timing in an event-driven manner. We show that DeepSNNs trained with the proposed single spike time-based learning algorithm can achieve state-of-the-art classification accuracy. Furthermore, by utilizing the trained model parameters obtained from the proposed STDBP learning algorithm, we demonstrate ultra-lowpower inference operations on a recently proposed neuromorphic inference accelerator. The experimental results also show that the neuromorphic hardware consumes 0.751 mW of the total power consumption and achieves a low latency of 47.71 ms to classify an image from the MNIST dataset. Overall, this work investigates the contribution of spike timing dynamics for information encoding, synaptic plasticity and decision making, providing a new perspective to the design of future DeepSNNs and neuromorphic hardware.</p>
        <p>Index Terms-Spiking neural networks, Deep neural networks, Spike-timing-dependent learning, Event-driven, Neuromorphic hardwareIndex Terms-Spiking neural networks, Deep neural networks, Spike-timing-dependent learning, Event-driven, Neuromorphic hardware</p>
        <p>T HE success of Deep Neural Networks (DNNs) is attributed to their underlying deep hierarchical structure that is capable of learning representations of big data with multiple levels of abstraction [1]. Recent advances in DNNs have witnessed an increasing attention both from academia and industry with widespread applications in various areas such as image recognition [2], speech recognition [3], natural language processing [4], [5], and medical diagnosis [6]. However, training DNNs generally requires high-performance computing hardware (e.g., GPUs and computing clusters). Therefore, in power-critical computing platforms, such as edge computing, the deployment of DNNs remains drastically limited [7], [8]. Motivated by the principles of brain computing, Spiking Neural Networks (SNNs) offer a low-power alternative for neural network implementation and provide great computing potential equivalent to that of DNNs on an ultra-low-power spike-driven neuromorphic hardware [9]- [14].T HE success of Deep Neural Networks (DNNs) is attributed to their underlying deep hierarchical structure that is capable of learning representations of big data with multiple levels of abstraction [1]. Recent advances in DNNs have witnessed an increasing attention both from academia and industry with widespread applications in various areas such as image recognition [2], speech recognition [3], natural language processing [4], [5], and medical diagnosis [6]. However, training DNNs generally requires high-performance computing hardware (e.g., GPUs and computing clusters). Therefore, in power-critical computing platforms, such as edge computing, the deployment of DNNs remains drastically limited [7], [8]. Motivated by the principles of brain computing, Spiking Neural Networks (SNNs) offer a low-power alternative for neural network implementation and provide great computing potential equivalent to that of DNNs on an ultra-low-power spike-driven neuromorphic hardware [9]- [14].</p>
        <p>However, due to the complex temporal dynamics of spiking neuronal models and the non-differentiable nature of their spiking activity, the well-known error back-propagation (BP) learning algorithm cannot not be directly applied to deep SNNs [15]. As such, SNNs have yet to match the performance of their DNN counterparts in pattern classification tasks [16]. To address the aforementioned problems, many solutions have been proposed recently [17]- [23], which can be grouped into the following three categories.However, due to the complex temporal dynamics of spiking neuronal models and the non-differentiable nature of their spiking activity, the well-known error back-propagation (BP) learning algorithm cannot not be directly applied to deep SNNs [15]. As such, SNNs have yet to match the performance of their DNN counterparts in pattern classification tasks [16]. To address the aforementioned problems, many solutions have been proposed recently [17]- [23], which can be grouped into the following three categories.</p>
        <p>The first category includes ANN-to-SNN conversion methods where we first train an ANN, and subsequently map the pretrained ANN weights to an SNN equivalent [24]- [34]. By carefully designing the hyperparameters of the SNN, e.g., spiking neuron model, firing threshold, we can approximate well the neural representation of the pre-trained ANN. Such ANN-to-SNN conversion benefits from the state-of-the-art ANN training algorithms. However, the conversion often leads to loss of accuracy due to approximation. A number of studies attempted to mitigate such approximation effect. For instance, the weight normalization [29]- [31] technique rescales the pretrained weights to prevent the approximation errors from either becoming excessive or resulting in too little firing of the spiking neurons. In addition, existing studies show that training ANNs with noise-augmented data can improve the robustness and accuracy of the converted SNNs [27], [28]. Yet, despite considerable progress in improving SNN training and their applicability, their performance is still lags behind their ANN counterparts. As the firing rate of spiking neurons is typically used to encode the activation value of artificial neurons, the spike timing information is not effectively exploited and the run-time computation is also energy-intensive when deployed on neuromorphic hardware [15], [35], [36].The first category includes ANN-to-SNN conversion methods where we first train an ANN, and subsequently map the pretrained ANN weights to an SNN equivalent [24]- [34]. By carefully designing the hyperparameters of the SNN, e.g., spiking neuron model, firing threshold, we can approximate well the neural representation of the pre-trained ANN. Such ANN-to-SNN conversion benefits from the state-of-the-art ANN training algorithms. However, the conversion often leads to loss of accuracy due to approximation. A number of studies attempted to mitigate such approximation effect. For instance, the weight normalization [29]- [31] technique rescales the pretrained weights to prevent the approximation errors from either becoming excessive or resulting in too little firing of the spiking neurons. In addition, existing studies show that training ANNs with noise-augmented data can improve the robustness and accuracy of the converted SNNs [27], [28]. Yet, despite considerable progress in improving SNN training and their applicability, their performance is still lags behind their ANN counterparts. As the firing rate of spiking neurons is typically used to encode the activation value of artificial neurons, the spike timing information is not effectively exploited and the run-time computation is also energy-intensive when deployed on neuromorphic hardware [15], [35], [36].</p>
        <p>The second category consists of membrane potential-driven learning algorithms, which treat the neuron's membrane potential as a differentiable signal. The discontinuities of the membrane potential at spike timings are addressed with continuous surrogate derivatives [15]. In [37]- [39], the errors are back-propagated based on the membrane potential at a single time step, which totally ignores the temporal dependency. To address this problem, 
            <rs type="software">SLAYER</rs> [8] and 
            <rs type="software">STBP</rs> [40], [41] train DeepSNNs with surrogate derivatives based on the idea of Back-propagation Through Time (BPTT) algorithm. While competitive accuracies are reported on the MNIST and CIFAR-10 datasets [41], the computational and memory requirements of BPTT-based approaches remain high because the intermediate activation values must be stored for the gradient computation.
        </p>
        <p>The third category includes the spike-driven learning algorithms, which consider the timing of spikes as a relevant signal for synaptic update [36], [42]- [49]. The typical examples include SpikeProp [43] and its derivatives [44]- [47]. These algorithms rely on the assumption that the neuron membrane potential increases linearly over the infinitesimal time interval around the spike time. This allows the calculation of derivatives at each spike time and facilitates the implementation of backpropagation in multi-layer SNNs. Grounded on the time-to-firstspike (TTFS) coding, Mostafa [36] applied non-leaky integrateand-fire neurons to avoid the problem of the non-differentiable spike function, and demonstrated competitive performance on the MNIST dataset. Along the same direction, the performance of SNNs with spike-driven learning algorithms is further improved in [48] and [49]. However, the existing spike-driven learning algorithms face several problems, for example, dead neurons and gradient exploding [36]. In [36], some constraints are imposed on the synaptic weights to overcome the dead neuron problem, and a gradient normalization strategy is used to overcome the problem of gradient exploding. These complex training strategies, however, limit the scalability of the learning algorithm.The third category includes the spike-driven learning algorithms, which consider the timing of spikes as a relevant signal for synaptic update [36], [42]- [49]. The typical examples include SpikeProp [43] and its derivatives [44]- [47]. These algorithms rely on the assumption that the neuron membrane potential increases linearly over the infinitesimal time interval around the spike time. This allows the calculation of derivatives at each spike time and facilitates the implementation of backpropagation in multi-layer SNNs. Grounded on the time-to-firstspike (TTFS) coding, Mostafa [36] applied non-leaky integrateand-fire neurons to avoid the problem of the non-differentiable spike function, and demonstrated competitive performance on the MNIST dataset. Along the same direction, the performance of SNNs with spike-driven learning algorithms is further improved in [48] and [49]. However, the existing spike-driven learning algorithms face several problems, for example, dead neurons and gradient exploding [36]. In [36], some constraints are imposed on the synaptic weights to overcome the dead neuron problem, and a gradient normalization strategy is used to overcome the problem of gradient exploding. These complex training strategies, however, limit the scalability of the learning algorithm.</p>
        <p>Among the existing learning algorithms, only the spikedriven learning algorithms perform the SNNs training in a strictly event-driven manner, and are compatible with the temporal coding in which the information is carried by the timing of individual spikes that has a high level of sparsity. Hence, spike-driven learning algorithms hold great potentials to enable efficient training and inference on low-power neuromorphic devices. Recently, several neuromorphic architectures have been proposed to accelerate the SNN inference and training [24], [50]- [57]. Notably, Intel Loihi [50] can support on-chip learning with a wide range of spike-timing-dependent-plasticity (STDP) rules. During SNN inference, Loihi is claimed to be 1000× faster than the general-purpose processors such as CPUs and GPUs, while using much less power. Additionally, Srivatsa et al. [58] recently proposed a neuromorphic accelerator called You Only Spike Once (YOSO) that can leverage the sparse spiking activity to achieve ultra-low-power inferences with high classification accuracies [58].Among the existing learning algorithms, only the spikedriven learning algorithms perform the SNNs training in a strictly event-driven manner, and are compatible with the temporal coding in which the information is carried by the timing of individual spikes that has a high level of sparsity. Hence, spike-driven learning algorithms hold great potentials to enable efficient training and inference on low-power neuromorphic devices. Recently, several neuromorphic architectures have been proposed to accelerate the SNN inference and training [24], [50]- [57]. Notably, Intel Loihi [50] can support on-chip learning with a wide range of spike-timing-dependent-plasticity (STDP) rules. During SNN inference, Loihi is claimed to be 1000× faster than the general-purpose processors such as CPUs and GPUs, while using much less power. Additionally, Srivatsa et al. [58] recently proposed a neuromorphic accelerator called You Only Spike Once (YOSO) that can leverage the sparse spiking activity to achieve ultra-low-power inferences with high classification accuracies [58].</p>
        <p>In this work, we develop an effective spike-driven learning algorithm for training high-performance deep SNNs. This paper makes the following main contributions:In this work, we develop an effective spike-driven learning algorithm for training high-performance deep SNNs. This paper makes the following main contributions:</p>
        <p>1) A comprehensive study of what makes the BP algorithm incompatible for training SNNs, including the problems of non-differentiable spike generation function, exploding gradient and dead neuron. Building on the insights from this study, we put forward a Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons to resolve these problems.1) A comprehensive study of what makes the BP algorithm incompatible for training SNNs, including the problems of non-differentiable spike generation function, exploding gradient and dead neuron. Building on the insights from this study, we put forward a Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons to resolve these problems.</p>
        <p>2) Based on the proposed ReL-PSP, we derive a novel spiketiming-dependent BP algorithm (STDBP) for DeepSNNs. In this algorithm, the timing of spikes is used as the information carrier, and learning happens only at the spike times in a fully event-driven manner.2) Based on the proposed ReL-PSP, we derive a novel spiketiming-dependent BP algorithm (STDBP) for DeepSNNs. In this algorithm, the timing of spikes is used as the information carrier, and learning happens only at the spike times in a fully event-driven manner.</p>
        <p>3) We demonstrate the effectiveness and scalability of STDBP with ReL-PSP in convolutional spiking neural networks (C-SNN), which achieves a classification accuracy of 99.5% on the MNIST dataset.3) We demonstrate the effectiveness and scalability of STDBP with ReL-PSP in convolutional spiking neural networks (C-SNN), which achieves a classification accuracy of 99.5% on the MNIST dataset.</p>
        <p>4) Lastly, using the SNN trained with the proposed STDBP learning algorithm, we demonstrate ultra-low-power and rapid inference on the recently proposed neuromorphic accelerator, YOSO.4) Lastly, using the SNN trained with the proposed STDBP learning algorithm, we demonstrate ultra-low-power and rapid inference on the recently proposed neuromorphic accelerator, YOSO.</p>
        <p>In summary, this work not only provides novel perspectives on the significance of neuronal dynamics in information coding, synaptic plasticity, and decision making within a SNN-based computing paradigm, but also demonstrates the possibility of realizing ultra-low-power inference with high classification accuracy on the emerging neuromorphic hardware.In summary, this work not only provides novel perspectives on the significance of neuronal dynamics in information coding, synaptic plasticity, and decision making within a SNN-based computing paradigm, but also demonstrates the possibility of realizing ultra-low-power inference with high classification accuracy on the emerging neuromorphic hardware.</p>
        <p>The remainder of this paper is organized as follows. In section II, we establish an understanding as to why error backpropagation does not work well in DeepSNNs. Section III presents a detailed description of the proposed ReL-PSP and the STDBP learning algorithm. In section IV, we introduce the recently proposed neuromorphic hardware architecture, YOSO. Section V presents a comprehensive experimental evaluation of the proposed ReL-PSP and STDBP learning algorithm. Finally, section VI discusses the results and draw conclusions.The remainder of this paper is organized as follows. In section II, we establish an understanding as to why error backpropagation does not work well in DeepSNNs. Section III presents a detailed description of the proposed ReL-PSP and the STDBP learning algorithm. In section IV, we introduce the recently proposed neuromorphic hardware architecture, YOSO. Section V presents a comprehensive experimental evaluation of the proposed ReL-PSP and STDBP learning algorithm. Finally, section VI discusses the results and draw conclusions.</p>
        <p>Error back-propagation, specifically stochastic gradient descent, is the workhorse of learning in DNNs. However, as shown in Fig. 1, the dynamics of a typical artificial neuron used in DNNs differ greatly from their spiking neuron counterparts, therefore the well-known BP algorithm cannot be directly applied to DeepSNNs due to the non-differentiable nature of the neuron spiking activity, exploding gradients and dead neurons problems. All these issues will be discussed in more depth in the following.Error back-propagation, specifically stochastic gradient descent, is the workhorse of learning in DNNs. However, as shown in Fig. 1, the dynamics of a typical artificial neuron used in DNNs differ greatly from their spiking neuron counterparts, therefore the well-known BP algorithm cannot be directly applied to DeepSNNs due to the non-differentiable nature of the neuron spiking activity, exploding gradients and dead neurons problems. All these issues will be discussed in more depth in the following.</p>
        <p>Consider a fully connected DeepSNN. For simplicity, each neuron is assumed to emit at most one spike. In general, the membrane potential V l j of neuron j in layer l with N presynaptic connections can be expressed as, where t l-1 i is the spike of the ith neuron in layer l-1, and ω l ij is the synaptic weight of the connection from neuron i (in l-1 layer) to neuron j (in l layer). Each incoming spike from neuron i will induce a postsynaptic potential (PSP) at neuron j, and the kernel ε(t -t l-1 i ) is used to describe the PSP generated by the spike t l-1 i . Hence each input spike makes a contribution to the membrane potential of the neuron as described by ω l ij ε(t -t l-1 i ) in Eq. 1. There are several PSP functions, and a commonly used one is alpha function which is defined asConsider a fully connected DeepSNN. For simplicity, each neuron is assumed to emit at most one spike. In general, the membrane potential V l j of neuron j in layer l with N presynaptic connections can be expressed as, where t l-1 i is the spike of the ith neuron in layer l-1, and ω l ij is the synaptic weight of the connection from neuron i (in l-1 layer) to neuron j (in l layer). Each incoming spike from neuron i will induce a postsynaptic potential (PSP) at neuron j, and the kernel ε(t -t l-1 i ) is used to describe the PSP generated by the spike t l-1 i . Hence each input spike makes a contribution to the membrane potential of the neuron as described by ω l ij ε(t -t l-1 i ) in Eq. 1. There are several PSP functions, and a commonly used one is alpha function which is defined as</p>
        <p>Fig. 2(a) shows the alpha-PSP response function. As shown in Fig. 2(b), integrating the weighted PSPs gives the dynamics of the membrane potential V l j (t). The neuron j will emit a spike when its membrane potential V l j (t) reaches the firing threshold ϑ, as mathematically defined in the spike generation function F:Fig. 2(a) shows the alpha-PSP response function. As shown in Fig. 2(b), integrating the weighted PSPs gives the dynamics of the membrane potential V l j (t). The neuron j will emit a spike when its membrane potential V l j (t) reaches the firing threshold ϑ, as mathematically defined in the spike generation function F:</p>
        <p>Once a spike is emitted, the refractory kernel η(t -t l j ) is used to reset the membrane potential to resting.Once a spike is emitted, the refractory kernel η(t -t l j ) is used to reset the membrane potential to resting.</p>
        <p>To train SNNs using BP, we need to compute the derivative of the postsynaptic spike time t l j with respect to a presynaptic spike time t l-1 i and synaptic weight ω l ij of the corresponding connection:To train SNNs using BP, we need to compute the derivative of the postsynaptic spike time t l j with respect to a presynaptic spike time t l-1 i and synaptic weight ω l ij of the corresponding connection:</p>
        <p>Due to the discrete nature of the spike generation function (Eq. 3), we face a challenge in solving the partial derivative ∂t l j /∂V l j (t l j ) in Eq. 4, which we refer to as the problem of nondifferentiable spike function. Existing spike-driven learning algorithms [43], [59] assume that the membrane potential V l j (t) increases linearly in the infinitesimal time interval before spike time t j . Then, ∂t j /V j (t) can be expressed asDue to the discrete nature of the spike generation function (Eq. 3), we face a challenge in solving the partial derivative ∂t l j /∂V l j (t l j ) in Eq. 4, which we refer to as the problem of nondifferentiable spike function. Existing spike-driven learning algorithms [43], [59] assume that the membrane potential V l j (t) increases linearly in the infinitesimal time interval before spike time t j . Then, ∂t j /V j (t) can be expressed as</p>
        <p>The exploding gradient problem occurs when ∂V l j (t l j )/∂t l j ≈ 0 i.e. the membrane potential is reaching the firing threshold, emitting a spike (Fig. 2b). Since ∂V j (t j )/∂t j is the denominator in Eq. 6, this causes Eq. 6 to explode with large weight updates. Despite the progress made to address this problem such as adaptive learning rate [60] and dynamic firing threshold [47], the problem has not been fully resolved.The exploding gradient problem occurs when ∂V l j (t l j )/∂t l j ≈ 0 i.e. the membrane potential is reaching the firing threshold, emitting a spike (Fig. 2b). Since ∂V j (t j )/∂t j is the denominator in Eq. 6, this causes Eq. 6 to explode with large weight updates. Despite the progress made to address this problem such as adaptive learning rate [60] and dynamic firing threshold [47], the problem has not been fully resolved.</p>
        <p>It is shown from Eq. 4 and Eq. 5 that when the presynaptic neuron does not emit a spike, the error cannot be backpropagated through ∂V l j (t l j )/∂t l-1 i , which then results in dead neurons. This problem also exists with analog neurons with ReLU activation function in DNNs. However, due to the leaky nature of the PSP kernel and the spike generation mechanism, spiking neurons encounter a more serious dead neuron problem. As shown in Fig. 2(c), there are three input spikes, and the neuron emits a spike with large synaptic weights (blue curve). With slightly reduced synaptic weights, the membrane potential stays below the threshold hence becoming a dead neuron (green curve). When the neuron does not spike, no errors can backpropagate through it. The dead neuron problem is fatal in spike-driven learning algorithms.It is shown from Eq. 4 and Eq. 5 that when the presynaptic neuron does not emit a spike, the error cannot be backpropagated through ∂V l j (t l j )/∂t l-1 i , which then results in dead neurons. This problem also exists with analog neurons with ReLU activation function in DNNs. However, due to the leaky nature of the PSP kernel and the spike generation mechanism, spiking neurons encounter a more serious dead neuron problem. As shown in Fig. 2(c), there are three input spikes, and the neuron emits a spike with large synaptic weights (blue curve). With slightly reduced synaptic weights, the membrane potential stays below the threshold hence becoming a dead neuron (green curve). When the neuron does not spike, no errors can backpropagate through it. The dead neuron problem is fatal in spike-driven learning algorithms.</p>
        <p>In this section, we describe how the above challenges may be overcome, thereby a DeepSNN may still be trained using BP. To this end, we introduce Rectified Linear Postsynaptic Potential function (ReL-PSP) as a new spiking neuron model, and present a spike-timing-dependent back propagation (STDBP) learning algorithm that is based on ReL-PSP.In this section, we describe how the above challenges may be overcome, thereby a DeepSNN may still be trained using BP. To this end, we introduce Rectified Linear Postsynaptic Potential function (ReL-PSP) as a new spiking neuron model, and present a spike-timing-dependent back propagation (STDBP) learning algorithm that is based on ReL-PSP.</p>
        <p>As discussed earlier in Section II, BP cannot be directly applied in DeepSNNs due to problems of non-differentiable spike function, exploding gradient and dead neuron. To overcome these problems, we propose a simple yet efficient Rectified Linear Postsynaptic Potential (ReL-PSP) based spiking neuron model, whose dynamics is defined as follows,As discussed earlier in Section II, BP cannot be directly applied in DeepSNNs due to problems of non-differentiable spike function, exploding gradient and dead neuron. To overcome these problems, we propose a simple yet efficient Rectified Linear Postsynaptic Potential (ReL-PSP) based spiking neuron model, whose dynamics is defined as follows,</p>
        <p>whereby K(t -t l-1 i ) is the kernel of the PSP function, which is defined aswhereby K(t -t l-1 i ) is the kernel of the PSP function, which is defined as</p>
        <p>As shown in Fig. 3(a), given an input spike at t l-1 i , the membrane potential after t l-1 i is a linear function of time t. Since the shape of the proposed PSP function resembles that of a rectified linear function, we name it the ReL-PSP function.As shown in Fig. 3(a), given an input spike at t l-1 i , the membrane potential after t l-1 i is a linear function of time t. Since the shape of the proposed PSP function resembles that of a rectified linear function, we name it the ReL-PSP function.</p>
        <p>In the following, we will analyze how the proposed neuron model solves the above-mentioned problems.In the following, we will analyze how the proposed neuron model solves the above-mentioned problems.</p>
        <p>1) Non-differentiable spike function: As shown in Fig. 3b, due to the linearity of the ReL-PSP, the membrane potential V l j (t) increases linearly prior to spike time t l j . The linearity is a much desired property from a postsynaptic potential function. We can now directly use Eq. 10 to compute ∂t l j /∂V l j (t l j ). This resolves the issue of non-differentiable spike generation.1) Non-differentiable spike function: As shown in Fig. 3b, due to the linearity of the ReL-PSP, the membrane potential V l j (t) increases linearly prior to spike time t l j . The linearity is a much desired property from a postsynaptic potential function. We can now directly use Eq. 10 to compute ∂t l j /∂V l j (t l j ). This resolves the issue of non-differentiable spike generation.</p>
        <p>The precise gradients in BP provide the necessary information for network optimization, which is the key to the performance of DNNs. Without having to assume linearity, we use the precise value of ∂t l j /∂V l j (t l j ) instead of approximating it, and avoid accumulating errors across multiple layers.The precise gradients in BP provide the necessary information for network optimization, which is the key to the performance of DNNs. Without having to assume linearity, we use the precise value of ∂t l j /∂V l j (t l j ) instead of approximating it, and avoid accumulating errors across multiple layers.</p>
        <p>2) Gradient explosion: Exploding gradient occurs when the denominator in Eq. 6 approaches 0. In this case, the membrane potential just reaches the firing threshold at spike time, and is caused by the combined effect of ω l ij and partial derivative of the PSP function. As N i ω l ij may still be close to 0, the exploding gradient problem may not be completely solved. However, from Eqs. 3 and 8, we obtain the spike time t l j as a function of input spikes and synaptic weights ω l ij , and the spike time t l j can be calculated as,2) Gradient explosion: Exploding gradient occurs when the denominator in Eq. 6 approaches 0. In this case, the membrane potential just reaches the firing threshold at spike time, and is caused by the combined effect of ω l ij and partial derivative of the PSP function. As N i ω l ij may still be close to 0, the exploding gradient problem may not be completely solved. However, from Eqs. 3 and 8, we obtain the spike time t l j as a function of input spikes and synaptic weights ω l ij , and the spike time t l j can be calculated as,</p>
        <p>Should the N i ω l ij be close to 0, the spike t l j will be emitted late, and may not contribute to the spike t l+1 j in the next layer. Therefore, the neuron j in the l layer does not participate in error BP, and does not result in exploding gradient.Should the N i ω l ij be close to 0, the spike t l j will be emitted late, and may not contribute to the spike t l+1 j in the next layer. Therefore, the neuron j in the l layer does not participate in error BP, and does not result in exploding gradient.</p>
        <p>3) Dead neuron: In neural networks, sparse representation (few activated neurons) has many advantages, such as information disentangling, efficient variable-size representation, linear separability etc. However, sparsity may also adversely affect the predictive performance. Given the same number of neurons, sparsity reduces the effective capacity of the model [61]. Unfortunately, as shown in Fig. 2(c), due to the leaky nature of the alpha-PSP and the spike generation mechanism, such a spiking neuron is more likely to suffer from the dead neuron problem.3) Dead neuron: In neural networks, sparse representation (few activated neurons) has many advantages, such as information disentangling, efficient variable-size representation, linear separability etc. However, sparsity may also adversely affect the predictive performance. Given the same number of neurons, sparsity reduces the effective capacity of the model [61]. Unfortunately, as shown in Fig. 2(c), due to the leaky nature of the alpha-PSP and the spike generation mechanism, such a spiking neuron is more likely to suffer from the dead neuron problem.</p>
        <p>As shown in Fig. 3(c), with the ReL-PSP kernel, the PSP increases over time within the simulation window T max until the postsynaptic neuron fires a spike . Hence the neuron with a more positive sum of weights fires earlier than one with a less positive sum, with lower probability of becoming a dead neuron. Overall, the proposed ReL-PSP greatly alleviates the dead neuron problem as the PSP does not decay over time, while maintaining a sparse representation to the same extent of the ReLU activation function.As shown in Fig. 3(c), with the ReL-PSP kernel, the PSP increases over time within the simulation window T max until the postsynaptic neuron fires a spike . Hence the neuron with a more positive sum of weights fires earlier than one with a less positive sum, with lower probability of becoming a dead neuron. Overall, the proposed ReL-PSP greatly alleviates the dead neuron problem as the PSP does not decay over time, while maintaining a sparse representation to the same extent of the ReLU activation function.</p>
        <p>Given a classification task with n categories, each neuron in the output layer is assigned to a category. When a training sample is presented to the neural network, the corresponding output neuron should fire the earliest. Several loss functions can be constructed to achieve this goal [36], [48], [49]. In this work, the cross-entropy loss function is used. To minimise the spike time of the target neuron, at the same time, maximise the spike time of non-target neurons, we use the softmax function on the negative values of the spike times in the output layer: p j = exp(-t j )/ n i exp(-t i ). The loss function is given by, where t o is the vector of the spike times in the output layer and g is the target class index [36].Given a classification task with n categories, each neuron in the output layer is assigned to a category. When a training sample is presented to the neural network, the corresponding output neuron should fire the earliest. Several loss functions can be constructed to achieve this goal [36], [48], [49]. In this work, the cross-entropy loss function is used. To minimise the spike time of the target neuron, at the same time, maximise the spike time of non-target neurons, we use the softmax function on the negative values of the spike times in the output layer: p j = exp(-t j )/ n i exp(-t i ). The loss function is given by, where t o is the vector of the spike times in the output layer and g is the target class index [36].</p>
        <p>The loss function is minimised by updating the synaptic weights across the network. This has the effect of delaying or advancing spike times across the network. The derivatives of the first spike time t l j with respect to synaptic weights ω l ij and input spike times t l-1 i are given byThe loss function is minimised by updating the synaptic weights across the network. This has the effect of delaying or advancing spike times across the network. The derivatives of the first spike time t l j with respect to synaptic weights ω l ij and input spike times t l-1 i are given by</p>
        <p>Following Eq. 13 and Eq. 14, a standard BP can be applied for DeepSNNs training.Following Eq. 13 and Eq. 14, a standard BP can be applied for DeepSNNs training.</p>
        <p>To put the proposed spiking neuron model and STDBP learning algorithm into action, we evaluate their deployment on a hardware architecture for pattern classification tasks. We first implement them in spiking neural networks for inference operations on YOSO platform [58] which is specifically designed for accelerating temporal coding based SNN models with sparse spiking activity. It was shown that YOSO facilitates ultralow-power inference operations (&lt; 1 mW) as a neuromorphic accelerator with state-of-the-art performance.To put the proposed spiking neuron model and STDBP learning algorithm into action, we evaluate their deployment on a hardware architecture for pattern classification tasks. We first implement them in spiking neural networks for inference operations on YOSO platform [58] which is specifically designed for accelerating temporal coding based SNN models with sparse spiking activity. It was shown that YOSO facilitates ultralow-power inference operations (&lt; 1 mW) as a neuromorphic accelerator with state-of-the-art performance.</p>
        <p>In this section, we will describe the hardware architecture of YOSO, the technique that maps SNNs on YOSO, and the simulation methodology that evaluates the YOSO performance during inference operations.In this section, we will describe the hardware architecture of YOSO, the technique that maps SNNs on YOSO, and the simulation methodology that evaluates the YOSO performance during inference operations.</p>
        <p>As shown in Fig. 4 (a), YOSO is a Network-On-Chip (NoC) architecture with multiple processing elements (PEs) connected in a mesh topology. Each PE has a router, which facilitates the propagation of spikes from one PE to another. As shown in Fig. 4 (b), each PE in the NoC consists of four static randomaccess memories (SRAMs), a memory interface, a core, a router interface, first-in-first-out (FIFO) buffers to facilitate communication between router and router interface. The router sends or receives the spikes to or from other PEs in the NoC. During the initialization phase, i.e. when the accelerator is downloading the model to be run, the router sends the spikes to SRAMs directly via router and memory interfaces. During the inference phase, the router sends the spikes to the core, where all the computations take place. Note that YOSO [58] can only accelerate inference operations and training needs to be performed offline.As shown in Fig. 4 (a), YOSO is a Network-On-Chip (NoC) architecture with multiple processing elements (PEs) connected in a mesh topology. Each PE has a router, which facilitates the propagation of spikes from one PE to another. As shown in Fig. 4 (b), each PE in the NoC consists of four static randomaccess memories (SRAMs), a memory interface, a core, a router interface, first-in-first-out (FIFO) buffers to facilitate communication between router and router interface. The router sends or receives the spikes to or from other PEs in the NoC. During the initialization phase, i.e. when the accelerator is downloading the model to be run, the router sends the spikes to SRAMs directly via router and memory interfaces. During the inference phase, the router sends the spikes to the core, where all the computations take place. Note that YOSO [58] can only accelerate inference operations and training needs to be performed offline.</p>
        <p>The SRAMs are used to store all the information required for processing the incoming spikes and for generating the output spikes. As shown in Fig. 4 (b), the SRAMs can communicate with the core and router interface via a memory interface. The accumulated weight, neuron, weight, and spike address SRAMs store accumulated weights, neuron potentials, weights between two layers, and the spike addresses of the neurons allocated for that PE, respectively.The SRAMs are used to store all the information required for processing the incoming spikes and for generating the output spikes. As shown in Fig. 4 (b), the SRAMs can communicate with the core and router interface via a memory interface. The accumulated weight, neuron, weight, and spike address SRAMs store accumulated weights, neuron potentials, weights between two layers, and the spike addresses of the neurons allocated for that PE, respectively.</p>
        <p>As shown in Fig. 4 (c), the core consists of three modulesload, compute, and store. The load module is responsible for decoding the information encoded in incoming spikes. A spike processing algorithm introduced in [58] is used to encode the information in spike packets that can read by the load module. After decoding the information in incoming spikes, the load module generates read requests to different SRAM blocks. After sending the read requests generated by one incoming spike, it transits to the idle state and waits for the next input spike. The compute module receives the data requested by the load module from the SRAMs, updates the data using the saturated adder, and sends the results to the FIFO connected to the store module. The store module receives addresses of the loaded data from the load module and the results from the compute module. Using the data and addresses received, it writes the data back to the SRAMs. It also generates spikes when a condition, depending on the chosen techniques, TTFS or softmax, is met to be sent to the next layer. In addition, the core is designed based on the decoupled access-execute model [62], which enables it to hide memory access latency by performing different computations parallelly.As shown in Fig. 4 (c), the core consists of three modulesload, compute, and store. The load module is responsible for decoding the information encoded in incoming spikes. A spike processing algorithm introduced in [58] is used to encode the information in spike packets that can read by the load module. After decoding the information in incoming spikes, the load module generates read requests to different SRAM blocks. After sending the read requests generated by one incoming spike, it transits to the idle state and waits for the next input spike. The compute module receives the data requested by the load module from the SRAMs, updates the data using the saturated adder, and sends the results to the FIFO connected to the store module. The store module receives addresses of the loaded data from the load module and the results from the compute module. Using the data and addresses received, it writes the data back to the SRAMs. It also generates spikes when a condition, depending on the chosen techniques, TTFS or softmax, is met to be sent to the next layer. In addition, the core is designed based on the decoupled access-execute model [62], which enables it to hide memory access latency by performing different computations parallelly.</p>
        <p>In this work, we plan to accelerate our trained SNN models on YOSO [58]. To map a m × n fully connected SNN layer on YOSO [58], a minimum of C = M AX( n N , m×n W ) PEs are needed where N is the maximum number of neurons that can be mapped to a single core and W is the maximum number of weights that the core can contain. The PEs are placed within a √ C by √ C grid. Each PE in the grid will be allocated to a layer or a part of layer for processing received by that layer.In this work, we plan to accelerate our trained SNN models on YOSO [58]. To map a m × n fully connected SNN layer on YOSO [58], a minimum of C = M AX( n N , m×n W ) PEs are needed where N is the maximum number of neurons that can be mapped to a single core and W is the maximum number of weights that the core can contain. The PEs are placed within a √ C by √ C grid. Each PE in the grid will be allocated to a layer or a part of layer for processing received by that layer.</p>
        <p>We implemented the YOSO's NoC architecture using the OpenSMART NoC generator [63]. The X-Y routing mechanism is used to send the spike packets from one PE to another. In addition, we designed the PEs in such a way that each PE can meet the memory and computational requirements of at least 256 neurons [58]. To evaluate the performance of YOSO [58], we synthesized the hardware architecture shown in Fig. 4 using 
            <rs type="software">Synopsys Design Compiler</rs> version P-2019.03-SP
            <rs type="version">5</rs> targeting a 22nm technology node with a 6 × 7 PE configuration. Gatelevel simulations are performed using the Synopsys VCS-MX K-2015.09-SP2-9 and power analysis is performed using the 
            <rs type="software">Synopsys PrimePower</rs> version P-2019
            <rs type="version">.03-SP5</rs>.
        </p>
        <p>In this section, we evaluate both fully connected SNNs and convolutional SNNs on the image classification task based on the MNIST [64], Fashion-MNIST [65] and Caltech 101 face/motorbike datasets 1 . We benchmark their learning capabilities against existing spike-driven learning algorithms. In addition, we evaluate the inference speed and energy efficiency of our fully connected SNN on the YOSO neuromorphic accelerator.In this section, we evaluate both fully connected SNNs and convolutional SNNs on the image classification task based on the MNIST [64], Fashion-MNIST [65] and Caltech 101 face/motorbike datasets 1 . We benchmark their learning capabilities against existing spike-driven learning algorithms. In addition, we evaluate the inference speed and energy efficiency of our fully connected SNN on the YOSO neuromorphic accelerator.</p>
        <p>We employ an efficient temporal coding scheme that encodes information into spike timing, following the assumption that strongly activated neurons tend to fire early [66]. In practice, the input pixel intensity value is encoded into spike timing according to t i = α(-s i + 255)/255, where t i is the firing time of the ith neuron, s i is intensity value of the ith pixel with s i ∈ [0, 255], and α is a scaling factor. In this way, more salient information is encoded into an earlier spike by the corresponding input neuron. These encoded spikes are propagated to subsequent layers following the dynamics of the SNN. Similar to the input layer, the neurons in the hidden and output layer that are strongly activated will fire first. As such, the temporal coding is maintained throughout the DeepSNN, and the output neuron that fires first categorizes the input sample.We employ an efficient temporal coding scheme that encodes information into spike timing, following the assumption that strongly activated neurons tend to fire early [66]. In practice, the input pixel intensity value is encoded into spike timing according to t i = α(-s i + 255)/255, where t i is the firing time of the ith neuron, s i is intensity value of the ith pixel with s i ∈ [0, 255], and α is a scaling factor. In this way, more salient information is encoded into an earlier spike by the corresponding input neuron. These encoded spikes are propagated to subsequent layers following the dynamics of the SNN. Similar to the input layer, the neurons in the hidden and output layer that are strongly activated will fire first. As such, the temporal coding is maintained throughout the DeepSNN, and the output neuron that fires first categorizes the input sample.</p>
        <p>1 http: //www.vision.caltech.edu1 http: //www.vision.caltech.edu</p>
        <p>The MNIST dataset comprises of 70,000 28 × 28 grayscaled images, with 60,000 and 10,000 for training and testing, respectively. We first train an ANN model to initialize the the corresponding SNN. Following the temporal coding in section V-A., we encode the images into spike patterns, then use them to train a fully connected and convolutional SNN using the proposed STDBP algorithm. A complete convolutional SNN network structure is provided in Fig. 5. During training, we add jittering noise to the input spike patterns so as to improve the performance of the trained model. The jitter intervals are randomly drawn from a Gaussian distribution with zero mean and variance σ j = 0.14. The SNN is trained for 150 epochs using the Adam optimizer, with a batch size of 128. The hyperparameters β 1 and β 2 of the Adam optimizer are set to 0.9 and 0.999, respectively. The learning rate starts at 0.0002 and gradually decreases to 0.00005 by the end of the training.The MNIST dataset comprises of 70,000 28 × 28 grayscaled images, with 60,000 and 10,000 for training and testing, respectively. We first train an ANN model to initialize the the corresponding SNN. Following the temporal coding in section V-A., we encode the images into spike patterns, then use them to train a fully connected and convolutional SNN using the proposed STDBP algorithm. A complete convolutional SNN network structure is provided in Fig. 5. During training, we add jittering noise to the input spike patterns so as to improve the performance of the trained model. The jitter intervals are randomly drawn from a Gaussian distribution with zero mean and variance σ j = 0.14. The SNN is trained for 150 epochs using the Adam optimizer, with a batch size of 128. The hyperparameters β 1 and β 2 of the Adam optimizer are set to 0.9 and 0.999, respectively. The learning rate starts at 0.0002 and gradually decreases to 0.00005 by the end of the training.</p>
        <p>As the experimental results summarised in Table I, the proposed STDBP learning algorithm could reach accuracies of 98.1% and 98.5% with the network structures of 784-400-10 and 784-800-10, respectively. They outperform previously reported results of SNNs with the same network structures. For example, with the structure of 784-400-10, the classification accuracy of our method is 98.1%, while the accuracy achieved by Mostafa [36] is 97.5%. Another advantage of our algorithm is that it does not need additional training strategies, such as constraints on weights and gradient normalization, which are widely used in previous works to improve their performance [36], [48], [49]. This facilitates large-scale implementation of STDBP, and makes it possible to train more complex CNN structure. The proposed convolutional SNN achieves an accuracy of 99.4%, much higher than all the results obtained by the fully connected SNNs. To our best knowledge, this is the first implementation of a convolutional SNN structure with the single-spike-timing-based supervised learning algorithm.As the experimental results summarised in Table I, the proposed STDBP learning algorithm could reach accuracies of 98.1% and 98.5% with the network structures of 784-400-10 and 784-800-10, respectively. They outperform previously reported results of SNNs with the same network structures. For example, with the structure of 784-400-10, the classification accuracy of our method is 98.1%, while the accuracy achieved by Mostafa [36] is 97.5%. Another advantage of our algorithm is that it does not need additional training strategies, such as constraints on weights and gradient normalization, which are widely used in previous works to improve their performance [36], [48], [49]. This facilitates large-scale implementation of STDBP, and makes it possible to train more complex CNN structure. The proposed convolutional SNN achieves an accuracy of 99.4%, much higher than all the results obtained by the fully connected SNNs. To our best knowledge, this is the first implementation of a convolutional SNN structure with the single-spike-timing-based supervised learning algorithm.</p>
        <p>Fig. 6 shows the distribution of spike timing in the hidden layers and of the earliest spike time in the output layer across 10,000 test images for two SNNs, namely 784-400-10 and 784-800-10. In both cases, the SNN makes a decision after only a fraction of hidden layer neurons are activated. For the 784-400-10 topology, an output neuron spikes (a class is selected) after only 48.6% of the hidden neurons have spiked. Model Coding Network Architecture Additional Strategy Acc. (%) Mostafa [36] Temporal 784-800-10 Weight and Gradient Constraint 97.5 Tavanaei et al [67] Rate 784-1000-10 None 96.6 Comsa et al [49] Temporal 784-340-10 Weight and Gradient Constraint 97.9 Kheradpisheh et al [48] The network is thus able to make rapid decisions about the input class. In addition, during the simulation time, only 66.3% of the hidden neurons have spiked. Therefore, the experimental results suggest that the proposed learning algorithm works in an accurate, fast and sparse manner.Fig. 6 shows the distribution of spike timing in the hidden layers and of the earliest spike time in the output layer across 10,000 test images for two SNNs, namely 784-400-10 and 784-800-10. In both cases, the SNN makes a decision after only a fraction of hidden layer neurons are activated. For the 784-400-10 topology, an output neuron spikes (a class is selected) after only 48.6% of the hidden neurons have spiked. Model Coding Network Architecture Additional Strategy Acc. (%) Mostafa [36] Temporal 784-800-10 Weight and Gradient Constraint 97.5 Tavanaei et al [67] Rate 784-1000-10 None 96.6 Comsa et al [49] Temporal 784-340-10 Weight and Gradient Constraint 97.9 Kheradpisheh et al [48] The network is thus able to make rapid decisions about the input class. In addition, during the simulation time, only 66.3% of the hidden neurons have spiked. Therefore, the experimental results suggest that the proposed learning algorithm works in an accurate, fast and sparse manner.</p>
        <p>To investigate whether the proposed ReL-PSP solves the problems of exploding gradient and dead neuron, we take the fully connected SNNs as an example and 20 independent experiments are conducted with different initial synaptic weights. The data distribution of error gradients, learned synaptic weights and dead neuron counts are reported in Fig. 7, Fig. 8 and Fig. 9, respectively. Fig. 7(a) shows the error gradients of the SNN with an alpha-PSP function, where gradients generally take values that are much larger than those in ReL-PSP (Fig. 7(b)) and ANNs (Fig. 7(c)). The results confirm our findings in the problem analysis in section II, that is, the alpha-PSP function is prone to the gradient exploding problem. We note that the previous studies only partly alleviate this problem, for example, the adaptive learning rate [60] and dynamic firing threshold [47] methods. We are encouraged to see that the gradients of the ReL-PSP function follows a normal distribution that is close to that of ANN, thus ensuring a stable gradient propagation in DeepSNNs. In addition, as shown in Fig. 8(a), the gradient exploding problem also leads to skewed weights, which may adversely affect the performance of the trained model, and is now addressed by the ReL-PSP function.To investigate whether the proposed ReL-PSP solves the problems of exploding gradient and dead neuron, we take the fully connected SNNs as an example and 20 independent experiments are conducted with different initial synaptic weights. The data distribution of error gradients, learned synaptic weights and dead neuron counts are reported in Fig. 7, Fig. 8 and Fig. 9, respectively. Fig. 7(a) shows the error gradients of the SNN with an alpha-PSP function, where gradients generally take values that are much larger than those in ReL-PSP (Fig. 7(b)) and ANNs (Fig. 7(c)). The results confirm our findings in the problem analysis in section II, that is, the alpha-PSP function is prone to the gradient exploding problem. We note that the previous studies only partly alleviate this problem, for example, the adaptive learning rate [60] and dynamic firing threshold [47] methods. We are encouraged to see that the gradients of the ReL-PSP function follows a normal distribution that is close to that of ANN, thus ensuring a stable gradient propagation in DeepSNNs. In addition, as shown in Fig. 8(a), the gradient exploding problem also leads to skewed weights, which may adversely affect the performance of the trained model, and is now addressed by the ReL-PSP function.</p>
        <p>For artificial neural networks, typically only a subset of neurons are activated at the same time [61]. We note that excessive inactivation will lead to the dead neuron problem, which reduces the effective capacity of the network and thus the predictive capability. To assess the severity of the dead neuron problem, we record the percentage of dead neurons during the training process, and report both the mean and the standard deviations across 20 independent runs in Fig. 9. As the training progresses, the percentage of dead neurons increases across both SNNs and ANN. After 100 training epochs, the SNN with ReL-PSP has 30% active neurons and achieves a test accuracy of 98.5%. In contrast, the SNN with alpha-PSP only has 5% active neurons with a test accuracy of 92.5%. As we discussed in section III-A, due to the leaky nature of traditional alpha-PSP function and the spike generation mechanism, SNNs with alpha-PSP function are more likely to suffer from the dead neuron problem. The experimental results corroborate our hypothesis and suggest that ReL-PSP function effectively addresses this problem. After 100 training epochs, the ANN has about 40% active neurons with an accuracy of 98.6%. Overall, the results show that the SNN model with ReL-PSP achieves competitive results with sparse neuronal activities.For artificial neural networks, typically only a subset of neurons are activated at the same time [61]. We note that excessive inactivation will lead to the dead neuron problem, which reduces the effective capacity of the network and thus the predictive capability. To assess the severity of the dead neuron problem, we record the percentage of dead neurons during the training process, and report both the mean and the standard deviations across 20 independent runs in Fig. 9. As the training progresses, the percentage of dead neurons increases across both SNNs and ANN. After 100 training epochs, the SNN with ReL-PSP has 30% active neurons and achieves a test accuracy of 98.5%. In contrast, the SNN with alpha-PSP only has 5% active neurons with a test accuracy of 92.5%. As we discussed in section III-A, due to the leaky nature of traditional alpha-PSP function and the spike generation mechanism, SNNs with alpha-PSP function are more likely to suffer from the dead neuron problem. The experimental results corroborate our hypothesis and suggest that ReL-PSP function effectively addresses this problem. After 100 training epochs, the ANN has about 40% active neurons with an accuracy of 98.6%. Overall, the results show that the SNN model with ReL-PSP achieves competitive results with sparse neuronal activities.</p>
        <p>Fashion-MNIST dataset [65] has same number of training/testing samples as MNIST, but is more challenging than MNIST. The samples are associated with 10 different classes like T-shirt/top, Pullover, Trouser, Dress, Sandal, Coat, Shirt, Bag, Sneaker and Ankle boot. Here we use Fashion-MNIST to compare our method with the existing fully-connected feedforward SNNs and convolutional SNNs.Fashion-MNIST dataset [65] has same number of training/testing samples as MNIST, but is more challenging than MNIST. The samples are associated with 10 different classes like T-shirt/top, Pullover, Trouser, Dress, Sandal, Coat, Shirt, Bag, Sneaker and Ankle boot. Here we use Fashion-MNIST to compare our method with the existing fully-connected feedforward SNNs and convolutional SNNs.</p>
        <p>Table II shows the classification accuracies and characteristics of different methods on Fashion-MNIST dataset. The proposed learning algorithm still deliver the best test accuracy in temporal coding based SNN methods. For example, our method can achieve accuracies of 88.1% and 90.1% with the fully-connected SNN and convolutional SNN, respectively. These results outperforms the best reported result of 88.0% [48] in temporal coding based SNN models.Table II shows the classification accuracies and characteristics of different methods on Fashion-MNIST dataset. The proposed learning algorithm still deliver the best test accuracy in temporal coding based SNN methods. For example, our method can achieve accuracies of 88.1% and 90.1% with the fully-connected SNN and convolutional SNN, respectively. These results outperforms the best reported result of 88.0% [48] in temporal coding based SNN models.</p>
        <p>In this experiment, the performance of the proposed STDBP is evaluated on the face/motobike categories of the Caltech 101 dataset (http://www.vision.caltech.edu). For each category, the training and validation set consist of 200 and 50 randomly selected samples, respectively, and the others are regard as testing samples. Before training, all images are rescaled and converted to 160 × 250 grayscale images. Fig. 10 shows some samples of the converted images. The converted images are then encoded into spike patterns by temporal coding.In this experiment, the performance of the proposed STDBP is evaluated on the face/motobike categories of the Caltech 101 dataset (http://www.vision.caltech.edu). For each category, the training and validation set consist of 200 and 50 randomly selected samples, respectively, and the others are regard as testing samples. Before training, all images are rescaled and converted to 160 × 250 grayscale images. Fig. 10 shows some samples of the converted images. The converted images are then encoded into spike patterns by temporal coding.</p>
        <p>The classification accuracies of different SNN-based computational models are shown in Table III. The proposed STDBP learning algorithm achieves an accuracy of 99.2% with the fully connected SNN structure and an accuracy of 99.5% with the convolutional SNN structure. The accuracy obtained by STDBP outperforms the previously reported SNN-based methods on this dataset. For example, In Kheradpisheh et al [74], an convolutional SNN structure with a SVM classifier achieves an accuracy of 99.1% on the same dataset. Moreover, it is not a fully spike-based computational model that the membrane potential is used as the classification signal. Recently, a spikebased fully connected SNNs model achieves an accuracy of 99.2%. However, the proposed method with convolutional SNN structure reaches an accuracy of 99.5%, which is the state-ofthe-art performance on this benchmark.The classification accuracies of different SNN-based computational models are shown in Table III. The proposed STDBP learning algorithm achieves an accuracy of 99.2% with the fully connected SNN structure and an accuracy of 99.5% with the convolutional SNN structure. The accuracy obtained by STDBP outperforms the previously reported SNN-based methods on this dataset. For example, In Kheradpisheh et al [74], an convolutional SNN structure with a SVM classifier achieves an accuracy of 99.1% on the same dataset. Moreover, it is not a fully spike-based computational model that the membrane potential is used as the classification signal. Recently, a spikebased fully connected SNNs model achieves an accuracy of 99.2%. However, the proposed method with convolutional SNN structure reaches an accuracy of 99.5%, which is the state-ofthe-art performance on this benchmark.</p>
        <p>We will now focus on evaluating the power and energy efficiency of our SNN models by accelerating their inference operations on YOSO. As a baseline for comparison with other neuromorphic accelerators, we considered a fully connected SNN with network architecture 784-800-10 and trained it on MNIST data with proposed STDBP learning algorithm. The learned weights are then transferred to YOSO for accelerating the inference operations. As shown in Table IV, YOSO consumes 0.751 mW of total power consumption and 47.71 ms of latency to classify an image from MNIST dataset. In addition, YOSO achieves 399×, 159.8×, 143.8×, and 1.67× power savings as compared to Spinnaker [56], Tianji [57], TrueNorthb [24], and Shenjing [55], respectively. Though, TrueNortha consumes less power than YOSO, there is a significant difference between the classification accuracies of TrueNorth-a and YOSO (See Table IV). Moreover, YOSO provides 108.7×, 6×, and 3× energy efficiency as compared to Spinnaker [56], SNNwt [76], and TrueNorth-b [24], respectively.We will now focus on evaluating the power and energy efficiency of our SNN models by accelerating their inference operations on YOSO. As a baseline for comparison with other neuromorphic accelerators, we considered a fully connected SNN with network architecture 784-800-10 and trained it on MNIST data with proposed STDBP learning algorithm. The learned weights are then transferred to YOSO for accelerating the inference operations. As shown in Table IV, YOSO consumes 0.751 mW of total power consumption and 47.71 ms of latency to classify an image from MNIST dataset. In addition, YOSO achieves 399×, 159.8×, 143.8×, and 1.67× power savings as compared to Spinnaker [56], Tianji [57], TrueNorthb [24], and Shenjing [55], respectively. Though, TrueNortha consumes less power than YOSO, there is a significant difference between the classification accuracies of TrueNorth-a and YOSO (See Table IV). Moreover, YOSO provides 108.7×, 6×, and 3× energy efficiency as compared to Spinnaker [56], SNNwt [76], and TrueNorth-b [24], respectively.</p>
        <p>In this work, we analysed the problems that BP faces in a DeepSNN, namely the non-differentiable spike function, the exploding gradient, and the dead neuron problems. To address these issues, we proposed the Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons and the STDBP learning algorithm for DeepSNNs. We evaluated the proposed method on both a multi-layer fully connected SNN and a convolutional SNN. The conducted experiments on MNIST showed an accuracy of 98.5% in the case of the the fully connected SNN and 99.4% with the convolutional SNN, which is the state-of-art in spike-driven learning algorithms for DeepSNNs.In this work, we analysed the problems that BP faces in a DeepSNN, namely the non-differentiable spike function, the exploding gradient, and the dead neuron problems. To address these issues, we proposed the Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons and the STDBP learning algorithm for DeepSNNs. We evaluated the proposed method on both a multi-layer fully connected SNN and a convolutional SNN. The conducted experiments on MNIST showed an accuracy of 98.5% in the case of the the fully connected SNN and 99.4% with the convolutional SNN, which is the state-of-art in spike-driven learning algorithms for DeepSNNs.</p>
        <p>There have been a number of learning algorithms for Deep-SNNs, such as conversion methods [24]- [33], and surrogate gradients methods [15], [37]- [41]. These methods are not compatible with temporal coding and spike-based learning mechanism. To overcome the non-differentiability of spike function, many methods have been studied [36], [43]- [49] to facilitate backpropagation. Two common drawbacks of these methods are exploding gradients and dead neurons, Other than what is discussed in this paper, the proposed ReL-PSP neuron model and STDBP have other attributes that might make it more energy-efficient and (neuromorphic) hardware friendly. Firstly, the linear ReL-PSP function is simpler than alpha-PSP for hardware implementation. Secondly, unlike rate-based encoding methods that require more time to generate enough output spikes for classification, our method takes advantage of temporal coding and uses a single spike, which is more sparse and energy-efficient, given energy is mainly consumed during spike generation and transmission. Thirdly, without additional training techniques, on-chip training in neuromorphic chips would be much easier to realize. Finally, even if the training is performed offline, inference of our SNN models can be accelerated with much less power and energy consumption as compared to other state-of-the-art neuromorphic accelerators.There have been a number of learning algorithms for Deep-SNNs, such as conversion methods [24]- [33], and surrogate gradients methods [15], [37]- [41]. These methods are not compatible with temporal coding and spike-based learning mechanism. To overcome the non-differentiability of spike function, many methods have been studied [36], [43]- [49] to facilitate backpropagation. Two common drawbacks of these methods are exploding gradients and dead neurons, Other than what is discussed in this paper, the proposed ReL-PSP neuron model and STDBP have other attributes that might make it more energy-efficient and (neuromorphic) hardware friendly. Firstly, the linear ReL-PSP function is simpler than alpha-PSP for hardware implementation. Secondly, unlike rate-based encoding methods that require more time to generate enough output spikes for classification, our method takes advantage of temporal coding and uses a single spike, which is more sparse and energy-efficient, given energy is mainly consumed during spike generation and transmission. Thirdly, without additional training techniques, on-chip training in neuromorphic chips would be much easier to realize. Finally, even if the training is performed offline, inference of our SNN models can be accelerated with much less power and energy consumption as compared to other state-of-the-art neuromorphic accelerators.</p>
        <p>**</p>
        <p>This research work is supported by Programmatic Grant No. A1687b0033 and Programmatic Grant No. I2001E0053 from the Singapore Government's Research, Innovation and Enterprise 2020 plan (Advanced Manufacturing and Engineering domain), and by the Science and Engineering Research Council, Agency of Science, Technology and Research, Singapore, through the National Robotics Program under Grant No. 192 25 00054. The work of J. Wu was also partially supported by the Zhejiang Lab (No.2019KC0AB02). The work of M. Zhang was aslo partially supported by the China Postdoctoral Science Foundation under Grant No.2020M680148, Zhejiang Lab's International Talent Found for Young Professionals, and National Key R&amp;D Program of China under Grant No. 2018AAA0100202. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the supporting institutions and companies.This research work is supported by Programmatic Grant No. A1687b0033 and Programmatic Grant No. I2001E0053 from the Singapore Government's Research, Innovation and Enterprise 2020 plan (Advanced Manufacturing and Engineering domain), and by the Science and Engineering Research Council, Agency of Science, Technology and Research, Singapore, through the National Robotics Program under Grant No. 192 25 00054. The work of J. Wu was also partially supported by the Zhejiang Lab (No.2019KC0AB02). The work of M. Zhang was aslo partially supported by the China Postdoctoral Science Foundation under Grant No.2020M680148, Zhejiang Lab's International Talent Found for Young Professionals, and National Key R&amp;D Program of China under Grant No. 2018AAA0100202. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of the supporting institutions and companies.</p>
    </text>
</tei>

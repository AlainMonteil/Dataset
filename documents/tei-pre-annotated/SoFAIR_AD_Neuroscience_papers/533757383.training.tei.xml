<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:12+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>While the visual environment contains massive amounts of information, we should not and cannot pay attention to all events. Instead, we need to direct attention to those events that have proven to be important in the past and suppress those that were distracting and irrelevant. Experiences molded through a learning process enable us to extract and adapt to the statistical regularities in the world. While previous studies have shown that visual statistical learning (VSL) is critical for representing higher order units of perception, here we review the role of VSL in attentional selection. Evidence suggests that through VSL, attentional priority settings are optimally adjusted to regularities in the environment, without intention and without conscious awareness.While the visual environment contains massive amounts of information, we should not and cannot pay attention to all events. Instead, we need to direct attention to those events that have proven to be important in the past and suppress those that were distracting and irrelevant. Experiences molded through a learning process enable us to extract and adapt to the statistical regularities in the world. While previous studies have shown that visual statistical learning (VSL) is critical for representing higher order units of perception, here we review the role of VSL in attentional selection. Evidence suggests that through VSL, attentional priority settings are optimally adjusted to regularities in the environment, without intention and without conscious awareness.</p>
        <p>Extracting regularities from the environment in service of automatic behavior is one of the most fundamental human abilities and is often referred to as statistical learning. Abundant research supports the idea that observers easily learn the underlying structure of auditory and/or visual sensory input. One of the most well-known findings is that infants exposed to continuous nonsense streams of speech for only two minutes, react differently to hearing novel 'words' that repeatedly occurred in the stream as opposed to 'nonwords' which recombined the same syllables. This indicates that infants can use information about syllable co-occurrences to discover word boundaries [1]. Following the classic work on speech segmentation, several studies used sequentially presented shapes in which temporal patterns were embedded, for example, by randomly intermixing sets of triplets (i.e., three shapes presented in sequence; see example in Figure 1A; left panel) into a long sequence [2,4,5]. Analogous to auditory statistical learning, at a surprise test following learning, participants typically showcase larger familiarity with triplets they were exposed to relative to foil triplets, indicating sensitivity to the visual patterns. Because evidence for statistical learning was found when stimuli were observed passively without any explicit task, it has been argued that the mere exposure to these streams is enough to passively absorb these regularities ([2,7,8], but see [4]). Statistical learning is assumed to be an implicit process that assimilated the statistical regularities in the input, operating without intent and outside explicit awareness [4]. This impressive learning ability has been the subject of investigation in many cognitive domains (see [9] for a discussion). Crucially, the acquisition of our most fundamental abilities, such as language [10] (see [11] for a review), motor learning [12], object recognition [2,13], scene and object perception [14,15], and conditioning [16] relies on such implicit adaptations to regularities [17] (see [18] for a review).Extracting regularities from the environment in service of automatic behavior is one of the most fundamental human abilities and is often referred to as statistical learning. Abundant research supports the idea that observers easily learn the underlying structure of auditory and/or visual sensory input. One of the most well-known findings is that infants exposed to continuous nonsense streams of speech for only two minutes, react differently to hearing novel 'words' that repeatedly occurred in the stream as opposed to 'nonwords' which recombined the same syllables. This indicates that infants can use information about syllable co-occurrences to discover word boundaries [1]. Following the classic work on speech segmentation, several studies used sequentially presented shapes in which temporal patterns were embedded, for example, by randomly intermixing sets of triplets (i.e., three shapes presented in sequence; see example in Figure 1A; left panel) into a long sequence [2,4,5]. Analogous to auditory statistical learning, at a surprise test following learning, participants typically showcase larger familiarity with triplets they were exposed to relative to foil triplets, indicating sensitivity to the visual patterns. Because evidence for statistical learning was found when stimuli were observed passively without any explicit task, it has been argued that the mere exposure to these streams is enough to passively absorb these regularities ([2,7,8], but see [4]). Statistical learning is assumed to be an implicit process that assimilated the statistical regularities in the input, operating without intent and outside explicit awareness [4]. This impressive learning ability has been the subject of investigation in many cognitive domains (see [9] for a discussion). Crucially, the acquisition of our most fundamental abilities, such as language [10] (see [11] for a review), motor learning [12], object recognition [2,13], scene and object perception [14,15], and conditioning [16] relies on such implicit adaptations to regularities [17] (see [18] for a review).</p>
        <p>Recently, it has become clear that visual statistical learning (VSL) (see Glossary) plays an important role not only for learning sequentially presented individual shapes in which there is no competition for selection (as in triplet learning; Figure 1A) but also when regularities are embedded within visual search displays containing multiple elements (Figure 1B). While these two forms of Highlights Effects of VSL on visual attention have proven more ubiquitous than previously assumed; people do not only learn statistical regularities concerning task-relevant targets, they also adapt to regularities concerning task-irrelevant distractors.Recently, it has become clear that visual statistical learning (VSL) (see Glossary) plays an important role not only for learning sequentially presented individual shapes in which there is no competition for selection (as in triplet learning; Figure 1A) but also when regularities are embedded within visual search displays containing multiple elements (Figure 1B). While these two forms of Highlights Effects of VSL on visual attention have proven more ubiquitous than previously assumed; people do not only learn statistical regularities concerning task-relevant targets, they also adapt to regularities concerning task-irrelevant distractors.</p>
        <p>Eye-tracking research has demonstrated that observers make fewer eye movements to locations that are suppressed regardless of whether a distractor or a target is presented at that location.Eye-tracking research has demonstrated that observers make fewer eye movements to locations that are suppressed regardless of whether a distractor or a target is presented at that location.</p>
        <p>Explicit awareness of the regularities present in the search displays has repeatedly been shown to have no effect on learning, confirming the notion that statistical learning is largely unconscious.Explicit awareness of the regularities present in the search displays has repeatedly been shown to have no effect on learning, confirming the notion that statistical learning is largely unconscious.</p>
        <p>A wide range of regularities have been found to modulate attentional prioritization. These include distributional regularities regarding the most probable location of a target or distractor, regular trial-totrial transitions (where the location of a target or distractor on trial n-1 predicts its subsequent location on trial n), and regularities that associate a particular moment in time with a particular location.A wide range of regularities have been found to modulate attentional prioritization. These include distributional regularities regarding the most probable location of a target or distractor, regular trial-totrial transitions (where the location of a target or distractor on trial n-1 predicts its subsequent location on trial n), and regularities that associate a particular moment in time with a particular location.</p>
        <p>Studies have shown that statistical distractor learning requires little to no executive control resources. Regardless of whether (spatial) working memory was fully loaded, learning to suppress was equally effective.Studies have shown that statistical distractor learning requires little to no executive control resources. Regardless of whether (spatial) working memory was fully loaded, learning to suppress was equally effective.</p>
        <p>statistical learning arguably tap into mechanisms that track regularities across time and concur in the implicit nature of learning, there are also clear differences. Whereas triplet learning induces expectations regarding a temporal sequence of particular stimuli such that the next object on screen can be predicted based on the current visual input [4,19], spatial regularities embedded in visual search displays may shape attentional priority in space such that task relevant locations are boosted and task irrelevant locations ignored [20] (Box 1).statistical learning arguably tap into mechanisms that track regularities across time and concur in the implicit nature of learning, there are also clear differences. Whereas triplet learning induces expectations regarding a temporal sequence of particular stimuli such that the next object on screen can be predicted based on the current visual input [4,19], spatial regularities embedded in visual search displays may shape attentional priority in space such that task relevant locations are boosted and task irrelevant locations ignored [20] (Box 1).</p>
        <p>In many everyday activities, it is critical to dissociate between information that is relevant and information that is distracting. For example, picture yourself driving. While trying to focus on theIn many everyday activities, it is critical to dissociate between information that is relevant and information that is distracting. For example, picture yourself driving. While trying to focus on the</p>
        <p>Target for this search trialTarget for this search trial</p>
        <p>Task: search for unique shapeTask: search for unique shape</p>
        <p>T T D DT T D D</p>
        <p>Task: press for target shape Task: watch the stream, optional cover task In a familiarity judgment test afterwards, participants show greater familiarity for triplets than for foils. In a target detection test, participants are faster to respond to a target shape when it is the second or third shape in a triplet. In this task, participants implicitly learn the statistical temporal relationships among visual stimuli. (B) Distractor location learning; participants perform the additional singleton task searching for a unique shape (diamond between circles or a circle between diamonds) while ignoring a color singleton distractor. The color distractor, if present, appeared more likely in one location than in the other locations. The typical finding is that relative to baseline (i.e., no distractor present; gray bar) distractors captured attention (slower response times: dark blue bar), but critically this attentional capture was significantly attenuated when the distractor was presented at the high-probability location (light blue bar). In this task, participants implicitly learn that the distractor is presented more likely in one location and suppress this location in order to attenuate distraction.Task: press for target shape Task: watch the stream, optional cover task In a familiarity judgment test afterwards, participants show greater familiarity for triplets than for foils. In a target detection test, participants are faster to respond to a target shape when it is the second or third shape in a triplet. In this task, participants implicitly learn the statistical temporal relationships among visual stimuli. (B) Distractor location learning; participants perform the additional singleton task searching for a unique shape (diamond between circles or a circle between diamonds) while ignoring a color singleton distractor. The color distractor, if present, appeared more likely in one location than in the other locations. The typical finding is that relative to baseline (i.e., no distractor present; gray bar) distractors captured attention (slower response times: dark blue bar), but critically this attentional capture was significantly attenuated when the distractor was presented at the high-probability location (light blue bar). In this task, participants implicitly learn that the distractor is presented more likely in one location and suppress this location in order to attenuate distraction.</p>
        <p>task at hand, your senses are constantly bombarded with new information: the blare of a car horn, a traffic light turning red, pedestrians crossing the street, blinking advertisements alongside the road, a buzz from your phone. Attention makes it possible to selectively prioritize relevant information (e.g., the crossing pedestrians) while suppressing task-irrelevant information (e.g., blinking advertisements). Through experience, attentional selection is facilitated as you learn to direct attention to those events that have proven to be important in the past and suppress those that were distracting. Despite its prominent role within a broad range of fundamental cognitive abilities, for many years the role of statistical learning within attentional selection has been ignored. While there is a long history of studying how previous selection episodes influence selection on the current trial, in particular in the context of low-level priming [21,22], these effects are typically not considered 'statistical learning' because of their short-term nature [23]. It is becoming increasingly clear, however, that selection priority is often influenced by previous selection experiences (i.e., selection history) in a manner that is disconnected from both the observer's current goals [i.e., goal-driven (top-down) selection] and stimulus-driven (bottom-up) selection (i.e., bottom-up salience) [24].task at hand, your senses are constantly bombarded with new information: the blare of a car horn, a traffic light turning red, pedestrians crossing the street, blinking advertisements alongside the road, a buzz from your phone. Attention makes it possible to selectively prioritize relevant information (e.g., the crossing pedestrians) while suppressing task-irrelevant information (e.g., blinking advertisements). Through experience, attentional selection is facilitated as you learn to direct attention to those events that have proven to be important in the past and suppress those that were distracting. Despite its prominent role within a broad range of fundamental cognitive abilities, for many years the role of statistical learning within attentional selection has been ignored. While there is a long history of studying how previous selection episodes influence selection on the current trial, in particular in the context of low-level priming [21,22], these effects are typically not considered 'statistical learning' because of their short-term nature [23]. It is becoming increasingly clear, however, that selection priority is often influenced by previous selection experiences (i.e., selection history) in a manner that is disconnected from both the observer's current goals [i.e., goal-driven (top-down) selection] and stimulus-driven (bottom-up) selection (i.e., bottom-up salience) [24].</p>
        <p>A classic study by Biederman [25] demonstrated that statistical learning is not limited to temporal and spatial relationships among auditory and visual stimuli [1][2][3][4]6,61], but also affects attentional selection. This seminal study showed that objects violating visual regularities learned over a lifetime are more difficult to find, for example, when these objects are presented at inconsistent locations within scenes (e.g., a water cooker on the floor, see also [14]). Building on this work, research known under the term 'contextual cueing' has shown that search is facilitated when the target appears at the same location in a visual layout that was encountered previously relative to visual layouts that were never seen before [26,27] (for a review see [28]). The classic finding is that targets are located faster in display configurations that are fixed throughout the experiment relative to configurations that are only seen once, indicating that observers learned the association between the spatial configuration and the target location. In addition, so-called 'probability cuing' studies have shown that participants can learn which location (or area within the display) Box 1. VSL: a unitary learning system or a collection of paradigms?A classic study by Biederman [25] demonstrated that statistical learning is not limited to temporal and spatial relationships among auditory and visual stimuli [1][2][3][4]6,61], but also affects attentional selection. This seminal study showed that objects violating visual regularities learned over a lifetime are more difficult to find, for example, when these objects are presented at inconsistent locations within scenes (e.g., a water cooker on the floor, see also [14]). Building on this work, research known under the term 'contextual cueing' has shown that search is facilitated when the target appears at the same location in a visual layout that was encountered previously relative to visual layouts that were never seen before [26,27] (for a review see [28]). The classic finding is that targets are located faster in display configurations that are fixed throughout the experiment relative to configurations that are only seen once, indicating that observers learned the association between the spatial configuration and the target location. In addition, so-called 'probability cuing' studies have shown that participants can learn which location (or area within the display) Box 1. VSL: a unitary learning system or a collection of paradigms?</p>
        <p>Whether statistical learning in different sensory modalities and cognitive domains within a same modality is supported by a single set of underlying mechanisms is a topic of debate [113][114][115]. While we discuss different 'visual statistical learning' (VSL) tasks that all show sensitivity to statistical regularities in a visual environment, we do not imply that VSL is necessarily a unified concept. The nature of regularities in different VSL tasks differs substantially: for example, regularities can be in time versus space and transitional (i.e., after A follows B) versus distributional (i.e., A is more frequent) [116]. In addition, the behavioral outcomes of learning also differ vastly (e.g., recognition of a group of objects that form a pattern versus attentional suppression of a certain location).Whether statistical learning in different sensory modalities and cognitive domains within a same modality is supported by a single set of underlying mechanisms is a topic of debate [113][114][115]. While we discuss different 'visual statistical learning' (VSL) tasks that all show sensitivity to statistical regularities in a visual environment, we do not imply that VSL is necessarily a unified concept. The nature of regularities in different VSL tasks differs substantially: for example, regularities can be in time versus space and transitional (i.e., after A follows B) versus distributional (i.e., A is more frequent) [116]. In addition, the behavioral outcomes of learning also differ vastly (e.g., recognition of a group of objects that form a pattern versus attentional suppression of a certain location).</p>
        <p>A theoretical model can be proposed in which the extraction of these different types of regularities relies on shared computations in the hippocampus [117]. Such a unitary learning system would be parsimonious, yet there is currently no convincing empirical evidence in support of this view. Alternatively, sensitivity to specific regularities could be an emergent property of different learning mechanisms (see [9] for a discussion). For example, learning to suppress the location where a distractor is likely to occur could rely on different computations compared with learning to predict the upcoming object in a sequence. In this case, VSL would be a label for a range of phenomena that appear to be similar in the sense that they all concern the extraction of structure in visual input but are not mechanistically similar. As such, an individual's learning ability could differ substantially across VSL tasks, to a larger extent than what could be explained by basic differences in, for example, encoding the stimuli [9].A theoretical model can be proposed in which the extraction of these different types of regularities relies on shared computations in the hippocampus [117]. Such a unitary learning system would be parsimonious, yet there is currently no convincing empirical evidence in support of this view. Alternatively, sensitivity to specific regularities could be an emergent property of different learning mechanisms (see [9] for a discussion). For example, learning to suppress the location where a distractor is likely to occur could rely on different computations compared with learning to predict the upcoming object in a sequence. In this case, VSL would be a label for a range of phenomena that appear to be similar in the sense that they all concern the extraction of structure in visual input but are not mechanistically similar. As such, an individual's learning ability could differ substantially across VSL tasks, to a larger extent than what could be explained by basic differences in, for example, encoding the stimuli [9].</p>
        <p>One way to tackle this question would be to investigate individual differences: systematic positive correlations between performance across a range of VSL tasks such as embedded triplet learning and learned distractor suppression would be consistent with the view of VSL as a unitary learning system [9,116]. Relatedly, one could look at special populations or neurological patients with hypothesized deficits in statistical learning. A unitary learning system would predict difficulties in acquiring sensitivity to statistical structure that are general in the sense that they emerge across all visual tasks that tap the learning of statistical structure [9,118].One way to tackle this question would be to investigate individual differences: systematic positive correlations between performance across a range of VSL tasks such as embedded triplet learning and learned distractor suppression would be consistent with the view of VSL as a unitary learning system [9,116]. Relatedly, one could look at special populations or neurological patients with hypothesized deficits in statistical learning. A unitary learning system would predict difficulties in acquiring sensitivity to statistical structure that are general in the sense that they emerge across all visual tasks that tap the learning of statistical structure [9,118].</p>
        <p>Additional singleton paradigm: a visual search task in which observers search for a unique shape (usually a diamond between circles or a circle between diamonds) while an element with a unique color is also present. The uniquely colored element is called a color singleton distractor and its presence interferes with search for the target. Attentional capture: the automatic and involuntary direction of attention towards an object irrespective of whether the object is relevant for the task at hand. Attentional selection: a collection of processes that allow the prioritization of particular input for further processing while simultaneously suppressing irrelevant or distracting information. Feature search mode: a search strategy in which the observer selectively searches for a specific target-defining property such as a specific color or specific shape (see also 'Singleton detection mode'). Goal-driven (top-down) selection: selection is volitional, completely driven by the momentary, current (task) goals of the observer. History-driven selection: selection is driven by previous selection experiences; not necessarily related to the observer's current goals (i.e., top-down attention) or the display characteristics (bottom-up salience). Proactive suppression: the priority signal at a specific location is suppressed before the actual stimulus is presented (proactive control) (see also 'Reactive suppression'). Reactive suppression: the priority signal at a specific location is suppressed following the initial allocation of attention to that location (reactive control) (see also 'Proactive suppression'). Singleton detection mode: a search strategy in which the observer searches for any unique salient item that is unique within a display (any stimulus that 'pops out' from the display) (see also 'Feature search mode'). Spatial priority map: a representation of a topographic space encoding the priority of individual locations combining signals from sensory input (bottom-up), current goal states (top-down or behavioral relevance), and statistical learning (history driven). Stimulus-driven (bottom-up) selection: when selection is automatic, is more likely to contain the target, which results in an attentional bias as evidenced by participants being faster to detect a target positioned in high-probability locations than in lowprobability locations [29][30][31].Additional singleton paradigm: a visual search task in which observers search for a unique shape (usually a diamond between circles or a circle between diamonds) while an element with a unique color is also present. The uniquely colored element is called a color singleton distractor and its presence interferes with search for the target. Attentional capture: the automatic and involuntary direction of attention towards an object irrespective of whether the object is relevant for the task at hand. Attentional selection: a collection of processes that allow the prioritization of particular input for further processing while simultaneously suppressing irrelevant or distracting information. Feature search mode: a search strategy in which the observer selectively searches for a specific target-defining property such as a specific color or specific shape (see also 'Singleton detection mode'). Goal-driven (top-down) selection: selection is volitional, completely driven by the momentary, current (task) goals of the observer. History-driven selection: selection is driven by previous selection experiences; not necessarily related to the observer's current goals (i.e., top-down attention) or the display characteristics (bottom-up salience). Proactive suppression: the priority signal at a specific location is suppressed before the actual stimulus is presented (proactive control) (see also 'Reactive suppression'). Reactive suppression: the priority signal at a specific location is suppressed following the initial allocation of attention to that location (reactive control) (see also 'Proactive suppression'). Singleton detection mode: a search strategy in which the observer searches for any unique salient item that is unique within a display (any stimulus that 'pops out' from the display) (see also 'Feature search mode'). Spatial priority map: a representation of a topographic space encoding the priority of individual locations combining signals from sensory input (bottom-up), current goal states (top-down or behavioral relevance), and statistical learning (history driven). Stimulus-driven (bottom-up) selection: when selection is automatic, is more likely to contain the target, which results in an attentional bias as evidenced by participants being faster to detect a target positioned in high-probability locations than in lowprobability locations [29][30][31].</p>
        <p>These studies are consistent with classic Posner cueing studies [32] showing that people are faster to detect targets appearing in probable locations than improbable locations [33]. Notably however, unlike Posner cueing studies in which people are asked to explicitly direct attention in a top-down, goal-driven way to a location in space, in contextual and probability cueing studies the effect occurs without instruction and without the intention to learn. Typically, observers show little awareness for what they had learned [34] (Box 2). Contextual and probability cueing studies hence reveal that the visual system is sensitive to regularities in the environment and that it will encode and retrieve information that is relevant for the task.These studies are consistent with classic Posner cueing studies [32] showing that people are faster to detect targets appearing in probable locations than improbable locations [33]. Notably however, unlike Posner cueing studies in which people are asked to explicitly direct attention in a top-down, goal-driven way to a location in space, in contextual and probability cueing studies the effect occurs without instruction and without the intention to learn. Typically, observers show little awareness for what they had learned [34] (Box 2). Contextual and probability cueing studies hence reveal that the visual system is sensitive to regularities in the environment and that it will encode and retrieve information that is relevant for the task.</p>
        <p>The research discussed earlier highlights how statistical learning helps to extract task-relevant properties by learning the underlying structure of visual input, either when that information is presented in isolation or when it is embedded in a search display. More recent research, however, demonstrates that history-driven selection biases resulting from this learning do not only prioritize target properties, such as the location and features of the target [26], as human observers are also sensitive to properties of objects that are task irrelevant [20,[35][36][37][38][39][40][41] (for recent reviews see [42,43]).The research discussed earlier highlights how statistical learning helps to extract task-relevant properties by learning the underlying structure of visual input, either when that information is presented in isolation or when it is embedded in a search display. More recent research, however, demonstrates that history-driven selection biases resulting from this learning do not only prioritize target properties, such as the location and features of the target [26], as human observers are also sensitive to properties of objects that are task irrelevant [20,[35][36][37][38][39][40][41] (for recent reviews see [42,43]).</p>
        <p>In a series of experiments, Wang and Theeuwes [39][40][41] examined whether observers could learn regularities regarding the location of the distractor and, if so, how it influences attentional priority. For this purpose, they made a small modification to the additional singleton paradigm [44,45], in which participants search for a unique shape (either a diamond between circles or a circle between diamonds), such that the salient, yet irrelevant, color singleton, when present, appeared with a higher probability at one specific location (Figure 1B; left panel). The typical finding is that even though the salient color singleton distractor is completely irrelevant to the task, it captures attention, evidenced by slower responses to the target when a singleton is present relative to when it is absent [44,45]. As visualized in Figure 1B (right panel), relative to baseline (i.e., no distractor present; gray bar) distractors indeed capture attention, but critically this attentional Box 2. Aware or unaware?In a series of experiments, Wang and Theeuwes [39][40][41] examined whether observers could learn regularities regarding the location of the distractor and, if so, how it influences attentional priority. For this purpose, they made a small modification to the additional singleton paradigm [44,45], in which participants search for a unique shape (either a diamond between circles or a circle between diamonds), such that the salient, yet irrelevant, color singleton, when present, appeared with a higher probability at one specific location (Figure 1B; left panel). The typical finding is that even though the salient color singleton distractor is completely irrelevant to the task, it captures attention, evidenced by slower responses to the target when a singleton is present relative to when it is absent [44,45]. As visualized in Figure 1B (right panel), relative to baseline (i.e., no distractor present; gray bar) distractors indeed capture attention, but critically this attentional Box 2. Aware or unaware?</p>
        <p>Numerous studies have shown that the extraction of regularities from the environment and the adaptations to these regularities can proceed without the intention to learn and without conscious awareness [17,47,119,120]. With 'without conscious awareness' we imply that participants are not able to explicitly indicate the regularities present in the display. In most experiments investigating statistical learning of the location of the distractor, at most, only a few participants are aware of the regularities present in the display [39][40][41]60,121,122]. In experiments in which regularities regarding the target are manipulated, awareness is much higher, with about two-thirds of the participants able to report the high-probability target location [31]. Note, however, that this all depends on the probabilities used, the number of trials, and the way awareness is assessed. Yet, everything being equal, it may not be surprising that the awareness regarding the likely target location is much higher than that of the distractor location as the target is task relevant.Numerous studies have shown that the extraction of regularities from the environment and the adaptations to these regularities can proceed without the intention to learn and without conscious awareness [17,47,119,120]. With 'without conscious awareness' we imply that participants are not able to explicitly indicate the regularities present in the display. In most experiments investigating statistical learning of the location of the distractor, at most, only a few participants are aware of the regularities present in the display [39][40][41]60,121,122]. In experiments in which regularities regarding the target are manipulated, awareness is much higher, with about two-thirds of the participants able to report the high-probability target location [31]. Note, however, that this all depends on the probabilities used, the number of trials, and the way awareness is assessed. Yet, everything being equal, it may not be surprising that the awareness regarding the likely target location is much higher than that of the distractor location as the target is task relevant.</p>
        <p>While in triplet learning explicit knowledge regarding the sequences of visual stimuli improves performance [123] and larger search biases have been found sometimes for individuals that are aware of a target regularity in visual search [124], no such effect was reported with learning to suppress a location. In a study that explicitly tested this, it was shown that the amount of suppression was exactly the same, irrespective of whether the experimental manipulation highlighted or masked the underlying statistical regularity [48]. Related to this is the finding that the explicit instruction to suppress a location in a top-down way does not work; if anything this resulted in an attentional prioritization of the location that needed to be suppressed [39]. It should be noted, however, that the current approach to determine awareness of particular processes has been criticized on methodological grounds as inferences about being aware or not are made on the basis of null effects in relatively small samples [125]. In addition, recent work using alternative awareness measures suggests that the awareness of distractor regularities might have been underestimated in previous work [126]. capture is significantly attenuated at the high-probability location (light blue bar) relative to lowprobability distractor locations (dark blue bar).While in triplet learning explicit knowledge regarding the sequences of visual stimuli improves performance [123] and larger search biases have been found sometimes for individuals that are aware of a target regularity in visual search [124], no such effect was reported with learning to suppress a location. In a study that explicitly tested this, it was shown that the amount of suppression was exactly the same, irrespective of whether the experimental manipulation highlighted or masked the underlying statistical regularity [48]. Related to this is the finding that the explicit instruction to suppress a location in a top-down way does not work; if anything this resulted in an attentional prioritization of the location that needed to be suppressed [39]. It should be noted, however, that the current approach to determine awareness of particular processes has been criticized on methodological grounds as inferences about being aware or not are made on the basis of null effects in relatively small samples [125]. In addition, recent work using alternative awareness measures suggests that the awareness of distractor regularities might have been underestimated in previous work [126]. capture is significantly attenuated at the high-probability location (light blue bar) relative to lowprobability distractor locations (dark blue bar).</p>
        <p>These results are interpreted as evidence that through statistical learning, locations that are likely to contain a distractor become suppressed such that they compete less for attention than other locations (for similar conclusions see [20,46]). Consequently, the priority signal of any object, be it relevant or irrelevant to the task at hand, presented at that location is attenuated. If the distractor is presented at that location, attentional capture is reduced. But also, when the target happens to be presented at that high-probability distractor location, its selection is less efficient, as reflected by longer response times. This hampered processing of targets at high-probability distractor locations suggests that learned spatial suppression is basically feature-blind (Box 3).These results are interpreted as evidence that through statistical learning, locations that are likely to contain a distractor become suppressed such that they compete less for attention than other locations (for similar conclusions see [20,46]). Consequently, the priority signal of any object, be it relevant or irrelevant to the task at hand, presented at that location is attenuated. If the distractor is presented at that location, attentional capture is reduced. But also, when the target happens to be presented at that high-probability distractor location, its selection is less efficient, as reflected by longer response times. This hampered processing of targets at high-probability distractor locations suggests that learned spatial suppression is basically feature-blind (Box 3).</p>
        <p>The implicit nature of statistical learning in visual search Several follow-up studies have further characterized the learned distractor suppression effect. In line with the classical studies showcasing that statistical regularities are extracted passively, without any intention to learn and without explicit awareness of the learned associations [47], suppression at the high-probability distractor location is observed independent of participants' explicit awareness of the regularity, although procedures to dissociate between 'aware' and 'unaware' participants using post hoc questionnaires has been criticized (Box 2). While future work is necessary to establish to what extent the effect is truly independent of conscious awareness, it is noteworthy that the observed suppression effect appears identical in groups of participants in which the experimental manipulation either highlighted or masked the underlying statistical regularity [48]. Moreover, distractor regularities are learned even when the regularities are unrelated to the current task and goals [49,50], again suggesting that this form of suppression relies on implicit learning that requires little to no executive control. Indeed, high-probability distractor locations are suppressed to the same extent, irrespective of whether working memory is loaded with a visual-spatial memory task [51,52]. Also, distractor suppression is observed independent of whether participants search for a unique feature (e.g., a shape singleton) favoring singleton detection mode, or search for a specific target feature (feature search mode) [41,53]. Together these findings indicate that oftentimes learning regularities about distracting information occurs implicitly, with little awareness, and is independent from the current search goals and the availability of executive resources.The implicit nature of statistical learning in visual search Several follow-up studies have further characterized the learned distractor suppression effect. In line with the classical studies showcasing that statistical regularities are extracted passively, without any intention to learn and without explicit awareness of the learned associations [47], suppression at the high-probability distractor location is observed independent of participants' explicit awareness of the regularity, although procedures to dissociate between 'aware' and 'unaware' participants using post hoc questionnaires has been criticized (Box 2). While future work is necessary to establish to what extent the effect is truly independent of conscious awareness, it is noteworthy that the observed suppression effect appears identical in groups of participants in which the experimental manipulation either highlighted or masked the underlying statistical regularity [48]. Moreover, distractor regularities are learned even when the regularities are unrelated to the current task and goals [49,50], again suggesting that this form of suppression relies on implicit learning that requires little to no executive control. Indeed, high-probability distractor locations are suppressed to the same extent, irrespective of whether working memory is loaded with a visual-spatial memory task [51,52]. Also, distractor suppression is observed independent of whether participants search for a unique feature (e.g., a shape singleton) favoring singleton detection mode, or search for a specific target feature (feature search mode) [41,53]. Together these findings indicate that oftentimes learning regularities about distracting information occurs implicitly, with little awareness, and is independent from the current search goals and the availability of executive resources.</p>
        <p>While statistical learning has long been described as a learning mechanism that operates automatically across ages and modalities [1,2], growing evidence suggests that allocating attention to the individual events in a stream can boost statistical learning and is sometimes even necessary [4,54,55]. For learning to occur in visual search, we also assume that spatial attention towards a BoxWhile statistical learning has long been described as a learning mechanism that operates automatically across ages and modalities [1,2], growing evidence suggests that allocating attention to the individual events in a stream can boost statistical learning and is sometimes even necessary [4,54,55]. For learning to occur in visual search, we also assume that spatial attention towards a Box</p>
        <p>There is ample evidence that through statistical learning, locations that are likely to contain a distractor are suppressed proactively [31,67,127,128], indicating that the location within the priority map is suppressed before the display is presented. While this suppression is assumed to be implemented at a spatial priority map and thus generic, there is also evidence that under specific conditions spatial suppression can become tuned to specific distractor features and/or dimensions [37,60,129]. It is less clear, however, whether through statistical learning, particular features can be suppressed proactively, independent of their location. Some have argued that it is indeed possible to suppress distractor features without first directing attention to them [69,108,[130][131][132][133][134][135]]. Yet, there has been some controversy about this, as others have argued that this suppression is, at least partly, reactive [66,136,137], suggesting that attention needs to be directed to the feature before it can be suppressed (known as rapid disengagement [137]). Also, it has been argued that in specific conditions when observers are able to selectively attend the relevant (target) feature, it may appear that the irrelevant distractor feature is suppressed [43,138]. Yet, enhancing the target feature does not necessarily imply proactive feature suppression. For a detailed discussion on this issue see [139].There is ample evidence that through statistical learning, locations that are likely to contain a distractor are suppressed proactively [31,67,127,128], indicating that the location within the priority map is suppressed before the display is presented. While this suppression is assumed to be implemented at a spatial priority map and thus generic, there is also evidence that under specific conditions spatial suppression can become tuned to specific distractor features and/or dimensions [37,60,129]. It is less clear, however, whether through statistical learning, particular features can be suppressed proactively, independent of their location. Some have argued that it is indeed possible to suppress distractor features without first directing attention to them [69,108,[130][131][132][133][134][135]]. Yet, there has been some controversy about this, as others have argued that this suppression is, at least partly, reactive [66,136,137], suggesting that attention needs to be directed to the feature before it can be suppressed (known as rapid disengagement [137]). Also, it has been argued that in specific conditions when observers are able to selectively attend the relevant (target) feature, it may appear that the irrelevant distractor feature is suppressed [43,138]. Yet, enhancing the target feature does not necessarily imply proactive feature suppression. For a detailed discussion on this issue see [139].</p>
        <p>target or a distractor is a prerequisite for learning to occur [53,56,57]. For example, in 'probability cuing' studies [29][30][31]58] in which a target appears more often in one region than in other regions, each time attention is directed to the target, associations are formed between the target and its location within the visual field. In a recent study investigating across trials learning it was shown that if there is no initial direction of attention to the target (for example, when search is serial rather than parallel), learning across trial regularities does not occur, but that learning can be instantiated when targets are made salient such that they pop out from the display [59]. We assume that spatial statistical learning operates by continuously adjusting weights within an assumed spatial priority map, which at any moment in time dynamically controls the deployment of covert attention and gaze [20]. When a location contained relevant information in the past, that location is upregulated, whereas a location is downregulated when it has a higher probability of containing distracting information. In this view, selection simply follows the priority landscape that arises after combining a variety of signals, such as current goals and bottom-up saliency, within which priority weights are induced by previous selection episodes. The premise that statistical learning of distractor (and target) location reflects weight changes in spatial priority maps also means that learning is not restricted to a single location, as weights at multiple locations can be adjusted in parallel. Indeed, learned suppression is concurrently observed at multiple high-probability distractor locations [57,60] and learning of target and distractor probabilities can co-occur, even when the two are manipulated independently [31].target or a distractor is a prerequisite for learning to occur [53,56,57]. For example, in 'probability cuing' studies [29][30][31]58] in which a target appears more often in one region than in other regions, each time attention is directed to the target, associations are formed between the target and its location within the visual field. In a recent study investigating across trials learning it was shown that if there is no initial direction of attention to the target (for example, when search is serial rather than parallel), learning across trial regularities does not occur, but that learning can be instantiated when targets are made salient such that they pop out from the display [59]. We assume that spatial statistical learning operates by continuously adjusting weights within an assumed spatial priority map, which at any moment in time dynamically controls the deployment of covert attention and gaze [20]. When a location contained relevant information in the past, that location is upregulated, whereas a location is downregulated when it has a higher probability of containing distracting information. In this view, selection simply follows the priority landscape that arises after combining a variety of signals, such as current goals and bottom-up saliency, within which priority weights are induced by previous selection episodes. The premise that statistical learning of distractor (and target) location reflects weight changes in spatial priority maps also means that learning is not restricted to a single location, as weights at multiple locations can be adjusted in parallel. Indeed, learned suppression is concurrently observed at multiple high-probability distractor locations [57,60] and learning of target and distractor probabilities can co-occur, even when the two are manipulated independently [31].</p>
        <p>The time course of learning Consistent with demonstrations that statistical learning in triplet learning tasks is extremely rapid [19,61], current evidence suggests that the updating of local priority accrues extremely fast, only needing few trials to become manifest [62]. These time course analyses are complicated, however, because statistical learning and short-term intertrial priming effects are naturally conflated, making it impossible to exclusively attribute the observed suppression/enhancement to learning. Critically, even though in VSL studies there are contributions of different types of intertrial effects (e.g., location, feature, or response repetitions) [21,22], effects attributed to statistical learning remain in place when all these priming effects are taken into account [30,63]. Interestingly, intertrial repetitions are not required for learning to occur [35], nor does their absence modulate the learning speed. Indeed, learning is equally fast regardless of the ratio between high-and low-probability distractor locations [64].The time course of learning Consistent with demonstrations that statistical learning in triplet learning tasks is extremely rapid [19,61], current evidence suggests that the updating of local priority accrues extremely fast, only needing few trials to become manifest [62]. These time course analyses are complicated, however, because statistical learning and short-term intertrial priming effects are naturally conflated, making it impossible to exclusively attribute the observed suppression/enhancement to learning. Critically, even though in VSL studies there are contributions of different types of intertrial effects (e.g., location, feature, or response repetitions) [21,22], effects attributed to statistical learning remain in place when all these priming effects are taken into account [30,63]. Interestingly, intertrial repetitions are not required for learning to occur [35], nor does their absence modulate the learning speed. Indeed, learning is equally fast regardless of the ratio between high-and low-probability distractor locations [64].</p>
        <p>In the absence of any regularities, suppression of distractors often occurs reactively, implying that after the distractor captured attention, active suppression mechanisms rapidly intervene [65,66]. For example, it was shown that suppression at a particular location was selectively adjusted on a trial-by-trial basis to the saliency level of the distractor presented at that location [57] (Box 3). In the case of suppression stemming from local weight changes in priority maps of space, however, it is assumed that suppression is brought into force proactively. To test this, Huang and colleagues [31] combined the additional singleton paradigm with a probe detection task, which made it possible to take a peek at selection priorities just before the actual search display was presented. Probe reaction times indicated that already before the display was presented, the location that was likely to contain a target was enhanced, while the location that was most likely to contain a distractor was suppressed. These findings confirm that contingencies regarding targets and distractors can be learned simultaneously, possibly via a proactive adjustment of the weights within the priority map. It remains controversial, however, whether such proactive suppression is evident in active neural tuning [67] or alternatively relies on activity silent mechanisms that require sensory input to come into play [43,68].In the absence of any regularities, suppression of distractors often occurs reactively, implying that after the distractor captured attention, active suppression mechanisms rapidly intervene [65,66]. For example, it was shown that suppression at a particular location was selectively adjusted on a trial-by-trial basis to the saliency level of the distractor presented at that location [57] (Box 3). In the case of suppression stemming from local weight changes in priority maps of space, however, it is assumed that suppression is brought into force proactively. To test this, Huang and colleagues [31] combined the additional singleton paradigm with a probe detection task, which made it possible to take a peek at selection priorities just before the actual search display was presented. Probe reaction times indicated that already before the display was presented, the location that was likely to contain a target was enhanced, while the location that was most likely to contain a distractor was suppressed. These findings confirm that contingencies regarding targets and distractors can be learned simultaneously, possibly via a proactive adjustment of the weights within the priority map. It remains controversial, however, whether such proactive suppression is evident in active neural tuning [67] or alternatively relies on activity silent mechanisms that require sensory input to come into play [43,68].</p>
        <p>Eye-movement studies confirm the idea that there is proactive suppression [69]. There is a strong link between attentional spatial selection and saccadic eye movements [70]: the eyes typically land at the location to which attention is directed [71]. This implies that if one changes the additional singleton paradigm in such a way that eye movements are required to find the target (i.e., making the line inside the target shape very small so that foveation is needed to determine its orientation), one would expect that participants are less likely to make eye movements toward the high-probability location when this location is suppressed. This is exactly what was found: there are fewer saccades, and with longer latencies, directed towards the high-probability distractor location, irrespective of whether that location contained a target or a distractor on the current trial [72]. In addition to proactive suppression, however, there was also evidence for some reactive suppression: on trials where the eyes were captured by the distractor, subsequent disengagement was faster at the high-compared with the low-probability distractor location [73] (see also [69,74]).Eye-movement studies confirm the idea that there is proactive suppression [69]. There is a strong link between attentional spatial selection and saccadic eye movements [70]: the eyes typically land at the location to which attention is directed [71]. This implies that if one changes the additional singleton paradigm in such a way that eye movements are required to find the target (i.e., making the line inside the target shape very small so that foveation is needed to determine its orientation), one would expect that participants are less likely to make eye movements toward the high-probability location when this location is suppressed. This is exactly what was found: there are fewer saccades, and with longer latencies, directed towards the high-probability distractor location, irrespective of whether that location contained a target or a distractor on the current trial [72]. In addition to proactive suppression, however, there was also evidence for some reactive suppression: on trials where the eyes were captured by the distractor, subsequent disengagement was faster at the high-compared with the low-probability distractor location [73] (see also [69,74]).</p>
        <p>The current evidence shows that once learned suppression is established it remains stable, even though from that moment on distractors capture little to no attention [41]. Thus, while capture to a location is likely needed for initial learning to occur [53,56,57], when contingencies are learned and suppression is in place, the mere presentation of a distractor at the suppressed location is sufficient to maintain the local weight changes. Also, when search for the target continues but the distractor is temporarily removed from the display, learned spatial suppression reappears when the distractor is presented again [62]. To reset the current learned priority landscape, either a new high-probability location needs to be introduced [75], or within the same display the spatial imbalance needs to be removed altogether (i.e., all locations of target and distractors are randomly assigned). Although it is unclear how quickly unlearning in these conditions occurs, with some studies reporting longer persistence [38] than others [49,76], there is a clear asymmetry in the rate of learning during acquisition and extinction, the latter being much slower. This makes sense from an ecological perspective: while one wants to quickly adapt to clear changes in the environment, learned suppression should not immediately return to baseline if the spatial imbalance is temporarily absent as, in that specific context, suppression was useful in the past and likely will be again in the near future.The current evidence shows that once learned suppression is established it remains stable, even though from that moment on distractors capture little to no attention [41]. Thus, while capture to a location is likely needed for initial learning to occur [53,56,57], when contingencies are learned and suppression is in place, the mere presentation of a distractor at the suppressed location is sufficient to maintain the local weight changes. Also, when search for the target continues but the distractor is temporarily removed from the display, learned spatial suppression reappears when the distractor is presented again [62]. To reset the current learned priority landscape, either a new high-probability location needs to be introduced [75], or within the same display the spatial imbalance needs to be removed altogether (i.e., all locations of target and distractors are randomly assigned). Although it is unclear how quickly unlearning in these conditions occurs, with some studies reporting longer persistence [38] than others [49,76], there is a clear asymmetry in the rate of learning during acquisition and extinction, the latter being much slower. This makes sense from an ecological perspective: while one wants to quickly adapt to clear changes in the environment, learned suppression should not immediately return to baseline if the spatial imbalance is temporarily absent as, in that specific context, suppression was useful in the past and likely will be again in the near future.</p>
        <p>Our discussion of regularities so far has focused on a stationary high-probability location and therefore is easily described through local weight changes in a priority map. In this sense, this research differs from typical triplet learning studies, where the regularities are sequential in nature: a particular event A is followed by event B and C [2,4,55,61]. Regularities in daily life are often much more dynamic and dependent on what happened before, the timing of an event, and the context in which it occurs.Our discussion of regularities so far has focused on a stationary high-probability location and therefore is easily described through local weight changes in a priority map. In this sense, this research differs from typical triplet learning studies, where the regularities are sequential in nature: a particular event A is followed by event B and C [2,4,55,61]. Regularities in daily life are often much more dynamic and dependent on what happened before, the timing of an event, and the context in which it occurs.</p>
        <p>Learning trial-to-trial transitions Recent work shows that attentional selection is also sensitive to regularities across trials, which are more similar to the regularities in other statistical learning tasks. For example, participants can learn simple patterns across pairs of trials such that prioritization at a given location on the current trial was instantiated by encountering a predictive target at another location on the preceding trial (e.g., a target at the 3 o'clock position is always followed by a target at the 9 o'clock position and vice versa) [77]. Even though these regularities were randomly mixed within random trial sequences, the visual system was nevertheless sensitive to these regularities and adjusted the priority landscape on a trial-by-trial basis. As indicated, a follow-up study showed that in order to learn these across-trial regularities, the target must be salient and needs to pop out from the display [59].Learning trial-to-trial transitions Recent work shows that attentional selection is also sensitive to regularities across trials, which are more similar to the regularities in other statistical learning tasks. For example, participants can learn simple patterns across pairs of trials such that prioritization at a given location on the current trial was instantiated by encountering a predictive target at another location on the preceding trial (e.g., a target at the 3 o'clock position is always followed by a target at the 9 o'clock position and vice versa) [77]. Even though these regularities were randomly mixed within random trial sequences, the visual system was nevertheless sensitive to these regularities and adjusted the priority landscape on a trial-by-trial basis. As indicated, a follow-up study showed that in order to learn these across-trial regularities, the target must be salient and needs to pop out from the display [59].</p>
        <p>A recent study, where attentional capture was reduced relative to a random baseline condition when the distractor was presented according to a consistent pattern across trials (i.e., moving clockwise or anticlockwise across the display), suggests that this type of intertrial learning is not unique to targets, but also holds for learned distractor suppression [78]. Yet, it still needs to be established whether such across-trial distractor sequence learning also holds for sequence pairs. Nevertheless, these experiments show that while learned attentional biases can be quite persistent, the priority landscape is highly flexible and can be up-and down-weighted for specific locations across trials if need be. This suggests that changing weights to adjust attentional priority at a given location is not a slow and effortful process, but instead can be extremely flexible. This suggests that either changes can be implemented so fast that weights can be increased and decreased on a trial-by-trial basis, or multiple priority maps can be entertained so that the correct one can be turned 'on' for the upcoming trial. Importantly, recent evidence does suggest that trial-to-trial distractor regularities need to concern specific properties of the distractor (such as its location) to modulate attention: search is not affected by the trial-to-trial predictability of the presence of a distractor when its characteristics are not predictable [79].A recent study, where attentional capture was reduced relative to a random baseline condition when the distractor was presented according to a consistent pattern across trials (i.e., moving clockwise or anticlockwise across the display), suggests that this type of intertrial learning is not unique to targets, but also holds for learned distractor suppression [78]. Yet, it still needs to be established whether such across-trial distractor sequence learning also holds for sequence pairs. Nevertheless, these experiments show that while learned attentional biases can be quite persistent, the priority landscape is highly flexible and can be up-and down-weighted for specific locations across trials if need be. This suggests that changing weights to adjust attentional priority at a given location is not a slow and effortful process, but instead can be extremely flexible. This suggests that either changes can be implemented so fast that weights can be increased and decreased on a trial-by-trial basis, or multiple priority maps can be entertained so that the correct one can be turned 'on' for the upcoming trial. Importantly, recent evidence does suggest that trial-to-trial distractor regularities need to concern specific properties of the distractor (such as its location) to modulate attention: search is not affected by the trial-to-trial predictability of the presence of a distractor when its characteristics are not predictable [79].</p>
        <p>Learning where and when Humans do not only learn to expect particular locations across trials [59,77], they also learn to orient attention to particular locations at specific moments in time [80,81]. In a pioneering modification of the Posner's orienting task by Coull and Nobre [82], instead of cueing the likely target location, cues predicted the time interval after which a target would likely occur. Better performance for valid trials (i.e., predicted time interval) than for invalid trials was observed, indicating that humans can orient attention to particular moments in time in a goal-driven way.Learning where and when Humans do not only learn to expect particular locations across trials [59,77], they also learn to orient attention to particular locations at specific moments in time [80,81]. In a pioneering modification of the Posner's orienting task by Coull and Nobre [82], instead of cueing the likely target location, cues predicted the time interval after which a target would likely occur. Better performance for valid trials (i.e., predicted time interval) than for invalid trials was observed, indicating that humans can orient attention to particular moments in time in a goal-driven way.</p>
        <p>Recently it was shown that orienting in time cannot only be done by cueing in a top-down way, but also by learning to expect the moment in time of the occurrence of an event [83]. This study examined whether learned suppression at high-probability locations would benefit from expectations in time. Critically, there were two high-probability locations (maximally distant from one another), of which one location only had a high distractor probability in displays that appeared early in time (after 500 ms), whereas the other location was more likely to contain a distractor later in time (after 1500 ms). The results showed higher search efficiency for targets when the distractor appeared at a high-probability location after the predicted time interval than when it appeared at that same location after the nonpredicted interval. These findings suggest that suppression can wax and wane, depending on learned expectations in time.Recently it was shown that orienting in time cannot only be done by cueing in a top-down way, but also by learning to expect the moment in time of the occurrence of an event [83]. This study examined whether learned suppression at high-probability locations would benefit from expectations in time. Critically, there were two high-probability locations (maximally distant from one another), of which one location only had a high distractor probability in displays that appeared early in time (after 500 ms), whereas the other location was more likely to contain a distractor later in time (after 1500 ms). The results showed higher search efficiency for targets when the distractor appeared at a high-probability location after the predicted time interval than when it appeared at that same location after the nonpredicted interval. These findings suggest that suppression can wax and wane, depending on learned expectations in time.</p>
        <p>Similar results were reported in a study measuring eye movements to explore spatial orienting in time [84]. Participants performed a search task in which the presentation of targets was spatiotemporally predictable, such that the target appeared in a specific quadrant at a specific time point in time within a trial. Performance was significantly better for spatiotemporally predictable than unpredictable targets. Overall, these findings suggest that observers can learn to anticipate the moment in time when targets or distractors arrive within a scene. This learning will result in the momentary enhancement (in case of targets) or momentary suppression (in case of distractors) of the location where presentation of the target or distractor occurs. It reveals a remarkable flexible tuning of the attentional biases to the learned regularities present in time and space.Similar results were reported in a study measuring eye movements to explore spatial orienting in time [84]. Participants performed a search task in which the presentation of targets was spatiotemporally predictable, such that the target appeared in a specific quadrant at a specific time point in time within a trial. Performance was significantly better for spatiotemporally predictable than unpredictable targets. Overall, these findings suggest that observers can learn to anticipate the moment in time when targets or distractors arrive within a scene. This learning will result in the momentary enhancement (in case of targets) or momentary suppression (in case of distractors) of the location where presentation of the target or distractor occurs. It reveals a remarkable flexible tuning of the attentional biases to the learned regularities present in time and space.</p>
        <p>The experiments discussed so far each contained only one regularity set, be it static or dynamic. For statistical learning to be of value in daily life, however, it is important to consider what happens when regularity A is learned in context A, regularity B is learned in context B, and so on. Is only theThe experiments discussed so far each contained only one regularity set, be it static or dynamic. For statistical learning to be of value in daily life, however, it is important to consider what happens when regularity A is learned in context A, regularity B is learned in context B, and so on. Is only the</p>
        <p>last-learned regularity available, or can learners intelligently differentiate between the context of each regularity? Without some form of contextual binding, selection history effects would require a constant relearning of biases for contexts that have already been encountered. Indeed, in classical auditory statistical learning, contextual cues such as a change in voice or pitch have been shown to help listeners track multiple sets of embedded patterns in continuous speech [85,86]. Also, contextual cuing and reward learning have been shown to involve highly contextspecific learning [87,88]. Counter to these observations, when learning tracks spatial probabilities of targets across search displays, learning generalizes to new contexts, especially when the tasks involve similar search behaviors [89,90]. However, learning becomes context specific when processing of the context is necessary to perform the task [89], or when the two tasks differ in attentional demands [91]. Although less well characterized, the current evidence also suggests that spatial suppression effects are not context dependent [76], not even when the contexts are made task-relevant [92]. However, context-dependent learning effects in those studies may have been obscured because the contexts were randomly intermixed. Statistical learning by definition requires integration across multiple trials, so that any regularity possibly needs to be sustained for some period in order to be learned. When learning is blocked, each regularity is linked to its context, making it possible to retrieve it when that context is encountered later on (J. de Waard et al., unpublished). Future research needs to establish to what extent the experimental context, as well as the task relevance of the associated regularity needs to be fixed for learning to occur in order to further our insight into the mechanisms underlying statistically learned (de)prioritization.last-learned regularity available, or can learners intelligently differentiate between the context of each regularity? Without some form of contextual binding, selection history effects would require a constant relearning of biases for contexts that have already been encountered. Indeed, in classical auditory statistical learning, contextual cues such as a change in voice or pitch have been shown to help listeners track multiple sets of embedded patterns in continuous speech [85,86]. Also, contextual cuing and reward learning have been shown to involve highly contextspecific learning [87,88]. Counter to these observations, when learning tracks spatial probabilities of targets across search displays, learning generalizes to new contexts, especially when the tasks involve similar search behaviors [89,90]. However, learning becomes context specific when processing of the context is necessary to perform the task [89], or when the two tasks differ in attentional demands [91]. Although less well characterized, the current evidence also suggests that spatial suppression effects are not context dependent [76], not even when the contexts are made task-relevant [92]. However, context-dependent learning effects in those studies may have been obscured because the contexts were randomly intermixed. Statistical learning by definition requires integration across multiple trials, so that any regularity possibly needs to be sustained for some period in order to be learned. When learning is blocked, each regularity is linked to its context, making it possible to retrieve it when that context is encountered later on (J. de Waard et al., unpublished). Future research needs to establish to what extent the experimental context, as well as the task relevance of the associated regularity needs to be fixed for learning to occur in order to further our insight into the mechanisms underlying statistically learned (de)prioritization.</p>
        <p>To date, very little is known about the underlying neural substrates driving learned distractor suppression and target enhancement.To date, very little is known about the underlying neural substrates driving learned distractor suppression and target enhancement.</p>
        <p>Although not yet experimentally confirmed, based on other studies examining different forms of statistical learning, the medial temporal lobe (MTL), and in particular the hippocampus, arguably plays a prominent role in tuning attentional priority in response to regularities in the environment [93][94][95][96][97]. fMRI studies using triplet or pair learning have shown larger hemodynamic responses in the hippocampus to predictive first stimuli of repeatedly presented pairs [97]. It seems feasible that during visual search, activity pattern of MTL also represents the learned regularities of the environment; yet, instead of plastic stimulus representations [98], we claim plasticity of the settings of the priority map representing space (Figure 2). The MTL and, specifically, the hippocampus, which has also been linked to contextual cueing [99], is known to represent space, particularly allocentric spatial location as demonstrated by the discovery of 'place cells' in both rodents [100,101] and humans [102].Although not yet experimentally confirmed, based on other studies examining different forms of statistical learning, the medial temporal lobe (MTL), and in particular the hippocampus, arguably plays a prominent role in tuning attentional priority in response to regularities in the environment [93][94][95][96][97]. fMRI studies using triplet or pair learning have shown larger hemodynamic responses in the hippocampus to predictive first stimuli of repeatedly presented pairs [97]. It seems feasible that during visual search, activity pattern of MTL also represents the learned regularities of the environment; yet, instead of plastic stimulus representations [98], we claim plasticity of the settings of the priority map representing space (Figure 2). The MTL and, specifically, the hippocampus, which has also been linked to contextual cueing [99], is known to represent space, particularly allocentric spatial location as demonstrated by the discovery of 'place cells' in both rodents [100,101] and humans [102].</p>
        <p>We propose that the statistical regularities present in the spatial environment change the representation of this space within the hippocampus and, possibly, other subcortical structures such as the basal ganglia [103]. These might then change the attentional priority settings throughout networks of cortical and subcortical nodes exhibiting properties of priority maps of space (e.g., frontal eye field, lateral intraparietal area, inferotemporal cortex, the superior colliculus) and in turn indirectly (via the aforementioned nodes) in lower-order areas with strong retinotopic organization. Although our focus is on priority maps of space, consistent with this perspective it has been shown that learned expectations can attenuate distractor processing across the visual hierarchy based on overall distractor probability [104] and learned feature expectations [105] (Box 3). Similarly, early visual cortex blood-oxygen-level-dependent signals are reduced for distractors (as well as targets) occurring in high-versus low-probability regions [106]. This suggests that specific suppression due to learning is implemented all the way down the visual stream. Similarly, consistent with generic proactive suppression, studies using electroencephalography showed that both targets and distractors elicit a P D , a neural marker of suppression Trends in Cognitive Sciences [107,108], at high-probability distractor locations [53,67]. Critically, the same stimuli presented at low-probability distractor locations elicited an N2pc, a neural marker of selection [109].We propose that the statistical regularities present in the spatial environment change the representation of this space within the hippocampus and, possibly, other subcortical structures such as the basal ganglia [103]. These might then change the attentional priority settings throughout networks of cortical and subcortical nodes exhibiting properties of priority maps of space (e.g., frontal eye field, lateral intraparietal area, inferotemporal cortex, the superior colliculus) and in turn indirectly (via the aforementioned nodes) in lower-order areas with strong retinotopic organization. Although our focus is on priority maps of space, consistent with this perspective it has been shown that learned expectations can attenuate distractor processing across the visual hierarchy based on overall distractor probability [104] and learned feature expectations [105] (Box 3). Similarly, early visual cortex blood-oxygen-level-dependent signals are reduced for distractors (as well as targets) occurring in high-versus low-probability regions [106]. This suggests that specific suppression due to learning is implemented all the way down the visual stream. Similarly, consistent with generic proactive suppression, studies using electroencephalography showed that both targets and distractors elicit a P D , a neural marker of suppression Trends in Cognitive Sciences [107,108], at high-probability distractor locations [53,67]. Critically, the same stimuli presented at low-probability distractor locations elicited an N2pc, a neural marker of selection [109].</p>
        <p>Although highly speculative, we reason that the observed tuning of attentional priority in response to more dynamic regularities such as trial-to-trial transitions (rather than a stationary high-probability location) might also be driven by hippocampal learning. While place cells in the hippocampus were traditionally believed to encode the observer's current location in space, the successor representation has been recently proposed wherein hippocampal place cell firing represents the current state in terms of its future (successor) states [110]. Moreover, recent evidence highlights that relational knowledge between objects or experiences is encoded into cognitive spaces, map-like structures relying on firing patterns that also encode maps in physical space [111,112]. An intriguing possibility thus is that learning about regularities, be it a temporal sequence of objects as in classic triplet/pair learning or regularities across visual searches (Figure 1), turn hippocampal representations into a predictive map-like structure. In the case of regularities in visual search, rather than representing the anticipated object [97], these successor representations could entertain different priority maps, such that the correct priority landscape can be activated based on the current successor representation.Although highly speculative, we reason that the observed tuning of attentional priority in response to more dynamic regularities such as trial-to-trial transitions (rather than a stationary high-probability location) might also be driven by hippocampal learning. While place cells in the hippocampus were traditionally believed to encode the observer's current location in space, the successor representation has been recently proposed wherein hippocampal place cell firing represents the current state in terms of its future (successor) states [110]. Moreover, recent evidence highlights that relational knowledge between objects or experiences is encoded into cognitive spaces, map-like structures relying on firing patterns that also encode maps in physical space [111,112]. An intriguing possibility thus is that learning about regularities, be it a temporal sequence of objects as in classic triplet/pair learning or regularities across visual searches (Figure 1), turn hippocampal representations into a predictive map-like structure. In the case of regularities in visual search, rather than representing the anticipated object [97], these successor representations could entertain different priority maps, such that the correct priority landscape can be activated based on the current successor representation.</p>
        <p>The topic of statistical learning has been heavily investigated in the last two decades, yet with a large focus on the learning of embedded temporal patterns, following in the footsteps of the We propose that statistical regularities present in prior searches change the representation of space within the hippocampus and possibly other subcortical structures, such as the basal ganglia, such that priority at a given location is either upregulated (in case of target learning) or downregulated (in case of distractor learning). Within spatial priority maps this information is then integrated with bottom-up saliency information encoded in early visual areas and structures like the superior colliculus [140] and top-down attention as controlled by the frontalparietal network [141]. The current evidence suggests that there is not a single common map in the brain and hence we do not link the spatial priority map to a specific brain structure. Instead, a number of cortical areas arguably work together to generate resulting behavior. Specifically, the lateral intraparietal area of posterior parietal cortex, inferotemporal cortex, frontal eye fields, and intermediate layers of the superior colliculus have each been described as priority maps [142,143]. Figure is adapted from [144].The topic of statistical learning has been heavily investigated in the last two decades, yet with a large focus on the learning of embedded temporal patterns, following in the footsteps of the We propose that statistical regularities present in prior searches change the representation of space within the hippocampus and possibly other subcortical structures, such as the basal ganglia, such that priority at a given location is either upregulated (in case of target learning) or downregulated (in case of distractor learning). Within spatial priority maps this information is then integrated with bottom-up saliency information encoded in early visual areas and structures like the superior colliculus [140] and top-down attention as controlled by the frontalparietal network [141]. The current evidence suggests that there is not a single common map in the brain and hence we do not link the spatial priority map to a specific brain structure. Instead, a number of cortical areas arguably work together to generate resulting behavior. Specifically, the lateral intraparietal area of posterior parietal cortex, inferotemporal cortex, frontal eye fields, and intermediate layers of the superior colliculus have each been described as priority maps [142,143]. Figure is adapted from [144].</p>
        <p>TrendsTrends</p>
        <p>Is there statistical learning within objects? It is likely that people implicitly learn to expect a particular part of an object (the handle of a hammer) to be at a specific location within the object, regardless of the actual spatiotopic or retinotopic location.Is there statistical learning within objects? It is likely that people implicitly learn to expect a particular part of an object (the handle of a hammer) to be at a specific location within the object, regardless of the actual spatiotopic or retinotopic location.</p>
        <p>Are there individual differences in learning to suppress a location? Even though there is ample evidence that there are large individual differences in VSL, it is unclear whether the standard measures of attentional capture and learned suppression are reliable and stable at the level of the individual.Are there individual differences in learning to suppress a location? Even though there is ample evidence that there are large individual differences in VSL, it is unclear whether the standard measures of attentional capture and learned suppression are reliable and stable at the level of the individual.</p>
        <p>Is the mechanism underlying attentional suppression and attentional enhancement the same? The question is whether attentional suppression of a location is different from a process in which all other locations are attentionally enhanced.Is the mechanism underlying attentional suppression and attentional enhancement the same? The question is whether attentional suppression of a location is different from a process in which all other locations are attentionally enhanced.</p>
        <p>How do changes in MTL due to statistical learning change the weights within the priority maps of space? Does the activity pattern in MTL represent the learned regularities of the environment, and how does this feed into the spatial priority map? What is the relationship between the modulation of attention by statistical regularities and predictive coding? Through VSL people learn to implicitly expect targets to be presented at a particular location and direct their attention accordingly. According to predictive coding, the opposite is expected, as people should direct attention to locations where large prediction errors are generated (i.e., unexpected locations). seminal work by Saffran and colleagues [1]. Only much more recently have researchers started to study the role of statistical learning in attentional selection. This review discusses recent behavioral, eye-tracking, and neuroimaging work, which demonstrated that attentional priority settings are adjusted in a dynamic way based on the environmental regularities that the observer encounters. We highlighted the similarities between traditional statistical pattern learning and the statistical learning in visual search, such as the observation that learning is largely implicit and unintentional, as well as the range of regularities (across both time and space) that can be assimilated. Nonetheless, it has also outlined distinct challenges for potential learning mechanisms and the extent to which the extraction of and adaptation to regularities in different cognitive domains are tackled by overlapping neural systems remains largely an open question (see Outstanding questions).How do changes in MTL due to statistical learning change the weights within the priority maps of space? Does the activity pattern in MTL represent the learned regularities of the environment, and how does this feed into the spatial priority map? What is the relationship between the modulation of attention by statistical regularities and predictive coding? Through VSL people learn to implicitly expect targets to be presented at a particular location and direct their attention accordingly. According to predictive coding, the opposite is expected, as people should direct attention to locations where large prediction errors are generated (i.e., unexpected locations). seminal work by Saffran and colleagues [1]. Only much more recently have researchers started to study the role of statistical learning in attentional selection. This review discusses recent behavioral, eye-tracking, and neuroimaging work, which demonstrated that attentional priority settings are adjusted in a dynamic way based on the environmental regularities that the observer encounters. We highlighted the similarities between traditional statistical pattern learning and the statistical learning in visual search, such as the observation that learning is largely implicit and unintentional, as well as the range of regularities (across both time and space) that can be assimilated. Nonetheless, it has also outlined distinct challenges for potential learning mechanisms and the extent to which the extraction of and adaptation to regularities in different cognitive domains are tackled by overlapping neural systems remains largely an open question (see Outstanding questions).</p>
        <p>Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 861Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 861</p>
        <p>Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 865Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 865</p>
        <p>Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 867Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 867</p>
        <p>Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 871Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10 871</p>
        <p>Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10Trends in Cognitive Sciences, October 2022, Vol. 26, No. 10</p>
        <p>This work is supported by a European Research Council (ERC) advanced grant 833029 -(LEARNATTEND) to J.T. L.B. is supported by an Odysseus grant from Research Foundation Flanders (FWO) (G0F3121N).This work is supported by a European Research Council (ERC) advanced grant 833029 -(LEARNATTEND) to J.T. L.B. is supported by an Odysseus grant from Research Foundation Flanders (FWO) (G0F3121N).</p>
        <p>No interests are declared.No interests are declared.</p>
    </text>
</tei>

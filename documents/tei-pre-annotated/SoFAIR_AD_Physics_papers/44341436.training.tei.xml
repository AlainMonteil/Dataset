<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T07:02+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>We reproduce the galaxy clustering catalogue from the SDSS-III Baryon Oscillation Spectroscopic Survey Final Data Release (BOSS DR11&amp;DR12) with high fidelity on all relevant scales in order to allow a robust analysis of baryon acoustic oscillations and redshift space distortions. We have generated (6000) 12 288 MultiDark PATCHY BOSS (DR11) DR12 light cones corresponding to an effective volume of ∼192 000 [h -1 Gpc] 3 (the largest ever simulated volume), including cosmic evolution in the redshift range from 0.15 to 0.75. The mocks have been calibrated using a reference galaxy catalogue based on the halo abundance matching modelling of the BOSS DR11&amp;DR12 galaxy clustering data and on the data themselves. The production follows three steps. First, we apply the PATCHY code to generate a dark matter field and an object distribution including non-linear stochastic galaxy bias. Secondly, we run the halo/stellar distribution reconstruction HADRON code to assign masses to the various objects. This step uses the mass distribution as a function of local density and non-local indicators (i.e. tidal field tensor eigenvalues and relative halo exclusion separation for massive objects) from the reference simulation applied to the corresponding patchy dark matter and galaxy distribution. Finally, we apply the SUGAR code to build the light cones. The resulting MultiDarkPATCHY mock light cones reproduce the number density, selection function, survey geometry, and in general within 1σ , for arbitrary stellar mass bins, the power spectrum up to k = 0.3 h Mpc -1 , the two-point correlation functions down to a few Mpc scales, and the three-point statistics of the BOSS DR11&amp;DR12 galaxy samples.</p>
        <p>as independent measurements, and computing the errors with jackknife or bootstrap estimates. While this approach continues being relevant as a way to obtain error estimates directly from the data (see e.g. Norberg et al. 2009), it also implies several disadvantages. First, it does not include systematic errors present in all subvolumes, secondly it does not lead to a physical understanding of the data by itself, and thirdly it introduces variance beyond the one already present in the data on scales larger than the subvolumes. The last point is especially critical when the signal sought has a large characteristic scale and its detection significance crucially depends on the volume, as is the case for baryon acoustic oscillations (BAOs; see e.g. Seo &amp; Eisenstein 2005;White, Song &amp; Percival 2009). During the past decades, there has been a huge effort to encode our physical knowledge of structure formation in computational algorithms, and compare the theoretical models to the actual observations. Pioneering works started with qualitative comparisons (see e.g. Klypin &amp; Shandarin 1983;Blumenthal et al. 1984;Davis et al. 1985). Since then simulations have grown and such comparisons have turned increasingly more quantitative (see e.g. Klypin et al. 2003;Springel et al. 2005;Boylan-Kolchin et al. 2009;Klypin, Trujillo-Gomez &amp; Primack 2011). These efforts are essential to understand structure formation and yet they suffer from a strong limitation: as simulations always push the computational limits, they are not suited for massive production. In fact, the number of current state-of-the-art large-volume N-body simulations is of order 10 ( Kim et al. 2009;Alimi et al. 2012;Angulo et al. 2012;Prada et al. 2012;Fosalba et al. 2015;Ishiyama et al. 2015;Klypin et al. 2014;Skillman et al. 2014;Watson et al. 2014). However, an ideal approach to determine the uncertainties from current and upcoming surveys scanning large sky areas, and hence covering huge volumes, such as BOSSfoot_0 (White et al. 2011), DESIfoot_1 /BigBOSS (Schlegel et al. 2011), DESfoot_2 (Frieman &amp; Dark Energy Survey Collaboration 2013), LSSTfoot_3 (LSST Dark Energy Science Collaboration 2012), J-PASfoot_4 (Benitez et al. 2014), 4MOST 6 (de Jong et al. 2012), or Euclidfoot_6 (Cimatti et al. 2009;Laureijs 2009), requires thousands of such simulations if the simplest error determination methods are used (Dodelson &amp; Schneider 2013;Taylor, Joachimi &amp; Kitching 2013;Percival et al. 2014). Alternative more efficient methods need to be considered to face this challenge. A few pioneering works explored a viable strategy more than a decade ago relying on simplified fast gravity solvers using perturbation theory (PT): PINOCCHIO (Monaco et al. 2002(Monaco et al. , 2013) and PTHALOS (Scoccimarro &amp; Sheth 2002). Nevertheless, these methods are not trivial, need calibration with N-body simulations, and still demand high computational efforts. For this reason, some of the first analysis of large surveys (Percival et al. 2001;Cole et al. 2005) was done based on lognormal realizations (see also Percival, Verde &amp; Peacock 2004;Beutler et al. 2011), which match the two-point statistics by construction (Coles &amp; Jones 1991), although their three-point statistics is very different from the true one (see e.g. White, Tinker &amp; McBride 2014;Chuang et al. 2015b). It is also not clear that their four-point statistics will be accurate (Cooray &amp; Hu 2001;Takada &amp; Hu 2013).</p>
        <p>The analysis of past data releases of the BOSS collaboration utilized 1000 mocks, created based on an improved version of 
            <rs type="software">PTHALOS</rs> (Manera et al. 2013(Manera et al. , 2015)). The use of approximate gravity solvers in these methods came at the expense of only matching clustering statistics on a wide range of scales to ∼10 per cent precision (and strongly deviating towards small scales 20 h -1 Mpc).
        </p>
        <p>This sets the agenda for the current BOSS data release DR11&amp;DR12 and the requirements for a new generation of mock galaxy catalogues. Ideally, one would like to base these on efficient solvers that are trained on exact solutions and deliver a comparable precision. A new generation of methods that can meet these high requirements have been developed during the past two years, in particular, 
            <rs type="software">PATCHY</rs> (Kitaura, Yepes &amp; Prada 2014), QPM (White et al. 2014), and 
            <rs type="software">EZMOCKS</rs> (Chuang et al. 2015a). The key concept exploited by these methods is to rely only on the large-scale density field obtained from approximate gravity solvers and use biasing prescriptions to populate it with mock galaxies, in a similar way to the methods proposed to augment the resolution of N-body simulations (de la Torre &amp; Peacock 2013; de la Torre et al. 2013;Angulo et al. 2014;Ahn et al. 2015). One should however be careful, as computing an accurate dark matter field is a necessary, but not sufficient condition to reproduce the correct halo/galaxy three-point statistics. The bias parameters are degenerate in the two-point statistics and need to be additionally constrained to reproduce higher order statistics (Kitaura et al. 2015). We will rely in this work on the 
            <rs type="software">PATCHY</rs> method due to its verified accuracy in the two-and three-point statistics for different populations of objects (see application of the 
            <rs type="software">HADRON</rs> code to 
            <rs type="software">PATCHY</rs> and 
            <rs type="software">EZMOCK</rs>; Zhao et al. 2015). An additional set of galaxy mocks fitting the BOSS DR11&amp;DR12 (CMASS and LOWZ) data at two mean redshifts (respectively) based on QPM have been produced in an unprecedented effort. These are constructed with a different structure formation model based on low-resolution particle mesh solvers, and a different galaxy bias, based on a rank-ordering scheme assigning most massive objects to the highest density peaks (for a comparison of both sets of catalogues, see Section 3 and Gil-Marín et al. 2015a).
        </p>
        <p>Another approach uses approximate PT-based solutions to speed up N-body solvers (see COLA method; Tassev, Zaldarriaga &amp; Eisenstein 2013;Howlett, Manera &amp; Percival 2015;Koda et al. 2015). This method is very promising to generate ensembles of reference mock catalogues; however, it has the drawback of requiring large computational memory for the force calculation and large number of particles to resolve the haloes (see Chuang et al. 2015b), and is therefore not suitable for the massive production aimed in this work. The speed of the method over N-body simulations comes at the expense of not resolving the substructures required to produce a realistic galaxy catalogue. This problem can be circumvented by, e.g., augmenting the missing objects with the halo occupation distribution (HOD) model, hereby losing some of the advantage of having a higher precise description of the non-linear clustering over the above-mentioned methods which rely only on the large-scale dark matter field, as shown in a comparison study (see Chuang et al. 2015b, and references therein). One may need an approach like COLA, to model the large-scale structure, combined with the galaxy bias presented in this work for future emission line galaxy-based surveys. We will, however, demonstrate here that this is not necessary to model the distribution of luminous red galaxies (LRGs) aimed in this work.</p>
        <p>One could argue whether mock catalogues are required at all, as analytical models may deliver an almost direct computation of error bars and covariance matrices (Hartlap, Simon &amp; Schneider 2007;Hamaus et al. 2010;Dodelson &amp; Schneider 2013;Taylor et al. 2013;Kalus, Percival &amp; Samushia 2016). It still remains to be shown that these methods making simple assumptions, such as that the density field is Gaussian distributed, yield the same accuracy as covariance matrices based on large sets of mock catalogues.</p>
        <p>Nevertheless, the purpose of mock catalogues is manifold, as they not only serve to provide error estimates, but also to provide an understanding of the systematics of the survey and of the methodology. Any analytical prediction or data analysis method should be cross-checked with large ensembles of mock galaxy catalogues for which the products of this work could be useful. One clear example is the case of BAO reconstruction techniques (see e.g. Eisenstein et al. 2007;Padmanabhan et al. 2012;Anderson et al. 2014;Ross et al. 2015).</p>
        <p>We exploit the efficiency and accuracy of the 
            <rs type="software">PATCHY</rs> code to produce 12 288 galaxy mock cataloguesfoot_8 including the light-cone evolution of galaxy bias based on the halo abundance matching (HAM) technique applied to the reference 
            <rs type="software">BigMultiDark</rs> N-body simulation (see Rodríguez-Torres et al. 2015, companion paper), and to the peculiar motions based on the observational data, matching the two-, three-point statistics, in real and redshift space of the BOSS DR11&amp;DR12 galaxy clustering data at different redshifts and for arbitrary stellar mass bins. Special care has been taken to include all relevant observational effects including selection functions and masking. The 
            <rs type="software">MultiDark</rs> PATCHY BOSS DR11 mock catalogues presented in this work are publicly available. 9This paper is structured as follows: in Section 2 we describe the methodology. This section starts with the generation of the reference catalogue using N-body simulations and the HAM technique. Subsequently, the scheme to massively generate mock catalogues is described. Then we show in Section 3 the statistical comparison between the mock catalogues and the BOSS DR12 data. Subsequently, we discuss future work (Section 4). Finally, in Section 5 we present the conclusions. The reader interested only in the results may skip Section 2 and directly go to Section 3.
        </p>
        <p>To construct high-fidelity mock light cones for interpreting the BOSS DR11&amp;DR12 galaxy clustering, we adopt an iterative training procedure in which a reference catalogue is statistically reproduced with approximate gravity solvers and analytical-statistical biasing models. The whole algorithm involves several steps and is summarized in the flow chart in Fig. 1.</p>
        <p>(i) The first step consists of the generation of an accurate reference catalogue. Here we rely on a large N-body simulation capable of resolving distinct haloes and the corresponding substructures. This permits us to apply the HAM technique to reproduce the clustering of the observations with only one parameter: the scatter in the stellar mass-to-halo mass relation (see Rodríguez-Torres et al. 2015, companion paper;and Section 2.1). This technique is applied at different redshift bins to obtain a detailed galaxy bias evolution spanning the redshift range covered by BOSS DR11&amp;DR12 galax-ies. In this way, we obtain mock galaxy catalogues in full cubical volumes of 2.5 h -1 Gpc side at different redshifts.</p>
        <p>(ii) In the second step, we train the PATCHY code (Kitaura et al. 2014(Kitaura et al. , 2015) ) to match the two-and three-point clustering of the full mock galaxy catalogues for each redshift bin. Here we consider all the mock galaxies together in a single bin irrespectively of their stellar mass.</p>
        <p>(iii) In the third step, we apply the HADRON code (Zhao et al. 2015) to assign stellar masses to the individual objects.</p>
        <p>(iv) In the fourth step, we apply the SUGAR code (see Rodríguez-Torres et al. 2015, companion paper) which includes selection effects, masking, and combines different boxes at different redshifts into a light cone.</p>
        <p>(v) In the fifth step, the resulting MultiDark PATCHY mock catalogues are compared to the observations. The process is iterated until the desired accuracy for different statistical measures is reached.</p>
        <p>In the next sections, we will describe in detail these steps described above for the massive generation of accurate mock galaxy catalogues. The reader interested only in the results may directly go to Section 3.</p>
        <p>The reference catalogues are extracted from one of the BigMul-tiDark 
            <rs type="software">simulationsfoot_10</rs> ( Klypin et al. 2014), which was performed using 
            <rs type="software">GADGET</rs>-2 (Springel et al. 2005) with 3840 3 particles on a volume of (2.5 h -1 Mpc ) 3 assuming cold dark matter Planck cosmology with { M = 0.307 115, b = 0.048 206, σ 8 = 0.8288, n s = 0.9611} , and a Hubble constant (H 0 = 100 h km s -1 Mpc -1 ) given by h = 0.6777. Haloes were defined based on the Bound Density Maximum halo finder (Klypin &amp; Holtzman 1997).
        </p>
        <p>We rely here on the HAM technique to connect haloes to galaxies (Kravtsov et al. 2004;Neyrinck, Hamilton &amp; Gnedin 2004;Tasitsiomi et al. 2004;Vale &amp; Ostriker 2004;Conroy, Wechsler &amp; Kravtsov 2006;Kim, Park &amp; Choi 2008;Guo et al. 2010;Wetzel &amp; White 2010;Trujillo-Gomez et al. 2011;Nuza et al. 2013).</p>
        <p>We note that there are alternative methods connecting haloes to galaxies like the HOD model, which we are not going to consider here (e.g. Berlind &amp; Weinberg 2002;Kravtsov et al. 2004;Zehavi et al. 2005;Zentner et al. 2005;Zheng, Coil &amp; Zehavi 2007;Ross &amp; Brunner 2009;Skibba &amp; Sheth 2009;Zheng et al. 2009;White et al. 2011). These methods are based on a statistical relation describing the probability that a halo of virial mass M hosts N galaxies with some specified properties. In general, theoretical HODs require the fitting of a function with several parameters, which we want to avoid here.</p>
        <p>At first order HAM assumes a one-to-one correspondence between the luminosity and stellar or dynamical masses: galaxies with more stars are assigned to more massive haloes or subhaloes. The luminosity in a red band is sometimes used instead of stellar mass. There should be some degree of stochasticity in the relation between stellar and dynamical masses due to deviations in the merger history, angular momentum, halo concentration, and even observational errors (Tasitsiomi et al. 2004;Behroozi, Conroy &amp; Wechsler 2010;Leauthaud et al. 2011;Trujillo-Gomez et al. 2011). Therefore, we include a scatter in that relation necessary to accurately fit the clustering of the BOSS data (see Rodríguez-Torres et al. 2015, companion paper). To do this, we modify the maximum Figure 1. Flowchart of the methodology applied in this work for the generation of high-fidelity BOSS DR11&amp;DR12 mock galaxy catalogues: i) starting from a reference mock catalogue calibrated with the observations, ii) followed by the reproduction of the whole catalogue, iii) with the subsequent mass assignment, iv) and survey generation. v) The final catalogues are compared with the observations and the simulation, and the previous steps are repeated until the mock catalogues are compatible with the observations within 1σ for the monopole and quadrupole up to k ∼ 0.3 h Mpc -1 . circular velocity (V max ) of each object adding a Gaussian noise:</p>
        <p>, where N (0, σ ) is a Gaussian random number with mean 0 and standard deviation σ . Then, we sort all objects by V scat max , and then we selected objects starting from the one with larger V scat max and we continue until we get the proper number density at different redshifts bins.</p>
        <p>By construction, the method reproduces the observed luminosity function (or stellar mass function). It also reproduces the scale dependence of galaxy clustering over a large range of epochs (Conroy et al. 2006;Guo et al. 2010). When abundance matching is used for the observed stellar mass function (Li &amp; White 2009), it gives also a reasonable fit to lensing results (Mandelbaum et al. 2006) and to the relation between stellar and virial mass (Guo et al. 2010).</p>
        <p>All covariance matrix estimates based on a finite number of mock catalogues, N s , are affected by noise, which must be propagated into the final constraints. The impact of the uncertainties in the covariance matrix on the derived cosmological constraints has been subject of several recent analyses (Dodelson &amp; Schneider 2013;Taylor et al. 2013;Percival et al. 2014). In particular, Dodelson &amp; Schneider (2013) showed that this additional uncertainty can be described by a rescaling of the parameter covariances derived from the distribution of measurements from a set of mocks with a factor given by</p>
        <p>where N b is the number of bins in the corresponding clustering measurements and N p is the number of parameters measured. This implies that a large number of mock catalogues are necessary for a robust analysis of the galaxy clustering data.</p>
        <p>For the anisotropic BAO measurements of Cuesta et al. (2015), the estimation of the full covariance matrix of the monopole and quadrupole of the two-dimensional correlation function from the ensemble of 1000 QPM corresponds to an additional uncertainty of 2 per cent on the constraints on H(z)r d and D A (z)/r d . Using the 2048 MultiDark PATCHY mock catalogues, the effect is reduced to the order of 1 per cent. Large sets of catalogues are even more important for full-shape fits of anisotropic clustering measurements, where the inclusion of information from smaller scales can significantly improve the constraints based on redshift space distortion (RSD; requiring a larger number of bins). For example, in the analysis of Sánchez et al. (in preparation), based on measurements of the clustering wedges statistic (Kazin, Sánchez &amp; Blanton 2012), the use of mock catalogues corresponds to a rescaling of the parameter covariances by m = 1.04 and 1.085 when using 1000 or 2048 catalogues, respectively. This additional uncertainty corresponds to a degradation of the true constraining power of the clustering measurements, which should be minimized by using a larger number of mock catalogues. For this reason, we have made the effort in the BOSS collaboration of producing at least 1000 mocks for each BOSS DR11&amp;DR12 subsample.</p>
        <p>The strategy for the massive production of mock galaxy catalogues relies on generating dark matter fields with approximate gravity solvers on a mesh. We use grids of 960 3 cells with volumes of (2.5 h -1 Gpc) 3 and resolutions of 2.6 h -1 Mpc for which the structure formation model can be considered to be accurate (see Section 2.2.1). Then the galaxies are populated on the mesh according to a combined non-linear deterministic (see Section 2.2.2) and stochastic bias model (see Section 2.2.3). In a post-processing step, we assign halo/stellar masses to each object (see Section 2.2.5). Finally, we apply the survey geometry and selection functions (see Section 2.2.6).</p>
        <p>Let us start describing the PATCHY code (PerturbAtion Theory Catalog generator of Halo and galaxY distributions).</p>
        <p>We rely on augmented Lagrangian perturbation theory (ALPT) to simulate structure formation. Let us recap the basics of this method and refer for details to Kitaura &amp; Heß (2013). In this approximation, the displacement field (q, z), mapping a distribution of dark matter particles at initial Lagrangian positions q to the final Eulerian positions x(z) at redshift z (x(z) = q + (q, z)), is split into a long-range L (q, z) and a short-range component S (q, z), i.e.</p>
        <p>(q, z) = L (q, z) + S (q, z). We rely on second order LPT (2LPT) for the long-range component 2LPT (for details on 2LPT, see Buchert 1994;Bouchet et al. 1995;Catelan 1995).</p>
        <p>The resulting displacement field is filtered with a kernel K: L (q, z) = K(q, r S ) • 2LPT (q, z). We apply a Gaussian filter K(q, r S ) = exp (-|q| 2 /(2r 2 S )), with r S being the smoothing radius. We use the spherical collapse approximation to model the shortrange component SC (q, z) (see Bernardeau 1994;Mohayaee et al. 2006;Neyrinck 2013). The combined ALPT displacement field ALPT (q, z) = K(q, r S ) • 2LPT (q, z)</p>
        <p>is used to move a set of homogeneously distributed particles from Lagrangian initial conditions to the Eulerian final ones. We then grid the particles following a clouds-in-cell scheme to produce a smooth density field δ ALPT . One may get some improvements preventing voids within larger collapsing regions, which essentially extends the collapsing region towards moderate underdensities (see 
            <rs type="software">MUSCLE</rs> method in Neyrinck 2016). This approach requires about eight additional convolutions being about twice as expensive, as the approached used here. Moreover, we have checked that the improvement provided by including 
            <rs type="software">MUSCLE</rs> is not perceptible when using grids with cell sizes of 2.6 h -1 Mpc.
        </p>
        <p>In this section, we describe the deterministic part of our bias model. This is combined with a stochastic element, described in Section 2.2.3, and a non-local element, described in Section 2.2.5, to produced the full model. The deterministic bias relates the expected number counts of haloes or galaxies ρ g ≡ N g ∂V at a given finite volume to the underlying dark matter field ρ M , with Ahn et al. 2015). In general, this bias relation will be arbitrarily complex:</p>
        <p>with B(ρ M ) being a general bias function,</p>
        <p>B(ρ M ) V , ρ g V being the number density Ng , and [•••] V being the ensemble average over the whole considered volume V (in our case the volume of the considered mesh).</p>
        <p>The deterministic bias model we consider in this work has the following form:</p>
        <p>with</p>
        <p>and { ρ th , α, , ρ , τ } the parameters of the model. We have modelled threshold bias (Kaiser 1984;Bardeen et al. 1986;Cole &amp; Kaiser 1989;Sheth, Mo &amp; Tormen 2001;Mo &amp; White 2002) as a combination of a step function θ(ρ Mρ th ) (Kitaura et al. 2014) and an exponential cut-off exp -ρ M ρ (Neyrinck et al. 2014). The local bias expansion (Cen &amp; Ostriker 1993;Fry &amp; Gaztanaga 1993) is summarized by a power law (de la Torre &amp; Peacock 2013; Kitaura et al. 2014). In addition, we consider a bias (ρ Mρ th ) τ which compensates for the missing power of PT-based methods.</p>
        <p>Non-local bias has been recently found to be relevant (McDonald &amp; Roy 2009;Baldauf et al. 2012;Chan, Scoccimarro &amp; Sheth 2012;Sheth, Chan &amp; Scoccimarro 2013;Saito et al. 2014). A non-local bias introduces a scatter in the local deterministic bias relations described above. In this work, the scatter is first described by a stochastic bias relation (see Section 2.2.3). We have investigated second-order non-local bias with PATCHY without finding that this can have a relevant effect on the mock catalogues considering stochastic bias and the full (one single mass bin) catalogue (see Autefage et al., in preparation). In fact, once one considers different populations of halo or stellar mass objects, then non-local bias plays an important role. We solve this in a post-processing step when assigning the masses to each galaxy (see Section 2.2.5 and Zhao et al. 2015).</p>
        <p>The halo distribution is a discrete sample N g, i of the continuous underlying dark matter distribution ρ g, i :</p>
        <p>for each cell i and {p SB } being the set of stochastic bias parameters.</p>
        <p>To account for the shot noise, one could do Poissonian realizations of the halo density field as given by the deterministic bias and the dark matter field (see e.g. de la Torre &amp; Peacock 2013). However, it is known that the excess probability of finding haloes in highdensity regions generates overdispersion (Somerville et al. 2001;Casas-Miranda et al. 2002).</p>
        <p>The strategy up to now has been to generate a mock catalogue which reproduces the clustering of the whole population of galaxies for a given redshift. This has the advantage that by mixing massive and low-mass galaxies we will always be dominated by overdispersion, which is much easier to model than underdispersion. In particular, we consider the negative binomial probability distribution function (for non-Poissonian distributions, see Saslaw &amp; Hamilton 1984;Sheth 1995) including an additional parameter β to model overdispersion (tends towards the Poisson probability distribution function for β → ∞ and for low λ values).</p>
        <p>We note that a proper treatment of the deviation from Poissonity is also crucial to get accurate density reconstructions (see Ata, Kitaura &amp; Müller 2015 and Ata et al., in preparation).</p>
        <p>We will need, however, to take care of the different statistical nature of each population of galaxies when we assign masses to each object (see Section 2.2.5).</p>
        <p>Let us recap here the way in which RSDs are treated in the PATCHY code (see Kitaura et al. 2014).</p>
        <p>The mapping between Eulerian real space x(z) and redshift space s(z) is given by s(z) = x(z) + v r (z), with v r ≡ (v • r)r/(H a), where r is the unit sightline vector, H the Hubble constant, a the scale factor, and v = v(x) the 3D velocity field interpolated at the position of each halo in Eulerian space x using the displacement field ALPT (q, z). We split the peculiar velocity field into a coherent v coh and a (quasi-) virialized component v σ : v = v coh + v σ . The coherent peculiar velocity field is computed in Lagrangian space from the linear Gaussian field δ (1) (q) using the ALPT formulation consistently with the displacement field (see equation 2):</p>
        <p>with v 2LPT (q, z) being the second-order and v SC (q, z) being the spherical collapse component (for details see Kitaura et al. 2014).</p>
        <p>We use the high correlation between the local density field and the velocity dispersion to model the displacement due to (quasi-) virialized motions. Effectively, we sample a Gaussian distribution function (G) with a dispersion given by σ</p>
        <p>Consequently,</p>
        <p>For the Gaussian streaming model see Reid &amp; White (2011), and for non-Gaussian models see e.g. Tinker (2007). In closely virialized systems, the kinetic energy approximately equals the gravitational energy and a Keplerian law predicts γ close to 0.5, leaving only the proportionality constant g as a free parameter in the model (see also Heß, Kitaura &amp; Gottlöber 2013). We assign larger dispersion velocities to low-mass objects considered to be satellites. The parameters g and γ have been adjusted to fit the damping effect in the monopole and quadrupole as found in the BigMul-tiDark N-body simulation first and later further constrained with the BOSS DR12 data for different redshift bins (see discussion in Section 3).</p>
        <p>Once we have a spatial distribution of objects {r g } which accurately reproduce the clustering of the whole galaxy sample at a given redshift, we assign the halo/stellar masses M l g to each object l according to the statistical information extracted from the BigMulti-Dark simulation using the Halo mAss Distribution ReconstructiON (HADRON) code (for technical details see Zhao et al. 2015). In particular, we sample the following conditional probability distribution function</p>
        <p>where ρ M is the local density, T the tidal field tensor (in particular the eigenvalues), r M min a minimum separation between massive objects due to exclusion effects, {p c } a set of cosmological parameters, and z the redshift at which we want to apply the mass reconstruction. We note that at this stage we consider non-local biasing through the tidal field tensor and the minimum separation of objects. Using all this information, it has been proven that one can recover compatible clustering for arbitrary halo mass cuts with the N-body simulation up to scales of about k = 0.3 h -1 Mpc (Zhao et al. 2015). We extend the algorithm to stellar masses including the rank-ordering relation and scatter described in Section 2.1.</p>
        <p>The 
            <rs type="software">SUrvey GenerAtoR (SUGAR)</rs> code is an openMP code which constructs light cones from mock galaxy catalogues (see Rodríguez-Torres et al. 2015, companion paper). This code applies geometrical features of the survey, including the geometry (using the publicly available MANGLE mask; Swanson et al. 2008), sector completeness, veto masks, and radial selection functions.
        </p>
        <p>The SUGAR code can construct light cones using a single box or multiples boxes at different redshifts, in order to include the redshift evolution in the final catalogue. The first step in the construction of the light cone is to locate the observer (z = 0) and to transform from comoving Cartesian coordinates to equatorial coordinates (RA,Dec.) and redshift. To compute the observed redshift (redshift space) of an object, first we compute the comoving distance from the observer to the object, and then we transform it to redshift space following s = r c + (v • r)/aH (z real ) (see Section 2.2.4), where r c (z) is computed from r c (z) =</p>
        <p>Once we compute the redshift of each galaxy, we consider two options to select objects in the radial direction:</p>
        <p>(i) downsampling: this option preserves the clustering of the input box selecting objects randomly to have the desired number density.</p>
        <p>(ii) selecting by halo property: this consists of rank ordering objects by a halo property and selecting them sequentially until the correct number density is obtained.</p>
        <p>Following the method described in Section 2, we generate 12 288 mock light-cone galaxy catalogues for BOSS DR12 11 (2048 for each LOWZ, CMASS, combined, southern, and northern galactic cap). We call these catalogues 
            <rs type="software">MultiDark</rs> PATCHY mocks, MD PATCHY mocks in short. The corresponding computations required about 500 000 CPU hours (30-50 min for each box on 16 cores and a total of 40 960 boxes). Since each PATCHY+HADRON run requires less than 24 Gb shared memory for a grid with 960 3 cells, we were able to make use of 128 nodes with 32 Gb each in parallel from the BSC Marenostrum facilities, taking about one week wall clock time for all 40 960 catalogues. The light-cone generation with SUGAR required an additional ∼1000 CPU hours. The equivalent computations based on N-body simulations would have required about 9000 million CPU hours (∼2.3 million CPU hours for each light cone). The effective number of particles is ∼(61 440) 3 (given that the reference catalogue required 3840 3 particles to resolve the objects we reproduce in the MD PATCHY catalogues).
        </p>
        <p>We used 10 redshift bins to construct the light cones. This permits us to obtain the galaxy bias, the growth, and the peculiar motion evolution as a function of redshift. A visualization of the BOSS DR12 and one MD PATCHY mock realization is shown in Fig. 2. We can clearly see from this plot that both the data and the mocks follow the same selection criteria including the survey mask (the colour code stands for the stellar mass), and there are no obvious visual differences beyond cosmic variance. The empty regions seem to be similarly distributed for both cases, indicating that the three-point statistics should be close, and the statistical comparison between the MD PATCHY mock galaxy catalogues and the observations of BOSS DR12 yields good agreement. The number densities for LOWZ and CMASS galaxy samples are recovered by construction (see Fig. 3). We investigate the performance of the mock galaxy catalogues in detail in the following subsections. To avoid redundancy, we show only the results for BOSS DR12, as the only difference with respect to the BOSS DR11 mocks is the applied mask and selection function.</p>
        <p>We perform first an analysis in configuration space computing the two-and three-point correlation functions. To compute the clustering signal in the correlation function for the MD PATCHY mock light cones and the observed data, we rely on the Landy &amp; Szalay (1993) estimator. We will follow their notation referring to the data sample (either simulation or observed data) as D and to the random catalogue as R.</p>
        <p>The correlation function is then constructed in the following way:</p>
        <p>as a function of separation between pairs of galaxies in redshift space s.</p>
        <p>The three-point correlation function gives a description of the probability of finding three objects in three different volumes, and can be computed following Szapudi &amp; Szalay (1998),</p>
        <p>as a function of separation between the vertices of triangles spanned by triplets of galaxies in redshift space s 12 , s 23 , s 13 . Fig. 4 shows that we accurately recover the clustering (monopole) for arbitrary stellar mass bins showing almost perfect agreement with observations. Only for the two largest stellar mass bin, we find deviations larger than 1σ . This is mainly due to the 'halo exclusion effect', which is only approximately modelled, assuming a minimum separation for massive galaxies, and not the full separation distribution function (Zhao et al. 2015). We find, however, that these differences are not critical, as they are restricted to small scales ( 20 h -1 Mpc) and only a low number of objects are affected. We further compute the monopole and quadrupole for LOWZ and CMASS (see Fig. 5 and Section 3.3). The monopole agrees towards small scales down to a few Mpc within 1σ .</p>
        <p>There is a deviation of the monopole around the BAO peak and towards larger scales. While the galaxy mock catalogues cross zero right after the BAO peak, the observations do not. In this study, we have applied all of the systematic weights, such as the stellar density contamination, detailed in Reid et al. (2016) and Ross et al. (in preparation). The correlation function measurements are quite covariant between s bins at these scales, making the deviations less significant than one would expect by the visual impression. The significance and potential causes of the large-scale excess are studied in Ross et al. (in preparation), where it is also shown that it has no significant impact on BAO measurements. This is even more so, as the overall shape of ξ (s) in BAO measurements is marginalized over with a polynomial (see e.g. Anderson et al. 2014). See also Ross et al. (2012) and Chuang et al. (2013) for similar studies on an earlier BOSS data set and Huterer et al. (2013) for potential photometric calibration systematics, which have not been accounted for in this analysis.</p>
        <p>In the case of RSD measurements, one has to make sure that the analysis is performed on scales which are not affected by systematics (Gil-Marín et al. 2015a, companion paper). The quadrupole is in very good agreement on all scales, further supporting that RSD analysis should be safe, even in case there are some remnant systematics in the data.</p>
        <p>An investigation of the three-point function demonstrates that the MD PATCHY mocks have a quality very similar to those based on N-body simulations after calibration (see the left-hand and central panels in Fig. 6). We have constrained the galaxy bias parameters (see Sections 2.2.2, 2.2.3, and2.2.5) based on the reference catalogues from the BigMultiDark simulation on cubical full volumes at each of the 10 redshift bins, matching the two-and the three-point statistics. To fit the latter, we focused on matching the higher order correlation functions through the probability distribution function of galaxies in the reference catalogues following the approach presented in Kitaura et al. (2015). Using the observations to constrain the three-point statistics is not trivial, due to incompleteness effects. This explains why the MD PATCHY mock catalogues better fit the reference catalogue than the data, especially for the CMASS galaxies. The three-point statistics performs worse for the QPM mocks, possibly because they do not include an iterative validation step fitting higher order statistics (beyond the two-point correlation function). The non-linear RSD parameter (see Section 2.2.4) was iteratively constrained based on the observations, as we explain in the next section.</p>
        <p>The galaxy power spectrum P and the galaxy bispectrum B are the two-and three-point correlation functions in Fourier space. Given the Fourier transform of the galaxy overdensity, δ g (x) ≡ ρ g (x)/ ρg -1,</p>
        <p>where ρ g (x) is the number density of objects and ρg its mean value, and the galaxy power spectrum and galaxy bispectrum are defined as</p>
        <p>with δ D being the Dirac delta function. Note that the bispectrum is only well defined when the set of k-vectors, k 1 , k 2 , and k 3 , close to form a triangle,</p>
        <p>where α 12 is the angle between k 1 and k 2 . This quantity is independent of the overall scale k and redshift at large scales and for a power spectrum that follows a power law. Moreover, it presents a characteristic 'U-shape' predicted by gravitational instability. Mode coupling and power-law deviations in the actual power spectrum induce a slight scale and time dependence in this quantity. However, in practice it has been observed that at scales of the order of k ∼ 0.1 h Mpc -1 the reduced bispectrum does not present a high variation in its amplitude. The measurement of the bispectrum is performed in the same way as the approach described in Gil-Marín et al. (2015c). This method consists of generating k-triangles and randomly orientating them in k-space. When the number of random triangles is sufficiently large, the mean value of their bispectra tends to the fiducial bispectrum (for details see Gil-Marín et al. 2015c).</p>
        <p>Discreteness adds a shot noise contribution to the measured power spectrum and bispectrum. In this paper, we assume that these contributions are of Poisson type and therefore are given by</p>
        <p>where</p>
        <p>and n is the number density of haloes.</p>
        <p>For both power spectrum and bispectrum, we present the BOSS DR12 data error bars computed from the dispersion among 2048 and 100 realizations of MD PATCHY mock catalogues, respectively.</p>
        <p>The Fourier space analysis has been used to improve the modelling of the RSDs in the galaxy mock catalogues. We have assigned higher peculiar random motions to about 10 per cent of the galaxies to fit the quadrupole of the data with a specific value for each of the 10 redshift bins. The resulting monopoles and the quadrupoles show good agreement with the observations over the range relevant to BAOs and RSDs up to at least k 0.3 h -1 Mpc for both LOWZ and CMASS (see Fig. 7). This agreement is further supported after BAO reconstruction, as can be seen in Fig. 8. Only towards the very large scales (k 0.02 h -1 Mpc), we can find that the observed monopole tends to be larger than the mock catalogues (both MD PATCHY and QPM). This hints towards the discrepancy in the monopole found in configuration space (see the previous section). Although the PATCHY method can potentially yield accurate two-point statistics up to k ∼ 1 h -1 Mpc (see Kitaura et al. 2014;Chuang et al. 2015b), we have restricted the study to lower ks, as the analysis of BAOs and RSDs will not be done beyond k = 0.3 h -1 Mpc, and the computation of power spectra for thousands of mocks with large grids becomes very expensive.</p>
        <p>This fitting procedure had, however, as a consequence that the three-point correlation function is slightly less precise at angles close to θ ∼ 0 and θ ∼ π, as can be seen in Fig. 6, which prior to this operation was fully compatible with the reference catalogue. In fact, the reference BigMultiDark catalogue used in this study showed a highly discrepant quadrupole, as compared to the observations. This has been deeply analysed and better agreement has been found based on an improved HAM procedure applied to the BigMultiDark simulation (see Rodríguez-Torres et al. 2015, companion paper), which however was not available at the moment of the generation of the MD PATCHY mocks. The HOD model adopted in the QPM mock catalogues assumed about 10 per cent satellite galaxies. This yields a compatible quadrupole for the CMASS galaxies. However, as these catalogues were not iteratively calibrated for different redshift slices, their agreement with the LOWZ galaxies is less accurate.</p>
        <p>A detailed analysis of the bispectra is presented in Figs 9 and 10 demonstrating reasonable agreement between the mocks and the observations for different configurations of triangles across a wide range of scales, given the high uncertainties introduced by the mask, selection function, and cosmic variance.</p>
        <p>The cosmic evolution modelled in the MD PATCHY mocks was achieved by fitting the clustering of 10 redshift bins for the full redshift range spanning about 5 Gyr. This implied running structure formation with ALPT for each redshift, i.e. modelling the growth of structures and the growth rate, and additionally fitting the galaxy bias evolution and the non-linear RSDs. The evolution of clustering for both sets of mocks in the full redshift range is shown in Fig. 11. While the correlation function for CMASS galaxies does not show strong differences along the CMASS redshift range, this evolution is very apparent for the LOWZ sample. Fig. 12 shows the comparison between the mocks and the observations for different LOWZ in more detail. The QPM mocks do not include a detailed cosmic evolution within LOWZ or CMASS being based on mean redshifts for each case. This explains why these mocks lose accuracy in the two-point statistics towards low redshifts.</p>
        <p>We investigate now the cosmic evolution of the covariance matrices derived from the MD PATCHY mocksfoot_12 computed as in Anderson et al. (2014):</p>
        <p>with bins i and j, mock sample l, and N s being the number of simulations.</p>
        <p>The correlation matrices for different redshift bins shown in Fig. 13 were constructed upon the covariance matrices following</p>
        <p>We find that the correlation matrices vary in subsequent redshift bins. First, the correlation matrices are increasingly correlated close to the diagonal for both the monopole and the quadrupole towards lower redshifts, as expected from gravitational evolution coupling different scales. This is seen in Fig. 13 as the diagonal red band becomes broader especially comparing the highest redshift bin with the lower ones. Secondly, we find that moderate off-diagonal correlations present at higher redshifts disappear towards lower redshifts.</p>
        <p>And thirdly, we can see that the correlation between the monopole and the quadrupole at large scales becomes maximal in the redshift bin 0.43 &lt; z &lt; 0.55, as can be seen in the white region in the lowerright and upper-left blocks. This 'triangular' correlation is expected from linear theory (see equations 7 and 9 in Chuang &amp; Wang 2013). Further calculations of the correlation functions including QPM mocks are shown in companion publications (Gil-Marín et al. 2015a,b, companion papers).</p>
        <p>Additionally, we show in Fig. 14 the angular correlation function and in Fig. 15 the multipole moments (including the hexadecapole) for different redshift bins based on the combined sample showing good agreement between the MD PATCHY mocks and the data.</p>
        <p>We have taken advantage in this survey of the characteristic bias of LRGs, being massive objects residing in high-density regions. This work confirms that threshold bias is an essential ingredient to explain the clustering of LRGs. This facilitates our analysis, since the low-density filamentary network did not need to be accurately described, and it has permitted us to rely on low-resolution (augmented Lagrangian) PT-based methods. This will no longer apply for upcoming surveys based on emission line galaxies residing in the whole cosmic web. One could improve the methodology presented in this work by substituting the structure formation model based on PT with a more accurate one (e.g. COLA). Whether this is necessary, or whether more efficient alternative approaches are sufficient (e.g. ALPT with MUSCLE corrections), will be investigated in future works.</p>
        <p>Non-local bias was only considered in the mass assignment step, but neglected in the generation of the full galaxy population. This may become important to model for emission line galaxies, and needs a deeper analysis.</p>
        <p>The approximate 'halo exclusion' modelling is mainly responsible for the deviation in the clustering of the most massive objects, and could be improved by taking their full distribution of relative distances, instead of taking a sharp minimum separation for each mass bin, as is done here.</p>
        <p>Another aspect which still needs to be improved in the catalogues is the clustering on sub-Mpc scales. We have randomly assigned positions of dark matter particles to the mock galaxies without considering that some of them are satellites of central galaxies. This implies that these mocks are not appropriate for fibre-collision analysis. For the time being, we will leave the mock catalogues as they are, since most of the studies are not affected by this. Nevertheless, we would like to stress that this aspect can easily be corrected by assigning to a fraction of the mock galaxies close positions to the major most massive ones in the neighbourhood, without the need of redoing the catalogues. The QPM mocks better model fibre collisions, as the HOD adopted in this work successfully reproduced the fraction of close satellites and central galaxies (Gil-Marín et al. 2015a, companion paper).</p>
        <p>Also the photometric calibration systematics, presumably responsible for the excess of power in the data towards large scales, require further investigation.</p>
        <p>We have considered one fiducial cosmology. It would be, however, interesting to provide sets of mock catalogues running over different combinations of cosmological parameters.</p>
        <p>Let us finally mention that we have ignored in this study supersurvey modes, which may be especially relevant for the analysis of the power spectrum at very large scales (Takada &amp; Hu 2013;Li, Hu &amp; Takada 2014a,b;Carron &amp; Szapudi 2015).</p>
        <p>We aim at addressing all these issues in future works.</p>
        <p>We have presented 12 288 mock galaxy catalogues for the BOSS DR12, including all relevant physical and observational effects, to enable a robust analysis of BAOs and RSDs.</p>
        <p>The main features of these mock catalogues are as follows:</p>
        <p>(i) large number of catalogues: 2048 for each LOWZ, CMASS, and combined LOWZ+CMASS and northern and southern galactic cap, (ii) accurate structure formation model on scales of a few Mpc, (iii) accurate galaxy bias model including non-linear, stochastic, threshold bias, and a non-local bias dependence on the tidal field tensor and the exclusion effect separation of massive objects, (iv) modelling redshift evolution of galaxy bias, growth of structures, growth rate, and non-linear RSDs, (v) and additional survey features, such as geometry, sector completeness, veto masks, and radial selection functions.</p>
        <p>The same degree of accuracy is achieved for the BOSS DR11 MD PATCHY mocks, for which only 6000 light-cone mock catalogues were produced (1000 for each LOWZ, CMASS, and combined LOWZ+CMASS and northern and southern galactic cap).</p>
        <p>The MD PATCHY mocks have shown a better match to the data than the QPM mocks in terms of two-and three-point statistics. Investigating the origin for these differences can be interesting as the physical models, and in particular the galaxy bias, adopted in each method are quite different.</p>
        <p>We note that neglecting the stochastic bias considered in the MD PATCHY mocks, modelling the deviation from Poisson shot noise (predominantly overdispersion), could underestimate the clustering uncertainties.</p>
        <p>The mock catalogues have enabled a robust analysis of the BOSS data yielding the necessary error estimates and the validation of the analysis methods. In particular, the studies include the following:</p>
        <p>(i) a full clustering analysis (Grieb et al., in preparation; Sánchez et al., in preparation: see Fig. 15), (ii) a tomographic analysis of the large-scale angular galaxy clustering, where full light-cone effects (e.g. growth, bias, and velocity field evolution) are essential (Salazar-Albornoz et al., in preparation: see Fig. 14), (iii) a study of the BAO reconstructions (see in preparation,and Fig. 8 showing the performance on the MD PATCHY mocks), (iv) and an RSD analysis (Gil-Marín et al. 2015a, companion paper;Beutler et al., in preparation).</p>
        <p>We have demonstrated that the MD PATCHY BOSS DR12 mock galaxies match, in general within 1σ , the clustering properties of the BOSS LRGs for the monopole, quadrupole, and hexadecapole of the two-point correlation function both in configuration and Fourier space. In particular, we achieve a high accuracy in the modelling of the monopole up to k ∼ 0.3 h Mpc -1 . We have furthermore shown that we also obtain three-point statistics with the same level of accuracy as N-body-based catalogues at scales larger than a few Mpc, which are close to the observations. The good agreement between the models and the observations demonstrates the level of accuracy reached in cosmology, our understanding of structure formation, galaxy bias, and observational systematics.</p>
        <p>All the mock galaxy catalogues and the corresponding covariance matrices will be made publicly available together with the release of the BOSS DR12 galaxy catalogue.</p>
        <p>http://www.sdss3.org/surveys/boss.php</p>
        <p>http://desi.lbl.gov/</p>
        <p>http://www.darkenergysurvey.org</p>
        <p>http://www.lsst.org/lsst/</p>
        <p>http://j-pas.org/</p>
        <p>http://www.aip.de/en/research/research-area-ea/research-groups-andprojects/4most</p>
        <p>http://www.euclid-ec.org</p>
        <p>MNRAS 456,4156-4173 (2016) at University of Portsmouth Library on May 3, 2016 http://mnras.oxfordjournals.org/ Downloaded from</p>
        <p>This corresponds to an effective volume of ∼192 000 [h -1 Gpc] 3 , a factor of ∼20 times larger than the volume of the DEUS FUR simulation(Alimi et al. 2012), and a factor of ∼375 times larger than the DarkSky ds14 simulation(Skillman et al. 2014).</p>
        <p>http://data.sdss3.org/sas/dr11/boss/lss/dr11_patchy_mocks/ The BOSS DR12 mock catalogues will be made publicly available together with the data catalogue: http://data.sdss3.org/sas/dr12/boss/lss/dr12_patchy_mocks/.</p>
        <p>http://www.multidark.org/MultiDark/ MNRAS 456, 4156-4173 (2016) at University of Portsmouth Library on May 3, 2016 http://mnras.oxfordjournals.org/ Downloaded from</p>
        <p>We have produced half the amount of mock catalogues for DR11, i.e. 1024 for each LOWZ, CMASS, combined, southern, and northern galactic cap.</p>
        <p>Covariance matrices for the different catalogues (LOWZ, CMASS, and combined sample) will be made publicly available with the publication of the galaxy catalogue.MNRAS 456,4156-4173 (2016) at University of Portsmouth Library on May 3, 2016 http://mnras.oxfordjournals.org/ Downloaded from</p>
        <p>This paper has been typeset from a T E X/L A T E X file prepared by the author.MNRAS 456,4156-4173 (2016) at University of Portsmouth Library on May 3, 2016 http://mnras.oxfordjournals.org/ Downloaded from</p>
        <p>and JNG acknowledge support from the Transregional Collaborative Research Centre TR33 'The Dark Universe' of the German Research Foundation (DFG). AJC is supported by supported by the European Research Council under the European Community's Seventh Framework Programme FP7-IDEAS-Phys.LSS 240117. Funding for this work was partially provided by the Spanish MINECO under project MDM-2014-0369 of ICCUB (Unidad de Excelencia 'María de Maeztu').</p>
        <p>The massive production of all MultiDark PATCHY BOSS DR12 mocks has been performed at the BSC Marenostrum supercomputer, the Hydra cluster at the Instituto de Física Teórica UAM/CSIC, and NERSC at the Lawrence Berkeley National Laboratory.</p>
        <p>The 
            <rs type="software">BigMultiDark</rs> simulations have been performed on the Su-perMUC supercomputer at the Leibniz-Rechenzentrum (LRZ) in Munich, using the computing resources awarded to the PRACE project number 2012060963. We want to thank V. Springel for providing us with the optimized version of Numerical computations for the power spectrum multipoles and bispectrum were performed on the Sciama High Performance Compute (HPC) cluster which is supported by the ICG, SEPNet, and the University of Portsmouth. This research also used resources of the National Energy Research Scientific Computing Center, a DOE Office of Sci-ence User Facility supported by the Office of Science of the US Department of Energy under Contract No. DE-AC02-05CH11231. Funding for SDSS-III has been provided by the Alfred P. Sloan Foundation, the Participating Institutions, the National Science Foundation, and the US Department of Energy Office of Science. The SDSS-III website is http://www.sdss3.org/. SDSS-III is managed by the Astrophysical Research Consortium for the Participating Institutions of the SDSS-III Collaboration including the University of Arizona, the Brazilian Participation Group, Brookhaven National Laboratory, University of Cambridge, Carnegie Mellon University, University of Florida, the French Participation Group, the German Participation Group, Harvard University, the Instituto de Astrofisica de Canarias, the Michigan State/Notre Dame/JINA Participation Group, Johns Hopkins University, Lawrence Berkeley National Laboratory, Max Planck Institute for Astrophysics, Max Planck Institute for Extraterrestrial Physics, New Mexico State University, New York University, Ohio State University, Pennsylvania State University, University of Portsmouth, Princeton University, the Spanish Participation Group, University of Tokyo, University of Utah, Vanderbilt University, University of Virginia, University of Washington, and Yale University.
        </p>
    </text>
</tei>

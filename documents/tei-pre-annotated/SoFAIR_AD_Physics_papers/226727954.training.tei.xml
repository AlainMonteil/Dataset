<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:58+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Gravitational lensing by clusters of galaxies offers a powerful probe of their structure and mass distribution. Several research groups have developed techniques independently to achieve this goal. While these methods have all provided remarkably high-precision mass maps, particularly with exquisite imaging data from the Hubble Space Telescope (HST), the reconstructions themselves have never been directly compared. In this paper, we present for the first time a detailed comparison of methodologies for fidelity, accuracy and precision. For this collaborative exercise, the lens modelling community was provided simulated cluster images that mimic the depth and resolution of the ongoing HST Frontier Fields. The results of the submitted reconstructions with the un-blinded true mass profile of these two clusters are presented here. Parametric, free-form and hybrid techniques have been deployed by the participating groups and we detail the strengths and trade-offs in accuracy and systematics that arise for each methodology. We note in conclusion that several properties of the lensing clusters are recovered equally well by most of the lensing techniques compared in this study. For example, the reconstruction of azimuthally averaged density and mass profiles by both parametric and freeform methods matches the input models at the level of ∼10 per cent. Parametric techniques are generally better at recovering the 2D maps of the convergence and of the magnification. For the best-performing algorithms, the accuracy in the magnification estimate is ∼10 per cent at μ true = 3 and it degrades to ∼30 per cent at μ true ∼ 10.</p>
        <p>Gravitational lensing has become an increasingly popular method to constrain the matter distribution in clusters (see e.g. Soucail et al. 1987;Fort et al. 1988;Lynds &amp; Petrosian 1989;Kneib et al. 1996;Broadhurst et al. 2005;Smith et al. 2005;Jullo et al. 2007;Limousin et al. 2007Limousin et al. , 2008Limousin et al. , 2012Limousin et al. , 2016;;Jullo &amp; Kneib 2009;Richard et al. 2010a;Postman et al. 2012). Strong lensing, as it turns out, is particularly suited to probing the dense central regions of clusters. Constraining the structure of the cluster cores and their density profiles is critical to our understanding of structure formation; probing the nature of dark matter and fully comprehending the interplay between baryons and dark matter. Lensing by massive clusters has proved to be an invaluable tool to study their properties, in particular the detailed dark matter distribution within the cluster, as well as the faint, distant background population of galaxies that they bring into view (see Kneib &amp; Natarajan 2011;Bartelmann et al. 2013;Meneghetti et al. 2013;Natarajan et al. 2017, and references therein for recent reviews). The magnification provided by lensing therefore affords the determination of the luminosity function of these high-redshift sources down to faint luminosities, thus helping inventory and identify galaxies that might have re-ionized the Universe (Seitz et al. 1998;Stark et al. 2007;Richard et al. 2008Richard et al. , 2011;;Vanzella et al. 2012Vanzella et al. , 2014Vanzella et al. , 2015Vanzella et al. , 2016;;Bouwens et al. 2014Bouwens et al. , 2015;;Robertson et al. 2015;Huang et al. 2016).</p>
        <p>Over the past two decades the Hubble Space Telescope (HST) has revolutionized the study of cluster lenses; and, with the deployment of ever more sensitive cameras from the Wide Field Planetary Cam-era2 to the Advanced Camera for Surveys (ACS), the data have become exquisite in terms of resolution. By 2005, mass distributions derived from lensing data were available for about 30 clusters. More recently, galaxy clusters were the primary targets of two multicycle treasury programs of the HST aiming at finding signatures of strong gravitational lensing in their cores. These are the 'Cluster Lensing And Supernova survey with Hubble' [CLASH; PI M. Postman (GO 12065); see Postman et al. 2012] and the ongoing Frontier Fields Initiative (FFI; PI: Lotz; see Lotz et al. 2017).</p>
        <p>As part of the Frontier Fields programme, HST is currently collecting data of unprecedented depth on fields that harbour six massive clusters that act as powerful gravitational lenses. This programme utilizes orbits under the Director's Discretionary observing time. The FFI is a revolutionary deep field observing program aimed at peering deeper into the universe than ever before to not only help understand better these dramatic lenses and their properties, but also simultaneously bring into view faint, distant background galaxies that would otherwise remain unseen without the magnification provided by the foreground lens. These high-redshift sources that can be accessed due to gravitational lensing provide a first glimpse likely of the earliest galaxies to have formed in the universe, and offer a preview of coming attractions that await unveiling by the upcoming James Webb Space Telescope. These Frontier Fields uniquely combine the power of HST with that of nature's gravitational telescopes -the high magnifications produced by these massive clusters of galaxies.</p>
        <p>Utilizing both the Wide Field Camera 3 (WFC3) and ACS in parallel in this current program, HST has been producing the deepest observations of clusters and the background galaxies that they lens; as well as observations of flanking blank fields that are located near these selected clusters. These images have revealed the presence of distant galaxy populations that are ∼10-100 times fainter than any previously observed (Atek et al. 2015a,b;Livermore, Finkelstein &amp; Lotz 2017). The magnifying power of these clusters is proving to be invaluable in helping improve our statistical understanding of early galaxies that are likely responsible for the re-ionization of the universe, and are providing unprecedented measurements of the spatial distribution of dark matter within massive clusters. These six clusters span the redshift range z = 0.3-0.55. The program devotes 140 orbits to each cluster/blank field pair, achieving a limiting AB magnitude of M AB ≈ 28.7-29 mag in the optical (ACS) and nearinfrared (WFC3) bands.</p>
        <p>The fundamental ingredient for exploiting the science outlined above is the construction of robust and reliable lens models. The ongoing FFI is an unprecedented test-bed for lens modelling techniques. Given the depth of these HST observations, hundreds of multiple images, covering a broad redshift range, have been newly unveiled behind each of the observed clusters (Jauzac et al. 2014;Diego et al. 2015b;Grillo et al. 2015;Jauzac et al. 2015;Wang et al. 2015;Hoag et al. 2016;Kawamata et al. 2016). In a rather unique case, even time delay measurements from a serendipitously multiply imaged supernova 'Refsdal' observed by the Grism Lens-Amplified Survey from Space (GLASS) team (Treu et al. 2015) in the FFI cluster MACSJ1149.5+2223 became available for testing and refining the lens models (Kelly et al. 2015;Jauzac et al. 2016;Rodney et al. 2016;Treu et al. 2016). Most importantly, FFI data were made publicly available immediately. Five teams were contracted by STScI to produce gravitational lensing models for all six Frontier Fields clusters to be made available to the astronomical community at large to enable wide use of this incredible data set. All teams share the latest observational constraints, including positions and redshifts of multiple imagesfoot_0 before working independently to produce lensing models which are also made publicly available. 2 Several additional groups have also been working on the data and producing mass models. In short, the whole community of strong lensing modellers has been actively collaborating to maximally exploit the FFI data.</p>
        <p>The process of converting the observed strong lensing constraints into matter distributions is called lens inversion. Several groups have developed algorithms that perform the lens inversion employing different methodologies and using various combinations of input constraints. These include other tracers of the cluster gravitational potential, such as weak lensing, galaxy kinematics and the X-ray emission from the intracluster medium (see e.g. Bradač et al. 2005;Donnarumma et al. 2011;Medezinski et al. 2013;Newman et al. 2013;Umetsu 2013;Umetsu et al. 2014;Merten et al. 2015). Over the years, it has become clear that while all methods are equally well motivated, they do not always converge to consistent reconstructions, even when applied to the same lens system (e.g. Smith et al. 2009;Zitrin &amp; Broadhurst 2009). In several cases strong-lensing masses for the same cluster lens were found to be in tension (by a factor 2-3) with other independent measurements, based e.g. on the modelling of the X-ray emission by the intracluster gas (Ebeling et al. 2009;Richard et al. 2010b;Donahue et al. 2014). The constraints from strong lensing need to be combined and fit simultaneously with stellar kinematic data and with weak lensing measurements (Newman et al. 2011) to improve accuracy. Using constraints on the mass profile arising from probes other than lensing also helps break the mass-sheet degeneracy (see e.g. Treu &amp; Koopmans 2002;Nordin et al. 2014;Zitrin, Redlich &amp; Broadhurst 2014;Rodney et al. 2015). Finally, in several clusters, lensing data alone seems unable to discriminate between various density profiles (Shu et al. 2008). Therefore, in some clusters the data favours steep inner density profile slopes, while, in others it favours extremely shallow density profiles. This is in contrast with the predictions from the cold dark matter paradigm (Sand, Treu &amp; Ellis 2002;Sand et al. 2005;Newman et al. 2013;Newman, Ellis &amp; Treu 2015, but see also Bartelmann &amp; Meneghetti 2004;Meneghetti et al. 2007) where a universal density profile is expected with minor modification due to the aggregation of baryons in the inner regions.</p>
        <p>In this paper, we challenge these lens inversion methods to reconstruct synthetic lenses with known input mass distributions. The goals of this exercise are twofold. First, we aim to provide concrete feedback to the lens modellers on how they may improve the performance of their codes. And secondly, we aim to provide potential users of the FFI models and the astronomical community at large a sharper, more quantitative view of how robustly specific properties of lenses are recovered and the sources of error that plague each method. Such a comparison with numerical simulations and contrasting of lens mapping methodologies has not been undertaken before.</p>
        <p>The outline of the paper is as follows. In Section 2, we outline the lens modelling challenge. In Section 3, we briefly introduce the various lens modelling techniques that were employed by participants in this study. In Section 4, we discuss the results of the reconstructions. Section 5 is dedicated to the detailed comparison of the independent modelling techniques through suitably defined metrics. Finally, in Section 7, we summarize the main results of this study and present our conclusions.</p>
        <p>The challenge that we presented to various groups of lens modellers comprised of analysing simulated observations of two mock galaxy clusters and producing magnification and mass maps for them. In generating these simulated (mock) clusters, we attempted to reproduce the depth, colour and spatial resolution of HST observations of the FFI cluster images including the gravitational lensing effects. While the comparison of lensing reconstructions of real clusters using the same input observational constraints strongly indicates that currently developed lens inversion techniques are robust (see e.g. Jullo et al. 2007;Grillo et al. 2015), the analysis of simulated data involving a large degree of realism where the true underlying mass distribution is known can help the lens reconstruction community to greatly improve their understanding of the modelling systematics. This view of using mocks to calibrate methodologies is widely supported by a number of extensive investigations carried out in the last few years.</p>
        <p>There are multiple advantages to such calibration exercises. First of all, we are able to produce reasonably realistic cluster mass distributions in simulations (although up to some limit) that can be used as lensing clusters. Building on an extensive analysis of N-body/hydrodynamical simulations to improve the knowledge of strong lensing clusters, we have identified the important properties of the lenses which need to be taken into account during the construction of a lens model: cluster galaxies (Meneghetti et al. 2000;Meneghetti, Bartelmann &amp; Moscardini 2003a), ellipticity and asymmetries (Meneghetti, Bartelmann &amp; Moscardini 2003b), substructures (Meneghetti et al. 2007), baryonic physics (Puchwein et al. 2005;Killedar et al. 2012) and the dynamical state (Torri et al. 2004). In fact, we can simulate the lensing effects of galaxy clusters accounting for all these important properties, using both state-of-the-art hydrodynamical simulations and semi-analytic models. Secondly, we have developed tools to produce mock observations of these simulated lenses. Our image simulator 
            <rs type="software">SKYLENS</rs> (Meneghetti et al. 2008(Meneghetti et al. , 2010a) ) can mimic observations taken virtually with any telescope, but here we have used it primarily to produce simulations of HST images taken with both the ACS and the WFC3. In a small-scale realization of the experiment that we present here, we applied the lens inversion techniques to a limited number of simulated observations of our mock lenses. By doing so, we highlighted some key limits of the strong lensing methods. For example, we note that strong lensing alone is powerful at constraining the cluster mass within the Einstein radius (∼100 kpc for a massive cluster) but the addition of further constraints at larger radii are required in order to appropriately measure the shape of the density profiles out to the cluster outskirts (Meneghetti et al. 2010a;Rasia et al. 2012). In what follows, we describe in detail how we generate the mock data set for the challenge, and what kind of high-level products were distributed to the participants.
        </p>
        <p>For the exercise reported here, we generated mass distributions for two massive cluster lenses. These two lenses are generated following substantially different approaches, as outlined below. In order to easily distinguish them, we assigned them names -Ares and Hera.</p>
        <p>The mass distribution of the first simulated galaxy cluster, Ares, is generated using the semi-analytic code 
            <rs type="software">MOKAfoot_2</rs> (Giocoli et al. 2012a). This software package builds up mock galaxy clusters by treating them as being comprised of three components: (i) the main dark matter halo -assumed to be smooth, triaxial and well fit with an NFW profile, (ii) cluster members -subhaloes, distributed to follow the main halo and to have a truncated singular isothermal sphere profile (Hernquist 1990), and (iii) the brightest cluster galaxy (BCG) modelled with a separate Hernquist (1990) profile. The axial ratios, a/b and a/c, of the main halo ellipsoid are randomly drawn from the Jing &amp; Suto (2002) distributions requiring abc = 1. We note that the observed FFI clusters typically consist of merging subclusters that cause them to be particularly efficient and spectacular lenses. As shown by Torri et al. (2004), the strong lensing cross-section is boosted significantly during the merging processes.
        </p>
        <p>In the attempt to generate a mass distribution that adequately replicates the complexity of the Frontier Fields clusters, Ares was produced by combining two large-scale mass distributions at z = 0.5. The two clumps have virial masses M 1 = 1.32 × 10 15 h -1 M and M 2 = 8.8 × 10 14 h -1 M and their centres are separated by ∼400 h -1 kpc. In each of the two cases, we start by assigning the same projected ellipticity to the smooth component, to the stellar density and to the subhalo spatial distribution. This is motivated by the hierarchical clustering scenario wherein the BCG and the substructures are related to the cluster as a whole and retain memory of the directions of the accretion of repeated merging events (Kazantzidis et al. 2004(Kazantzidis et al. , 2008(Kazantzidis et al. , 2009;;Fasano et al. 2010). In order to introduce some level of asymmetry, we then added in a small twist to the surface density contours. The degree of twisting adopted reproduces variations of the orientation of iso-surface density contours measured in numerically simulated galaxy clusters (see e.g. Meneghetti et al. 2007). The two large-scale haloes combined to create Ares are nearly aligned. The difference between the position angles of the two clumps is ∼21 deg. The central region of Ares contains large baryonic concentrations to mimic the presence of BCGs. We account for the possible adiabatic contraction of the dark matter caused by the presence of BCGs for Ares (although several empirical studies find no evidence of adiabatic contraction on these scales, see e.g. Newman et al. 2013;Dutton &amp; Treu 2014). The adiabatic contraction as described by Keeton &amp; Madau (2001) for Hernquist (1990) was implemented. For further details of the 
            <rs type="software">MOKA</rs> code, we refer to Giocoli et al. (2012a,b). 
            <rs type="software">MOKA</rs> also takes into account the correlation between assembly history and various halo properties that are expected in CDM: (i) less massive haloes typically tend to be more concentrated than the more massive ones, and (ii) at fixed mass, earlier forming haloes are more concentrated and contain fewer substructures. These recipes have been implemented in consonance with recent results from numerical simulations. In particular, we assume the Zhao et al. (2009) relation to link the concentration to mass and the Giocoli et al. (2010) relation for the subhalo abundance. When substructures are included we define the smooth mass as M smooth = M viri m sub, i and its concentration c s are defined such that the total (smooth+clumps) mass density profile has a concentration c vir , equal to that of the total virial mass of the halo. Throughout the paper, the quoted masses and concentrations are evaluated at the virial radius, M vir and c vir . For these definitions, we adopt derivations from the spherical collapse model:
        </p>
        <p>where ρ c = 2.77 × 10 11 h 2 M Mpc -1 represents the critical density of the Universe, 0 = m (0) is the matter density parameter at the present time, vir is the virial overdensity (Eke, Cole &amp; Frenk 1996;Bryan &amp; Norman 1998) and R vir symbolizes the virial radius of the halo, i.e. the distance from the halo centre that encloses the desired density contrast; and</p>
        <p>with r s the radius at which the NFW profile approaches a logarithmic slope of -2. The concentrations assigned to the two main mass components of Ares are c 1 = 5.39 and c 2 = 5.46, respectively.</p>
        <p>Ares is generated in a flat CDM cosmological model with matter density parameter m, 0 = 0.272. The Hubble parameter at the present epoch is H 0 = 70.4 km s -1 Mpc -1 .</p>
        <p>In the left-hand panels of Fig. 1, we show the convergence maps of Ares, calculated for a source redshift z s = 9. The cluster is elongated in the SE-NW direction and contains several massive substructures. Since Ares was generated using semi-analytical methods (SAMs), the small-scale substructures of its mass distribution are very well resolved, as shown in the bottom-left panel. The substructure mass function is shown in the right-hand panel of Fig. 2. As expected, this scales as N(M) ∝ M -0.8 , consistent with results of numerical simulations (Giocoli et al. 2010) of the CDM model. The convergence profile, measured from the centre of the most massive clump, is shown in the left-hand panel of Fig. 2.</p>
        <p>In the image simulations described later, we also include the light emission from cluster members. MOKA populates the dark matter subhaloes with galaxies using the Halo Occupation Distribution (HOD) technique. Stellar masses and B-band luminosities are subsequently assigned to each galaxy accordingly to the mass of the dark matter (sub)halo within which it formed, following Wang et al. (2006). The morphological type and the Spectral Energy Distribution (SED) of each galaxy is then defined on the basis of the stellar mass so as to reproduce the observed morphology-density and morphology-radius relations in galaxy clusters (e.g. van der Wel 2008;Ma et al. 2010).</p>
        <p>The mass distribution of the second galaxy cluster, Hera, is instead directly derived from a high-resolution N-body simulation of a cluster-sized dark matter halo. More precisely, Hera is part of the set of simulated clusters presented in Planelles et al. (2014). The cluster halo was first identified in a low-resolution simulation box with a periodic comoving size of 1 h -1 Gpc for a flat CDM model with present matter density parameter m, 0 = 0.24 and baryon density parameter b, 0 = 0.04. The Hubble constant adopted was Figure 2. Key properties of Ares and Hera (blue and red colours, respectively). Left-hand panel: convergence profiles (for source redshift z s = 9). In both cases, the centre has been chosen to coincide with the most massive dark matter clump in the simulation; Right-hand panel: subhalo mass function (built considering all subhaloes within 1 h -1 Mpc from the centre of the most massive clump.</p>
        <p>H 0 = 72 km s -1 Mpc -1 and the normalization of the matter power spectrum σ 8 = 0.8. The primordial power spectrum of the density fluctuations is P(k) ∝ k n with n = 0.96. The parent simulation followed 1024 3 collisionless particles in the box. Hera was identified at z = 0 using a standard Friends-of-Friends (FoF) algorithm, and its Lagrangian region was re-simulated at higher resolution employing the Zoomed Initial Conditions code (ZIC; Tormen, Bouchet &amp; White 1997). The resolution is progressively degraded outside this region to save computational time while still providing a correct description of the large-scale tidal field. The Lagrangian region was taken to be large enough to ensure that only high-resolution particles are present within five virial radii of the cluster.</p>
        <p>The re-simulation was then carried out using the TreePM-SPH GADGET-3 code, a newer version of the original 
            <rs type="software">GADGET</rs>-
            <rs type="version">2</rs> code by Springel (2005) that adopted a more efficient domain decomposition to improve the work load balance. Although, the parent Hera halo exists in several flavours in various simulation runs (several assumptions for the nature of dark matter particles), including several baryonic processes, the simulation used in this paper uses only the version that utilized collisionless dark matter particles. This has allowed us to increase the mass resolution by about an order of magnitude compared to the hydrodynamical versions of the simulation. The particle mass is m DM = 10 8 h -1 M . Therefore, the virial region of Hera is resolved with ∼10 million particles, with a total cluster mass of M = 9.4 × 10 14 h -1 M , comparable to that inferred for observed cluster lenses. The redshift of this halo is z l = 0.507. During the re-simulation, the Plummer-equivalent comoving softening length for gravitational force in the high-resolution region is fixed to Pl = 2.3 h -1 kpc physical at z &lt; 2 while being fixed to Pl = 6.9 h -1 kpc comoving at higher redshift. The properties of cluster galaxies used for creating the simulated observations are derived from SAMs of galaxy formation (De Lucia &amp; Blaizot 2007). The process starts by using the algorithm 
            <rs type="software">SUBFIND</rs> (Springel et al. 2001) to decompose each FOF group previously found in the simulation into a set of disjoint substructures. These are identified as locally overdense regions in the density field of the background halo. Only substructures that retain at least 20 bound particles after a gravitational unbinding procedure are considered to be genuine substructures. Merging histories are constructed for all self-bound structures, using the same post-processing algorithm that has been employed for the Millennium Simulation (Springel, Frenk &amp; White 2006). The merger-tree is then used to construct a mock catalogue of galaxies. The evolution of the galaxy population is described by a modified version of the semi-analytic model presented in De Lucia &amp; Blaizot (2007), that included the implementation of the generation of intracluster light described in Contini et al. (2014), given by the combination of Model Tidal Radius and Merger channels presented in that paper.
        </p>
        <p>Note that, even in the case of Hera, the galaxy positions trace reasonably well the mass. Several reconstruction methods assume that light traces the mass, a reasonable assumption that is thus satisfied both in Ares and in Hera. To increase the level of uncertainty, the galaxy shapes and orientations are chosen to be uncorrelated with the underlying mass distribution.</p>
        <p>The convergence map of Hera with its complex morphology and the abundance of substructures is shown in the right-hand panels of Fig. 1. The convergence profile and the substructure mass function are displayed in Fig. 2. Compared to Ares, the small-scale structures of Hera are smoother as they are less well resolved. Nevertheless, the substructure mass function scales very similarly with halo mass. As in the case of Ares, Hera has a bimodal mass distribution. A massive substructure (M ∼ 5 × 10 13 h -1 M ) is located ∼30 arcsec (∼130 h -1 kpc) from the cluster centre, producing a secondary peak in the convergence map and elongating the iso-density contours in the south-west-north-easterly direction.</p>
        <p>In order to generate lensing effects in the simulated images, it is necessary to compute the deflections produced by the cluster. This allows us to then use ray-tracing methods to map the surfacebrightness distribution of the sources on the camera of our virtual telescope, which is HST in this case. In practice, we shoot a bundle of light rays through a dense grid covering the field of view (FOV), starting from the position of the observer. Then, we use the computed deflection angles to trace the path of the light back to the sources. When simulating HST observations, we compute the deflection angles on a regular grid of 2048 × 2048 points, covering a FOV of 250 arcsec × 250 arcsec centred on the cluster. The pixel scale of this grid corresponds to 0.12 arcsec or ∼0.5 h -1 kpc.</p>
        <p>In the case of Ares, MOKA produces a map of the convergence, κ(θ). This can be converted into a map of the deflection angles, α(θ), by solving the equation</p>
        <p>Since this is a convolution of the convergence, κ(θ), with the kernel function</p>
        <p>this task can be achieved numerically by means of fast-Fouriertransform (FFT) methods. To do so, we use the FFT routines implemented in the 
            <rs type="software">gsl</rs> library.
        </p>
        <p>In the case of Hera, the mass distribution of the cluster is described by a collection of dark matter particles. Instead of mapping them on a grid to construct the convergence map, we use our consolidated lensing simulation pipeline (see e.g. Meneghetti et al. 2010b, and references therein) to compute the deflections. To briefly summarize, the procedure involves the following steps:</p>
        <p>(i) We project the particles belonging to the halo along the desired line of sight on the lens plane. To select particles, we define a slice of the simulated volume around the cluster, corresponding to a depth of 10 h -1 Mpc.</p>
        <p>(ii) Starting from the position of the virtual observer, we trace a bundle of light rays through a regular grid of 2048 × 2048 covering a region around the halo centre on the lens plane. In the case of strong lensing simulations (e.g. for HST observations), we restrict our analysis to a region of 1 × 1 h -2 Mpc 2 . The pixel scale of this grid is 0.11 arcsec, corresponding to ∼0.47 h -1 kpc. In the case of simulations extending into the weak-lensing regime (e.g. for Subaru-like observations), the grid of light rays covers a much wider area (∼8 × 8 h -2 Mpc 2 ). In this case, we use a grid of 4096 × 4096 rays, whose resolution is 0.44 arcsec pixel -1 , corresponding to ∼1.87 h -1 kpc pixel -1 .</p>
        <p>(iii) Using our code 
            <rs type="software">GLFAST</rs> (Meneghetti et al. 2010a), we compute the total deflection α(x) at each light-ray position x, accounting for the contributions from all particles on the lens plane. Even in the case of strong-lensing simulations, where light rays are shot through a narrower region of the lens plane, the deflections account for all particles projected out to ∼4 h -1 Mpc from the cluster centre. The code is based on a Tree algorithm, where the contributions to the deflection angle of a light ray by the nearby particles are summed directly, while those from distant particles are calculated using higher order Taylor expansions of the deflection potential around the light-ray positions.
        </p>
        <p>(iv) The resulting deflection field is used to derive several relevant lensing quantities. In particular, we use the spatial derivatives of α(θ) to construct the shear maps, γ = (γ 1 , γ 2 ), defined as</p>
        <p>The convergence, κ(θ ), may also be reconstructed as</p>
        <p>The lensing critical lines yield formally infinite magnification for a given source redshift. They are defined as the curves along which the determinant of the lensing Jacobian is zero (e.g. Schneider, Ehlers &amp; Falco 1992):</p>
        <p>In particular, the tangential critical line is defined by the condition (1κ -|γ |) = 0, whereas the radial critical line corresponds to the line along which (1κ + |γ |) = 0. In the following sections, we will often use the term Einstein radius to refer to the size of the tangential critical line. As discussed in Meneghetti et al. (2013), there are several possible definitions for the Einstein radius. Here, we adopt the effective Einstein radius definition (see also Redlich et al. 2012) given by</p>
        <p>where S is the area enclosed by the tangential critical line and d L is the angular diameter distance to the lens plane.</p>
        <p>We simulate observations of galaxy cluster fields using the code 
            <rs type="software">SKYLENS</rs>, which is described in detail in Meneghetti et al. (2008) and in Meneghetti et al. (2010a). The creation of the simulated images involves the following steps:
        </p>
        <p>(i) we generate a past light-cone populated with source galaxies resembling the luminosity and the redshift distribution of the galaxies in the Hubble Ultra-Deep-Field (HUDF; Coe et al. 2006);</p>
        <p>(ii) we model the morphologies of the sources using shapelet decompositions of the galaxies in the HUDF (Melchior, Meneghetti &amp; Bartelmann 2007). Their spectral energy distributions were obtained as part of the photometric redshift measurements of these galaxies described in Coe et al. (2006);</p>
        <p>(iii) the deflection fields of the lensing clusters are used to trace a bundle of rays from a virtual CCD, resembling the properties of the ACS or of the WFC3, back to the sources;</p>
        <p>(iv) by associating each pixel of the virtual CCD to the emitting elements of the sources, we reconstruct their lensed surface brightness distributions on the CCD;</p>
        <p>(v) we model the shape and the surface brightness distribution of the cluster galaxies using elliptical single or double Sérsic models (Sérsic 1963). These are obtained by fitting real cluster galaxies in a set of low to intermediate redshift clusters (Gonzalez, Zabludoff &amp; Zaritsky 2005). The match between the observed and the simulated galaxies is done via the galaxy luminosity. The BCGs all include a large-scale component used to model the intracluster light produced by the BCG stellar haloes and by free-floating stars;</p>
        <p>(vi) the SEDs of the cluster galaxies are modelled according to prescriptions from semi-analytic models or from the HOD technique, as explained earlier;</p>
        <p>(vii) we convert the surface brightness distributions into counts per pixel assuming a telescope throughput curve, which accounts for the optics, the camera and the filter used in carrying out the simulated observations. In each band, we simulate the exposure times (in units of HST orbitsfoot_5 ) used to carry out the mock Frontier Fields observations;</p>
        <p>(viii) the images are then convolved with a point spread function (PSF) model, obtained using the 
            <rs type="software">TINYTIM</rs> HST PSF modelling software (Krist, Hook &amp; Stoehr 2011). Finally, realistic noise is added
        </p>
        <p>This is the first phase of a comparison project, and in the next phase we intend to include additional simulations with an even greater level of realism. For this first exercise, we proceed as follows.</p>
        <p>(i) For both Ares and Hera, we generate simulated HST observations in all bands that are deployed for the FFI, mimicking the same exposure times (or number of orbits) as the real observations. The level of the background is set to values provided by the ACS and WFC3 exposure time calculators in each band. The details of these simulations are provided in Table 1. Each image covers a FOV of 204 × 204 arcsec 2 . All images are co-aligned and corotated. Effects like gaps between chips, pixel defects, charge transfer inefficiency, cosmic rays, etc. are not included. The resolution of the ACS and WFC3 simulations are 0.05 and 0.13 arcsec pixel -1 , respectively. These images were made available to the modellers.</p>
        <p>(ii) In addition to the images, we provided the list of all multiple images obtained from the ray-tracing procedure (see Figs 3 and4, upper panels). Each multiple-image system is characterized by the redshift of its source, which is also provided to the modellers. Thus, in this exercise we assume that all images can be identified without errors and that all their redshifts can be measured 'spectroscopically'. This is certainly a very optimistic assumption that will never be satisfied in the real world. In the next round of this comparison project, the assumption will be relaxed, but for the moment we decided to release this information because our objective is to determine possible systematics of the various reconstruction algorithms. Other issues related to the approaches used to search for multiple images or the impact of redshift uncertainties on the results will be studied in a future work. Some of these systematics have already been investigated for some lens modelling methods, i.e. Johnson &amp; Sharon (2016).</p>
        <p>(iii) We also released a catalogue of cluster members (circled in the bottom-right panels of Figs 3 and4), containing positions and photometry in all bands. Several reconstruction methods (in particular those employing the parametric approach) build the lens model by combining smooth dark matter haloes with substructures associated with the cluster members akin to our construction of Ares. In this simplified test, modellers are provided with the list of all cluster members with m AB, F814W &lt; 24. Again, this is an oversimplification that will be removed in the next round of simulations, and which implicitly favours those methods that use this information. In reality, such methods have to deal with the risks of misidentification of cluster members.</p>
        <p>(iv) For those groups that use weak lensing measurements to complement the strong lensing constraints, we produced a single Subaru-like R-band image of both Ares and Hera covering a much larger FOV of 30 × 30 arcmin 2 . The provided image contained only background galaxies (i.e. lensed by the clusters) and stars, so that shape measurements could be made using any weaklensing pipeline without worrying about the separation of background sources from the cluster members or contamination by foreground galaxies. We also use the publicly available pipeline KSBf90foot_7 (Heymans et al. 2006) based on the Kaiser, Squires and Broadhurst method (Kaiser, Squires &amp; Broadhurst 1995) to derive a catalogue containing galaxy positions and ellipticities. The resulting number density of galaxies useful for the weak lensing analysis is ∼14 galaxies arcmin -2 . This is significantly smaller than the number density achievable with HST.</p>
        <p>All these data for the mock cluster lenses were shared with lens modellers participating in the project via a dedicated website. 7 We emphasize that the input mass distributions of the lenses and the techniques used to generate them were initially kept blinded to all groups. The strong lensing constraints amounted to 242 multiple images produced by 85 sources in the case of Ares and 65 images of 19 sources in the case of Hera.</p>
        <p>A large fraction of lens modellers currently working actively on the analysis of the FFI data accepted the challenge and participated in this project. The two cluster simulations were not released simultaneously. We initially released only the data for Ares, and we received reconstructed models for this cluster from seven groups. These groups performed a fully blind analysis of the data set. Two additional models were submitted by A. Zitrin after the input mass distributions were already revealed, under the assurance that the reconstruction was actually performed blindly.</p>
        <p>In a second stage of the comparison exercise, we released the simulation of Hera, and received eight models from six participating groups. Also for this cluster, we received additional reconstructions after we revealed the input mass distribution of the lens. These models were submitted by A. Zitrin and by D. Lam.</p>
        <p>There are two general classes of inversion algorithms. They comprise parametric models wherein the mass distribution is reconstructed by combining clumps of matter, often positioned where the BCGs are located, each of which is characterized by an ensemble of parameters including the density profile and shape. The parameter spaces of these models are explored in an effort to best reproduce the observed positions, shapes and magnitudes of the multiple images and arcs. The second approach is called free form (a.k.a. non-parametric): wherein now the cluster is subdivided into a mesh on to which the lensing observables are mapped, and which We overlay to the optical image the critical lines for z s = 1 (red) and z s = 9 (white). In addition, we display the location of the multiple image systems (numbered yellow circles). In the bottom panels, we show the image of the cluster in the F814W band. On the left, the galaxies identified as cluster members are indicated by red circles. On the right, we display the surface density iso-contours. is then transformed into a pixelized mass distribution following several methods to link the observable to the underlying lens potential or deflection field.</p>
        <p>Both these approaches were amply represented in the challenge. A summary of all submitted models, with the indication of whether they are parametric or free form and built before or after the input mass distribution of the lenses was revealed, is given in Table 2. Each model is given a reference name used throughout the paper. Each modelling technique is briefly described below. In Table 3, we summarize the details of each reconstruction of Ares and Hera.</p>
        <p>Table 2. Models submitted by the groups participating in the project. The table lists the name of the submitting group/author of the reconstruction, the reference name of the model, the type of algorithm (free-form, parametric or hybrid) and whether the model was submitted blind, i.e. before the input mass distribution of the lens was revealed.</p>
        <p>The Bradac-Hoag model employs the method named SWUnited: Strong and weak lensing mass reconstruction on a non-uniform adapted grid. This combined strong and weak lensing analysis method reconstructs the gravitational potential ψ k = ψ(θ k ) on a set of points θ k , which can be randomly distributed over the entire FOV. From the potential, any desired gravitational lensing quantity (e.g. surface mass density, deflection angle, magnification, flexion, etc.) can be readily calculated. Such an approach therefore does not require an assumption of e.g. a particular model of the potential/mass distribution. The potential is reconstructed by maximizing a log-likelihood log P (see equation 2 of Bradač et al. 2009), which uses image positions of multiply imaged sources and source plane minimization (corrected by magnification); weak lensing ellipticities, and regularization as constraints. Current implementation also includes flexion measurements, however, the data was not used in this paper.</p>
        <p>The implementation of the method follows the algorithm first proposed by Bartelmann et al. (1996) and is described in detail in Bradač et al. (2005) and Bradač et al. (2009). It starts with a uniform grid of points θ k at coarse resolution (25-30 pixels on a side) and refine it by two to four times in progressively smaller regions enclosing the BCGs and by eight times around multiple images, making sure there are no sharp transitions in the density of grid points anywhere. This generally results in a grid that is coarse in the outskirts of the cluster and fine in the central regions where the multiple images exist. These are the points where the potential is evaluated. From the set of potential values all observables are determined using derivatives. For example, the convergence κ is related to ψ via the Poisson equation, 2κ = ∇ 2 ψ (where the physical surface mass density is = κ crit and crit depends upon the angular diameter distances between the observer, the lens and the source). The details on how the derivatives on a non-uniform grid are evaluated can be found in Bradač et al. (2009). By using a reconstruction grid whose pixel scale varies across the field, the method is able to achieve increased resolution in the cluster centre (close to where we see strongly lensed images), and hence the magnification map in the regions of high magnification is more detailed. The posterior peak values of the potential ψ k are found by solving the non-linear equation ∂ log P /∂ψ k = 0. This set of equations is linearized and a solution is reached in an iterative fashion (keeping the non-linear terms fixed at each iteration step). This requires an initial guess for the gravitational potential; the systematic effects arising from various choices of this initial model were discussed in Bradač et al. (2006). The choice of particular grid geometry, the regularization parameter and the hyper-parameters that set the relative weighting between the contributions to log P all become critical when weak lensing data on large scales ( 1Mpc) are included, and a full-field mass reconstruction is needed. This is not the case in this work, as we are only interested in the magnification of the inner region.</p>
        <p>The reconstruction is performed in a two-level iteration process, outlined in Fig. 5. The inner-level iteration process described above for solving the non-linear system of equations ∂ log P /∂ψ k = 0 is solved in iterative fashion and repeated until convergence of κ. The outer-level iteration is performed for the purpose of regularization. In order to penalize small-scale fluctuations in the surface mass density, the reconstruction is started with a coarse grid (large cell size). Then, for each n 2 step the number of grid points is increased in the field and the new reconstructed κ (n 2 ) is compared with the one from the previous iteration κ (n 2 -1) (or with the initial input value κ (0) for n 2 = 0), penalizing any large deviations. The secondlevel iterations are performed until the final grid size is reached and convergence is achieved.</p>
        <p>Regularization is used. It is adjusted so that the model converges and the final χ 2 for weak lensing is ∼2N weak (number of weak lensing galaxies). If the regularization term is too small, the method fits to the noise and does not converge to a single solution.</p>
        <p>The main strength of the method as discussed above is that instead of fitting a specific set of family of models to the data, the method is free of such an assumption. Furthermore, the positions of the points where the potential is reconstructed (θ k ) can be chosen arbitrarily, which allows us to increase the density of points in the regions where signal to noise is the highest (i.e. where multiple images are present). A coarser sampling can be employed in the areas where this is not the case (e.g. at large radii from the centre). The algorithm reconstructs the potential (rather than traditionally used surface mass density), since it locally determines both the lensing distortion (for weak lensing and flexion) as well as the deflection (for strong lensing) and there is no need to assume the surface mass density values beyond the observed field. 4) profile used to model the dark matter halo components; (5) number of dark matter haloes assumed; (6) type of constraints used (SL, WL); ( 7) number of multiple images used as constraints; (8) number of additional constraints (e.g. image ellipticity, flux, etc.); (9) number of free parameters. In the case of free-form methods, the number of grid points of the reconstructed map is given; (10) density profile used to model the cluster galaxies; (11) shape of the cluster galaxies (circular or elliptical); ( 12) scaling relations used to model the ensemble of cluster galaxies; (13) number of free parameters used to model the cluster galaxies; (14) catalogue of cluster galaxies used; (15) number of cluster galaxies optimized individually (i.e. independently on the scaling relations); ( 16) external shear (yes or no); (17) high-order perturbations (yes or no) and ( 18) regularization term (yes or no). The number of free parameters depends on the number of Plummer spheres effectively used. This is not known a priori as it is determined by the genetic algorithm during each run. The starting number of Plummer spheres is 600-1000 depending on the run; (g)</p>
        <p>The cluster mass distribution is described by a combination of RBFs whose form is given in equations 17, 20 and 21 of Coe et al. (2008); (h)</p>
        <p>The CATS Ares catalogue of cluster members is obtained by selecting the galaxies with m F160W &lt; 22; (i) Positions and luminosities were taken from the official catalogue of cluster members distributed to all teams. Ellipticity and position angle of each galaxy were measured using 
            <rs type="software">SEXTRACTOR</rs>;
        </p>
        <p>(j) The two brightest galaxies are modelled with Hernquist profiles and optimized individually. The other galaxies are modelled with PIEMDs and their masses and sizes scale with luminosity. The main weakness of the method on the other hand is the fact that it maximizes a function with a large number of parameters. Therefore, the method is inherently unstable. The inversion of the matrix satisfying the equation ∂ log P /∂ψ k = 0 is also very noisy. The method is therefore very likely to diverge or land in a secondary minimum. Regularization needs to be employed, which adds additional parameters (relative weighting of regularization term) to the rest of log P and a choice of regularization method itself. The optimal choices need to be determined using simulation data.</p>
        <p>A recent improvement to the method is the addition of the measurement of flexion to the input constraints. The code has been adapted (see also Cain, Bradac &amp; Levinson 2016) and tested on simulated data. This improvement is currently being tested using HST data. In the future, the code will be ported in 
            <rs type="software">PYTHON</rs>, to make the interface user friendly, and released to the community.
        </p>
        <p>All Diego models (Diego-multires, Diego-overfit and Diego-reggrid models) and the Lam model are built using 
            <rs type="software">WSLAP+</rs>, a freeform or non-parametric method that includes also a compact mass component associated with the cluster members (thus, classified as hybrid in this paper). The main part of the 
            <rs type="software">code</rs> is written in FORTRAN and compiles with standard compilers (like 
            <rs type="software">gfortran</rs>) included in the most common 
            <rs type="software">linux</rs> distributions. 
            <rs type="software">Plotting routines</rs> written in 
            <rs type="software">IDL</rs> are available to display the intermediate results as the code runs. A 
            <rs type="software">script</rs> interface allows the user to define the input and output files, select the parts of the code to be run and control the plotting routines. A detailed description of the code and of its features can be found in the papers by Diego et al. (2005), Diego et al. (2007), Sendra et al. (2014) and Diego et al. (2016). The code is not publicly available yet but a companion code 
            <rs type="software">LENSEXPLORER</rs> is available. 
            <rs type="software">LENSEXPLORER</rs> allows the user to easily explore the lens models derived for the Frontier Fields clusters, search for new counter images, compute magnifications or predict the location and shape of multiple images. The 
            <rs type="software">WSLAP+</rs> code is still under development. It already includes certain features that were not used in the analysis presented in this paper. They will be included in the future 'unblinded' version of this work. Among these features, the code incorporates spatial information about knots in resolved systems greatly improving the accuracy and robustness of the results (see Diego et al. 2016, for practical demonstration). In this work, only long elongated arcs were considered as resolved systems. Multiple knots identified in these arcs are assumed to originate from a very compact region in the source plane. For the shorter arcs, only the central knot was used.
        </p>
        <p>A diagram describing the steps involved in the analysis performed by WSLAP+ is shown in Fig. 6. The algorithm divides the mass distribution in the lens plane into two components. The first is a compact one and is associated with the member galaxies. The member galaxies are selected from the red sequence. The second component is diffuse and is distributed as a superposition of Gaussians on a regular (or adaptive) grid. For the compact component, the mass associated with the galaxies is assumed to be proportional to their luminosity. If all the galaxies are assumed to have the same massto-light (M/L) ratio, the compact component (galaxies) contributes with just one (N g = 1) extra free parameter that corresponds to the correction that needs to be applied to the fiducial M/L ratio. In some particular cases, some galaxies (like the BCG or massive galaxies very close to an arclet) are allowed to have their own M/L ratio adding additional free parameters to the lens model but typically no more than a few (N g ∼ O(1)). For this component associated with the galaxies, the total mass is assumed to follow either an NFW profile (with fixed concentration and scale radius scaling with the fiducial halo mass) or be proportional to the observed surface brightness. The diffuse or 'soft' component is described by as many free parameters as grid (or cell) points. This number (N c ) varies but is typically between a few hundred to one thousand (N c ∼ O(100)-O(1000)) depending on the resolution and/or use of the adaptive grid. The size of the adaptive grid, when used, is based on a solution obtained with the regular grid. Areas with more mass result in smaller grid cells. In addition to the free parameters describing the lens model, the method includes as unknowns the original positions of the lensed galaxies in the source plane. For the clusters included in the FFI program the number of background sources, N s , is typically a few tens (N s ∼ O(10)), each contributing with two unknowns (β x and β y ). All the unknowns are then combined into a single array X with N x elements (N x ∼ O(1000)).</p>
        <p>The observables are both strong lensing and weak lensing (shear) measurements. For strong lensing data, the inputs are the pixel positions of the strongly lensed galaxies (not just the centroids). In the case of long elongated arcs near the critical curves with no features, the entire arc is mapped and included as a constraint. If the arclets have individual features, these can be incorporated as semi-independent constraints but with the added condition that they need to form the same source in the source plane. Incorporating this information acts as an anchor constraining the range of possible solutions and reducing the risk of a bias due to the minimization being carried in the source plane. For the weak lensing, we use shear measurements (γ 1 and γ 2 ). The weak lensing constraints normally complement the lack of strong lensing constraints beyond the central region allowing for a mass reconstruction on a wider scale. When weak lensing information is used, the code typically uses an adaptive grid to extend the range up to the larger distances covered by the weak lensing data (Diego et al. 2015a) The solution, X, which is an array containing the unknowns in the problem (the mass in each grid cell, the re-scaling factor of the assumed light-to-mass ratio for the member galaxies and the positions of the lensed galaxies in the source plane) is obtained after solving the system of linear equations</p>
        <p>where the N o observations (strong lensing, weak lensing, time delays) are included in the array and the matrix is known and has dimension N o x(N c + N g + 2N s ). This matrix contains the deflection field and shear components (if weak lensing data is available) produced by each grid cell with a fiducial mass and the fiducial distribution of member galaxies at each observed location of an arc. In practice, X, is obtained by solving the set of linear equations described in equation ( 10) via a fast bi-conjugate algorithm, or inverted with a singular value decomposition (after setting a threshold for the eigenvalues) or solved with a more robust quadratic algorithm (slower). The quadratic algorithm is the preferred method as it imposes the physical constrain that the solution X must be positive. This eliminates un-physical solutions with negative masses and reduces the space of possible solutions. Errors in the solution are derived by minimizing the quadratic function multiple times, after varying the initial conditions of the minimization process, and/or modifying the grid, and/or changing the fiducial deflection field associated with the member galaxies.</p>
        <p>The code implements a free-form modelling component. This implies that no strong assumptions are necessary about the distribution of dark matter. This is particularly useful if DM is not linked to the galaxies or if the baryons are also dissociated from the galaxies. The latter seems to be the case in the FFI clusters which are in a merging phase. Evidence that the solution obtained by the algorithm may be sensitive to the mass of the X-ray emitting plasma was presented in Lam et al. (2014) and Diego et al. (2015bDiego et al. ( ,c, 2016)). The algorithm is very fast. Several methods are implemented to search for a solution. Using the bi-conjugate gradient algorithm a solution can be obtained in seconds. Using the slower, but more reliable, quadratic optimization approaches a robust solution that can be obtained in minutes. Other fast approaches have been implemented as well like singular value decomposition.</p>
        <p>An adaptive grid can be used that transforms the method into a multiresolution code. Different adaptive grids can be implemented that introduce a small degree of freedom but also allows us to explore other possible solutions and hence constrain better the variability of the solution.</p>
        <p>The code is prepared to combine weak and strong lensing. The relative weight of the two data sets is given by the intrinsic errors in the data sets (typically small in the strong lensing regime and larger in the weak lensing regime). Correlations between the lensing data can be incorporated through a covariance matrix that naturally weights the different data sets.</p>
        <p>The minimization is made in the source plane which may result in biases towards larger magnifications (e.g. Kochanek 1991). To avoid this, the minimization algorithm needs to be stopped after a given number of iterations. Even better, including information about the size and shape of the sources in the source plane seems to solve this problem and the solution remains stable and unbiased even after a very large number of iterations. These prior information on the size and shape of the source galaxies is only possible when well-resolved lensed images are available and at least one of the multiple images is not highly magnified.</p>
        <p>The compact component is pixelized usually into a 512×512 image that covers the FOV. For the small member galaxies, this pixelization results in a loss of resolution that have a small impact on lensed images that happen to be located near this small member galaxies. A possible solution to alleviate this problem is to pre-compute the deflection field of these galaxies prior to the minimization at higher resolution and later interpolate at the position of the observed lensed galaxies. This approach has not been implemented yet but it is expected to eliminate this problem.</p>
        <p>In the Lam model of Hera, the cluster members are modelled using analytical mass profiles (NFW). In this case, there is no need to produce a pixelated map of the deflection angles. Instead, these are computed analytically.</p>
        <p>A weakness of this approach is that the code can also predict more multiple images than observed. This is not being factored in at the moment but will be the subject of the null space implementation described in Section 3.3.3. One systematic bias is known to affect the results at large distances from the centre. The reconstructed solution systematically underpredicts the mass (and magnification) in the regions where there is no lensing constraints. These regions are normally located beyond the corresponding Einstein radius for a high-redshift background source. Addition of weak lensing to then constraints can reduce or eliminate this problem.</p>
        <p>The addition of time delays is being implemented to the reconstruction of the solution. Time delays will be included in a similar footing as the other observables (weak and strong lensing observables) with a weight that is proportional to their associated observational error.</p>
        <p>The addition of the null space was proven to be a useful and powerful way of improving the robustness of the derived solution (Diego et al. 2005). This direction has not been explored fully and we plan to incorporate the null space as an additional constraint. This will eliminate additional counter images that are predicted by the model but not observed in the data.</p>
        <p>The Diego models use both a regular grid with 32 × 32 = 1024 grid points (Diego-reggrid model) and a multiresolution grid with approximately half the number of grid points (Diego-multires model).</p>
        <p>A comparison between these grids is shown in Diego et al. (2016). The compact component of the defection field is constructed based on the brightest elliptical galaxies in the cluster. We include 50 such bright ellipticals for each cluster. The mass profile for each galaxy is taken either as an NFW with scale radius (and total mass) scaling with the galaxy luminosity or directly as the observed surface brightness. This choice plays a small role in the final solution.</p>
        <p>Depending on the number of iterations, different solutions can be obtained. Earlier work based on simple simulations (Sendra et al. 2014) showed how in a typical situation (similar to the one in Ares and Hera), after 10 000 iterations of the code, the solution converges to a stable point. The code can be left iterating longer reaching a point that we refer as 'overfit' where the observed constraints are reproduced with great accuracy but sometimes at the expense of a model with fake structures. In the case of Hera, we computed the solution also in the overfit regime (90 000 iterations) for comparison purposes (Diego-overfit model). As explained in Sendra et al. (2014), the compact component helps also in regularizing the solution since it acts as an 'anchor' for the entire solution.</p>
        <p>The Lam model differs from the Diego models as follows. A regular grid of Gaussian functions is used instead of a multiresolution grid (as in Diego-multires). The code runs a minimization algorithm. In each step, a new solution is obtained and updated in an iterative way. The algorithm starts with an initial guess (a random realization of the iterative solution) and the minimization converges to a different solution each time. By varying the initial condition 100 times, 100 different lens models were obtained that are equally consistent with the data. The submitted model is an average of these 100 individual models. With the exception of the 10 BCGs, the relative masses of all cluster galaxies are fixed, and are derived using a stellar mass-dark matter mass relation found in the EAGLE cosmological hydrodynamical simulation (Schaller et al. 2015). The stellar masses of cluster galaxies are derived from fitting synthesized spectra to the measured photometry using FAST (Kriek et al. 2009). The contribution from cluster galaxies are parametrized by NFW haloes with scale radii derived from the dark matter mass using again a relation found in the same simulation.</p>
        <p>The GRALE models are based on the reconstruction code 
            <rs type="software">GRALE</rs>. 8 
            <rs type="software">GRALE</rs> is a flexible, free-form method, based on a genetic algorithm, which uses an adaptive grid to iteratively refine the mass model. As input it uses only the information about the lensed images, and nothing about cluster's visible mass (Liesenborgs, De Rijcke &amp; Dejonghe 2006). This last feature sets 
            <rs type="software">GRALE</rs> apart from many other lens mass reconstruction techniques, and gives it the ability to test how well mass follows light on small and large scales within clusters. 
            <rs type="software">GRALE</rs> has been used to reconstruct mass distributions in a number of clusters (Liesenborgs et al. 2008(Liesenborgs et al. , 2009;;Mohammed et al. 2014;Mohammed, Saha &amp; Liesenborgs 2015), quantify mass/light offsets in Abell 3827 (Mohammed et al. 2014;Massey et al. 2015), derive projected mass power spectra and compare to those of simulated clusters (Mohammed et al. 2016) and to study the relation between mass and light in MACS0416 (Sebesta et al. 2016). These papers used strong lensing constraints only, and so the analysis was confined to the central regions of galaxy clusters.
        </p>
        <p>GRALE starts out with an initial coarse uniform grid in the lens plane which is populated with a basis set, such as projected Plummer density spheres. A uniform mass sheet covering the whole modelling region is also added to supplement the basis set. As the code runs, the denser regions are resolved with a finer grid, with each cell given a Plummer with a proportionate width. The initial trial solution, as well as all later evolved solutions are evaluated for genetic fitness, and the fit ones are cloned, combined and mutated. The final map consists of a superposition of a mass sheet and many Plummers, typically several hundred to a couple of thousand, each with its own size and weight, determined by the genetic algorithm. Critical curves, caustics and magnifications for any given source redshift are automatically available.</p>
        <p>Multiple fitness measures are used in 
            <rs type="software">GRALE</rs>. These are as follows: (a) Image positions -a successful mass map would lens image-plane images of the same source back to the same source location and shape. A mass map has a better fitness measure if the images have a greater fractional degree of overlap. Using fractional overlap of extended images ensures against overfocusing or overmagnifying images. (b) Null space -regions of image plane that definitely do not contain any lensed features belong to the null space. (c) Critical lines -in some cases, it is known on astrophysical grounds that a critical line cannot go through certain image regions, but must pass between them. 
            <rs type="software">GRALE</rs> can incorporate this type of constraint, but we have not used this fitness measure in the Frontier Fields work so far. (d) Time delay measurements -though not used in this work, time delay measurements can also be incorporated into the fitness (Liesenborgs et al. 2009;Mohammed et al. 2015).
        </p>
        <p>Each GRALE run with the same set of images, but a different random seed, will produce a somewhat different final map. The dispersion between these quantifies mass uncertainties that are due to mass degeneracies present when all image information is held fixed. The best known among these, the mass sheet degeneracy, is broken in most clusters because of the multiple redshifts of background sources. The other, more numerous and less known degeneraciesdocumented (Saha 2000;Liesenborgs &amp; De Rijcke 2012) and not documented -are the ones that contribute to the uncertainties.</p>
        <p>The clusters Ares and Hera were modelled with multiple images as inputs, and using two fitness measures: (a) image positions and (b) null space for each source (image set) separately. For image sets where it was not entirely clear if or where the counter images might be present, the nulls were allowed to have large holes corresponding to the regions of possible additional images. 
            <rs type="software">GRALE</rs> can operate in two modes: with lensed images represented by points, or by extended images. The present reconstruction were done using the extended image mode.
        </p>
        <p>The main advantage of GRALE is its flexibility, and hence ability to explore a wide range of lensing mass degeneracies. Another important feature, which can be viewed as strength, is that GRALE does not use cluster galaxies, or any information about the distribution of luminous matter to do the mass reconstruction. This is useful if one wants to test how well mass follows light (Mohammed et al. 2014;Sebesta et al. 2016).</p>
        <p>GRALE's main weakness is that it is not an optimal tool for identifying lensed images. This is a direct consequence, or, one may say, the flip side of GRALE's flexibility. A technical feature of GRALE worth mentioning is that it requires significant computational resources: 
            <rs type="software">GRALE</rs> runs on a supercomputer.
        </p>
        <p>The GRALE team has carried out numerous tests of the code, to optimize the set of genetic algorithm and other code parameters. In the near future, GRALE will be extended to include fitness measure constraints from weak shear and flexion.</p>
        <p>The Coe model for Ares uses LensPerfect 9 (Coe et al. 2008(Coe et al. , 2010)). LensPerfect makes no assumptions about light tracing mass. The lens models perfectly reproduce the input observed positions of all strongly lensed multiple images. Redshifts may be either fixed to input spectroscopic redshifts or included in the model optimization based on input photometric redshifts and uncertainties.</p>
        <p>The image positions, redshifts and estimated source positions define the lensing deflection field sparsely at the multiple image positions. LensPerfect interpolates this vector field, obtaining a smooth model that exactly reproduces the image deflections at the input image positions. Based on this 2D deflection map, the mass distribution, magnification and all other model quantities may be derived.</p>
        <p>The curl-free vector interpolation scheme (Fuselier 2006;Fuselier 2008) uses direct matrix inversion to obtain a model composed of radial basis functions (RBFs) at the positions of the input vectors (our multiple image locations). Each 2D RBF has two free parameters -amplitude and rotation angle -which are determined uniquely by the matrix inversion (Coe et al. 2008; see their fig. 3 and equations 17, 20 and 21).</p>
        <p>After setting the width of the RBF, the free parameters are the source positions and any uncertain redshifts. LensPerfect performs an optimization routine searching for those parameters which yield the most 'physical' mass model according to a set of criteria described in Coe et al. (2008Coe et al. ( , 2010)). Briefly, these require that the mass should be positive everywhere and, less restrictively, prefer relatively smooth variations decreasing outward from the centre on average with rough azimuthal symmetry. Penalties for deviating from these ideals are calculated based on the mass map evaluated on an 81 × 81 pixel grid as detailed in Coe et al. (2008; their section 2.4 and appendix).</p>
        <p>In high-resolution HST ACS images, strongly lensed multiple image locations are observed and measured with accuracies of ∼1 pixel, or ∼0.05 arcsec. By fully utilizing this information, LensPerfect is able to obtain relatively high-resolution maps of galaxy cluster substructure without relying on any assumptions about light tracing mass. Large numbers of multiple images may be input, and the number of free parameters is always roughly equal to the number of constraints. The mass model spatial resolution increases with the density of multiple images on the sky. 9 http://www.stsci.edu/∼dcoe/LensPerfect/ Given current numbers of multiple images (up to ∼100 or so) for a single cluster (e.g. Coe et al. 2010), LensPerfect can accurately recover cluster mass profiles along with some larger subhaloes. Magnifications, however, are influenced by local mass density gradients, which are not accurately reproduced by LensPerfect given current constraints. Furthermore, LensPerfect mass models are only well constrained within the area enclosed by the multiple images and should generally be disregarded outside this region.</p>
        <p>LensPerfect is well suited to future data sets such as JWST imaging revealing still greater numbers of multiple images. Initial tests with hundreds to a thousand multiple images show great potential for resolving many individual cluster galaxy haloes without assuming light traces mass (LTM). The biggest hurdle (seen in tests with up to 10 000 multiple images) may be accounting for multiple lens plane deflections due to mass along the line of sight.</p>
        <p>One potential improvement would be to develop a hybrid method combining LTM assumptions with LensPerfect adding deviations to the mass distribution.</p>
        <p>LENSTOOL as an inversion algorithm deploys both strong and weak lensing data as input constraints. Below, we first briefly outline the available capabilities of the 
            <rs type="software">LENSTOOL</rs> software package and then describe the specific versions and assumptions that were used to reconstruct Ares and Hera by two groups: CATS (Cluster As Telescopes) and Johnson-Sharon. The CATS collaboration developed the LENSTOOL algorithm collectively over two decade. The code utilizes the positions, magnitudes, shapes, multiplicity and spectroscopic redshifts for the multiply imaged background galaxies to derive the detailed mass distribution of the cluster. The overall mass distribution in cluster lenses is modelled in LENSTOOL as a superposition of smoother large-scale potentials and smallscale substructure that is associated with the locations of bright, cluster member galaxies. Individual cluster galaxies are always described by parametric mass models, whereas the smoother, largescale mass distribution can be flexibly modelled non-parametrically or with specific profiles. This available multiscale approach is optimal, inasmuch as the input constraints required for this inversion exercise are derived from a range of scales. Further details of the methodology are outlined in Jullo &amp; Kneib (2009). In its current implementation in 
            <rs type="software">LENSTOOL</rs>, the optimization of the combined parametric and non-parametric model is computationally time intensive. And some degeneracies persist, despite the large number of stringent input constraints from the positions, shapes, brightnesses and measured spectroscopic redshifts of several families of multiple images. However, these degeneracies are well understood, in particular for specific parameters of models used to characterize the mass distribution. In order to tackle this challenge, an iterative strategy has been developed wherein initial models are derived with the best-fitting values solely from the parametric model, which are then optimized using the underlying multiscale grid. Both the multiscale and the parametric models are adjusted in a Bayesian way, i.e. their posterior probability density is probed with a Markov Chain Monte Carlo (MCMC) sampler. This process allows an easy and reliable estimate of the errors on derived quantities such as the amplification maps and the mass maps.
        </p>
        <p>The CATS and the Johnson-Sharon models are built using the 
            <rs type="software">LENSTOOL</rs> public modelling software (see e.g. Jullo et al. 2007). The public version of LENSTOOL deployed by Johnson-Sharon adopts the original modelling approach developed by Natarajan &amp; Kneib (1997) wherein a small-scale dark matter clump is associated with each bright cluster galaxy and a large-scale dark matter clump with prominent concentrations of cluster galaxies. This technique of associating mass and light has proven to be very reliable and results in mass distributions that are in very good agreement with theoretical predictions from high-resolution cosmological N-body simulations. The Johnson-Sharon models follow the methods described in Sharon et al. ( 2012) and Johnson et al. (2014).
        </p>
        <p>Typically, cluster lenses are represented by a few cluster-scale or group-scale haloes (representing the smooth component, with σ in the range of hundreds to ∼1500 km s -1 ), with contribution from galaxy-scale haloes (see below). Large-scale dark matter haloes are parametrized as pseudo-isothermal elliptical mass distribution (PIEMD),</p>
        <p>where ρ 0 is a normalization, and r core and r cut define a region r core r r cut in which the mass distribution is isothermal, i.e. ρ ∝ r -2 . In LENSTOOL, 
            <rs type="software">PIEMD</rs> has seven free parameters: x, y are the coordinates on which the halo is centred, e and θ are the ellipticity and the position angle, respectively, r core , r cut and effective velocity dispersion σ 0 that determines the normalization (we note that the σ 0 is not exactly the observed velocity dispersion, see Elíasdóttir et al. 2007). The parameters of cluster-scale haloes are kept free, with the exception of r cut which is usually unconstrained by the strong lensing data, and is thus fixed at an arbitrary value (typically 1500 kpc).
        </p>
        <p>CATS also model galaxies as 
            <rs type="software">PIEMD</rs>, whereas Johnson-Sharon model galaxies as circular isothermal distributions (see Section 3.6.5). To keep the number of free parameters reasonably small, the parameters of galaxy-scale haloes are determined from their photometric properties through scaling relations assuming a constant mass-to-light ratio for all galaxies,
        </p>
        <p>The positional parameters, x, y, e and θ, are fixed to their observed values as measured from the light distribution in the imaging data. CATS used the simulated strong lensing catalogues and the 
            <rs type="software">LENSTOOL</rs> software to perform a mass reconstruction of both simulated clusters, assuming a parametric model for the distribution of dark matter. The model is optimized with the Bayesian Markov chain Monte Carlo sampler, described in detail in Jullo et al. (2007). The mass distribution is optimized in the image plane by minimizing the distance between the observed and predicted multiple image positions. Weak lensing information is not taken into account. The image-plane root-mean-squared (rmsi) distance of the images predicted by the model were used to compare with the observed positions as an accuracy estimator of the model (Limousin et al. 2007).
        </p>
        <p>The CATS collaboration has modelled both clusters, Ares and Hera have been modelled as bimodal clusters with two smooth dark matter clumps and two BCGs lying in the centre of those main clumps. Each smooth component is modelled using a 
            <rs type="software">PIEMD</rs> profile. Cluster member galaxies are taken from the given simulated catalogues up to a magnitude of m F160W &lt; 22.0 for Ares and m F814W &lt; 24.0 for Hera. These are modelled with 
            <rs type="software">PIEMD</rs> profiles under the assumption that (i) their positions correspond to the observed ones, (ii) they have circular shape and (iii) their mass is proportional to the galaxy magnitudes in the F160W band. In the provided models, it is assumed that they all have the same M/L ratio, following the scaling relations in equation ( 12). Two free parameters are used to describe the ensemble of cluster galaxies, namely σ * 0 and r * cut . At the end of the optimization, the bestfitting values for these parameters are σ * 0 = 98 ± 1.2 km s -1 and r * cut = 262 ± 0.66 kpc for Ares and σ * 0 = 96.7 ± 3.3 km s -1 and r * cut = 33 ± 0.07 kpc for Hera. The reference luminosities are set to m * = 18.5 and m * = 19.82 for Ares and Hera, respectively. All multiple images provided were used in this model. In addition, a few (massive) cluster galaxies in both clusters were more carefully modelled in order to improve the rmsi of nearby multiple images. The RMSi is defined as
        </p>
        <p>where θ obs i and θ pred i are the positions of the multiple images observed and predicted by the model; N is the total number of images.</p>
        <p>In addition to the two BCGs, four other central cluster galaxies were modelled in this way in Hera (of which one is considered to be a foreground) and three, also central, galaxies in Ares.</p>
        <p>These reconstructions have a resulting rms in the image plane of 0.87 arcsec for Ares and 0.95 arcsec for Hera.</p>
        <p>The Johnson-Sharon models for Ares and Hera were constructed using techniques similar to those in Johnson et al. (2014), using the catalogues of multiple images that were provided to the lens modellers as positional constraints. The redshifts of the background sources were assumed to be known spectroscopically with no uncertainty or outliers. Both clusters were modelled with two 
            <rs type="software">PIEMD</rs> haloes, to represent the smooth dominant dark matter components, each centred close to the two peaks in the light distribution in the mock HST images with their exact positions set by the MCMC minimization process.
        </p>
        <p>Individual 
            <rs type="software">PIEMD</rs> haloes were assigned to each galaxy in the provided catalogue, with positional parameters, x, y, e and θ , fixed to their observed values as measured from the light distribution in the mock imaging data. The parameters that describe the slope of the projected mass density were scaled with the light in the F125W band assuming a constant M/L ratio for all the galaxies, following the scaling relations in equation ( 12). As both clusters are at z = 0.5, the same scaling relations were used for the cluster member galaxies: σ 0 = 120 km s -1 , r core = 0.15 kpc and r cut = 30 kpc, and m = 20.00, 19.87 for Ares and Hera, respectively.
        </p>
        <p>A few galaxies located near constraints were modelled independent of the scaling relations and their core radius and velocity dispersion were left as free parameters in the lens models. This includes the two bright cluster galaxies lying at the centres of the gravitational well of both clumps in the dark matter distribution in both clusters.</p>
        <p>We note that the PIEMD functional form of the cluster galaxies used in LENSTOOL differs from the function that was used in the simulation of Ares in the treatment of the truncation radius. While the PIEMD profile transitions smoothly from isothermal and asymptotes to zero at large radii, the simulated mass distribution truncates the mass function sharply to zero at r = r cut . This discrepancy is what causes the sharp circular residuals seen in Fig. 9. We thus do not expect the model to accurately reconstruct the mass distribution at radii larger than the truncation radius.</p>
        <p>In addition, the Johnson-Sharon model assumes that the ellipticity and position angle of the light of each mock galaxy follows the underlying mass distribution. In practice, all the galaxies in the underlying simulated mass distribution had circular geometry (i.e. no ellipticity) and the galaxies were painted on with arbitrary ellipticities and position angles. This feature of the blinded analysis contributes to residuals on small scales in the mass reconstruction. Finally, the Johnson-Sharon model does not use weak lensing information and does not include cluster-scale haloes outside of the main FOV if such haloes are not required by the strong lensing constraints alone.</p>
        <p>LENSTOOL strengths and weaknesses are typical of parametric models. This approach is useful in the sense that it directly compares physically motivated models to data, propagating errors in a fully consistent and Bayesian manner. It allows direct comparison with simulation outputs and the assessment of possible discrepancies. On the other hand, parametric models can significantly differ from reality and their lack of freedom introduces biases in the estimated masses, matter densities or errors. Regarding practical aspects, errors estimation implies running MCMC sampling, which can only be performed on supercomputer. LENSTOOL calculations can last for a couple of weeks on shared memory machines depending on the model complexity and the amount of multiple images. In the case of Hera and Ares, optimization lasts for about 10 h.</p>
        <p>CATS is currently working actively on two improvements that should significantly improve the accuracy of their mass reconstructions. First, LENSTOOL in its current revision does not permit radial variation of the ellipticity for the mass distribution, and this restricts the flexibility of models that can be generated. Current code development aims to include this additional degree of freedom in the modelling. Secondly, in order to maximally extract information from the exquisite image resolution afforded by the HST FFI, flexion measurements will be included as input constraints in the modelling. Finally, a new MCMC engine with MPI support and a GPU-based LENSTOOL are under development to decrease the computing time.</p>
        <p>The publicly available GLAFIC code (Oguri 2010) 10 is used for mass modelling in the GLAFIC models.</p>
        <p>GLAFIC adopts the so-called parametric lens modelling in which the lens mass distribution is assumed to consist of multiple components, each of which is characterized by a small number of parameters such as the centroid position, mass, ellipticity and position angle. Mass distributions of cluster member galaxies are modelled with 
            <rs type="software">PIEMD</rs> models. In order to reduce the number of parameters, the velocity dispersion σ and truncation radius r cut of each member galaxy are assumed to scale with the galaxy luminosity L as σ ∝ L 1/4 and r cut ∝ L η , and the normalizations of the scal-10 http://www.slac.stanford.edu/∼oguri/GLAFIC/ ing relations are treated as free parameters (see e.g. Oguri 2010). Ellipticities and position angles of individual member galaxies are fixed to values measured in the image. These parameters are optimized to reproduce positions of observed multiple images, either using the downhill simplex method or Markov Chain Monte Carlo. Examples of detailed cluster mass modelling with GLAFIC are found in Oguri (2010), Oguri et al. (2012), Oguri et al. (2013), Ishigaki et al. (2015) and Kawamata et al. (2016). GLAFIC can also simulate and fit lensed extended sources. This functionality has been used to, e.g. fit a lensed quasar host galaxy (Oguri et al. 2013), estimate a selection function of lensed high-redshift galaxies (Ishigaki et al. 2015) and derive sizes of lensed high-redshift galaxies (Kawamata et al. 2015).
        </p>
        <p>An advantage of GLAFIC is a wide range of lens potential implemented in the code, which enables flexible modelling of cluster mass distributions. For example, in addition to the standard external shear, one can add higher order perturbations with arbitrary multipole orders (see Oguri 2010). When necessary, in addition to observed multiple image positions, GLAFIC can also include flexible observational constraints such as time delays and flux ratios between multiple images, and (reduced) shear and magnification values at several sky positions measured by weak lensing and Type Ia supernovae, respectively.</p>
        <p>The source plane χ 2 minimization is often adopted for efficient model optimizations. In doing so, GLAFIC converts the distance between observed and model positions in the source plane to the corresponding distance in the image plane using the full magnification tensor. In appendix 2 of Oguri (2010), it has been shown that this source-plane χ 2 is accurate in the sense that it is very close to the image-plane χ 2 and therefore is sufficient for reliable mass modelling.</p>
        <p>Of course GLAFIC supports the image plane χ 2 minimization as well. Adaptive-meshing with increased resolution near critical curves is used for efficient computations of multiple images for a given source position. Multiple images and critical curves are computed for the best-fitting model from the source plane χ 2 minimization to check the robustness of the result.</p>
        <p>A known limitation of GLAFIC is that it can only handle single lens planes. Lens systems for which multiple deflections at different redshifts play a crucial role are difficult to be modelled by GLAFIC.</p>
        <p>Each halo component is modelled by the elliptical NFW profile. For Ares, five halo components are included, in addition to the member galaxies modelled by 
            <rs type="software">PIEMDs</rs> (see above). In addition, the η parameter (see above) is fixed to 0.5. For Hera, two NFW halo components are placed around two brightest galaxies. These two brightest galaxies are modelled by the Hernquist profile, separately from the other member galaxies. The formula for this profile is given by
        </p>
        <p>where the scale radius r b is related to the effective radius R e by r b = 0.551R e . We introduce the ellipticity in the projected mass density. Ellipticities and position angles of the brightest galaxies are fixed to observed values. To achieve better fit, external shear and third-order multipole perturbation are added for Hera. In modelling member galaxies, η in the scaling relation of truncation radii is fixed to 0.5 for Ares and is treated as a free parameter for Hera. Simulated F814W band images are used to measure luminosities, ellipticities and position angles of member galaxies with SEXTRACTOR for both Ares and Hera. Overall, a more elaborated lens model is adopted for Hera compared to Ares, because in the initial exploration period of mass modelling it was found that the lens potential of Hera appears to be much more complex. In particular, both external shear and high-order perturbations have been included in the model. The latter are simulated using a potential of the form</p>
        <p>where m = 3, n = 2 and pert and θ pert are left as free parameters in the fit. The best-fitting values for the external shear amplitude and orientation are γ ext = 0.072 and θ ext = 32.4 deg, respectively. For the third-order multipole perturbation, we found pert = 0.018 and θ pert = 25.9 deg. The resulting best-fitting models reproduce image positions very well, with rms of ∼0.27 arcsec for Ares and ∼0.43 arcsec for Hera.</p>
        <p>The Zitrin LTM method (Broadhurst et al. 2005;Zitrin et al. 2009) was designed primarily to be a very simple, straightforward modelling method with a minimal number of free parameters, relying only on the observable light distribution of cluster members (namely their positions and relative fluxes) to supply a well-guessed and highly predictive solution to the mass distribution of the lens and the location of multiple image systems (e.g. Zitrin et al. 2012Zitrin et al. , 2013)). Previous to the design of this method, it had been shown that (a) cluster galaxies must be included, typically with a mass in proportion to their luminosity, in order the solution to have predictive power to find multiple images and that (b) a dark matter component should be added (see Kneib et al. 2004;Broadhurst et al. 2005). This simple parametrization, as we detailed further below, has allowed us to identify systems of multiple images in an unprecedented number of clusters, where the images are physically matched also by the initially guessed model (which is then refined), and are not only matched by eye based on their colour information as is often accustomed.</p>
        <p>As mentioned above, this method was designed to include both a galaxy component and a dark matter component, yet to successfully do so with a minimal number of free parameters. To form the galaxy component, cluster galaxies (found following the red sequence in a colour-magnitude diagram) are assigned each with a power-law mass density distribution, where the normalization of each galaxy's weight is proportional to its (relative) flux, and the exponent is the same for all galaxies and is the first free parameter of this method. The superposition of all galaxy power-law mass distribution then constitutes the lumpy, galaxy component of the model. To describe the dark matter distribution, the galaxy component is smoothed with either a Spline interpolation or usually a Gaussian kernel whose degree or width is the second free parameter of this method. The smoothing yields a diffuse, smooth dark matter component that depends on the initial light distribution; therefore, the method is dubbed LTM as both the galaxy and dark matter components roughly follow simply the light distribution. Next, the two components are added with a relative weight (typically around few to a couple dozen percents for the galaxies), which is another free parameter in the modelling. The fourth parameter is an overall normalization of the lens model to a certain redshift or multipleimage system. In addition to the four parameters, we often introduce several other parameters that add some flexibility and help in refining the final solution given the set of input multiple images. These include a core and two-parameter ellipticity for the BCG(s), two parameter external shear (which mimics ellipticity for the critical curves) and chosen galaxies whose weights (or fluxes) are left free to be optimized in the modelling, meaning that they are allowed to deviate from the adopted mass-to-light relation. The minimization for the best-fitting solution and related errors, given a set of multiple images (often found with the aid of the initially guessed map from this method), is performed with a χ 2 criterion comparing the positions of multiple images with the predicted ones, in the image plane, via a few-dozen thousand MCMC steps with Metropolis-Hastings algorithm. The conversion between mass distribution and deflection angles is done in Fourier space, using FFT techniques.</p>
        <p>The resulting lens model from this procedure, as its name suggests, is strongly coupled to the input light distribution of the lens (cluster members positions and luminosities). This entails various strengths and weaknesses. The fact that the solution is coupled to the light distribution is what grants this method with the unprecedented prediction power to delineate the critical curves and locate multiple images in advance, even if no multiple-image system is used as constraint (see Zitrin et al. 2012). In fact, most of the free parameters in the initial solution are relatively well known, so that as a first step (i.e. to find multiple images) we can reduced these to one free parameter -namely the normalization of the lens, and obtain a well-guessed solution, that we have shown is not much different than the resulting solution for the same clusters when using many multiple images as constraints (Zitrin et al. 2012). This means that the method is capable to supply a well-guessed solution also in cases where HST high-quality data is lacking.</p>
        <p>The simplistic nature of this method also means that the solution is often faster to converge and compute than other grid-based methods or parametrizations, allowing the analysis of many dozens of cluster lenses in a relatively short time.</p>
        <p>Another advantage that this method encompasses is that the same very simple procedure applies to all clusters -from relaxed, small clusters and groups (such as the relatively smaller cluster lenses A383, MS2137 or A611, see Zitrin et al. 2015, for recent modelling), to the most complex merging clusters such as M0416, M1149 or M0717 (Zitrin et al. 2015), that often require multihalo fits in other parametric methods.</p>
        <p>But the coupling to the light distribution also means that the spatial flexibility of the model is small. While our parametrization does allow for a flexible mass profile in the sense that it is not limited to a certain analytic form, the solution is limited spatially by the light distribution. This means that the multiple-image reproduction accuracy is often smaller than in other more flexible parametric methods that model the dark matter independently of the light (such as other well-known methods listed in this work including our own second method listed below). This is manifested usually in clusters that have a large number of multiple images spread across the field; for these, the LTM method often reaches a finite rms value of ∼1-2 arcsec on which it cannot improve further.</p>
        <p>A second disadvantage stemming from our parametrizationsince we do not model the dark matter independently of the light and since the critical curve's ellipticity in our modelling is for the most part generated by the external shear -is that there is no ellipticity assigned directly to the mass distribution. This creates some discrepancy between the lens and mass models: the mass distribution can be often significantly rounder than implied by the critical curves, whose ellipticity comes from the external shear that does not contribute ellipticity to the convergence map. In simple words, this reveals a degeneracy regarding the true ellipticity of the mass distribution -as the ellipticity of the lens can be attributed to intrinsic ellipticity or to external shear.</p>
        <p>To summarize, we thus consider this method very reliable and robust, supplying especially well-guessed initial maps for any given cluster regardless of its complexity, and with unprecedented prediction power to find multiple images, but it can also be in some cases less accurate and spatially flexible. Also, given this is a light-tracing method, we do not expect this method to describe well numerical simulations whose mass-to-light relations are not completely representative.</p>
        <p>The code is currently being optimized in order to speed up the minimization procedure. We are also testing whether replacing the galaxy component with the more well-behaved 
            <rs type="software">PIEMD</rs> (see below), despite having a fixed isothermal slope, would be sufficient for our purposes. We have also implemented an option of smoothing with an elliptical Gaussian which then introduces ellipticity into the matter distribution itself.
        </p>
        <p>Note that our calculations are performed on an input grid matching an actual image of the field, with its native pixel scale. To speed the minimization procedure, we often reduce the resolution (especially in the case of HST that has high spatial resolution) by factors 4 to 10 on each axis. This contributes to the finite, non-negligible rms obtained often in this method (e.g. due to pixel coordinates round-ups etc.). We intend to investigate this further and try to improve the resolution in the crucial places, such as near the critical curves and when delensing to the source plane, where this lower resolution might prevent a further improved solution.</p>
        <p>To model Ares and Hera, we use the following setting in our LTM pipeline. We create a grid of 4080×4080 pixels covering the FOV, with an angular resolution of 0.5 arcsec pixel -1 . The calculation in practice is performed in two stages -first, we run many individual random MC chains with a grid resolution lower by factor 10 on each axis. From this we find the global minimum area and extract the covariance matrix. A proper, long MCMC is then run with a grid of four times lower resolution than the original input image. The final solution is then interpolated to match the original pixelscale map. Errors were derived using 50 random models form the MC chain, with a positional uncertainty of 1.4 arcsec for the χ 2 term. We use the input list of galaxies supplied by the simulators scaled by their light. In Ares, we allow five galaxies to deviate from the nominal mass-to-light ratio and be freely weighted by the MC chain, and for two of them -especially important where radial images are seen in the data -we allow for a free core radius as well. In the case of Hera, only three galaxies were modelled in this way. The ellipticity (and direction) of these bright galaxies are also left as free parameters. As constraints, we use the full list of multiple images. No weak lensing constraints were used. The final rms of the model is 1.8 and 1.2 arcsec for Ares and Hera, respectively, which, as we mention above, is in part limited by the finite lower resolution of the grid we work on. The best-fitting external shear amplitude and orientation are γ ext = 0.007, θ ext = 6.5 deg for Ares and γ ext = 0.114, θ ext = 72.1 deg for Hera.</p>
        <p>3.9 PIEMDeNFW: the Zitrin-NFW models Zitrin et al. (2013) expanded their pipeline to also allow for a fully parametric solution. This method in essence is similar to the other parametric techniques mentioned here such as LENSTOOL and GLAFIC. The main motivation for adding this parametric pipeline was to (a) allow for further flexibility and improved fits by having a semiindependent solution in which the dark matter is modelled independently of the light, and (b) test for the magnitude of systematic differences between these methods (Zitrin et al. 2015).</p>
        <p>As is usually accustomed in parametric modelling, in order to describe well the multiple-image positions with enough prediction power, this method also relies on a combination of galaxy and dark matter. The red sequence cluster galaxies are modelled each as PIEMDs based on the prescription and scaling relations used in LENSTOOL, and typically with a fixed mass-to-light ratio. Usually two or three parameters are left free to describe the galaxy component: the velocity dispersion, core radius and truncation radius, of an M* galaxy, which is used as reference for the scaling relations. The dark matter component is modelled also with an analytic, fully parametric recipe. We can choose either an elliptical Navarro et al. (1996) profile (eNFW), or, also 
            <rs type="software">PIEMD</rs> for the cluster's dark matter halo. Therefore, in this method, the dark matter is modelled with a symmetric analytic form, independent from the light distribution. Similar to our LTM method, the same minimization engine is used here: a long MCMC with a χ 2 image-plane criteria. Also here we can add other parameters to be optimized in the minimization, such as the ellipticities of the BCGs, their mass can be allowed to deviate from the adopted scaling relation, and so forth.
        </p>
        <p>Compared to our LTM technique, for example, we have found that the fully parametric technique is more spatially flexible and can thus often supply a more accurate solution with a (somewhat) smaller image-plane rms. On the other hand the method is less efficient at finding new sets of multiple images (especially before the model is initially constrained).</p>
        <p>In a similar sense, another main disadvantage of such parametrizations is the need to add dark matter haloes to model subhaloes for complex structures (such as merging clusters), without knowing if these are fundamental parameters, e.g. accounting truly for additional dark matter haloes, or just nuisance parameters that help add flexibility and refine the fit. Additionally, each such added halo adds several (usually four to six) free parameters to the minimization procedure rendering it significantly more cumbersome.</p>
        <p>Note that since we developed this method with the same infrastructure used for our LTM method, and in part, for comparison with it, the solutions given by the PIEMDeNFW method, despite being analytic in nature, are also calculated on a grid, the size of the input *.fits image, similar to our LTM procedure. This results in a somewhat slower procedure compared, for example, to our LTM technique, and also here to achieve higher converging speed we lower the grid resolution by factors of a few on each axis. Again this leads to a finite rms due to e.g. numerical round-ups in highmagnification regions.</p>
        <p>The main improvement we wish to implement is to speed up the procedure. This for start can be achieved if part of the calculation is done completely analytically/numerically (say, only around the positions of multiple images) rather than on a full-frame grid. We intend to explore such possibilities. Also, recently we added the possibility for an external shear to allow for further flexibility.</p>
        <p>To model Ares and Hera, we use the following setting in our 
            <rs type="software">PIEM-DeNFW pipeline</rs>. As done with the LTM-gauss method, we create a grid of 4080×4080 pixels covering the cluster. We start by running many individual random MC chains with a grid resolution lower by factor 20 on each axis. From this we find the global minimum area and extract the covariance matrix. A proper, long MCMC is then run with a grid of four times lower resolution than the original input image. The final solution is then interpolated to match the original pixel scale map. Errors were derived using 50 random models to form the MC chain, with a positional uncertainty of 1.4 arcsec for the χ 2 term. We use the input list of galaxies, scaled by their light. The brightest galaxies are optimized individually as done with the 
            <rs type="software">LTM-gauss</rs> pipeline. In this case, however, the ellipticity (and direction) of the four brightest galaxies in both clusters are also left as free parameters. In Ares, two cluster-scale DM haloes in the form of elliptical NFW mass densities are introduced, with fixed centring on the respective BCGs. In Hera, we used three such large haloes. As constraints we use the full list of multiple images. No weak lensing constraints were used. The final rms of the model is 1.8 arcsec, which as we mentioned above is in part limited by the finite lower resolution of the grid we work on.
        </p>
        <p>In this section, we describe how the different methods perform at recovering several properties of the lenses.</p>
        <p>The reconstructed convergence maps of Ares and Hera are shown for all models in Figs 7 and 8, respectively. The maps are all normalized to z S = 9. In both figures, the maps derived from the free-form algorithms are shown first (beginning from the upper-left panel). The last panel in each figure shows the true convergence map, for easy comparison. All maps cover the same FOV. This does not correspond to the size of the simulated images that were made available to the modellers. Indeed, for several technical reasons inherent to each methodology employed, the submissions by the different groups were different in size. To carry out a proper comparison between the models, we restrict our analysis to the area around each of the two lenses, which is covered by all the reconstructions. More precisely, we used as footprints for identifying the area of analysis the submissions by the GLAFIC and by the GRALE teams for Ares and Hera, respectively. In the first case, the FOV is ∼180 arcsec × 180 arcsec. In the second, the reconstructed area is ∼110 arcsec × 110 arcsec wide.</p>
        <p>Since Ares was constructed parametrically with light-tracing mass, it is particularly well suited for reconstruction by parametric techniques. The parametric CATS, GLAFIC, Johnson-Sharon, and Zitrin models and the hybrid Diego model all include mass substructure at the observed positions of cluster galaxies, recovering the Ares mass distribution with high fidelity. The free-form GRALE, Bradac-Hoag and Coe models do not assume LTM, reconstructing the mass distribution solely based on the observed lensing. They recover the main mass peaks, but smaller substructures are not constrained by the lensing data. The GRALE model accurately reproduces the cluster bimodality. The Bradac-Hoag and Coe models are less smooth, including noisy smaller substructure, especially outside the region constrained by strongly lensed multiple images.</p>
        <p>The Hera cluster, obtained from an N-body simulation, is less ideal for being reconstructed using parametric methods. Indeed, the performance of the parametric algorithms appears more consistent with that of the free-form ones. Hera is constructed assuming light traces its massive substructure, as assumed by the parametric and hybrid methods. The Bradac-Hoag and GRALE models do not make that assumption and thus recover fewer small subhaloes.</p>
        <p>The major differences between the models and the true convergence maps are found near substructures, but also the shape of the mass distributions, especially at large distances from the centre, show inconsistencies. We will discuss them in more detail in the next sections.</p>
        <p>To better highlight the differences between the maps, we show the relative differences (κκ true )/κ true between the reconstructed and the true convergence maps for Ares and Hera in Figs 9 and 10, respectively.</p>
        <p>We begin discussing the results on the mass and convergence (or surface density) profiles. Meneghetti et al. (2010a) already showed using only one of the methods employed in this paper (LENSTOOL, employed by both the CATS and the Johnson-Sharon teams) that strong lensing can potentially measure the mass inside the Einstein radius with an accuracy of the order of a few per cent. In the cases of Ares and Hera, the sizes of the Einstein radii are significantly different. In Fig. 11, we show how θ E grows as a function of the source redshift z s . The Einstein radius of Ares is ∼20 arcsec at z s = 1. Its size at z s ∼ 2 is more than double and it grows asymptotically to ∼55 arcsec at higher redshift. The reason of the steep rise between z s = 1 and z s = 2 is that Ares has a bimodal mass distribution. For sources at low redshift (z s ∼ 1), each of the two mass clumps has its own critical lines. These are shown by the red curves in the upper panel of Fig. 3. To draw the plot in Fig. 11, we use the centre of the most massive mass clump as reference, and only the critical line enclosing this point is used to measure θ E . By increasing the source redshift, the critical lines around the two mass clumps merge into a single, very extended critical line (see the white lines in the upper panel of Fig. 3, which shows the critical lines for sources at z s = 9).</p>
        <p>In the case of Hera, the Einstein radius grows from ∼12 arcsec at z s = 1 to ∼30 arcsec at z s = 9. The critical lines for these two source redshifts are shown in the upper panel of Fig. 4.</p>
        <p>In Fig. 11, we also show the redshift distributions of the multiple images identified in the background of the two clusters (red and blue histograms). These multiple images are marked with numbered circles in the upper panels of Figs 3 and4. The labels of each image are constructed as X.Y, where X is the ID of the source and Y is the ID of the multiple images belonging to the same system. Being such a powerful lens, Ares produces many more multiple images than Hera, some of which originate from galaxies at redshift z s ∼ 6.</p>
        <p>The most distant multiple image system in the field of Hera is only at z s ∼ 3.5. In both cases, however, the redshift distribution of the multiple images overlaps with the redshift range where the size of the Einstein radii has the strongest growth. Indeed, the relative variation of θ E between z s = 3 and z s = 9 is only 10 per cent. Thus, we expect that the models constructed using these constraints can be safely used to trace the growth of the cluster strong lensing region up to very high redshifts. Analogously, we expect that the mass profiles are recovered with higher precision in the radial ranges 20 θ 60 arcsec and 10 θ 30 arcsec for Ares and Hera, respectively. This is consistent with our findings. The upper panels of Figs 12 and 13 show the projected enclosed mass profiles of Ares and Hera, respectively. The bottom panels show the projected mass density profiles in units of convergence κ for z S = 9. The profiles are computed with respect to the centre of the most massive subclump in each cluster field. To facilitate the comparison between parametric and free-form methods, we show the results for these two classes of models separately (left-hand and right-hand panels).</p>
        <p>The mass distribution of Ares is generated in a very similar manner as used by the parametric techniques (except Zitrin-LTM) to model the lenses -as a combination of parametrized mass components, including subhaloes at the positions of cluster galaxies. Therefore, it is not surprising that these methods recover the true mass profile of Ares with very good accuracy. For example, the CATS, Johnson-Sharon and GLAFIC profiles differ from the true mass profile by ± 2 per cent. Larger differences are found for the Zitrin-LTM-gauss and the Zitrin-NFW approaches (perhaps because these are calculated on a lower resolution grid, see discussion in Sections 3.8 and 3.9), but even for these models, in the region probed by strong lensing, the deviations from the true mass profiles are within ∼±10 per cent.</p>
        <p>It is noteworthy that neither the CATS nor the Johnson-Sharon teams used the NFW density profile to model the smooth DM haloes of the two main mass components of Ares. On the contrary, they used cored isothermal profiles, which can of course be tweaked to match the lensing properties of NFW haloes. This is consis-tent with the findings of Shu et al. (2008), who showed that, in several cases, strong lensing clusters are equally well modelled with cored-isothermal and NFW density profiles. The additional constraints provided by complementary analysis, such as stellarkinematics in the BCG, could help break this degeneracy (Newman et al. 2013). Moreover, the adoption of an isothermal profile with core instead of the NFW profile does not prevent several models from recovering the correct slope of the surface mass density (i.e. convergence) profile over a relatively broad range of distances from the cluster centre. The constraints available to carry out the reconstructions include both radial and tangential features, with the former particularly sensitive to the slope of the projected density profile.</p>
        <p>Among the free-form methods, the reconstructed profiles generally deviate by 5-15 per cent from the true mass and convergence profiles. Some models (e.g. GRALE) have a very similar performance to parametric methods. The best agreement between the true profile and the models is found between 20 and 60 arcsec from the lens centre, which nicely corresponds to the size of the Einstein radius, as shown in Fig. 11. Hera is a less idealized test case for most of the parametric methods, but it still assumes that light traces the mass substructure. So also for this lens, the parametric models reproduce the input mass profiles more closely than the free-form methods, though the differences between the two approaches are now reduced. We find that the mass profiles obtained with the parametric methods differ from the input mass profile by less than 10 per cent within ∼80 arcsec from the assumed centre. The same level of accuracy is reached by the free-form methods within 10 r 30 arcsec. This radial range corresponds to the size of the region probed by strong lensing. Both parametric and free-form methods clearly converge to the true mass profiles within this range of distances, where the relative differences are of the order of few per cent.</p>
        <p>Having quantified the performance of the methods to reconstruct 1D mass profiles, we discuss now their ability to recover the 2D mass distributions of the lenses.</p>
        <p>To be more quantitative about how well the methods employed recover the true shape and orientation of the two clusters, we consider the projected mass distributions of the lenses in terms of their iso-surface-density (or convergence, κ) contours. We use the following procedure:</p>
        <p>(i) From the convergence maps, we extract the contours corresponding to κ-levels in the range 0.5-3.0. Since both Ares and Hera have bimodal mass distributions, we use the centre of the largest mass clump as the reference centre for this analysis and we consider only the contours enclosing it.</p>
        <p>(ii) We fit an ellipse to each contour and measure its ellipticity and position angle. We also measure the size of each contour by means of an equivalent radius r κ , defined as</p>
        <p>where a and b are the semi-axes of the best-fitting ellipse.</p>
        <p>(iii) Finally, we draw the radial profiles of both the ellipticity and the position angle. The radius used to produce the profiles is the equivalent radius of the iso-density contours.</p>
        <p>The procedure outlined above is shown in Fig. 14 for the cluster Ares.</p>
        <p>The radial profiles of the ellipticity and of the position angle for the two clusters are shown in Figs 15 and 16. As done in Figs 12 and 13, the results for parametric and free-form methods are displayed separately (left-hand and right-hand panels, respectively).</p>
        <p>In each panel, the true profile is given by the black dashed line. The two clusters investigated in this work exhibit quite different ellipticity profiles. Indeed, due to the larger spatial separation between the two mass clumps, Ares has a less elongated inner core (e = 1b/a ∼ 0.3) compared to Hera (e ∼ 0.7). Ares's ellipticity increases with radius, while Hera shows the opposite trend.</p>
        <p>Despite the fact that we have introduced some modest radial variation of the ellipticity of two main mass clumps in Ares, the largest jumps in the ellipticity profile of this cluster are produced by massive substructures. These variations of ellipticity are generally well reproduced in the parametric reconstructions, and, to some extent, also in the free-form model of GRALE. Clearly, the parametric techniques produce better measurements of the core shapes, both in the cases of Ares and Hera. Indeed, due to resolution limits, the convergence maps produced by the free-form methods are noisier, resulting in more irregular iso-density contours. Under these circumstances, the ellipticity measurements are more uncertain.</p>
        <p>Among the parametric reconstructions of Ares, the largest deviations from the true ellipticity profile are found for the Zitrin-NFW and for the Zitrin-LTM-gauss models within ∼40 and ∼20 arcsec, respectively. Interestingly, these same algorithms provide some of the most accurate measurements of the core shape in the case of Hera. These algorithms generally find higher halo ellipticities compared to the other parametric methods. Such behaviour is consistent with the results of Zitrin et al. (2015), where the Zitrin-NFW and Zitrin-LTM-gauss methods are both employed in the reconstruction of the galaxy clusters in the CLASH sample. As shown in their fig. 3, the first of these two methods leads to more elliptical mass distributions. The most likely interpretation of this behaviour is that external shear compensates the smaller ellipticity of the LTM models.</p>
        <p>All parametric methods except the Zitrin-LTM-gauss tend to overestimate the ellipticity of the mass distribution at large radii in the case of Hera. We shall recall that all these algorithms fit the data by combining multiple mass components, each of which has a fixed ellipticity. The results show that, within the region probed by strong lensing ( 40 arcsec for Hera), the combination of multiple mass clumps is effective in reproducing the overall ellipticity of the cluster. At larger radii, though, the models are unconstrained and the ellipticity is extrapolated from the inner region. Free-form methods do not show the same trend; their ellipticity profiles are more noisy.</p>
        <p>Also the orientation angles of the iso-density contours in the parametric reconstructions deviate from Hera's true orientations at large radii. Being a numerically simulated cluster, Hera is characterized by asymmetries and twists of the iso-density contours that result to be much stronger than in Ares. For example, the position angle of the iso-density contours changes by ∼20 deg between the very inner region of the cluster and a distance of ∼50 arcsec.</p>
        <p>As a result of the not perfectly reproduced shape and orientation of the cluster at large radii, the CATS, Johnson-Sharon, GLAFIC and Zitrin-NFW models have an excess of mass along the major axis of the cluster with respect to the true mass distribution of Hera (and consequently they lack mass in the perpendicular direction). Such peculiarities can be seen in Fig. 10, where the ratios between reconstructed and true convergence maps of Hera are shown.</p>
        <p>Figs 9 and 10 show that significant differences exist between the models near substructures. Measuring the mass of substructures is an important task that several authors have performed via strong lensing (see e.g. Natarajan, De Lucia &amp; Springel 2007;Natarajan et al. 2009;Grillo et al. 2015, and references therein). Therefore, it is interesting to quantify the lens model precision near these secondary mass clumps.</p>
        <p>From the perspective of strong lensing, substructures are often identified as massive haloes around cluster galaxies. This is particularly true for parametric methods: they use the luminous galaxies as tracers of the underlying mass distribution. Instead, free-form methods can in principle detect any kind of mass substructure, even if not traced by light. However, they cannot distinguish between the projected mass belonging to the cluster halo and bound to the substructures.</p>
        <p>Indeed, as part of their submissions, the groups did not provide estimates of the masses in substructures, nor substructure catalogues. Here, we perform the following analysis:</p>
        <p>(i) We start from the assumption that galaxies trace the substructures. This is not a strong assumption given the method employed to generate the galaxy populations of Ares and Hera. In both cases, galaxies tend indeed to coincide with dark matter substructures. In the case of Ares, there is a one-to-one correspondence between luminous galaxies and dark matter subhaloes. In the case of Hera, we have excluded from the image simulations those galaxies that had their dark matter haloes stripped off in the course of the cluster evolution.</p>
        <p>(ii) We create apertures centred on the cluster galaxies with m AB, F814W &lt; 24, with radii equal to twice the effective radius of the galaxies, and we measure the projected mass within each aperture from both the reconstructed and the true convergence maps.</p>
        <p>(iii) In the following, we will refer to these masses as substructure masses, keeping in mind that these are, however, the sum of the substructure mass and of the projected mass of the underlying cluster dark matter halo.</p>
        <p>In Figs 17 and 18, we show the distributions of the ratios between measured and true substructure masses. The two figures refer to Ares and Hera, respectively, and show the results for all the models. We characterize the distributions of the ratios r by means of their median r and of their 25th and 75th percentiles, p 25 and p 75 . The analysis is carried out on the same areas covered by the maps in Figs 7 and8. Therefore, the same number of substructures have been used to build the histograms (282 and 278 for Ares and Hera, respectively).</p>
        <p>The results found for Ares show that several methods recover nearly unbiased substructure masses with good accuracy. For example, the interpercentile range found for the CATS model is only 0.21 and the median is r = 1. Similar results are found for the Zitrin-LTM-gauss model, although with a median slightly larger than unity. Some parametric models, such as those of Johnson-Sharon and Zitrin-NFW and marginally GLAFIC, have skewed distributions with tails extending towards ratios larger than unity. Interestingly, Johnson-Sharon's model is based on the same modelling software employed by the CATS group.</p>
        <p>Among the free-form models, the distributions are generally broader than for the parametric methods. The distribution for the Bradac-Hoag model has median r = 1 and interpercentile range 0.32. Similar or slightly larger scatter is found for the GRALE and Coe models. The ratio distribution obtained for the Diego-reggrid model has a tail extending towards small values and its median is r = 0.8.</p>
        <p>The results found for Hera are quite in agreement with those found for Ares. Parametric methods perform very similarly among each other, providing mass measurements accurate at the level of few per cent. The Zitrin-LTM-gauss model has a median r = 0.89. The dispersions of the ratio distributions, as quantified by the interpercentile ranges, are ∼0.2-0.25. This is quite remarkable given the very different methods used to populate Ares and Hera with substructures and the significant differences between the density profiles of the substructures themselves in the two simulations, as shown in Fig. 1. This seems to indicate that the methods are flexible enough to account for even large variations in the substructure properties, provided they are traced by light. It is less surprising that the flexible free-form methods also behave so similarly in Hera and Ares.</p>
        <p>As one of the major goals of the Hubble Frontier Fields is to use the lensing power of galaxy clusters to detect and characterize very high redshift galaxies, we focus now on the magnification. Of course, the results shown in this section are not independent of those discussed The largest discrepancies between reconstructed and true magnifications appear around the lens critical lines. These are the loci where the magnification formally diverges. Therefore, even a small misalignment of the true and reconstructed critical lines will result in potentially large magnification differences. Most of the models recover the shape and the size of the critical lines well. Others, as the Bradac-Hoag, the Coe and the Diego-multires models are characterized by critical lines with very irregular shapes.</p>
        <p>In Figs 23 and 24, the measured magnifications are plotted as a function of the true magnifications. As anticipated, the scatter around the median increases as a function of the true magnification for all models. The scatters for parametrically reconstructed models of Hera are factors of 2-3 larger than for the corresponding models of Ares, the mock cluster that was generated parametrically. Besides, we note that Hera was inherently less well constrained as the cluster had fewer multiple images than Ares. So a slightly lower fidelity in the reconstruction was anticipated and found as expected.</p>
        <p>In the best scenario obtained for Hera (i.e. the GLAFIC model, see also Fig. 25), we find very high accuracy (a few per cent bias at most) and precision: ∼10 per cent uncertainty for μ = 3, growing to ∼30 per cent at μ = 10, and increasing further at higher magnifications.</p>
        <p>In other cases, median magnifications are biased low or high by as much as ∼40-50 per cent. Some of these biases are due to the models' inability to reproduce the correct magnification patterns interior to the tangential critical lines. In other cases, the gradient of the magnification around the critical lines is significantly different from that in the true magnification maps, reflecting the incorrect shape and orientation of the projected mass distribution or the incorrect slope of the convergence profile.</p>
        <p>Regions around substructures sometimes are characterized by large uncertainties on magnification estimates. For example, the large substructure located south of the cluster Hera is not well constrained by any of the models, which all systematically underestimate the magnification around it. As shown in the upper panel of Fig. 4, there are no multiple images located near this substructure, which may explain why no model is able to constrain its mass properly.</p>
        <p>In Fig. 25, we plot the precision and the accuracy of the magnification measurements attained by each model as a function of the model magnification. The results are shown for Hera only, and they do not account for the location of the lensing constraints. Fig. 26 shows that the precision achieved by the model at the location of the constraints is higher than in other regions with similar magnifications. The distributions of the relative differences between model and true magnifications at the location of the 65 multiple images in the cluster Hera are shown using green boxplots for all the lens models. The median true magnification at the location of the lensing constraints is μ(z s = 9) ∼ 6. We sample each model magnification maps at other 65 positions, making sure that the selected points have magnification distributions identical to those of the multiple images. The distributions of the relative differences between model and true magnifications at these locations are shown by the blue boxplots. As said above the precision of the models is usually better near the lensing constraints, although the accuracy does not generally improve.</p>
        <p>In order to be more quantitative in estimating the ability of the different methods employed in this work to measure several relevant properties of Ares and Hera, we have defined metrics for the lens properties discussed above. More precisely, we introduce metrics for the 1D radial profiles of the following:</p>
        <p>(i) the 2D projected mass enclosed within radius R, (ii) the surface mass density, or convergence κ(R), (iii) the ellipticity, as fit to iso-density contours, (iv) and the orientation, as given by the position angle of the convergence contours.</p>
        <p>We also define metrics to quantify the goodness of the reconstruction of the 2D convergence and magnification maps. Finally, we define a metric for the projected subhalo masses in apertures centred on the cluster galaxies.</p>
        <p>Thus, we have seven metrics that can be used for a more quantitative comparison between the lens models of both clusters. We can also evaluate how the performance of each algorithm changes when switching from a simulation based on a lens obtained from SAMs (Ares) and one obtained from a fully numerical simulation (Hera).</p>
        <p>The metrics are defined as follows. Given a set of measured values v and a set of true values v true , we derive the distribution of v/v true . Then, we compute the median, ζ and the 25th and 75th percentiles of the distribution, p 25 and p 75 . The metric is finally defined as By adopting this definition, the metric penalizes those reconstructions that are biased and/or affected by a large scatter. Of course, the metrics are not fully independent. For example, a model that is able to reproduce the convergence profile of the lens with a good accuracy will also provide a robust measurement of the mass profile. Similarly, models whose reconstructed convergence maps show little deviation from the true convergence maps will also provide a good match with the simulation in terms of converge profile or shape (ellipticity and position angle). Nevertheless, the ranking among the models with respect to correlated lens properties is not always the same. For example, the Johnson-Sharon reconstruction of Hera ranks second in terms of convergence profiles and fourth in terms of mass profiles. In addition, the different lens properties that are discussed here are often used individually, and it may be interesting for the reader to establish which modelling technique is better suited to their scientific purposes.</p>
        <p>In Fig. 27, we show radar plots that summarize the metric values recorded by each reconstruction. The overall performance of each model corresponds to the area of each polygon. When one model is good at measuring some of the lens properties, but less effective at capturing others, the polygon appears elongated towards one or more of the chart vertexes.</p>
        <p>The first eight charts correspond to free-form or hybrid methods. The remaining five charts refer to parametric techniques. As we have pointed out several times earlier, there is larger discrepancy between the performances of parametric and non-parametric methods in the case of Ares than in the case of Hera. This leads us to the conclusion that, despite our attempts to make the Ares mass distribution less ideal for parametric methods (e.g. by simulating adiabatic contraction or by introducing some twist of the iso-density contours, including some radial dependence), the simple fact that this cluster is assembled by combining mass components traced by the cluster galaxies, consistently with the basic assumptions of most parametric techniques, gives a huge advantage to these methods. The good news, in this case, is the following. First, these algorithms work as they are supposed to. Secondly, they provide very accurate reconstructions even if the parametrization chosen for the lens halo density profile is not fully consistent with the true profile of the lens. For example, none of the parametric techniques, except the Zitrin-NFW method, used the NFW profile for fitting the smooth dark matter halo components of Ares. Even so, models such as those submitted by the CATS, Johnson-Sharon and GLAFIC teams pro-duce an overall better fit to the input mass distribution compared to the Zitrin-NFW reconstruction. This suggests that pseudo-elliptical, cored halo models provide the right flexibility to account for most of the effects we have introduced in the simulation, such as the adiabatic contraction, which steepens the density profile in the central region of the cluster. Alternatively, these results may be interpreted as evidence for a lack of sensitivity of lensing alone to the precise share of the halo density profiles, being mostly sensitive to the mass enclosed within the Einstein radius rather than the slope of the density profile. Another possible cause may be that the Zitrin's models are calculated on a low-resolution grid and perhaps their accuracy is limited by this resolution compared to higher resolution or completely analytic parametrizations.</p>
        <p>When switching to a fully numerical simulation, the differences between parametric and free-form methods become weaker. At least for some of the metrics, some free-form/hybrid reconstructions of Hera (see e.g. the GRALE or Lam models) appear to be as good as the best parametric reconstructions of this cluster. This indicates that several parametric methods still cannot fully account for deviations of the mass distributions from a symmetric shape, which are, instead, more naturally captured by free-form methods. Asymmetries could be mimicked by suitable combinations of substructures in parametric models. Indeed, a degeneracy exists between these two properties of the mass distribution. However, the number of constraints in these simulations is high enough that this degeneracy is partially broken, as shown by how well the mass is constrained around the cluster galaxies in at least some of the parametric reconstructions.</p>
        <p>The model provided by the CATS team for Hera has significantly smaller values of all metrics (except for the cluster orientation), compared to the model submitted by the same team for Ares. The metrics agree with those of other parametric reconstructions of the same cluster (e.g. Johnson-Sharon). On the contrary, the reconstructions provided by the GLAFIC team for the two clusters The comparison between the metrics of parametric and freeform methods also shows that the latter techniques are generally less accurate in reconstructing the 2D maps of convergence and magnification and in measuring the mass around substructures. In fact, the spatial resolution that can be achieved with these methods is generally lower. On the contrary, radial profiles of the convergence and of the enclosed mass are measured by several of the free-form methods employed in this experiment with accuracy comparable to parametric techniques.</p>
        <p>We would like to remark that the tests outlined in this paper suffer of some limitations. First of all, we make the assumption that the simulations reproduce the properties of real clusters. While some methods (e.g. the free-form ones) do not care about the correlation between dark matter and baryons, other methods strongly rely on the assumption that LTM. Both Ares and Hera implement this property, which, at least in some cases, has been questioned by observations (Hoag et al. 2015;Wang et al. 2015). In particular, the results we report on substructures are sensitive to this assumption. In a recent paper, Harvey, Kneib &amp; Jauzac (2016) have explored how assuming that LTM in strong gravitational models can lead to systematic errors in the predicted positions of multiple images. They find that images can be shifted by up to ∼1 arcsec, assuming physically motivated offsets between dark matter and stars. They quote a ∼0.5 arcsec rms error in the position of the multiple images due to breaking the assumption that mass traces light. Note, however, that, to some extent, we introduced some misalignment between matter and light in both Ares and Hera, by assigning to the observed galaxies a shape and an orientation that are not correlated with the underlying dark matter distribution.</p>
        <p>Other limitations regard some observable properties of the galaxies in the simulated observations (e.g. luminosities and sizes) and their correlation with their halo masses. It is known that the SAMs are not fully consistent with observations in this respect (see e.g. González et al. 2009;Ascaso, Mei &amp; Benítez 2015;Xie et al. 2015;Hirschmann, De Lucia &amp; Fontanot 2016), and thus the standard scaling relations adopted by some parametric techniques to translate the light into the mass or the size of the host halo might not equally applicable to observations and simulations.</p>
        <p>In this paper, we used simulated observations of two synthetic galaxy clusters to evaluate the performance of several algorithms MNRAS 472, 3177-3216 (2017) for mass reconstruction with strong lensing. Such algorithms are currently being used to deliver to the community the lens models for the six galaxy clusters being observed in the Frontier Fields programme of the HST.</p>
        <p>The two clusters used in this study were obtained using very different techniques. Ares was generated using the semi-analytical code 
            <rs type="software">MOKA</rs>. Hera is instead the output of a cosmological N-body simulation at high resolution. The observable properties of the cluster galaxies are modelled using HOD and SAM techniques in Ares and Hera, respectively. In both cases, the clusters have complex mass distributions, characterized by disturbed and bimodal morphology, similar to those of the FFI clusters.
        </p>
        <p>We used the code 
            <rs type="software">SKYLENS</rs> to simulate HST observations of the two mock clusters with both the ACS and the WFC3-IR camera. We produced images in all photometric bands used in the FFI, calibrating the exposure times such to reach the depth of the FFI observations. These HST simulated data were distributed to several groups of lens modellers for a blind analysis, i.e. without unveiling the true mass distribution of the lenses, neither the method used to simulate them.
        </p>
        <p>The simulated observations include lensing effects on a realistic distribution of background galaxies. We identified many strongly lensed galaxies and built a catalogue of multiple image systems, which was delivered together with the simulated observations. The catalogues also include the redshift of all the sources.</p>
        <p>We complemented the HST simulations with a simulated observation in the R c band with the Subaru telescope. The main purpose of this additional simulation was to allow the inclusion of weaklensing constraints at larger distances from the cluster centre than those probed by HST. Together with the image, we also distributed a shear catalogue obtained by processing the Subaru simulation through a public KSB pipeline.</p>
        <p>We received 9 reconstructions of Ares and 11 reconstructions of Hera, submitted by 10 different groups. Seven groups employed their techniques to reconstruct both clusters. The remainder groups reconstructed just one of the two clusters or submitted reconstructions based on different set-ups of their methods. This is the first time that such a large number of algorithms have been tested against known mass distributions. Similar to the spirit of our experiment, in the recent collaborative effort presented in Treu et al. (2016), several of the methods used to reconstruct the galaxy cluster MACSJ1149.5+2223 and to estimate the time delays between the multiple images of the SN 'Refsdal were compared. The recent reappearance of the SN, reported by Kelly et al. (2016), enabled the blind test of various model predictions, which were found to be in very accurate for several reconstructions. In addition, Rodney et al. (2015) compared the magnification predictions from 17 mass models of Abell 2744 using a lensed Type Ia supernova.</p>
        <p>The methods compared here include both parametric and freeform algorithms. We have investigated how they perform at recovering several properties of the lenses, namely: the radial profiles of the convergence and of the enclosed mass, the mass in substructures, the maps of the convergence and of the magnification. For each of these properties, we defined a metric aimed at quantifying the performance of the method.</p>
        <p>The key results of this phase of the comparison exercise of lens mapping methodologies can be summarized as follows.</p>
        <p>(i) Parametric methods are generally better at capturing 2D properties of the lens cores (shape, local values of the convergence and of the magnification). The free-form methods are as competitive as the parametric methods to measure convergence and mass profiles. It is worth mentioning, however, that, in both Ares and the Hera, the cluster galaxies were good tracers of the cluster mass distributions.</p>
        <p>(ii) The accuracy and precision of strong lensing methods to measure the mass within the Einstein radius (or more generally within the region probed by the strong lensing constraints) is very high. The measured profiles deviate from the true profiles by only a few per cent at these scales. Of course, larger deviations are found at radii larger and smaller than the Einstein radius. The determination of the mass enclosed within the Einstein radius was extremely robust for all methods.</p>
        <p>(iii) The largest uncertainties in the lens models are found near substructures and around the cluster critical lines. For some of the parametric models, the total mass around substructures (identified by cluster galaxies) is constrained with an accuracy of ∼10 per cent. However, other methods have much larger scatter. Uncertainties on the magnification grow as a function the magnification itself and are therefore more pronounced near the cluster critical lines. For the best-performing methods, the accuracy in the magnification estimate is ∼10 per cent at μ true = 3 and degrades to ∼30 per cent at μ true = 10.</p>
        <p>(iv) Switching from Ares to Hera, i.e. from a purely parametric to a more realistic lens mass distribution, the gap between parametric and free-form methods becomes smaller. Algorithms such as that used by the GLAFIC team, which include third-order multipoles in the lens mass distribution, have extra degrees of freedom which allow them to better reproduce asymmetries. These asymmetries, and possible variations of the halo ellipticity as a function of radius, seem to be the strongest limitations of parametric methods. The adoption of an hybrid approach, where parametric and free-form methods are combined also to describe the large-scale component of the clusters, could lead to a significant improvement of the mass reconstructions.</p>
        <p>(v) Some of the participating groups used the same code but adopted different set-ups to run them. For example, two groups (CATS and Johnson-Sharon) use the public code LENSTOOL with slight modifications. Similarly, Diego submitted several models of Hera using 
            <rs type="software">WSLAP+</rs>, which is the same code used by Lam et al. (2014). Despite using the same algorithms and using the same inputs (i.e. families of multiple images and redshifts), the reconstructions obtained by these groups are different, indicating that some choices made by the modellers when ingesting the data and hence set up priors influence the results. For example, the differences between the reconstructions performed with LENSTOOL by the CATS and Johnson-Sharon teams may be in part due to the priors on the normalization of the scaling relations used to model the cluster galaxies. This is the first of a series of papers in which we address the issue of the accuracy of lens modelling. In a second paper, currently in preparation, we will discuss the results of the unblinded modelling of Ares and Hera. The feedback from the unblinding was used by modellers to not only tweak their best fits to reach the best possible match to the input mass distributions of the lenses but to also incorporate and instigate improvements in their modelling procedure. This will provide information on the accuracy limits achievable by each method and will also give further hints on the steps that need to be taken to optimize reconstructions.
        </p>
        <p>Despite their complexity and the inclusion of several observational effects, the simulations used in this paper are still idealized in many respects. For example, the lenses are isolated and no additional lensing by matter along the line of sight is included. In addition, we alleviated the work of the lens modellers by identifying the strongly lensed sources and even providing redshifts for all of them. In the case of Ares, the number of available multiple images with known redshifts exceeds by a factor of ∼3-4 what is available in any of the Frontier Fields (e.g. MACSJ0416). We will include the uncertainties due to possible misidentification of multiple images and photometric redshifts as well as the noise added in by the intervening matter distribution along the line of sight in the next phase of this project in future work.</p>
        <p>The redshifts are mainly obtained in the framework of the GLASS and CLASH-VLT programs(Grillo et al.</p>
        <p>2015;Treu et al. 2015) and with the integral field spectrograph Multi Unit Spectroscopic Explorer (MUSE) on the VLT (see e.g.Karman et al. 2015). 2 https://archive.stsci.edu/prepds/frontier/lensmodels/ MNRAS 472,3177-3216 (2017)</p>
        <p>http://cgiocoli.wordpress.com/research-interests/moka MNRAS</p>
        <p>472,3177-3216 (2017)</p>
        <p>MNRAS 472,3177-3216 (2017)</p>
        <p>We assume an orbital visibility period of 2500 s.MNRAS 472,3177-3216 (2017)</p>
        <p>Two exposures per orbit per filter.</p>
        <p>http://www.roe.ac.uk/˜heymans/KSBf90/Home.html</p>
        <p>http://pico.bo.astro.it/˜massimo/Public/FF MNRAS 472,3177-3216 (2017)</p>
        <p>
            <rs type="software">GRALE</rs>'s description, software and installation instructions are available at 
            <rs type="url">http://research.edm.uhasselt.be/∼jori/grale</rs>.
        </p>
        <p>We thank T. Treu for the helpful discussion. MM acknowledges support from the Italian Ministry of Foreign Affairs and International Cooperation, Directorate General for Country Promotion, from INAF via PRIN-INAF 2014 C.R.A. 1.05.01.94.02, and from ASI via contract ASI/INAF/I/023/12/0. This work was supported in part by World Premier International Research Center Initiative (WPI Initiative), MEXT, Japan and JSPS KAKENHI Grant Number 26800093 and15H05892. AZ is supported by NASA through Hubble Fellowship grant #HST-HF2-51334.001-A awarded by STScI, which is operated by the Association of Universities for Research in Astronomy, Inc. under NASA contract NAS 5-26555. JMD acknowledges support of the consolider project CSD2010-00064 and AYA2012-39475-C02-01 funded by the Ministerio de Economia y Competitividad, Spain. We acknowledge the lens modelling community for enthusiastically participating in this collaborative project to compare and contrast mass models. Finally, we want to thank the anonymous referees for their comments and suggestions that helped to improve the quality of the manuscript significantly.</p>
    </text>
</tei>

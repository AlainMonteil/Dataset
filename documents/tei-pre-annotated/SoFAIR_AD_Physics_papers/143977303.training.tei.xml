<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:42+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>During 2015 the ATLAS experiment recorded 3.8 fb -1 of proton-proton collision data at a centre-of-mass energy of 13 TeV. The ATLAS trigger system is a crucial component of the experiment, responsible for selecting events of interest at a recording rate of approximately 1 kHz from up to 40 MHz of collisions. This paper presents a short overview of the changes to the trigger and data acquisition systems during the first long shutdown of the LHC and shows the performance of the trigger system and its components based on the 2015 proton-proton collision data.During 2015 the ATLAS experiment recorded 3.8 fb -1 of proton-proton collision data at a centre-of-mass energy of 13 TeV. The ATLAS trigger system is a crucial component of the experiment, responsible for selecting events of interest at a recording rate of approximately 1 kHz from up to 40 MHz of collisions. This paper presents a short overview of the changes to the trigger and data acquisition systems during the first long shutdown of the LHC and shows the performance of the trigger system and its components based on the 2015 proton-proton collision data.</p>
        <p>The trigger system is an essential component of any collider experiment as it is responsible for deciding whether or not to keep an event from a given bunch-crossing interaction for later study. During Run 1 (2009 to early 2013) of the Large Hadron Collider (LHC), the trigger system [1][2][3][4][5] of the ATLAS experiment [6] operated efficiently at instantaneous luminosities of up to 8 × 10 33 cm -2 s -1 and primarily at centre-of-mass energies, √ s, of 7 TeV and 8 TeV. In Run 2 (since 2015) the increased centre-of-mass energy of 13 TeV, higher luminosity and increased number of proton-proton interactions per bunch-crossing (pile-up) meant that, without upgrades of the trigger system, the trigger rates would have exceeded the maximum allowed rates when running with the trigger thresholds needed to satisfy the physics programme of the experiment. For this reason, the first long shutdown (LS1) between LHC Run 1 and Run 2 operations was used to improve the trigger system with almost no component left untouched.The trigger system is an essential component of any collider experiment as it is responsible for deciding whether or not to keep an event from a given bunch-crossing interaction for later study. During Run 1 (2009 to early 2013) of the Large Hadron Collider (LHC), the trigger system [1][2][3][4][5] of the ATLAS experiment [6] operated efficiently at instantaneous luminosities of up to 8 × 10 33 cm -2 s -1 and primarily at centre-of-mass energies, √ s, of 7 TeV and 8 TeV. In Run 2 (since 2015) the increased centre-of-mass energy of 13 TeV, higher luminosity and increased number of proton-proton interactions per bunch-crossing (pile-up) meant that, without upgrades of the trigger system, the trigger rates would have exceeded the maximum allowed rates when running with the trigger thresholds needed to satisfy the physics programme of the experiment. For this reason, the first long shutdown (LS1) between LHC Run 1 and Run 2 operations was used to improve the trigger system with almost no component left untouched.</p>
        <p>After a brief introduction of the ATLAS detector in Sect. 2, Sect. 3 summarises the changes to the trigger and data acquisition during LS1. Section 4 gives an overview of the trigger menu used during 2015 followed by an introduction to the reconstruction algorithms used at the high-level trigger in Sect. 5. The performance of the different trigger signatures is shown in Sect. 6 for the data taken with 25 ns bunchspacing in 2015 at a peak luminosity of 5 × 10 33 cm -2 s -1 with comparison to Monte Carlo (MC) simulation.After a brief introduction of the ATLAS detector in Sect. 2, Sect. 3 summarises the changes to the trigger and data acquisition during LS1. Section 4 gives an overview of the trigger menu used during 2015 followed by an introduction to the reconstruction algorithms used at the high-level trigger in Sect. 5. The performance of the different trigger signatures is shown in Sect. 6 for the data taken with 25 ns bunchspacing in 2015 at a peak luminosity of 5 × 10 33 cm -2 s -1 with comparison to Monte Carlo (MC) simulation.</p>
        <p>ATLAS is a general-purpose detector with a forwardbackward symmetry, which provides almost full solid angle coverage around the interaction point. 1 The main components of ATLAS are an inner detector (ID), which is surrounded by a superconducting solenoid providing a 2T axial magnetic field, a calorimeter system, and a muon spectrometer (MS) in a magnetic field generated by three large superconducting toroids with eight coils each. The ID provides track reconstruction within |η| &lt; 2.5, employing a pixel detector (Pixel) close to the beam pipe, a silicon microstrip detector (SCT) at intermediate radii, and a transition radiation tracker (TRT) at outer radii. A new innermost pixeldetector layer, the insertable B-layer (IBL), was added dur- 1 ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP) in the centre of the detector and the z-axis along the beam pipe. The x-axis points from the IP to the centre of the LHC ring, and the y-axis points upward. Cylindrical coordinates (r, φ) are used in the transverse plane, φ being the azimuthal angle around the z-axis. The pseudorapidity is defined in terms of the polar angle θ as η =ln tan(θ/2). ing LS1 at a radius of 33 mm around a new and thinner beam pipe [7]. The calorimeter system covers the region |η| &lt; 4.9, the forward region (3.2 &lt; |η| &lt; 4.9) being instrumented with a liquid-argon (LAr) calorimeter for electromagnetic and hadronic measurements. In the central region, a lead/LAr electromagnetic calorimeter covers |η| &lt; 3.2, while the hadronic calorimeter uses two different detector technologies, with steel/scintillator tiles (|η| &lt; 1.7) or lead/LAr (1.5 &lt; |η| &lt; 3.2) as absorber/active material. The MS consists of one barrel (|η| &lt; 1.05) and two end-cap sections (1.05 &lt; |η| &lt; 2.7). Resistive plate chambers (RPC, three doublet layers for |η| &lt; 1.05) and thin gap chambers (TGC, one triplet layer followed by two doublets for 1.0 &lt; |η| &lt; 2.4) provide triggering capability as well as (η, φ) position measurements. A precise momentum measurement for muons with |η| up to 2.7 is provided by three layers of monitored drift tubes (MDT), with each chamber providing six to eight η measurements along the muon trajectory. For |η| &gt; 2, the inner layer is instrumented with cathode strip chambers (CSC), consisting of four sensitive layers each, instead of MDTs.ATLAS is a general-purpose detector with a forwardbackward symmetry, which provides almost full solid angle coverage around the interaction point. 1 The main components of ATLAS are an inner detector (ID), which is surrounded by a superconducting solenoid providing a 2T axial magnetic field, a calorimeter system, and a muon spectrometer (MS) in a magnetic field generated by three large superconducting toroids with eight coils each. The ID provides track reconstruction within |η| &lt; 2.5, employing a pixel detector (Pixel) close to the beam pipe, a silicon microstrip detector (SCT) at intermediate radii, and a transition radiation tracker (TRT) at outer radii. A new innermost pixeldetector layer, the insertable B-layer (IBL), was added dur- 1 ATLAS uses a right-handed coordinate system with its origin at the nominal interaction point (IP) in the centre of the detector and the z-axis along the beam pipe. The x-axis points from the IP to the centre of the LHC ring, and the y-axis points upward. Cylindrical coordinates (r, φ) are used in the transverse plane, φ being the azimuthal angle around the z-axis. The pseudorapidity is defined in terms of the polar angle θ as η =ln tan(θ/2). ing LS1 at a radius of 33 mm around a new and thinner beam pipe [7]. The calorimeter system covers the region |η| &lt; 4.9, the forward region (3.2 &lt; |η| &lt; 4.9) being instrumented with a liquid-argon (LAr) calorimeter for electromagnetic and hadronic measurements. In the central region, a lead/LAr electromagnetic calorimeter covers |η| &lt; 3.2, while the hadronic calorimeter uses two different detector technologies, with steel/scintillator tiles (|η| &lt; 1.7) or lead/LAr (1.5 &lt; |η| &lt; 3.2) as absorber/active material. The MS consists of one barrel (|η| &lt; 1.05) and two end-cap sections (1.05 &lt; |η| &lt; 2.7). Resistive plate chambers (RPC, three doublet layers for |η| &lt; 1.05) and thin gap chambers (TGC, one triplet layer followed by two doublets for 1.0 &lt; |η| &lt; 2.4) provide triggering capability as well as (η, φ) position measurements. A precise momentum measurement for muons with |η| up to 2.7 is provided by three layers of monitored drift tubes (MDT), with each chamber providing six to eight η measurements along the muon trajectory. For |η| &gt; 2, the inner layer is instrumented with cathode strip chambers (CSC), consisting of four sensitive layers each, instead of MDTs.</p>
        <p>The Trigger and Data Acquisition (TDAQ) system shown in Fig. 1 consists of a hardware-based first-level trigger (L1) and a software-based high-level trigger (HLT). The L1 trigger decision is formed by the Central Trigger Processor (CTP), which receives inputs from the L1 calorimeter (L1Calo) and L1 muon (L1Muon) triggers as well as several other subsystems such as the Minimum Bias Trigger Scintillators (MBTS), the LUCID Cherenkov counter and the Zero-Degree Calorimeter (ZDC). The CTP is also responsible for applying preventive dead-time. It limits the minimum time between two consecutive L1 accepts (simple dead-time) to avoid overlapping readout windows, and restricts the number of L1 accepts allowed in a given number of bunch-crossings (complex dead-time) to avoid front-end buffers from overflowing. In 2015 running, the simple dead-time was set to 4 bunch-crossings (100 ns). A more detailed description of the L1 trigger system can be found in Ref. [1]. After the L1 trigger acceptance, the events are buffered in the Read-Out System (ROS) and processed by the HLT. The HLT receives Region-of-Interest (RoI) information from L1, which can be used for regional reconstruction in the trigger algorithms. After the events are accepted by the HLT, they are transferred to local storage at the experimental site and exported to the Tier-0 facility at CERN's computing centre for offline reconstruction.The Trigger and Data Acquisition (TDAQ) system shown in Fig. 1 consists of a hardware-based first-level trigger (L1) and a software-based high-level trigger (HLT). The L1 trigger decision is formed by the Central Trigger Processor (CTP), which receives inputs from the L1 calorimeter (L1Calo) and L1 muon (L1Muon) triggers as well as several other subsystems such as the Minimum Bias Trigger Scintillators (MBTS), the LUCID Cherenkov counter and the Zero-Degree Calorimeter (ZDC). The CTP is also responsible for applying preventive dead-time. It limits the minimum time between two consecutive L1 accepts (simple dead-time) to avoid overlapping readout windows, and restricts the number of L1 accepts allowed in a given number of bunch-crossings (complex dead-time) to avoid front-end buffers from overflowing. In 2015 running, the simple dead-time was set to 4 bunch-crossings (100 ns). A more detailed description of the L1 trigger system can be found in Ref. [1]. After the L1 trigger acceptance, the events are buffered in the Read-Out System (ROS) and processed by the HLT. The HLT receives Region-of-Interest (RoI) information from L1, which can be used for regional reconstruction in the trigger algorithms. After the events are accepted by the HLT, they are transferred to local storage at the experimental site and exported to the Tier-0 facility at CERN's computing centre for offline reconstruction.</p>
        <p>Several Monte Carlo simulated datasets were used to assess the performance of the trigger. Fully simulated pho-ton+jet and dijet events generated with 
            <rs type="software">Pythia8</rs> [8] using the NNPDF2.3LO [9] parton distribution function (PDF) set were used to study the photon and jet triggers. To study tau and b-jet triggers, Z → τ τ and t t samples generated with 
            <rs type="software">Powheg-Box</rs>
            <rs type="version">2.0</rs> [10][11][12] with the CT10 [13] PDF Fig. 1 The ATLAS TDAQ system in Run 2 with emphasis on the components relevant for triggering. L1Topo and FTK were being commissioned during 2015 and not used for the results shown here set and interfaced to 
            <rs type="software">Pythia</rs>
            <rs type="version">8</rs> or 
            <rs type="software">Pythia</rs>
            <rs type="version">6</rs> [14] with the CTEQ6L1 [15] PDF set were used.
        </p>
        <p>3 Changes to the Trigger/DAQ system for Run 23 Changes to the Trigger/DAQ system for Run 2</p>
        <p>The TDAQ system used during Run 1 is described in detail in Refs. [1,16]. Compared to Run 1, the LHC has increased its centre-of-mass energy from 8 to 13 TeV, and the nominal bunch-spacing has decreased from 50 to 25 ns. Due to the larger transverse beam size at the interaction point (β * = 80 cm compared to 60 cm in 2012) and a lower bunch population (1.15 × 10 11 instead of 1.6 × 10 11 protons per bunch) the peak luminosity reached in 2015 (5.0 × 10 33 cm -2 s -1 ) was lower than in Run 1 (7.7 × 10 33 cm -2 s -1 ). However, due to the increase in energy, trigger rates are on average 2.0 to 2.5 times larger for the same luminosity and with the same trigger criteria (individual trigger rates, e.g. jets, can have even larger increases). The decrease in bunch-spacing also increases certain trigger rates (e.g. muons) due to additional interactions from neighbouring bunch-crossings (out-of-time pile-up). In order to prepare for the expected higher rates in Run 2, several upgrades and additions were implemented during LS1. The main changes relevant to the trigger system are briefly described below.The TDAQ system used during Run 1 is described in detail in Refs. [1,16]. Compared to Run 1, the LHC has increased its centre-of-mass energy from 8 to 13 TeV, and the nominal bunch-spacing has decreased from 50 to 25 ns. Due to the larger transverse beam size at the interaction point (β * = 80 cm compared to 60 cm in 2012) and a lower bunch population (1.15 × 10 11 instead of 1.6 × 10 11 protons per bunch) the peak luminosity reached in 2015 (5.0 × 10 33 cm -2 s -1 ) was lower than in Run 1 (7.7 × 10 33 cm -2 s -1 ). However, due to the increase in energy, trigger rates are on average 2.0 to 2.5 times larger for the same luminosity and with the same trigger criteria (individual trigger rates, e.g. jets, can have even larger increases). The decrease in bunch-spacing also increases certain trigger rates (e.g. muons) due to additional interactions from neighbouring bunch-crossings (out-of-time pile-up). In order to prepare for the expected higher rates in Run 2, several upgrades and additions were implemented during LS1. The main changes relevant to the trigger system are briefly described below.</p>
        <p>In the L1 Central Trigger, a new topological trigger (L1Topo) consisting of two FPGA-based (Field-Programmable Gate Arrays) processor modules was added. The modules are identical hardware-wise and each is pro-grammed to perform selections based on geometric or kinematic association between trigger objects received from the L1Calo or L1Muon systems. This includes the refined calculation of global event quantities such as missing transverse momentum (with magnitude E miss T ). The system was fully installed and commissioned during 2016, i.e. it was not used for the data described in this paper. Details of the hardware implementation can be found in Ref. [17]. The Muon-to-CTP interface (
            <rs type="software">MUCPTI</rs>) and the CTP were upgraded to provide inputs to and receive inputs from L1Topo, respectively. In order to better address sub-detector specific requirements, the CTP now supports up to four independent complex dead-time settings operating simultaneously. In addition, the number of L1 trigger selections (512) and bunch-group selections (16), defined later, were doubled compared to Run 1. The changes to the L1Calo and L1Muon trigger systems are described in separate sections below.
        </p>
        <p>In Run 1 the HLT consisted of separate Level-2 (L2) and Event Filter (EF) farms. While L2 requested partial event data over the network, the EF operated on full event information assembled by separate farm nodes dedicated to Event Building (EB). For Run 2, the L2 and EF farms were merged into a single homogeneous farm allowing better resource sharing and an overall simplification of both the hardware and software. RoI-based reconstruction continues to be employed by time-critical algorithms. The functionality of the EB nodes was also integrated into the HLT farm. To achieve higher readout and output rates, the ROS, the data collection network and data storage system were upgraded. The on-detector front-end (FE) electronics and detector-specific readout drivers (ROD) were not changed in any significant way.In Run 1 the HLT consisted of separate Level-2 (L2) and Event Filter (EF) farms. While L2 requested partial event data over the network, the EF operated on full event information assembled by separate farm nodes dedicated to Event Building (EB). For Run 2, the L2 and EF farms were merged into a single homogeneous farm allowing better resource sharing and an overall simplification of both the hardware and software. RoI-based reconstruction continues to be employed by time-critical algorithms. The functionality of the EB nodes was also integrated into the HLT farm. To achieve higher readout and output rates, the ROS, the data collection network and data storage system were upgraded. The on-detector front-end (FE) electronics and detector-specific readout drivers (ROD) were not changed in any significant way.</p>
        <p>A new Fast TracKer (FTK) system [18] will provide global ID track reconstruction at the L1 trigger rate using lookup tables stored in custom associative memory chips for the pattern recognition. Instead of a computationally intensive helix fit, the FPGA-based track fitter performs a fast linear fit and the tracks are made available to the HLT. This system will allow the use of tracks at much higher event rates in the HLT than is currently affordable using CPU systems. This system is currently being installed and expected to be fully commissioned during 2017.A new Fast TracKer (FTK) system [18] will provide global ID track reconstruction at the L1 trigger rate using lookup tables stored in custom associative memory chips for the pattern recognition. Instead of a computationally intensive helix fit, the FPGA-based track fitter performs a fast linear fit and the tracks are made available to the HLT. This system will allow the use of tracks at much higher event rates in the HLT than is currently affordable using CPU systems. This system is currently being installed and expected to be fully commissioned during 2017.</p>
        <p>The details of the L1Calo trigger algorithms can be found in Ref. [19], and only the basic elements are described here. The electron/photon and tau trigger algorithm (Fig. 2) identifies an RoI as a 2 × 2 trigger tower cluster in the electromagnetic calorimeter for which the sum of the transverse energy from at least one of the four possible pairs of nearest neighbour towers (1 × 2 or 2 × 1) exceeds a predefined threshold. Isolation-veto thresholds can be set for the electromagnetic (EM) isolation ring in the electromagnetic calorimeter, as well as for hadronic tower sums in a central 2×2 core behind the EM cluster and in the 12-tower hadronic ring around it. The E T threshold can be set differently for different η regions at a granularity of 0.1 in η in order to correct for varying detector energy responses. The energy of the trigger towers is calibrated at the electromagnetic energy scale (EM scale). The EM scale correctly reconstructs the energy deposited by particles in an electromagnetic shower in the calorimeter but underestimates the energy deposited by hadrons. Jet RoIs are defined as 4 × 4 or 8 × 8 trigger tower windows for which the summed electromagnetic and hadronic transverse energy exceeds predefined thresholds and which surround a 2 × 2 trigger tower core that is a local maximum. The location of this local maximum also defines the coordinates of the jet RoI.The details of the L1Calo trigger algorithms can be found in Ref. [19], and only the basic elements are described here. The electron/photon and tau trigger algorithm (Fig. 2) identifies an RoI as a 2 × 2 trigger tower cluster in the electromagnetic calorimeter for which the sum of the transverse energy from at least one of the four possible pairs of nearest neighbour towers (1 × 2 or 2 × 1) exceeds a predefined threshold. Isolation-veto thresholds can be set for the electromagnetic (EM) isolation ring in the electromagnetic calorimeter, as well as for hadronic tower sums in a central 2×2 core behind the EM cluster and in the 12-tower hadronic ring around it. The E T threshold can be set differently for different η regions at a granularity of 0.1 in η in order to correct for varying detector energy responses. The energy of the trigger towers is calibrated at the electromagnetic energy scale (EM scale). The EM scale correctly reconstructs the energy deposited by particles in an electromagnetic shower in the calorimeter but underestimates the energy deposited by hadrons. Jet RoIs are defined as 4 × 4 or 8 × 8 trigger tower windows for which the summed electromagnetic and hadronic transverse energy exceeds predefined thresholds and which surround a 2 × 2 trigger tower core that is a local maximum. The location of this local maximum also defines the coordinates of the jet RoI.</p>
        <p>In preparation for Run 2, due to the expected increase in luminosity and consequent increase in the number of pileup events, a major upgrade of several central components of the L1Calo electronics was undertaken to reduce the trigger rates.In preparation for Run 2, due to the expected increase in luminosity and consequent increase in the number of pileup events, a major upgrade of several central components of the L1Calo electronics was undertaken to reduce the trigger rates.</p>
        <p>For the preprocessor system [20], which digitises and calibrates the analogue signals (consisting of ∼7000 trigger towers at a granularity of 0.1 × 0.1 in η × φ) from the calorimeter detectors, a new FPGA-based multi-chip module (nMCM) was developed [21] and about 3000 chips (including spares) were produced. They replace the old ASIC-based MCMs used during Run 1. The new modules provide additional flexibility and new functionality with respect to the old system. In particular, the nMCMs support the use of digital autocorrelation Finite Impulse Response (FIR) filters and the implementation of a dynamic, bunch-by-bunch pedestal correction, both introduced for Run 2. These improvements lead to a significant rate reduction of the L1 jet and L1 E miss T triggers. The bunch-by-bunch pedestal subtraction compensates for the increased trigger rates at the beginning of a bunch train caused by the interplay of in-time and out-oftime pile-up coupled with the LAr pulse shape [22], and linearises the L1 trigger rate as a function of the instantaneous luminosity, as shown in Fig. 3 for the L1 E miss T trigger. The autocorrelation FIR filters substantially improve the bunchcrossing identification (BCID) efficiencies, in particular for low energy deposits. However, the use of this new filtering scheme initially led to an early trigger signal (and incomplete events) for a small fraction of very high energy events. These events were saved into a stream dedicated to mistimed events and treated separately in the relevant physics analyses. The source of the problem was fixed in firmware by adapting the BCID decision logic for saturated pulses and was deployed at the start of the 2016 data-taking period.For the preprocessor system [20], which digitises and calibrates the analogue signals (consisting of ∼7000 trigger towers at a granularity of 0.1 × 0.1 in η × φ) from the calorimeter detectors, a new FPGA-based multi-chip module (nMCM) was developed [21] and about 3000 chips (including spares) were produced. They replace the old ASIC-based MCMs used during Run 1. The new modules provide additional flexibility and new functionality with respect to the old system. In particular, the nMCMs support the use of digital autocorrelation Finite Impulse Response (FIR) filters and the implementation of a dynamic, bunch-by-bunch pedestal correction, both introduced for Run 2. These improvements lead to a significant rate reduction of the L1 jet and L1 E miss T triggers. The bunch-by-bunch pedestal subtraction compensates for the increased trigger rates at the beginning of a bunch train caused by the interplay of in-time and out-oftime pile-up coupled with the LAr pulse shape [22], and linearises the L1 trigger rate as a function of the instantaneous luminosity, as shown in Fig. 3 for the L1 E miss T trigger. The autocorrelation FIR filters substantially improve the bunchcrossing identification (BCID) efficiencies, in particular for low energy deposits. However, the use of this new filtering scheme initially led to an early trigger signal (and incomplete events) for a small fraction of very high energy events. These events were saved into a stream dedicated to mistimed events and treated separately in the relevant physics analyses. The source of the problem was fixed in firmware by adapting the BCID decision logic for saturated pulses and was deployed at the start of the 2016 data-taking period.</p>
        <p>The preprocessor outputs are then transmitted to both the Cluster Processor (CP) and Jet/Energy-sum Processor (JEP) subsystems in parallel. The CP subsystem identifies electron/photon and tau lepton candidates with E T above a programmable threshold and satisfying, if required, certain isolation criteria. The JEP receives jet trigger elements, which Fig. 3 The per-bunch trigger rate for the L1 missing transverse momentum trigger with a threshold of 50 GeV (L1_XE50) as a function of the instantaneous luminosity per bunch. The rates are shown with and without pedestal correction applied are 0.2 × 0.2 sums in η × φ, and uses these to identify jets and to produce global sums of scalar and missing transverse momentum. Both the CP and JEP firmware were upgraded to allow an increase of the data transmission rate over the custom-made backplanes from 40 to 160 Mbps, allowing the transmission of up to four jet or five EM/tau trigger objects per module. A trigger object contains the E T sum, η -φ coordinates, and isolation thresholds where relevant. While the JEP firmware changes were only minor, substantial extra selectivity was added to the CP by implementing energydependent L1 electromagnetic isolation criteria instead of fixed threshold cuts. This feature was added to the trigger menu (defined in Sect. 4) at the beginning of Run 2. In 2015 it was used to effectively select events with specific signatures, e.g. EM isolation was required for taus but not for electrons.The preprocessor outputs are then transmitted to both the Cluster Processor (CP) and Jet/Energy-sum Processor (JEP) subsystems in parallel. The CP subsystem identifies electron/photon and tau lepton candidates with E T above a programmable threshold and satisfying, if required, certain isolation criteria. The JEP receives jet trigger elements, which Fig. 3 The per-bunch trigger rate for the L1 missing transverse momentum trigger with a threshold of 50 GeV (L1_XE50) as a function of the instantaneous luminosity per bunch. The rates are shown with and without pedestal correction applied are 0.2 × 0.2 sums in η × φ, and uses these to identify jets and to produce global sums of scalar and missing transverse momentum. Both the CP and JEP firmware were upgraded to allow an increase of the data transmission rate over the custom-made backplanes from 40 to 160 Mbps, allowing the transmission of up to four jet or five EM/tau trigger objects per module. A trigger object contains the E T sum, η -φ coordinates, and isolation thresholds where relevant. While the JEP firmware changes were only minor, substantial extra selectivity was added to the CP by implementing energydependent L1 electromagnetic isolation criteria instead of fixed threshold cuts. This feature was added to the trigger menu (defined in Sect. 4) at the beginning of Run 2. In 2015 it was used to effectively select events with specific signatures, e.g. EM isolation was required for taus but not for electrons.</p>
        <p>Finally, new extended cluster merger modules (CMX) were developed to replace the L1Calo merger modules (CMMs) used during Run 1. The new CMX modules transmit the location and the energy of identified trigger objects to the new L1Topo modules instead of only the threshold multiplicities as done by the CMMs. This transmission happens with a bandwidth of 6.4 Gbps per channel, while the total output bandwidth amounts to above 2 Tbps. Moreover, for most L1 triggers, twice as many trigger selections and isolation thresholds can be processed with the new CMX modules compared to Run 1, considerably increasing the selectivity of the L1Calo system.Finally, new extended cluster merger modules (CMX) were developed to replace the L1Calo merger modules (CMMs) used during Run 1. The new CMX modules transmit the location and the energy of identified trigger objects to the new L1Topo modules instead of only the threshold multiplicities as done by the CMMs. This transmission happens with a bandwidth of 6.4 Gbps per channel, while the total output bandwidth amounts to above 2 Tbps. Moreover, for most L1 triggers, twice as many trigger selections and isolation thresholds can be processed with the new CMX modules compared to Run 1, considerably increasing the selectivity of the L1Calo system.</p>
        <p>The muon barrel trigger was not significantly changed with respect to Run 1, apart from the regions close to the feet that support the ATLAS detector, where the presence of support structures reduces trigger coverage. To recover trigger acceptance, a fourth layer of RPC trigger chambers was installed before Run 1 in the projective region of the acceptance holes. These chambers were not operational during Run 1. During LS1, these RPC layers were equipped with trigger electronics. Commissioning started during 2015 and they are fully operational in 2016. Additional chambers were installed during LS1 to cover the acceptance holes corresponding to two elevator shafts at the bottom of the muon spectrometer but are not yet operational. At the end of the commissioning phase, the new feet and elevator chambers are expected to increase the overall barrel trigger acceptance by 2.8 and 0.8% points, respectively.The muon barrel trigger was not significantly changed with respect to Run 1, apart from the regions close to the feet that support the ATLAS detector, where the presence of support structures reduces trigger coverage. To recover trigger acceptance, a fourth layer of RPC trigger chambers was installed before Run 1 in the projective region of the acceptance holes. These chambers were not operational during Run 1. During LS1, these RPC layers were equipped with trigger electronics. Commissioning started during 2015 and they are fully operational in 2016. Additional chambers were installed during LS1 to cover the acceptance holes corresponding to two elevator shafts at the bottom of the muon spectrometer but are not yet operational. At the end of the commissioning phase, the new feet and elevator chambers are expected to increase the overall barrel trigger acceptance by 2.8 and 0.8% points, respectively.</p>
        <p>During Run 1, a significant fraction of the trigger rate from the end-cap region was found to be due to particles not originating from the interaction point, as illustrated in Fig. 4. To reject these interactions, new trigger logic was introduced in Run 2. An additional TGC coincidence requirement was deployed in 2015 covering the region 1.3 &lt; |η| &lt; 1.9 (TGC-FI). Further coincidence logic in the region 1.0 &lt; |η| &lt; 1.3 is being commissioned by requiring coincidence with the inner TGC chambers (EIL4) or the Tile hadronic calorimeter. Figure 5a shows the muon trigger rate as a function of the muon trigger pseudorapidity with and without the TGC-FI coincidence in separate data-taking runs. The asymmetry as a function of η is a result of the magnetic field direction and the background particles being mostly positively charged. In the region where this additional coincidence is applied, the trigger rate is reduced by up to 60%During Run 1, a significant fraction of the trigger rate from the end-cap region was found to be due to particles not originating from the interaction point, as illustrated in Fig. 4. To reject these interactions, new trigger logic was introduced in Run 2. An additional TGC coincidence requirement was deployed in 2015 covering the region 1.3 &lt; |η| &lt; 1.9 (TGC-FI). Further coincidence logic in the region 1.0 &lt; |η| &lt; 1.3 is being commissioned by requiring coincidence with the inner TGC chambers (EIL4) or the Tile hadronic calorimeter. Figure 5a shows the muon trigger rate as a function of the muon trigger pseudorapidity with and without the TGC-FI coincidence in separate data-taking runs. The asymmetry as a function of η is a result of the magnetic field direction and the background particles being mostly positively charged. In the region where this additional coincidence is applied, the trigger rate is reduced by up to 60%</p>
        <p>The trigger menu defines the list of L1 and HLT triggers and consists of:The trigger menu defines the list of L1 and HLT triggers and consists of:</p>
        <p>• primary triggers, which are used for physics analyses and are typically unprescaled; • support triggers, which are used for efficiency and performance measurements or for monitoring, and are typically operated at a small rate (of the order of 0.5 Hz each) using prescale factors; • alternative triggers, using alternative (sometimes experimental or new) reconstruction algorithms compared to the primary or support selections, and often heavily overlapping with the primary triggers; • backup triggers, with tighter selections and lower expected rate; • calibration triggers, which are used for detector calibration and are often operated at high rate but storing very small events with only the relevant information needed for calibration.• primary triggers, which are used for physics analyses and are typically unprescaled; • support triggers, which are used for efficiency and performance measurements or for monitoring, and are typically operated at a small rate (of the order of 0.5 Hz each) using prescale factors; • alternative triggers, using alternative (sometimes experimental or new) reconstruction algorithms compared to the primary or support selections, and often heavily overlapping with the primary triggers; • backup triggers, with tighter selections and lower expected rate; • calibration triggers, which are used for detector calibration and are often operated at high rate but storing very small events with only the relevant information needed for calibration.</p>
        <p>The primary triggers cover all signatures relevant to the ATLAS physics programme including electrons, photons, muons, tau leptons, (b-)jets and E miss T which are used for Standard Model (SM) precision measurements including decays of the Higgs, W and Z bosons, and searches for physics beyond the SM such as heavy particles, supersymmetry or exotic particles. A set of low transverse momentum ( p T ) dimuon triggers is used to collect B-meson decays, which are essential for the B-physics programme of ATLAS.The primary triggers cover all signatures relevant to the ATLAS physics programme including electrons, photons, muons, tau leptons, (b-)jets and E miss T which are used for Standard Model (SM) precision measurements including decays of the Higgs, W and Z bosons, and searches for physics beyond the SM such as heavy particles, supersymmetry or exotic particles. A set of low transverse momentum ( p T ) dimuon triggers is used to collect B-meson decays, which are essential for the B-physics programme of ATLAS.</p>
        <p>The trigger menu composition and trigger thresholds are optimised for several luminosity ranges in order to maximise the physics output of the experiment and to fit within the rate and bandwidth constraints of the ATLAS detector, TDAQ system and offline computing. For Run 2 the most relevant constraints are the maximum L1 rate of 100 kHz (75 kHz in Run 1) defined by the ATLAS detector readout capability and an average HLT physics output rate of 1000 Hz (400 Hz in Run 1) defined by the offline computing model. To ensure an optimal trigger menu within the rate constraints for a given LHC luminosity, prescale factors can be applied to L1 and HLT triggers and changed during data-taking in such a way that triggers may be disabled or only a certain fraction of events may be accepted by them. Supporting triggers may be running at a constant rate or certain triggers enabled later in the LHC fill when the luminosity and pile-up has reduced and the required resources are available. Further flexibility is provided by bunch groups, which allow triggers to include specific requirements on the LHC proton bunches colliding in ATLAS. These requirements include paired (colliding) bunch-crossings for physics triggers, empty or unpaired crossings for background studies or search for long-lived particle decays, and dedicated bunch groups for detector calibration.The trigger menu composition and trigger thresholds are optimised for several luminosity ranges in order to maximise the physics output of the experiment and to fit within the rate and bandwidth constraints of the ATLAS detector, TDAQ system and offline computing. For Run 2 the most relevant constraints are the maximum L1 rate of 100 kHz (75 kHz in Run 1) defined by the ATLAS detector readout capability and an average HLT physics output rate of 1000 Hz (400 Hz in Run 1) defined by the offline computing model. To ensure an optimal trigger menu within the rate constraints for a given LHC luminosity, prescale factors can be applied to L1 and HLT triggers and changed during data-taking in such a way that triggers may be disabled or only a certain fraction of events may be accepted by them. Supporting triggers may be running at a constant rate or certain triggers enabled later in the LHC fill when the luminosity and pile-up has reduced and the required resources are available. Further flexibility is provided by bunch groups, which allow triggers to include specific requirements on the LHC proton bunches colliding in ATLAS. These requirements include paired (colliding) bunch-crossings for physics triggers, empty or unpaired crossings for background studies or search for long-lived particle decays, and dedicated bunch groups for detector calibration.</p>
        <p>Trigger names used throughout this paper consist of the trigger level (L1 or HLT, the latter often omitted for brevity), multiplicity, particle type (e.g. g for photon, j for jet, xe for E miss T , te for E T triggers) and p T threshold value in GeV (e.g. L1_2MU4 requires at least two muons with p T &gt; 4 GeV at L1, HLT_mu40 requires at least one muon with p T &gt; 40 GeV at the HLT). L1 and HLT trigger items are written in upper case and lower case letters, respectively. Each HLT trigger is configured with an L1 trigger as its seed. The L1 seed is not explicitly part of the trigger name except when an HLT trigger is seeded by more than one L1 trigger, in which case the L1 seed is denoted in the suffix of the alternative trigger (e.g. HLT_mu20 and HLT_mu20_L1MU15 with the first one using L1_MU20 as its seed). Further selection criteria (type of identification, isolation, reconstruction algorithm, geometrical region) are suffixed to the trigger name (e.g. HLT_g120_loose).Trigger names used throughout this paper consist of the trigger level (L1 or HLT, the latter often omitted for brevity), multiplicity, particle type (e.g. g for photon, j for jet, xe for E miss T , te for E T triggers) and p T threshold value in GeV (e.g. L1_2MU4 requires at least two muons with p T &gt; 4 GeV at L1, HLT_mu40 requires at least one muon with p T &gt; 40 GeV at the HLT). L1 and HLT trigger items are written in upper case and lower case letters, respectively. Each HLT trigger is configured with an L1 trigger as its seed. The L1 seed is not explicitly part of the trigger name except when an HLT trigger is seeded by more than one L1 trigger, in which case the L1 seed is denoted in the suffix of the alternative trigger (e.g. HLT_mu20 and HLT_mu20_L1MU15 with the first one using L1_MU20 as its seed). Further selection criteria (type of identification, isolation, reconstruction algorithm, geometrical region) are suffixed to the trigger name (e.g. HLT_g120_loose).</p>
        <p>The main goal of the trigger menu design was to maintain the unprescaled single-electron and single-muon trigger p T thresholds around 25 GeV despite the expected higher trigger rates in Run 2 (see Sect. 3). This strategy ensures the collection of the majority of the events with leptonic W and Z boson decays, which are the main source of events for the study of electroweak processes. In addition, compared to using a large number of analysis-specific triggers, this trigger strategy is simpler and more robust at the cost of slightly higher trigger output rates. Dedicated (multi-object) triggers were added for specific analyses not covered by the above. Table 1 shows a comparison of selected primary trigger thresholds for L1 and the HLT used during Run 1 and 2015 together and tau groups. The rate increase around luminosity block 400 is due to the removal of prescaling of the B-physics triggers. The combined group includes multiple triggers combining different trigger signatures such as electrons with muons, taus, jets or E miss T with the typical thresholds for offline reconstructed objects used in analyses (the latter are usually defined as the p T value at which the trigger efficiency reached the plateau). Trigger thresholds at L1 were either kept the same as during Run 1 or slightly increased to fit within the allowed maximum L1 rate of 100 kHz. At the HLT, several selections were loosened compared to Run 1 or thresholds lowered thanks to the use of more sophisticated HLT algorithms (e.g. multivariate analysis techniques for electrons and taus).The main goal of the trigger menu design was to maintain the unprescaled single-electron and single-muon trigger p T thresholds around 25 GeV despite the expected higher trigger rates in Run 2 (see Sect. 3). This strategy ensures the collection of the majority of the events with leptonic W and Z boson decays, which are the main source of events for the study of electroweak processes. In addition, compared to using a large number of analysis-specific triggers, this trigger strategy is simpler and more robust at the cost of slightly higher trigger output rates. Dedicated (multi-object) triggers were added for specific analyses not covered by the above. Table 1 shows a comparison of selected primary trigger thresholds for L1 and the HLT used during Run 1 and 2015 together and tau groups. The rate increase around luminosity block 400 is due to the removal of prescaling of the B-physics triggers. The combined group includes multiple triggers combining different trigger signatures such as electrons with muons, taus, jets or E miss T with the typical thresholds for offline reconstructed objects used in analyses (the latter are usually defined as the p T value at which the trigger efficiency reached the plateau). Trigger thresholds at L1 were either kept the same as during Run 1 or slightly increased to fit within the allowed maximum L1 rate of 100 kHz. At the HLT, several selections were loosened compared to Run 1 or thresholds lowered thanks to the use of more sophisticated HLT algorithms (e.g. multivariate analysis techniques for electrons and taus).</p>
        <p>Figure 6a, b show the L1 and HLT trigger rates grouped by signatures during an LHC fill with a peak luminosity of 4.5 × 10 33 cm -2 s -1 . The preventive dead-time 2 The singleelectron and single-muon triggers contribute a large fraction to the total rate. While running at these relatively low luminosities it was possible to dedicate a large fraction of the bandwidth to the B-physics triggers. Support triggers contribute about 20% of the total rate. Since the time for trigger commissioning in 2015 was limited due to the fast rise of the LHC luminosity (compared to Run 1), several backup triggers, which contribute additional rate, were implemented in the menu in addition to the primary physics triggers. This is the case for electron, b-jet and E miss T triggers, which are discussed in later sections of the paper.Figure 6a, b show the L1 and HLT trigger rates grouped by signatures during an LHC fill with a peak luminosity of 4.5 × 10 33 cm -2 s -1 . The preventive dead-time 2 The singleelectron and single-muon triggers contribute a large fraction to the total rate. While running at these relatively low luminosities it was possible to dedicate a large fraction of the bandwidth to the B-physics triggers. Support triggers contribute about 20% of the total rate. Since the time for trigger commissioning in 2015 was limited due to the fast rise of the LHC luminosity (compared to Run 1), several backup triggers, which contribute additional rate, were implemented in the menu in addition to the primary physics triggers. This is the case for electron, b-jet and E miss T triggers, which are discussed in later sections of the paper.</p>
        <p>Events accepted by the HLT are written into separate data streams. Events for physics analyses are sent to a single Main stream replacing the three separate physics streams (Egamma, Muons, JetTauEtMiss) used in Run 1. This change reduces event duplication, thus reducing storage and CPU resources required for reconstruction by roughly 10%. A small fraction of these events at a rate of 10 to 20 Hz are also written to an Express stream that is reconstructed promptly offline and used to provide calibration and data quality information prior to the reconstruction of the full Main stream, which typically happens 36 h after the data are taken. In addition, there are about twenty additional streams for calibration, monitoring and detector performance studies. To reduce event size, some of these streams use partial event building (partial EB), which writes only a predefined subset of the ATLAS detector data per event. For Run 2, events that contain only HLT reconstructed objects, but no ATLAS detector data, can be recorded to a new type of stream. These events are of very small size, allowing recording at high rate. These streams are used for calibration purposes and Trigger-Level Analysis as described in Sect. 6.4.4. Figure 7 shows typical HLT stream rates and bandwidth during an LHC fill.Events accepted by the HLT are written into separate data streams. Events for physics analyses are sent to a single Main stream replacing the three separate physics streams (Egamma, Muons, JetTauEtMiss) used in Run 1. This change reduces event duplication, thus reducing storage and CPU resources required for reconstruction by roughly 10%. A small fraction of these events at a rate of 10 to 20 Hz are also written to an Express stream that is reconstructed promptly offline and used to provide calibration and data quality information prior to the reconstruction of the full Main stream, which typically happens 36 h after the data are taken. In addition, there are about twenty additional streams for calibration, monitoring and detector performance studies. To reduce event size, some of these streams use partial event building (partial EB), which writes only a predefined subset of the ATLAS detector data per event. For Run 2, events that contain only HLT reconstructed objects, but no ATLAS detector data, can be recorded to a new type of stream. These events are of very small size, allowing recording at high rate. These streams are used for calibration purposes and Trigger-Level Analysis as described in Sect. 6.4.4. Figure 7 shows typical HLT stream rates and bandwidth during an LHC fill.</p>
        <p>Events that cannot be properly processed at the HLT or have other DAQ-related problems are written to dedicated debug streams. These events are reprocessed offline with the same HLT configuration as used during data-taking and accepted events are stored into separate data sets for use in physics analyses. In 2015, approximately 339,000 events were written to debug streams. The majority of them (∼90%) are due to online processing timeouts that occur when the event cannot be processed within 2-3 min. Long processing times are mainly due to muon algorithms processing events with a large number of tracks in the muon spectrometer (e.g. due to jets not contained in the calorimeter). During the debugEvents that cannot be properly processed at the HLT or have other DAQ-related problems are written to dedicated debug streams. These events are reprocessed offline with the same HLT configuration as used during data-taking and accepted events are stored into separate data sets for use in physics analyses. In 2015, approximately 339,000 events were written to debug streams. The majority of them (∼90%) are due to online processing timeouts that occur when the event cannot be processed within 2-3 min. Long processing times are mainly due to muon algorithms processing events with a large number of tracks in the muon spectrometer (e.g. due to jets not contained in the calorimeter). During the debug</p>
        <p>Fig. 7 a HLT stream rates and b bandwidth during an LHC fill in October 2015 with a peak luminosity of 4.5 × 10 33 cm -2 s -1 . Partial Event Building (partial EB) streams only store relevant subdetector data and thus have smaller event sizes. The other physics-related streams contain events with special readout settings and are used to overlay with MC events to simulate pile-up stream reprocessing, 330,000 events were successfully processed by the HLT of which about 85% were accepted. The remaining 9000 events could not be processed due to data integrity issues.Fig. 7 a HLT stream rates and b bandwidth during an LHC fill in October 2015 with a peak luminosity of 4.5 × 10 33 cm -2 s -1 . Partial Event Building (partial EB) streams only store relevant subdetector data and thus have smaller event sizes. The other physics-related streams contain events with special readout settings and are used to overlay with MC events to simulate pile-up stream reprocessing, 330,000 events were successfully processed by the HLT of which about 85% were accepted. The remaining 9000 events could not be processed due to data integrity issues.</p>
        <p>The HLT processing time per event is mainly determined by the trigger menu and the number of pile-up interactions. The HLT farm CPU utilisation depends on the L1 trigger rate and the average HLT processing time. Figure 8 shows (a) the HLT processing time distribution for the highest luminosity run in 2015 with a peak luminosity of 5.2 × 10 33 cm -2 s -1 and (b) the average HLT processing time as a function of the instantaneous luminosity. At the highest luminosity point the average event processing time was approximately 235 ms. An L1 rate of 80 kHz corresponds to an average utilisation of 67% of a farm with 28,000 available CPU cores. About 40, 35 and 15% of the processing time are spent on inner detector tracking, muon spectrometer reconstruction and calorimeter reconstruction, respectively. The muon reconstruction time is dominated by the large rate of lowp T B-physics triggers. The increased processing time at low luminosities observed in Fig. 8b is due to additional triggers being enabled towards the end of an LHC fill to take advantage of the available CPU and bandwidth resources. Moreover, trigger prescale changes are made throughout the run giving rise to some of the observed features in the curve. The clearly visible scaling with luminosity is due to the pileup dependence of the processing time. It is also worth noting that the processing time cannot naively be scaled to higher luminosities as the trigger menu changes significantly in order to keep the L1 rate below or at 100 kHz.The HLT processing time per event is mainly determined by the trigger menu and the number of pile-up interactions. The HLT farm CPU utilisation depends on the L1 trigger rate and the average HLT processing time. Figure 8 shows (a) the HLT processing time distribution for the highest luminosity run in 2015 with a peak luminosity of 5.2 × 10 33 cm -2 s -1 and (b) the average HLT processing time as a function of the instantaneous luminosity. At the highest luminosity point the average event processing time was approximately 235 ms. An L1 rate of 80 kHz corresponds to an average utilisation of 67% of a farm with 28,000 available CPU cores. About 40, 35 and 15% of the processing time are spent on inner detector tracking, muon spectrometer reconstruction and calorimeter reconstruction, respectively. The muon reconstruction time is dominated by the large rate of lowp T B-physics triggers. The increased processing time at low luminosities observed in Fig. 8b is due to additional triggers being enabled towards the end of an LHC fill to take advantage of the available CPU and bandwidth resources. Moreover, trigger prescale changes are made throughout the run giving rise to some of the observed features in the curve. The clearly visible scaling with luminosity is due to the pileup dependence of the processing time. It is also worth noting that the processing time cannot naively be scaled to higher luminosities as the trigger menu changes significantly in order to keep the L1 rate below or at 100 kHz.</p>
        <p>Special trigger menus are used for particular data-taking conditions and can either be required for collecting a set of events for dedicated measurements or due to specific LHC bunch configurations. In the following, three examples of dedicated menus are given: menu for low number of bunches in the LHC, menu for collecting enhanced minimum-bias data for trigger rate predictions and menu during beam separation scans for luminosity calibration (van der Meer scans).Special trigger menus are used for particular data-taking conditions and can either be required for collecting a set of events for dedicated measurements or due to specific LHC bunch configurations. In the following, three examples of dedicated menus are given: menu for low number of bunches in the LHC, menu for collecting enhanced minimum-bias data for trigger rate predictions and menu during beam separation scans for luminosity calibration (van der Meer scans).</p>
        <p>When the LHC contains a low number of bunches (and thus few bunch trains), care is needed not to trigger at resonant frequencies that could damage the wire bonds of the IBL or SCT detectors, which reside in the magnetic field. The dangerous resonant frequencies are between 9 and 25 kHz for the IBL and above 100 kHz for the SCT detector. To avoid this risk, both detectors have implemented in the readout firmware a so-called fixed frequency veto that prevents triggers falling within a dangerous frequency range [23]. The IBL veto poses the most stringent limit on the acceptable L1 rate in this LHC configuration. In order to provide trigger menus appropriate to each LHC configuration during the startup phase, the trigger rate has been estimated after simulating the effect of the IBL veto. Figure 9 shows the simulated IBL rate limit for two different bunch configurations and the expected L1 trigger rate of the nominal physics trigger menu. At a low number of bunches the expected L1 trigger rate exceeds slightly the allowed L1 rate imposed by the IBL veto. In order not to veto important physics triggers, the required rate reduction was achieved by reducing the rate of supporting triggers.When the LHC contains a low number of bunches (and thus few bunch trains), care is needed not to trigger at resonant frequencies that could damage the wire bonds of the IBL or SCT detectors, which reside in the magnetic field. The dangerous resonant frequencies are between 9 and 25 kHz for the IBL and above 100 kHz for the SCT detector. To avoid this risk, both detectors have implemented in the readout firmware a so-called fixed frequency veto that prevents triggers falling within a dangerous frequency range [23]. The IBL veto poses the most stringent limit on the acceptable L1 rate in this LHC configuration. In order to provide trigger menus appropriate to each LHC configuration during the startup phase, the trigger rate has been estimated after simulating the effect of the IBL veto. Figure 9 shows the simulated IBL rate limit for two different bunch configurations and the expected L1 trigger rate of the nominal physics trigger menu. At a low number of bunches the expected L1 trigger rate exceeds slightly the allowed L1 rate imposed by the IBL veto. In order not to veto important physics triggers, the required rate reduction was achieved by reducing the rate of supporting triggers.</p>
        <p>Certain applications such as trigger algorithm development, rate predictions and validation require a data set that is The simulated rate limit is confirmed with experimental tests. The rate limit is higher for the 72-bunch train configuration since the bunches are more equally spread across the LHC ring. The rate limitation was only crucial for the low luminosity phase, where the required physics L1 rate was higher than the limit imposed by the IBL veto. The maximum number of colliding bunches in 2015 was 2232 minimally biased by the triggers used to select it. This special data set is collected using the enhanced minimum-bias trigger menu, which consists of all primary lowestp T L1 triggers with increasing p T threshold and a random trigger for very high cross-section processes. This trigger menu can be enabled in addition to the regular physics menu and records events at 300 Hz for a period of approximately one hour to obtain a data set of around one million events. Since the correlations between triggers are preserved, per-event weights can be calculated and used to convert the sample into a zerobias sample, which is used for trigger rate predictions during the development of new triggers [24]. This approach requires a much smaller total number of events than a true zero-bias data set.Certain applications such as trigger algorithm development, rate predictions and validation require a data set that is The simulated rate limit is confirmed with experimental tests. The rate limit is higher for the 72-bunch train configuration since the bunches are more equally spread across the LHC ring. The rate limitation was only crucial for the low luminosity phase, where the required physics L1 rate was higher than the limit imposed by the IBL veto. The maximum number of colliding bunches in 2015 was 2232 minimally biased by the triggers used to select it. This special data set is collected using the enhanced minimum-bias trigger menu, which consists of all primary lowestp T L1 triggers with increasing p T threshold and a random trigger for very high cross-section processes. This trigger menu can be enabled in addition to the regular physics menu and records events at 300 Hz for a period of approximately one hour to obtain a data set of around one million events. Since the correlations between triggers are preserved, per-event weights can be calculated and used to convert the sample into a zerobias sample, which is used for trigger rate predictions during the development of new triggers [24]. This approach requires a much smaller total number of events than a true zero-bias data set.</p>
        <p>During van der Meer scans [25], which are performed by the LHC to allow the experiments to calibrate their luminosity measurements, a dedicated trigger menu is used. ATLAS uses several luminosity algorithms (see Ref. [26]) amongst which one relies on counting tracks in the ID. Since the different LHC bunches do not have the exact same proton density, it is beneficial to sample a few bunches at the maximum possible rate. For this purpose, a minimum-bias trigger selects events for specific LHC bunches and uses partial event building to read out only the ID data at about 5 kHz for five different LHC bunches.During van der Meer scans [25], which are performed by the LHC to allow the experiments to calibrate their luminosity measurements, a dedicated trigger menu is used. ATLAS uses several luminosity algorithms (see Ref. [26]) amongst which one relies on counting tracks in the ID. Since the different LHC bunches do not have the exact same proton density, it is beneficial to sample a few bunches at the maximum possible rate. For this purpose, a minimum-bias trigger selects events for specific LHC bunches and uses partial event building to read out only the ID data at about 5 kHz for five different LHC bunches.</p>
        <p>After L1 trigger acceptance, the events are processed by the HLT using finer-granularity calorimeter information, precision measurements from the MS and tracking information from the ID, which are not available at L1. As needed, the HLT reconstruction can either be executed within RoIs identified at L1 or for the full detector. In both cases the data is retrieved on demand from the readout system. As in Run 1, in order to reduce the processing time, most HLT triggers use a two-stage approach with a fast first-pass reconstruction to reject the majority of events and a slower precision reconstruction for the remaining events. However, with the merging of the previously separate L2 and EF farms, there is no longer a fixed bandwidth or rate limitation between the two steps. The following sections describe the main reconstruction algorithms used in the HLT for inner detector, calorimeter and muon reconstruction.After L1 trigger acceptance, the events are processed by the HLT using finer-granularity calorimeter information, precision measurements from the MS and tracking information from the ID, which are not available at L1. As needed, the HLT reconstruction can either be executed within RoIs identified at L1 or for the full detector. In both cases the data is retrieved on demand from the readout system. As in Run 1, in order to reduce the processing time, most HLT triggers use a two-stage approach with a fast first-pass reconstruction to reject the majority of events and a slower precision reconstruction for the remaining events. However, with the merging of the previously separate L2 and EF farms, there is no longer a fixed bandwidth or rate limitation between the two steps. The following sections describe the main reconstruction algorithms used in the HLT for inner detector, calorimeter and muon reconstruction.</p>
        <p>For Run 1 the ID tracking in the trigger consisted of custom tracking algorithms at L2 and offline tracking algorithms adapted for running in the EF. The ID trigger was redesigned for Run 2 to take advantage of the merged HLT and include information from the IBL. The latter significantly improves the tracking performance and in particular the impact parameter resolution [7]. In addition, provision was made for the inclusion of FTK tracks once that system becomes available later in Run 2.For Run 1 the ID tracking in the trigger consisted of custom tracking algorithms at L2 and offline tracking algorithms adapted for running in the EF. The ID trigger was redesigned for Run 2 to take advantage of the merged HLT and include information from the IBL. The latter significantly improves the tracking performance and in particular the impact parameter resolution [7]. In addition, provision was made for the inclusion of FTK tracks once that system becomes available later in Run 2.</p>
        <p>The tracking trigger is subdivided into fast tracking and precision tracking stages. The fast tracking consists of triggerspecific pattern recognition algorithms very similar to those used at L2 during Run 1, whereas the precision stage relies heavily on offline tracking algorithms. Despite similar naming the fast tracking as described here is not related to the FTK hardware tracking that will only become available during 2017. The tracking algorithms are typically configured to run within an RoI identified by L1. The offline tracking was reimplemented in LS1 to run three times faster than in Run 1, making it more suitable to use in the HLT. To reduce CPU usage even further, the offline track-finding is seeded by tracks and space-points identified by the fast tracking stage.The tracking trigger is subdivided into fast tracking and precision tracking stages. The fast tracking consists of triggerspecific pattern recognition algorithms very similar to those used at L2 during Run 1, whereas the precision stage relies heavily on offline tracking algorithms. Despite similar naming the fast tracking as described here is not related to the FTK hardware tracking that will only become available during 2017. The tracking algorithms are typically configured to run within an RoI identified by L1. The offline tracking was reimplemented in LS1 to run three times faster than in Run 1, making it more suitable to use in the HLT. To reduce CPU usage even further, the offline track-finding is seeded by tracks and space-points identified by the fast tracking stage.</p>
        <p>The tracking efficiency with respect to offline tracks has been determined for electrons and muons. The reconstructed tracks are required to have at least two (six) pixel (SCT) clusters and lie in the region |η| &lt; 2.5. The closest trigger track within a cone of size R = ( η) 2 + ( φ) 2 = 0.05 of the offline reconstructed track is selected as the matching trigger track.The tracking efficiency with respect to offline tracks has been determined for electrons and muons. The reconstructed tracks are required to have at least two (six) pixel (SCT) clusters and lie in the region |η| &lt; 2.5. The closest trigger track within a cone of size R = ( η) 2 + ( φ) 2 = 0.05 of the offline reconstructed track is selected as the matching trigger track.</p>
        <p>Figure 10 shows the tracking efficiency for the 24 GeV medium electron trigger (see Sect. 6.2) as a function of the η and of the p T of the offline track. The tracking efficiency is measured with respect to offline tracks with p T &gt; 20 GeV for tight offline electron candidates from the 24 GeV electron support trigger, which does not use the trigger tracks in the selection, but is otherwise identical to the physics trigger. The efficiencies of the fast track finder and precision tracking exceed 99% for all pseudorapidities. There is a small efficiency loss at low p T due to bremsstrahlung energy loss by electrons.Figure 10 shows the tracking efficiency for the 24 GeV medium electron trigger (see Sect. 6.2) as a function of the η and of the p T of the offline track. The tracking efficiency is measured with respect to offline tracks with p T &gt; 20 GeV for tight offline electron candidates from the 24 GeV electron support trigger, which does not use the trigger tracks in the selection, but is otherwise identical to the physics trigger. The efficiencies of the fast track finder and precision tracking exceed 99% for all pseudorapidities. There is a small efficiency loss at low p T due to bremsstrahlung energy loss by electrons.</p>
        <p>Figure 11a shows the tracking performance of the ID trigger for muons with respect to loose offline muon candidates with p T &gt; 6 GeV selected by the 6 GeV muon support trigger as a function of the offline muon transverse momentum. The efficiency is significantly better than 99% for all p T for both the fast and precision tracking. Shown in Fig. 11b is the resolution of the transverse track impact parameter with respect to offline as a function of the offline muon p T . The resolution in the fast (precision) tracking is better than 17 µm (15 µm) for muon candidates with offline p T &gt; 20 GeV.Figure 11a shows the tracking performance of the ID trigger for muons with respect to loose offline muon candidates with p T &gt; 6 GeV selected by the 6 GeV muon support trigger as a function of the offline muon transverse momentum. The efficiency is significantly better than 99% for all p T for both the fast and precision tracking. Shown in Fig. 11b is the resolution of the transverse track impact parameter with respect to offline as a function of the offline muon p T . The resolution in the fast (precision) tracking is better than 17 µm (15 µm) for muon candidates with offline p T &gt; 20 GeV.</p>
        <p>For the hadronic tau and b-jet triggers, tracking is run in a larger RoI than for electrons or muons. To limit CPU usage, multiple stage track reconstruction was implemented.For the hadronic tau and b-jet triggers, tracking is run in a larger RoI than for electrons or muons. To limit CPU usage, multiple stage track reconstruction was implemented.</p>
        <p>A two-stage processing approach was implemented for the hadronic tau trigger. First, the leading track and its position along the beamline are determined by executing fast tracking in an RoI that is fully extended along the beam- (See the blue-shaded region in Fig. 12.) Using this position along the beamline, the second stage reconstructs all tracks in an RoI that is larger (0.4) in both η and φ but limited to | z| &lt; 10 mm with respect to the leading track. (See the green shaded region in Fig. 12.) At this second stage, fast tracking is followed by precision tracking. For evaluation purposes, the tau lepton signatures can also be executed in a single-stage mode, running the fast track finder followed by the precision tracking in an RoI of the full extent along the beam line and in eta and phi.A two-stage processing approach was implemented for the hadronic tau trigger. First, the leading track and its position along the beamline are determined by executing fast tracking in an RoI that is fully extended along the beam- (See the blue-shaded region in Fig. 12.) Using this position along the beamline, the second stage reconstructs all tracks in an RoI that is larger (0.4) in both η and φ but limited to | z| &lt; 10 mm with respect to the leading track. (See the green shaded region in Fig. 12.) At this second stage, fast tracking is followed by precision tracking. For evaluation purposes, the tau lepton signatures can also be executed in a single-stage mode, running the fast track finder followed by the precision tracking in an RoI of the full extent along the beam line and in eta and phi.</p>
        <p>Figure 13 shows the performance of the tau two-stage tracking with respect to the offline tau tracking for tracks with p T &gt; 1 GeV originating from decays of offline tau lepton candidates with p T &gt; 25 GeV, but with very loose track matching in R to the offline tau candidate. Figure 13a shows the efficiency of the fast tracking from the first and second stages, together with the efficiency of the precision tracking for the second stage. The second-stage tracking efficiency is higher than 96% everywhere, and improves to better than 99% for tracks with p T &gt; 2 GeV. The efficiency of the firststage fast tracking has a slower turn-on, rising from 94% at 2 GeV to better than 99% for p T &gt; 5 GeV. This slow turn-on arises due to the narrow width ( φ &lt; 0.1) of the first-stage RoI and the loose tau selection that results in a larger fraction of lowp T tracks from tau candidates that bend out of the RoI (and are not reconstructed) compared to a wider RoI. The transverse impact parameter resolution with respect to offline for loosely matched tracks is seen in Fig. 13b and is around 20 µm for tracks with p T &gt; 10 GeV reconstructed by the precision tracking. The tau selection algorithms based on this two-stage tracking are presented in Sect. 6.5.1.Figure 13 shows the performance of the tau two-stage tracking with respect to the offline tau tracking for tracks with p T &gt; 1 GeV originating from decays of offline tau lepton candidates with p T &gt; 25 GeV, but with very loose track matching in R to the offline tau candidate. Figure 13a shows the efficiency of the fast tracking from the first and second stages, together with the efficiency of the precision tracking for the second stage. The second-stage tracking efficiency is higher than 96% everywhere, and improves to better than 99% for tracks with p T &gt; 2 GeV. The efficiency of the firststage fast tracking has a slower turn-on, rising from 94% at 2 GeV to better than 99% for p T &gt; 5 GeV. This slow turn-on arises due to the narrow width ( φ &lt; 0.1) of the first-stage RoI and the loose tau selection that results in a larger fraction of lowp T tracks from tau candidates that bend out of the RoI (and are not reconstructed) compared to a wider RoI. The transverse impact parameter resolution with respect to offline for loosely matched tracks is seen in Fig. 13b and is around 20 µm for tracks with p T &gt; 10 GeV reconstructed by the precision tracking. The tau selection algorithms based on this two-stage tracking are presented in Sect. 6.5.1.</p>
        <p>For b-jet tracking a similar multi-stage tracking strategy was adopted. However, in this case the first-stage vertex tracking takes all jets identified by the jet trigger with E T &gt; 30 GeV and reconstructs tracks with the fast track finder in a narrow region in η and φ around the jet axis for each jet, but with |z| &lt; 225 mm along the beam line. Following this step, the primary vertex reconstruction [27] is performed using the tracks from the fast tracking stage. This vertex is used to define wider RoIs around the jet axes, with | η| &lt; 0.4 and | φ| &lt; 0.4 but with | z| &lt; 20 mm relative to the primary vertex z position. These RoIs are then used for the second-stage reconstruction that runs the fast track finder in the wider η and φ regions followed by the precision tracking, secondary vertexing and b-tagging algorithms.For b-jet tracking a similar multi-stage tracking strategy was adopted. However, in this case the first-stage vertex tracking takes all jets identified by the jet trigger with E T &gt; 30 GeV and reconstructs tracks with the fast track finder in a narrow region in η and φ around the jet axis for each jet, but with |z| &lt; 225 mm along the beam line. Following this step, the primary vertex reconstruction [27] is performed using the tracks from the fast tracking stage. This vertex is used to define wider RoIs around the jet axes, with | η| &lt; 0.4 and | φ| &lt; 0.4 but with | z| &lt; 20 mm relative to the primary vertex z position. These RoIs are then used for the second-stage reconstruction that runs the fast track finder in the wider η and φ regions followed by the precision tracking, secondary vertexing and b-tagging algorithms.</p>
        <p>The performance of the primary vertexing in the b-jet vertex tracking can be seen in Fig. 14a, which shows the vertex finding efficiency with respect to offline vertices in jet events with at least one jet with transverse energy above 55, 110, or 260 GeV and with no additional b-tagging requirement. The efficiency is shown as a function of the number of offline tracks with p T &gt; 1 GeV that lie within the boundary of the wider RoI (defined above) from the selected jets. The efficiency rises sharply and is above 90% for vertices with three or more tracks, and rises to more than 99.5% for vertices with five or more tracks. The resolution in z with respect to the offline z position as shown in Fig. 14b is better than 100 µm for vertices with two or more offline tracks and improves to 60 µm for vertices with ten or more offline tracks. 15 The CPU processing time for the fast and precision tracking per electron RoI for the 24 GeV electron trigger. The precision tracking is seeded by the tracks found in the fast tracking stage and hence requires less CPU timeThe performance of the primary vertexing in the b-jet vertex tracking can be seen in Fig. 14a, which shows the vertex finding efficiency with respect to offline vertices in jet events with at least one jet with transverse energy above 55, 110, or 260 GeV and with no additional b-tagging requirement. The efficiency is shown as a function of the number of offline tracks with p T &gt; 1 GeV that lie within the boundary of the wider RoI (defined above) from the selected jets. The efficiency rises sharply and is above 90% for vertices with three or more tracks, and rises to more than 99.5% for vertices with five or more tracks. The resolution in z with respect to the offline z position as shown in Fig. 14b is better than 100 µm for vertices with two or more offline tracks and improves to 60 µm for vertices with ten or more offline tracks. 15 The CPU processing time for the fast and precision tracking per electron RoI for the 24 GeV electron trigger. The precision tracking is seeded by the tracks found in the fast tracking stage and hence requires less CPU time</p>
        <p>The timing of the fast tracking and precision tracking stages of the electron trigger executed per RoI can be seen in Fig. 15 for events passing the 24 GeV electron trigger. The fast tracking takes on average 6.2 ms per RoI with a tail at the per-mille level at around 60 ms. The precision tracking execution time has a mean of 2.5 ms and a tail at the per-mille level of around 20 ms. The precision tracking is seeded by the tracks found in the fast tracking stage and hence requires less CPU time.The timing of the fast tracking and precision tracking stages of the electron trigger executed per RoI can be seen in Fig. 15 for events passing the 24 GeV electron trigger. The fast tracking takes on average 6.2 ms per RoI with a tail at the per-mille level at around 60 ms. The precision tracking execution time has a mean of 2.5 ms and a tail at the per-mille level of around 20 ms. The precision tracking is seeded by the tracks found in the fast tracking stage and hence requires less CPU time.</p>
        <p>The time taken by the tau tracking in both the singlestage and two-stage variants is shown in Fig. 16. Figure 16a shows the processing times per RoI for fast tracking stages: individually for the first and second stages of the two-stage tracking, and separately for the single-stage tracking with the wider RoI in η, φ and z. The fast tracking in the single-stage tracking has a mean execution time of approximately 66 ms, with a very long tail. In contrast, the first-stage tracking with an RoI that is wide only in the z direction has a mean execution time of 23 ms, driven predominantly by the narrower RoI width in φ. The second-stage tracking, although wider in η and φ, takes only 21 ms on average because of the significant reduction in the RoI z-width along the beam line. Figure 16b shows a comparison of the processing time per RoI for the precision tracking. The two-stage tracking executes faster, with a mean of 4.8 ms compared to 12 ms for the single-stage tracking. Again, this is due to the reduction in the number of tracks to be processed from the tighter selection in z along the beam line.The time taken by the tau tracking in both the singlestage and two-stage variants is shown in Fig. 16. Figure 16a shows the processing times per RoI for fast tracking stages: individually for the first and second stages of the two-stage tracking, and separately for the single-stage tracking with the wider RoI in η, φ and z. The fast tracking in the single-stage tracking has a mean execution time of approximately 66 ms, with a very long tail. In contrast, the first-stage tracking with an RoI that is wide only in the z direction has a mean execution time of 23 ms, driven predominantly by the narrower RoI width in φ. The second-stage tracking, although wider in η and φ, takes only 21 ms on average because of the significant reduction in the RoI z-width along the beam line. Figure 16b shows a comparison of the processing time per RoI for the precision tracking. The two-stage tracking executes faster, with a mean of 4.8 ms compared to 12 ms for the single-stage tracking. Again, this is due to the reduction in the number of tracks to be processed from the tighter selection in z along the beam line.</p>
        <p>A series of reconstruction algorithms are used to convert signals from the calorimeter readout into objects, specifically cells and clusters, that then serve as input to the reconstruction of electron, photon, tau, and jet candidates and the reconstruction of E miss T . These cells and clusters are also used in the determination of the shower shapes and the isolation properties of candidate particles (including muons), both of which are later used as discriminants for particle identification and the rejection of backgrounds. The reconstruction algorithms used in the HLT have access to full detector granularity and thus allow improved accuracy and precision in energy and position measurements with respect to L1.A series of reconstruction algorithms are used to convert signals from the calorimeter readout into objects, specifically cells and clusters, that then serve as input to the reconstruction of electron, photon, tau, and jet candidates and the reconstruction of E miss T . These cells and clusters are also used in the determination of the shower shapes and the isolation properties of candidate particles (including muons), both of which are later used as discriminants for particle identification and the rejection of backgrounds. The reconstruction algorithms used in the HLT have access to full detector granularity and thus allow improved accuracy and precision in energy and position measurements with respect to L1.</p>
        <p>The first stage in the reconstruction involves unpacking the data from the calorimeter. The unpacking can be done in two different ways: either by unpacking only the data from within the RoIs identified at L1 or by unpacking the data from the full calorimeter. The RoI-based approach is used for well- 16 The ID trigger tau tracking processing time for a the fast track finder and b the precision tracking comparing the single-stage and two-stage tracking approach separated objects (e.g. electron, photon, muon, tau), whereas the full calorimeter reconstruction is used for jets and global event quantities (e.g. E miss T ). In both cases the raw unpacked data is then converted into a collection of cells. Two different clustering algorithms are used to reconstruct the clusters of energy deposited in the calorimeter, the sliding-window and the topo-clustering algorithms [28]. While the latter provides performance closer to the offline reconstruction, it is also significantly slower (see Sect. 5.2.3).The first stage in the reconstruction involves unpacking the data from the calorimeter. The unpacking can be done in two different ways: either by unpacking only the data from within the RoIs identified at L1 or by unpacking the data from the full calorimeter. The RoI-based approach is used for well- 16 The ID trigger tau tracking processing time for a the fast track finder and b the precision tracking comparing the single-stage and two-stage tracking approach separated objects (e.g. electron, photon, muon, tau), whereas the full calorimeter reconstruction is used for jets and global event quantities (e.g. E miss T ). In both cases the raw unpacked data is then converted into a collection of cells. Two different clustering algorithms are used to reconstruct the clusters of energy deposited in the calorimeter, the sliding-window and the topo-clustering algorithms [28]. While the latter provides performance closer to the offline reconstruction, it is also significantly slower (see Sect. 5.2.3).</p>
        <p>The sliding-window algorithm operates on a grid in which the cells are divided into projective towers. The algorithm scans this grid and positions the window in such a way that the transverse energy contained within the window is the local maximum. If this local maximum is above a given threshold, a cluster is formed by summing the cells within a rectangular clustering window. For each layer the barycentre of the cells within that layer is determined, and then all cells within a fixed window around that position are included in the cluster. Although the size of the clustering window is fixed, the central position of the window may vary slightly at each calorimeter layer, depending on how the cell energies are distributed within them.The sliding-window algorithm operates on a grid in which the cells are divided into projective towers. The algorithm scans this grid and positions the window in such a way that the transverse energy contained within the window is the local maximum. If this local maximum is above a given threshold, a cluster is formed by summing the cells within a rectangular clustering window. For each layer the barycentre of the cells within that layer is determined, and then all cells within a fixed window around that position are included in the cluster. Although the size of the clustering window is fixed, the central position of the window may vary slightly at each calorimeter layer, depending on how the cell energies are distributed within them.</p>
        <p>The topo-clustering algorithm begins with a seed cell and iteratively adds neighbouring cells to the cluster if their energies are above a given energy threshold that is a function of the expected root-mean-square (RMS) noise (σ ). The seed cells are first identified as those cells that have energies greater than 4σ . All neighbouring cells with energies greater than 2σ are then added to the cluster and, finally, all the remaining neighbours to these cells are also added. Unlike the sliding-window clusters, the topo-clusters have no predefined shape, and consequently their size can vary from cluster to cluster.The topo-clustering algorithm begins with a seed cell and iteratively adds neighbouring cells to the cluster if their energies are above a given energy threshold that is a function of the expected root-mean-square (RMS) noise (σ ). The seed cells are first identified as those cells that have energies greater than 4σ . All neighbouring cells with energies greater than 2σ are then added to the cluster and, finally, all the remaining neighbours to these cells are also added. Unlike the sliding-window clusters, the topo-clusters have no predefined shape, and consequently their size can vary from cluster to cluster.</p>
        <p>The reconstruction of candidate electrons and photons uses the sliding-window algorithm with rectangular clustering windows of size η × φ = 0.075 × 0.175 in the barrel and 0.125 × 0.125 in the end-caps. Since the magnetic field bends the electron trajectory in the φ direction, the size of the window is larger in that coordinate in order to contain most of the energy. The reconstruction of candidate taus and jets and the reconstruction of E miss T all use the topo-clustering algorithm. For taus the topo-clustering uses a window of 0.8 × 0.8 around each of the tau RoIs identified at L1. For jets and E miss T , the topo-clustering is done for the full calorimeter. In addition, the E miss T is also determined based on the cell energies across the full calorimeter (see Sect. 6.6).The reconstruction of candidate electrons and photons uses the sliding-window algorithm with rectangular clustering windows of size η × φ = 0.075 × 0.175 in the barrel and 0.125 × 0.125 in the end-caps. Since the magnetic field bends the electron trajectory in the φ direction, the size of the window is larger in that coordinate in order to contain most of the energy. The reconstruction of candidate taus and jets and the reconstruction of E miss T all use the topo-clustering algorithm. For taus the topo-clustering uses a window of 0.8 × 0.8 around each of the tau RoIs identified at L1. For jets and E miss T , the topo-clustering is done for the full calorimeter. In addition, the E miss T is also determined based on the cell energies across the full calorimeter (see Sect. 6.6).</p>
        <p>The harmonisation between the online and offline algorithms in Run 2 means that the online calorimeter performance is now much closer to the offline performance. The E T resolutions of the sliding-window clusters and the topo-clusters with respect to their offline counterparts are shown in Fig. 17. The E T resolution of the sliding-window clusters is 3% for clusters above 5 GeV, while the E T resolution of the topoclustering algorithm is 2% for clusters above 10 GeV. The slight shift in cell energies between the HLT and offline is due to the fact that out-of-time pile-up effects were not corrected in the online reconstruction, resulting in slightly higher reconstructed cell energies in the HLT (this was changed for 2016). In addition, the topo-cluster based reconstruction shown in Fig. 17b suffered from a mismatch of some calibration constants between online and offline during most of 2015, resulting in a shift towards lower HLT cell energies. Fig. 18 The distributions of processing times for the topo-clustering algorithm executed a within an RoI and b on the full calorimeter. The processing times within an RoI are obtained from tau RoIs with a size of η × φ = 0.8 × 0.8The harmonisation between the online and offline algorithms in Run 2 means that the online calorimeter performance is now much closer to the offline performance. The E T resolutions of the sliding-window clusters and the topo-clusters with respect to their offline counterparts are shown in Fig. 17. The E T resolution of the sliding-window clusters is 3% for clusters above 5 GeV, while the E T resolution of the topoclustering algorithm is 2% for clusters above 10 GeV. The slight shift in cell energies between the HLT and offline is due to the fact that out-of-time pile-up effects were not corrected in the online reconstruction, resulting in slightly higher reconstructed cell energies in the HLT (this was changed for 2016). In addition, the topo-cluster based reconstruction shown in Fig. 17b suffered from a mismatch of some calibration constants between online and offline during most of 2015, resulting in a shift towards lower HLT cell energies. Fig. 18 The distributions of processing times for the topo-clustering algorithm executed a within an RoI and b on the full calorimeter. The processing times within an RoI are obtained from tau RoIs with a size of η × φ = 0.8 × 0.8</p>
        <p>Due to the optimisation of the offline clustering algorithms during LS1, offline clustering algorithms can be used in the HLT directly after the L1 selection. At the data preparation stage, a specially optimised infrastructure with a memory caching mechanism allows very fast unpacking of data, even from the full calorimeter, which comprises approximately 187,000 cells. The mean processing time for the data preparation stage is 2 ms per RoI and 20 ms for the full calorimeter, and both are roughly independent of pile-up. The topoclustering, however, requires a fixed estimate of the expected pile-up noise (cell energy contributions from pile-up interactions) in order to determine the cluster-building thresholds and, when there is a discrepancy between the expected pileup noise and the actual pile-up noise, the processing time can show some dependence on the pile-up conditions. The mean processing time for the topo-clustering is 6 ms per RoI and 82 ms for the full calorimeter. The distributions of the topoclustering processing times are shown in Fig. 18a for an RoI and Fig. 18b for the full calorimeter. The RoI-based topoclustering can run multiple times if there is more than one RoI per event. The topo-clustering over the full calorimeter runs at most once per event, even if the event satisfied both jet and E miss T selections at L1. The mean processing time of the sliding window clustering algorithm is not shown but is typically less than 2.5 ms per RoI.Due to the optimisation of the offline clustering algorithms during LS1, offline clustering algorithms can be used in the HLT directly after the L1 selection. At the data preparation stage, a specially optimised infrastructure with a memory caching mechanism allows very fast unpacking of data, even from the full calorimeter, which comprises approximately 187,000 cells. The mean processing time for the data preparation stage is 2 ms per RoI and 20 ms for the full calorimeter, and both are roughly independent of pile-up. The topoclustering, however, requires a fixed estimate of the expected pile-up noise (cell energy contributions from pile-up interactions) in order to determine the cluster-building thresholds and, when there is a discrepancy between the expected pileup noise and the actual pile-up noise, the processing time can show some dependence on the pile-up conditions. The mean processing time for the topo-clustering is 6 ms per RoI and 82 ms for the full calorimeter. The distributions of the topoclustering processing times are shown in Fig. 18a for an RoI and Fig. 18b for the full calorimeter. The RoI-based topoclustering can run multiple times if there is more than one RoI per event. The topo-clustering over the full calorimeter runs at most once per event, even if the event satisfied both jet and E miss T selections at L1. The mean processing time of the sliding window clustering algorithm is not shown but is typically less than 2.5 ms per RoI.</p>
        <p>Muons are identified at the L1 trigger by the spatial and temporal coincidence of hits either in the RPC or TGC chambers within the rapidity range of |η| &lt; 2.4. The degree of deviation from the hit pattern expected for a muon with infinite momentum is used to estimate the p T of the muon with six possible thresholds. The HLT receives this information together with the RoI position and makes use of the precision MDT and CSC chambers to further refine the L1 muon candidates.Muons are identified at the L1 trigger by the spatial and temporal coincidence of hits either in the RPC or TGC chambers within the rapidity range of |η| &lt; 2.4. The degree of deviation from the hit pattern expected for a muon with infinite momentum is used to estimate the p T of the muon with six possible thresholds. The HLT receives this information together with the RoI position and makes use of the precision MDT and CSC chambers to further refine the L1 muon candidates.</p>
        <p>The HLT muon reconstruction is split into fast (trigger specific) and precision (close to offline) reconstruction stages, which were used during Run 1 at L2 and EF, respectively.The HLT muon reconstruction is split into fast (trigger specific) and precision (close to offline) reconstruction stages, which were used during Run 1 at L2 and EF, respectively.</p>
        <p>In the fast reconstruction stage, each L1 muon candidate is refined by including the precision data from the MDT chambers in the RoI defined by the L1 candidate. A track fit is performed using the MDT drift times and positions, and a p T measurement is assigned using lookup tables, creating MS-only muon candidates. The MS-only muon track is backextrapolated to the interaction point using the offline track extrapolator (based on a detailed detector description instead of the lookup-table-based approach used in Run 1) and combined with tracks reconstructed in the ID to form a combined muon candidate with refined track parameter resolution.In the fast reconstruction stage, each L1 muon candidate is refined by including the precision data from the MDT chambers in the RoI defined by the L1 candidate. A track fit is performed using the MDT drift times and positions, and a p T measurement is assigned using lookup tables, creating MS-only muon candidates. The MS-only muon track is backextrapolated to the interaction point using the offline track extrapolator (based on a detailed detector description instead of the lookup-table-based approach used in Run 1) and combined with tracks reconstructed in the ID to form a combined muon candidate with refined track parameter resolution.</p>
        <p>In the precision reconstruction stage, the muon reconstruction starts from the refined RoIs identified by the fast stage, reconstructing segments and tracks using information from the trigger and precision chambers. As in the fast stage, muon candidates are first formed by using the muon detectors (MSonly) and are subsequently combined with ID tracks leading to combined muons. If no matching ID track can be found, combined muon candidates are searched for by extrapolating ID tracks to the MS. This latter inside-out approach is slower The combined muon candidates are used for the majority of the muon triggers. However, MS-only candidates are used for specialised triggers that cannot rely on the existence of an ID track, e.g. triggers for long-lived particles that decay within the ID volume.In the precision reconstruction stage, the muon reconstruction starts from the refined RoIs identified by the fast stage, reconstructing segments and tracks using information from the trigger and precision chambers. As in the fast stage, muon candidates are first formed by using the muon detectors (MSonly) and are subsequently combined with ID tracks leading to combined muons. If no matching ID track can be found, combined muon candidates are searched for by extrapolating ID tracks to the MS. This latter inside-out approach is slower The combined muon candidates are used for the majority of the muon triggers. However, MS-only candidates are used for specialised triggers that cannot rely on the existence of an ID track, e.g. triggers for long-lived particles that decay within the ID volume.</p>
        <p>Comparisons between online and offline muon track parameters using Z → μμ candidate events are presented in this section while muon trigger efficiencies are described in Sect. 6.3. Distributions of the residuals between online and offline track parameters (1/ p T , η and φ) are constructed in bins of p T and two subsequent Gaussian fits are performed on the core of the distribution to extract the widths, σ , of the residual distributions as a function of p T . The inversep T residual widths, σ ((1/ p online T -1/ p offline T )/(1/ p offline T )), are shown in Fig. 19 as a function of the offline muon p T for the precision MSonly and precision combined reconstruction. The resolution for combined muons is better than the resolution for MS-only muons due to the higher precision of the ID track measurements, especially at low p T . As the tracks become closer to straight lines at high p T , it becomes more difficult to precisely measure the p T of both the MS and ID tracks, and hence the resolution degrades. The p T resolution for lowp T MS-only muons is degraded when muons in the barrel are bent out of the detector before traversing the entire muon spectrometer. The resolution is generally better in the barrel than in the end-caps due to the difference in detector granularity. The η residual widths, σ (η online -η offline ), and φ residual widths, σ (φ online -φ offline ), are shown as a function of p T in Fig. 20 for both the MS-only and combined algorithms. As the trajectories are straighter at high p T , the precision of their position improves and so the spatial resolution decreases with p T . Good agreement between track parameters calculated online and offline is observed.Comparisons between online and offline muon track parameters using Z → μμ candidate events are presented in this section while muon trigger efficiencies are described in Sect. 6.3. Distributions of the residuals between online and offline track parameters (1/ p T , η and φ) are constructed in bins of p T and two subsequent Gaussian fits are performed on the core of the distribution to extract the widths, σ , of the residual distributions as a function of p T . The inversep T residual widths, σ ((1/ p online T -1/ p offline T )/(1/ p offline T )), are shown in Fig. 19 as a function of the offline muon p T for the precision MSonly and precision combined reconstruction. The resolution for combined muons is better than the resolution for MS-only muons due to the higher precision of the ID track measurements, especially at low p T . As the tracks become closer to straight lines at high p T , it becomes more difficult to precisely measure the p T of both the MS and ID tracks, and hence the resolution degrades. The p T resolution for lowp T MS-only muons is degraded when muons in the barrel are bent out of the detector before traversing the entire muon spectrometer. The resolution is generally better in the barrel than in the end-caps due to the difference in detector granularity. The η residual widths, σ (η online -η offline ), and φ residual widths, σ (φ online -φ offline ), are shown as a function of p T in Fig. 20 for both the MS-only and combined algorithms. As the trajectories are straighter at high p T , the precision of their position improves and so the spatial resolution decreases with p T . Good agreement between track parameters calculated online and offline is observed.</p>
        <p>Figure 21 shows the processing times per RoI for the (a) fast MS-only and fast combined algorithms and (b) precision muon algorithm. The large time difference between the fast and precision algorithms, with the precision reconstruction using too much time to be run by itself at the full L1 muon trigger rate, motivates the need for a two-stage reconstruction. 6 Trigger signature performanceFigure 21 shows the processing times per RoI for the (a) fast MS-only and fast combined algorithms and (b) precision muon algorithm. The large time difference between the fast and precision algorithms, with the precision reconstruction using too much time to be run by itself at the full L1 muon trigger rate, motivates the need for a two-stage reconstruction. 6 Trigger signature performance</p>
        <p>The following sections describe the different selection criteria placed upon the reconstructed objects described in Sect. 5 in order to form individual trigger signatures that identify leptons, hadrons, and global event quantities such as E miss T . For each case the primary triggers used during 2015 are listed together with their output rate and performance. Where possible the trigger efficiency measured in data is compared with MC simulation. The following methods are used to derive an unbiased measurement of the trigger efficiency:The following sections describe the different selection criteria placed upon the reconstructed objects described in Sect. 5 in order to form individual trigger signatures that identify leptons, hadrons, and global event quantities such as E miss T . For each case the primary triggers used during 2015 are listed together with their output rate and performance. Where possible the trigger efficiency measured in data is compared with MC simulation. The following methods are used to derive an unbiased measurement of the trigger efficiency:</p>
        <p>• Tag-and-probe method, which uses a sample of offlineselected events that contain a pair of related objects reconstructed offline, such as electrons from a Z → ee decay, where one has triggered the event and the other one is used to measure the trigger efficiency; • Bootstrap method, where the efficiency of a higher trigger threshold is determined using events triggered by a lower threshold.• Tag-and-probe method, which uses a sample of offlineselected events that contain a pair of related objects reconstructed offline, such as electrons from a Z → ee decay, where one has triggered the event and the other one is used to measure the trigger efficiency; • Bootstrap method, where the efficiency of a higher trigger threshold is determined using events triggered by a lower threshold.</p>
        <p>Trigger efficiencies are computed with respect to an offline-selected data sample. The ratio of the measured trigger efficiency to the simulated one is used as a correction factor in physics analyses. Unless otherwise specified, performance studies use good-quality data corresponding to an integrated luminosity of 3.2 fb -1 collected during 2015 with a bunch-spacing of 25 ns. Trigger rates shown in the following sections are usually extracted from multiple datataking runs to cover the maximum range in instantaneous luminosity. Due to different beam and detector conditions between runs, this can result in slightly different trigger rates for nearby luminosity values.Trigger efficiencies are computed with respect to an offline-selected data sample. The ratio of the measured trigger efficiency to the simulated one is used as a correction factor in physics analyses. Unless otherwise specified, performance studies use good-quality data corresponding to an integrated luminosity of 3.2 fb -1 collected during 2015 with a bunch-spacing of 25 ns. Trigger rates shown in the following sections are usually extracted from multiple datataking runs to cover the maximum range in instantaneous luminosity. Due to different beam and detector conditions between runs, this can result in slightly different trigger rates for nearby luminosity values.</p>
        <p>Studies of the total cross-section, hadronisation, diffraction, hadrons containing strange quarks and other nonperturbative properties of pp interactions require the use of a high-efficiency trigger for selecting all inelastic interactions that result in particle production within the detector. The MBTS minimum-bias trigger is highly efficient, even for events containing only two charged particles with p T &gt; 100 MeV and |η| &lt; 2.5.Studies of the total cross-section, hadronisation, diffraction, hadrons containing strange quarks and other nonperturbative properties of pp interactions require the use of a high-efficiency trigger for selecting all inelastic interactions that result in particle production within the detector. The MBTS minimum-bias trigger is highly efficient, even for events containing only two charged particles with p T &gt; 100 MeV and |η| &lt; 2.5.</p>
        <p>The primary minimum-bias and high-multiplicity data set at √ s = 13 TeV was recorded in June 2015. The average pile-up μ varied between 0.003 and 0.03, and the interaction rate had a maximum of about 15 kHz. More than 200 million interactions were recorded during a one-week datataking period. Most of the readout bandwidth was dedicated to the loosest L1_MBTS_1 trigger (described below) recording events at 1.0 to 1.5 kHz on average.The primary minimum-bias and high-multiplicity data set at √ s = 13 TeV was recorded in June 2015. The average pile-up μ varied between 0.003 and 0.03, and the interaction rate had a maximum of about 15 kHz. More than 200 million interactions were recorded during a one-week datataking period. Most of the readout bandwidth was dedicated to the loosest L1_MBTS_1 trigger (described below) recording events at 1.0 to 1.5 kHz on average.</p>
        <p>The MBTS are used as the primary L1 hardware triggers for recording inelastic events with minimum bias, as reported in Refs. [30,31]. The plastic scintillation counters composing the system were replaced during LS1 and consist of two planes of twelve counters, each plane formed of an inner ring of eight counters and an outer ring of four counters. These rings are sensitive to charged particles in the interval 2.07 &lt; |η| &lt; 3.86. Each counter is connected to a photomultiplier tube and provides a fast trigger via a constant fraction discriminator and is read out through the Tile calorimeter data acquisition system. The MBTS triggers require a certain multiplicity of counters to be above threshold in a bunch-crossing with colliding beams. The L1_MBTS_1 and L1_MBTS_2 triggers require any one or two of the 24 counters to be above threshold, respectively. The coincidence of two hits in the latter suppresses beam-induced backgrounds from low-energy neutrons and photons. The L1_MBTS_1_1 trigger requires at least one counter to be above threshold in both the +z and -z hemispheres of the detector and is used to seed the highmultiplicity HLT triggers. The same trigger selections are also applied to empty (no beam present) and unpaired (one beam present) beam-crossings to investigate beam-induced backgrounds. No additional HLT selection is applied to L1_MBTS_1 and L1_MBTS_2 triggered events.The MBTS are used as the primary L1 hardware triggers for recording inelastic events with minimum bias, as reported in Refs. [30,31]. The plastic scintillation counters composing the system were replaced during LS1 and consist of two planes of twelve counters, each plane formed of an inner ring of eight counters and an outer ring of four counters. These rings are sensitive to charged particles in the interval 2.07 &lt; |η| &lt; 3.86. Each counter is connected to a photomultiplier tube and provides a fast trigger via a constant fraction discriminator and is read out through the Tile calorimeter data acquisition system. The MBTS triggers require a certain multiplicity of counters to be above threshold in a bunch-crossing with colliding beams. The L1_MBTS_1 and L1_MBTS_2 triggers require any one or two of the 24 counters to be above threshold, respectively. The coincidence of two hits in the latter suppresses beam-induced backgrounds from low-energy neutrons and photons. The L1_MBTS_1_1 trigger requires at least one counter to be above threshold in both the +z and -z hemispheres of the detector and is used to seed the highmultiplicity HLT triggers. The same trigger selections are also applied to empty (no beam present) and unpaired (one beam present) beam-crossings to investigate beam-induced backgrounds. No additional HLT selection is applied to L1_MBTS_1 and L1_MBTS_2 triggered events.</p>
        <p>The mb_sptrk trigger is used to determine the efficiency of the MBTS. It is seeded using a random trigger on filled bunches and requires at least two reconstructed space-points in the Pixel system and three in the SCT, along with at least one reconstructed track with p T &gt; 200 MeV. Studies using MC simulation and a fully unbiased data sample have demonstrated that this control trigger is unbiased with respect to the offline selection.The mb_sptrk trigger is used to determine the efficiency of the MBTS. It is seeded using a random trigger on filled bunches and requires at least two reconstructed space-points in the Pixel system and three in the SCT, along with at least one reconstructed track with p T &gt; 200 MeV. Studies using MC simulation and a fully unbiased data sample have demonstrated that this control trigger is unbiased with respect to the offline selection.</p>
        <p>The primary high-multiplicity trigger (e.g. used in the measurement of two-particle correlations [32]) is mb_sp900_trk60_hmt_L1MBTS_1_1 and requires at least 900 reconstructed space-points in the SCT and at least 60 reconstructed tracks with p T &gt; 400 MeV. This higher p T requirement for the high-multiplicity trigger is compatible with the p T cut used for physics analysis and reduces the computational complexity of the track-finding algorithms in the HLT to an acceptable level.The primary high-multiplicity trigger (e.g. used in the measurement of two-particle correlations [32]) is mb_sp900_trk60_hmt_L1MBTS_1_1 and requires at least 900 reconstructed space-points in the SCT and at least 60 reconstructed tracks with p T &gt; 400 MeV. This higher p T requirement for the high-multiplicity trigger is compatible with the p T cut used for physics analysis and reduces the computational complexity of the track-finding algorithms in the HLT to an acceptable level.</p>
        <p>The MBTS trigger efficiency is defined as the ratio of events passing MBTS trigger, the control trigger (mb_sptrk) and offline selection to events passing the control trigger and offline selection. The efficiency is shown in Fig. 22 for two offline selections as a function of the number of selected tracks compatible in transverse impact parameter (|d 0 | &lt; 1.5 mm) with the beam line (n BL sel ) for (a) p T &gt; 100 MeV and (b) p T &gt; 500 MeV. The efficiency is close to 95% in the first bin, quickly rising to 100% for L1_MBTS_1 and L1_MBTS_2. The L1_MBTS_1_1 trigger, which requires at least one hit on both sides of the detector, only approaches 100% efficiency for events with around 15 tracks. The primary reason for the lower efficiency of the L1_MBTS_1_1 trigger compared to L1_MBTS_1 or L1_MBTS_2 is that at low multiplicities about 30% of the inelastic events are due to diffractive interactions where usually one proton stays intact and thus particles from the interactions are only produced on one side of the detector. Systematic uncertainties in the trigger efficiency are evaluated by removing the cut on the transverse impact parameter with respect to the beam line from the track selection and applying a longitudinal impact parameter cut with respect to the primary vertex (for events where a primary vertex is reconstructed). This results in a less than 0.1% shift. The difference in response between the two hemispheres is additionally evaluated to be at most 0.12%.The MBTS trigger efficiency is defined as the ratio of events passing MBTS trigger, the control trigger (mb_sptrk) and offline selection to events passing the control trigger and offline selection. The efficiency is shown in Fig. 22 for two offline selections as a function of the number of selected tracks compatible in transverse impact parameter (|d 0 | &lt; 1.5 mm) with the beam line (n BL sel ) for (a) p T &gt; 100 MeV and (b) p T &gt; 500 MeV. The efficiency is close to 95% in the first bin, quickly rising to 100% for L1_MBTS_1 and L1_MBTS_2. The L1_MBTS_1_1 trigger, which requires at least one hit on both sides of the detector, only approaches 100% efficiency for events with around 15 tracks. The primary reason for the lower efficiency of the L1_MBTS_1_1 trigger compared to L1_MBTS_1 or L1_MBTS_2 is that at low multiplicities about 30% of the inelastic events are due to diffractive interactions where usually one proton stays intact and thus particles from the interactions are only produced on one side of the detector. Systematic uncertainties in the trigger efficiency are evaluated by removing the cut on the transverse impact parameter with respect to the beam line from the track selection and applying a longitudinal impact parameter cut with respect to the primary vertex (for events where a primary vertex is reconstructed). This results in a less than 0.1% shift. The difference in response between the two hemispheres is additionally evaluated to be at most 0.12%.</p>
        <p>The L1_MBTS_1 trigger is used as the control trigger for the determination of the efficiency turn-on curves for the high-multiplicity data set. The efficiency is parameterised as a function of the number of offline tracks associated with the primary vertex. Figure 23 shows the efficiency for three different selections of the minimum number of SCT spacepoints and reconstructed tracks and for two selections of the offline track p T requirement (above 400 and 500 MeV). In the case of matching offline and trigger p T selections ( p T &gt; 400 MeV) shown in Fig. 23a, the triggers are 100% efficient for a value of five tracks above the offline threshold (e.g. trk60 becomes fully efficient for 65 offline tracks). If the offline requirement is raised to 500 MeV as shown in Fig. 23b, the trigger is 100% efficient for the required number of tracks.The L1_MBTS_1 trigger is used as the control trigger for the determination of the efficiency turn-on curves for the high-multiplicity data set. The efficiency is parameterised as a function of the number of offline tracks associated with the primary vertex. Figure 23 shows the efficiency for three different selections of the minimum number of SCT spacepoints and reconstructed tracks and for two selections of the offline track p T requirement (above 400 and 500 MeV). In the case of matching offline and trigger p T selections ( p T &gt; 400 MeV) shown in Fig. 23a, the triggers are 100% efficient for a value of five tracks above the offline threshold (e.g. trk60 becomes fully efficient for 65 offline tracks). If the offline requirement is raised to 500 MeV as shown in Fig. 23b, the trigger is 100% efficient for the required number of tracks.</p>
        <p>Events with electrons and photons in the final state are important signatures for many ATLAS physics analyses, from SM precision physics, such as Higgs boson, top quark, W and Z boson properties and production rate measurements, to searches for new physics. Various triggers cover the energy range between a few GeV and several TeV. Low-E T triggers are used to collect data for measuring the properties of J/ψ → ee, diphoton or low mass Drell-Yan production. Single-electron triggers with E T above 24 GeV, dielectron triggers with lower thresholds and diphoton triggers are used for the signal selection in a wide variety of ATLAS physics analyses such as studies of the Higgs boson.Events with electrons and photons in the final state are important signatures for many ATLAS physics analyses, from SM precision physics, such as Higgs boson, top quark, W and Z boson properties and production rate measurements, to searches for new physics. Various triggers cover the energy range between a few GeV and several TeV. Low-E T triggers are used to collect data for measuring the properties of J/ψ → ee, diphoton or low mass Drell-Yan production. Single-electron triggers with E T above 24 GeV, dielectron triggers with lower thresholds and diphoton triggers are used for the signal selection in a wide variety of ATLAS physics analyses such as studies of the Higgs boson.</p>
        <p>At L1 the electron and photon triggers use the algorithms described in Sect. 3.1. The isolation and hadronic leakage veto cuts are not required for EM clusters with transverse energy above 50 GeV.At L1 the electron and photon triggers use the algorithms described in Sect. 3.1. The isolation and hadronic leakage veto cuts are not required for EM clusters with transverse energy above 50 GeV.</p>
        <p>At the HLT, electron and photon candidates are reconstructed and selected in several steps in order to reject events as fast as possible, thus allowing algorithms which reproduce closely the offline algorithms and require more CPU time to run at a reduced rate later in the trigger sequence. At first, fast calorimeter algorithms build clusters from the calorimeter cells (covering 0.025 × 0.025 in η × φ space) within the RoI ( η × φ = 0.4 × 0.4) identified by L1. Since electrons and photons deposit most of their energy in the second layer of the EM calorimeter, this layer is used to find the cell with the largest deposited transverse energy in the RoI. EM calorimeter clusters of size 3 × 7 in the barrel (|η| &lt; 1.4) and 5 × 5 in the end-cap (1.4 &lt; |η| &lt; 2.47) are used to reconstruct electrons and photons. The identification of electrons and photons is based on the cluster E T as well as cluster shape parameters such as R had , R η and E ratio , 3 the latter being used for electron candidates and a few tight photon triggers. Electron candidates are required to have tracks from the fast tracking stage with p T &gt; 1 GeV and to match clusters within η &lt; 0.2.At the HLT, electron and photon candidates are reconstructed and selected in several steps in order to reject events as fast as possible, thus allowing algorithms which reproduce closely the offline algorithms and require more CPU time to run at a reduced rate later in the trigger sequence. At first, fast calorimeter algorithms build clusters from the calorimeter cells (covering 0.025 × 0.025 in η × φ space) within the RoI ( η × φ = 0.4 × 0.4) identified by L1. Since electrons and photons deposit most of their energy in the second layer of the EM calorimeter, this layer is used to find the cell with the largest deposited transverse energy in the RoI. EM calorimeter clusters of size 3 × 7 in the barrel (|η| &lt; 1.4) and 5 × 5 in the end-cap (1.4 &lt; |η| &lt; 2.47) are used to reconstruct electrons and photons. The identification of electrons and photons is based on the cluster E T as well as cluster shape parameters such as R had , R η and E ratio , 3 the latter being used for electron candidates and a few tight photon triggers. Electron candidates are required to have tracks from the fast tracking stage with p T &gt; 1 GeV and to match clusters within η &lt; 0.2.</p>
        <p>The second step relies on precise offline-like algorithms. The energy of the clusters is calibrated for electron and photon triggers separately using a multivariate technique where the response of the calorimeter layers is corrected in data and simulation [33]. Precision tracks extrapolated to the second layer of the EM calorimeter are required to match to clusters within η of 0.05 and φ of 0.05. Electron identification relies on a multivariate technique using a likelihood 3 R had = E had T /E EM T is the ratio of the cluster transverse energy in the hadronic calorimeter to that in the EM calorimeter. R η is based on the cluster shape in the second layer of the EM calorimeter and defined as the ratio of transverse energy in a core region of 3×7 cells in η×φ to that in a 7 × 7 region, expanded in η from the 3 × 7 core. E ratio is defined as the ratio of the energy difference between the largest and second-largest energy deposits in the cluster over the sum of these energies in the front layer of the EM calorimeter.The second step relies on precise offline-like algorithms. The energy of the clusters is calibrated for electron and photon triggers separately using a multivariate technique where the response of the calorimeter layers is corrected in data and simulation [33]. Precision tracks extrapolated to the second layer of the EM calorimeter are required to match to clusters within η of 0.05 and φ of 0.05. Electron identification relies on a multivariate technique using a likelihood 3 R had = E had T /E EM T is the ratio of the cluster transverse energy in the hadronic calorimeter to that in the EM calorimeter. R η is based on the cluster shape in the second layer of the EM calorimeter and defined as the ratio of transverse energy in a core region of 3×7 cells in η×φ to that in a 7 × 7 region, expanded in η from the 3 × 7 core. E ratio is defined as the ratio of the energy difference between the largest and second-largest energy deposits in the cluster over the sum of these energies in the front layer of the EM calorimeter.</p>
        <p>(LH) discriminant with three operating points named loose LH, medium LH and tight LH. An additional working point named very loose LH is used for supporting triggers. The LHbased identification makes use of variables similar to the cutbased identification employed during Run 1 [2] but has better background rejection for the same signal efficiency. The discriminating variables used offline are also used by the trigger, exploiting the characteristic features of energy deposits in the EM calorimeters (longitudinal and lateral shower shapes), track quality, track-cluster matching, and particle identification by the TRT. All variables are described in Refs. [34,35]. The composition of the likelihood is the same as in the offline reconstruction with the exception of momentum loss due to bremsstrahlung, p/ p, which is not accounted for in the online environment. The photon identification relies only on the cluster shower-shape variables and three working points are also defined: loose, medium and tight.(LH) discriminant with three operating points named loose LH, medium LH and tight LH. An additional working point named very loose LH is used for supporting triggers. The LHbased identification makes use of variables similar to the cutbased identification employed during Run 1 [2] but has better background rejection for the same signal efficiency. The discriminating variables used offline are also used by the trigger, exploiting the characteristic features of energy deposits in the EM calorimeters (longitudinal and lateral shower shapes), track quality, track-cluster matching, and particle identification by the TRT. All variables are described in Refs. [34,35]. The composition of the likelihood is the same as in the offline reconstruction with the exception of momentum loss due to bremsstrahlung, p/ p, which is not accounted for in the online environment. The photon identification relies only on the cluster shower-shape variables and three working points are also defined: loose, medium and tight.</p>
        <p>Not applied during 2015 but foreseen for higher luminosities during Run 2 is an additional requirement on isolation for the lowest-threshold unprescaled single-electron trigger. The isolation parameter is calculated as the sum of the p T values of all tracks in a cone of size R = 0.2 around the electron for tracks with p T &gt; 1 GeV and | z 0 sin θ | &lt; 0.3, where z 0 is the distance along z between the longitudinal impact parameter of the track and the leading track in the RoI. The ratio of this quantity to the EM cluster E T , namely p T /E T , is used to estimate the energy deposited by other particles.Not applied during 2015 but foreseen for higher luminosities during Run 2 is an additional requirement on isolation for the lowest-threshold unprescaled single-electron trigger. The isolation parameter is calculated as the sum of the p T values of all tracks in a cone of size R = 0.2 around the electron for tracks with p T &gt; 1 GeV and | z 0 sin θ | &lt; 0.3, where z 0 is the distance along z between the longitudinal impact parameter of the track and the leading track in the RoI. The ratio of this quantity to the EM cluster E T , namely p T /E T , is used to estimate the energy deposited by other particles.</p>
        <p>The primary L1 and HLT electron and photon triggers used in 2015 are listed in Table 1. The lowest-threshold singleelectron trigger (e24_lhmedium_L1EM20VH) applies a 24 GeV transverse energy threshold and requires the electron to pass medium LH identification requirements. The trigger is seeded by L1_EM20VH, which requires E T &gt; 20 GeV, and applies an E T -dependent veto against energy deposited in the hadronic calorimeter behind the electromagnetic cluster of the electron candidate (hadronic veto, denoted by H in the trigger name). The E T threshold varies slightly as a function of η to compensate for passive material in front of the calorimeter (denoted by V in the trigger name). To recover efficiency in the high transverse energy regime, this trigger is complemented by a trigger requiring a transverse energy above 120 GeV with loose LH identification (e120_lhloose). With a maximum instantaneous luminosity of 5.2 × 10 33 cm -2 s -1 reached during the 2015 datataking, the rates of electron triggers could be sustained without the use of additional electromagnetic or track isolation requirements at L1 or HLT. The lowest-threshold dielectron trigger (2e12_lhloose_L12EM10VH) applies a 12 GeV transverse energy threshold and requires the two electrons L dt = 3.2 fb ∫ = 13 TeV, s Fig. 24 L1 trigger rates as a function of the instantaneous luminosity for selected single-and multi-object triggers to pass loose LH identification requirements. The trigger is seeded by L1_2EM10VH, which requires two electrons with E T above 10 GeV and a hadronic energy veto.The primary L1 and HLT electron and photon triggers used in 2015 are listed in Table 1. The lowest-threshold singleelectron trigger (e24_lhmedium_L1EM20VH) applies a 24 GeV transverse energy threshold and requires the electron to pass medium LH identification requirements. The trigger is seeded by L1_EM20VH, which requires E T &gt; 20 GeV, and applies an E T -dependent veto against energy deposited in the hadronic calorimeter behind the electromagnetic cluster of the electron candidate (hadronic veto, denoted by H in the trigger name). The E T threshold varies slightly as a function of η to compensate for passive material in front of the calorimeter (denoted by V in the trigger name). To recover efficiency in the high transverse energy regime, this trigger is complemented by a trigger requiring a transverse energy above 120 GeV with loose LH identification (e120_lhloose). With a maximum instantaneous luminosity of 5.2 × 10 33 cm -2 s -1 reached during the 2015 datataking, the rates of electron triggers could be sustained without the use of additional electromagnetic or track isolation requirements at L1 or HLT. The lowest-threshold dielectron trigger (2e12_lhloose_L12EM10VH) applies a 12 GeV transverse energy threshold and requires the two electrons L dt = 3.2 fb ∫ = 13 TeV, s Fig. 24 L1 trigger rates as a function of the instantaneous luminosity for selected single-and multi-object triggers to pass loose LH identification requirements. The trigger is seeded by L1_2EM10VH, which requires two electrons with E T above 10 GeV and a hadronic energy veto.</p>
        <p>The primary single-photon trigger used in 2015 is g120_loose. It requires a transverse energy above 120 GeV and applies loose photon identification criteria. It is seeded by L1_EM22VHI, which requires an isolated electromagnetic cluster (denoted by I in the trigger name) with E T above 22 GeV and applies a hadronic veto and η-dependent E T thresholds as described above. As mentioned earlier, the electromagnetic isolation and hadronic veto requirements are not applied for E T above 50 GeV. The two main diphoton triggers are g35_loose_g25_loose, which requires two photons above 35 and 25 GeV thresholds and loose photon identification requirements, and 2g20_tight, which requires two photons with E T above 20 GeV and tight identification. Both triggers are seeded by L1_2EM15VH, which requires two electromagnetic clusters with E T above 15 GeV and a hadronic veto.The primary single-photon trigger used in 2015 is g120_loose. It requires a transverse energy above 120 GeV and applies loose photon identification criteria. It is seeded by L1_EM22VHI, which requires an isolated electromagnetic cluster (denoted by I in the trigger name) with E T above 22 GeV and applies a hadronic veto and η-dependent E T thresholds as described above. As mentioned earlier, the electromagnetic isolation and hadronic veto requirements are not applied for E T above 50 GeV. The two main diphoton triggers are g35_loose_g25_loose, which requires two photons above 35 and 25 GeV thresholds and loose photon identification requirements, and 2g20_tight, which requires two photons with E T above 20 GeV and tight identification. Both triggers are seeded by L1_2EM15VH, which requires two electromagnetic clusters with E T above 15 GeV and a hadronic veto.</p>
        <p>Figures 24 and25 show the rates of the electron and photon triggers as a function of the instantaneous luminosity. These trigger rates scale linearly with the instantaneous luminosity.Figures 24 and25 show the rates of the electron and photon triggers as a function of the instantaneous luminosity. These trigger rates scale linearly with the instantaneous luminosity.</p>
        <p>The performance of electron triggers is studied using a sample of Z → ee events. The tag-and-probe method utilises events triggered by a single-electron trigger and requires two offline reconstructed electrons with an invariant mass between 80 and 100 GeV. After identifying the electron that triggered the event (tag electron), the other electron (probe electron) is unbiased by the trigger selection, thus allowing its use to measure the electron trigger efficiency. HLT electrons (L1 EM objects) are matched to the probe electron if their separation is R &lt; 0.07(0.15). The trigger efficiency is calcu- The offline reconstructed electron candidate is required to have an E T value at least 1 GeV above the trigger threshold lated as the ratio of the number of probe electrons passing the trigger selection to the number of probe electrons. The efficiency of the combination of the lowest unprescaled singleelectron trigger e24_lhmedium_L1EM20VH and the high transverse momentum electron trigger e120_lhloose with respect to the offline objects is shown in Fig. 26 as a function of the offline reconstructed electron transverse energy and pseudorapidity. The figure also shows the efficiency of the L1 trigger (L1_EM20VH) seeding the lowest unprescaled single-electron trigger. A sharp turn-on can be observed for both the L1 and overall (L1 and HLT) efficiency, and the HLT inefficiency with respect to L1 is small. Inefficiencies observed around pseudorapidities of -1.4 and 1.4 are due to the transition region between the barrel and endcap calorimeter.The performance of electron triggers is studied using a sample of Z → ee events. The tag-and-probe method utilises events triggered by a single-electron trigger and requires two offline reconstructed electrons with an invariant mass between 80 and 100 GeV. After identifying the electron that triggered the event (tag electron), the other electron (probe electron) is unbiased by the trigger selection, thus allowing its use to measure the electron trigger efficiency. HLT electrons (L1 EM objects) are matched to the probe electron if their separation is R &lt; 0.07(0.15). The trigger efficiency is calcu- The offline reconstructed electron candidate is required to have an E T value at least 1 GeV above the trigger threshold lated as the ratio of the number of probe electrons passing the trigger selection to the number of probe electrons. The efficiency of the combination of the lowest unprescaled singleelectron trigger e24_lhmedium_L1EM20VH and the high transverse momentum electron trigger e120_lhloose with respect to the offline objects is shown in Fig. 26 as a function of the offline reconstructed electron transverse energy and pseudorapidity. The figure also shows the efficiency of the L1 trigger (L1_EM20VH) seeding the lowest unprescaled single-electron trigger. A sharp turn-on can be observed for both the L1 and overall (L1 and HLT) efficiency, and the HLT inefficiency with respect to L1 is small. Inefficiencies observed around pseudorapidities of -1.4 and 1.4 are due to the transition region between the barrel and endcap calorimeter.</p>
        <p>The photon trigger efficiency is computed using the bootstrap method as the efficiency of the HLT trigger relative to a trigger with a lower E T threshold. Figure 27 shows the efficiency of the main single-photon trigger and the photons of the main diphoton trigger as a function of the offline reconstructed photon transverse energy and pseudorapidity for data and MC simulation. Very good agreement is observed between data and simulation.The photon trigger efficiency is computed using the bootstrap method as the efficiency of the HLT trigger relative to a trigger with a lower E T threshold. Figure 27 shows the efficiency of the main single-photon trigger and the photons of the main diphoton trigger as a function of the offline reconstructed photon transverse energy and pseudorapidity for data and MC simulation. Very good agreement is observed between data and simulation.</p>
        <p>Muons are produced in many final states of interest to the ATLAS physics programme, from SM precision physics to searches for new physics. Muons are identified with high purity compared to other signatures and cover a wide trans- verse momentum range, from a few GeV to several TeV. Muon trigger thresholds in the p T range from 4 to 10 GeV are used to collect data for measurements of processes such as J/ψ → μμ, low-p T dimuons, and Z → τ τ [36,37]. Higher p T thresholds are used to collect data for new-physics searches as well as measuring the properties and production rates of SM particles such as the Higgs, W and Z bosons, and top quarks [38][39][40].Muons are produced in many final states of interest to the ATLAS physics programme, from SM precision physics to searches for new physics. Muons are identified with high purity compared to other signatures and cover a wide trans- verse momentum range, from a few GeV to several TeV. Muon trigger thresholds in the p T range from 4 to 10 GeV are used to collect data for measurements of processes such as J/ψ → μμ, low-p T dimuons, and Z → τ τ [36,37]. Higher p T thresholds are used to collect data for new-physics searches as well as measuring the properties and production rates of SM particles such as the Higgs, W and Z bosons, and top quarks [38][39][40].</p>
        <p>The trigger reconstruction algorithms for muons at L1 and the HLT are described in Sects. 3.2 and 5.3, respectively. The selection criteria depend on the algorithm used for reconstruction. The MS-only algorithm selects solely on the p T of the muon candidate measured by the muon spectrometer; the combined algorithm makes selections based on the match between the ID and MS tracks and their combined p T ; and the isolated muon algorithm applies selection criteria based on the amount of energy in the isolation cones.The trigger reconstruction algorithms for muons at L1 and the HLT are described in Sects. 3.2 and 5.3, respectively. The selection criteria depend on the algorithm used for reconstruction. The MS-only algorithm selects solely on the p T of the muon candidate measured by the muon spectrometer; the combined algorithm makes selections based on the match between the ID and MS tracks and their combined p T ; and the isolated muon algorithm applies selection criteria based on the amount of energy in the isolation cones.</p>
        <p>The lowest-threshold single-muon trigger (mu20_iloose_ L1MU15) requires a minimum transverse momentum of 20 GeV for combined muon candidates in addition to a loose isolation: the scalar sum of the track p T values in a cone of size R = 0.2 around the muon candidate is required to be smaller than 12% of the muon transverse momentum. The isolation requirement reduces the rate by a factor of approximately 2.5 with a negligible efficiency loss. The trigger is seeded by L1_MU15, which requires a transverse momen-tum above 15 GeV. At a transverse momentum above 50 GeV this trigger is complemented by a trigger not requiring isolation (mu50), to recover a small efficiency loss in the high transverse momentum region.The lowest-threshold single-muon trigger (mu20_iloose_ L1MU15) requires a minimum transverse momentum of 20 GeV for combined muon candidates in addition to a loose isolation: the scalar sum of the track p T values in a cone of size R = 0.2 around the muon candidate is required to be smaller than 12% of the muon transverse momentum. The isolation requirement reduces the rate by a factor of approximately 2.5 with a negligible efficiency loss. The trigger is seeded by L1_MU15, which requires a transverse momen-tum above 15 GeV. At a transverse momentum above 50 GeV this trigger is complemented by a trigger not requiring isolation (mu50), to recover a small efficiency loss in the high transverse momentum region.</p>
        <p>The lowest-threshold unprescaled dimuon trigger (2mu10) requires a minimum transverse momentum of 10 GeV for combined muon candidates. The trigger is seeded by L1_2MU10, which requires two muons with transverse momentum above 10 GeV. Figure 28 shows the rates of these triggers as a function of the instantaneous luminosity. The trigger rates scale linearly with the instantaneous luminosity. Dimuon triggers with lower p T thresholds and further selections (e.g. on the dimuon invariant mass) were also active and are discussed in Sect. 6.8. Additionally, an asymmetric dimuon trigger (mu18_mu8noL1) is included, where mu18 is seeded by L1_MU15 and mu8noL1 performs a search for a muon in the full detector at the HLT. By requiring only one muon at L1, the dimuon trigger does not suffer a loss of efficiency that would otherwise have if two muons were required at L1. This trigger is typically used by physics searches involving two relatively highp T muons to improve the acceptance with respect to the standard dimuon triggers.The lowest-threshold unprescaled dimuon trigger (2mu10) requires a minimum transverse momentum of 10 GeV for combined muon candidates. The trigger is seeded by L1_2MU10, which requires two muons with transverse momentum above 10 GeV. Figure 28 shows the rates of these triggers as a function of the instantaneous luminosity. The trigger rates scale linearly with the instantaneous luminosity. Dimuon triggers with lower p T thresholds and further selections (e.g. on the dimuon invariant mass) were also active and are discussed in Sect. 6.8. Additionally, an asymmetric dimuon trigger (mu18_mu8noL1) is included, where mu18 is seeded by L1_MU15 and mu8noL1 performs a search for a muon in the full detector at the HLT. By requiring only one muon at L1, the dimuon trigger does not suffer a loss of efficiency that would otherwise have if two muons were required at L1. This trigger is typically used by physics searches involving two relatively highp T muons to improve the acceptance with respect to the standard dimuon triggers.</p>
        <p>The L1 and HLT muon efficiencies are determined using a tag-and-probe method with Z → μμ candidate events. Events are required to contain a pair of reference muons with opposite charge and an invariant mass within 10 GeV of the Z mass. Reference muons reconstructed offline using both ID and MS information are required to be inside the fiducial volume of the muon triggers (|η| &lt; 2.4) and pass the medium identification requirements [41,42].The L1 and HLT muon efficiencies are determined using a tag-and-probe method with Z → μμ candidate events. Events are required to contain a pair of reference muons with opposite charge and an invariant mass within 10 GeV of the Z mass. Reference muons reconstructed offline using both ID and MS information are required to be inside the fiducial volume of the muon triggers (|η| &lt; 2.4) and pass the medium identification requirements [41,42].</p>
        <p>The absolute efficiency of the L1_MU15 trigger and the absolute and relative efficiencies of the logical 'or' of mu20_iloose and mu50 as a function of the p T of the offline muon track are shown in Fig. 29. The L1 muon trigger efficiency is close to 70% in the barrel and 90% in the end-caps. The different efficiencies are due to the different geometrical acceptance of the barrel and end-cap trigger systems and local detector inefficiencies. The HLT efficiency relative to L1 is close to 100% both in the barrel and in the end-caps. Figure 30 shows the muon trigger efficiency as a function of the azimuthal angle φ of the offline muon track for (a) the barrel and (b) the end-cap regions. The reduced barrel acceptance can be seen in the eight bins corresponding to the sectors containing the toroid coils and in the two feet sectors around φ ≈ -1.6 and φ ≈ -2.0, respectively.The absolute efficiency of the L1_MU15 trigger and the absolute and relative efficiencies of the logical 'or' of mu20_iloose and mu50 as a function of the p T of the offline muon track are shown in Fig. 29. The L1 muon trigger efficiency is close to 70% in the barrel and 90% in the end-caps. The different efficiencies are due to the different geometrical acceptance of the barrel and end-cap trigger systems and local detector inefficiencies. The HLT efficiency relative to L1 is close to 100% both in the barrel and in the end-caps. Figure 30 shows the muon trigger efficiency as a function of the azimuthal angle φ of the offline muon track for (a) the barrel and (b) the end-cap regions. The reduced barrel acceptance can be seen in the eight bins corresponding to the sectors containing the toroid coils and in the two feet sectors around φ ≈ -1.6 and φ ≈ -2.0, respectively.</p>
        <p>Jet triggers are used for signal selection in a wide variety of physics measurements and detector performance studies. Precision measurements of inclusive jet, dijet and multi-jet topologies rely on the events selected with the single-jet and multi-jet triggers. Events selected by the single-jet triggers are also used for the calibration of the calorimeter jet energy scale and resolution. All-hadronic decays of t t events can be studied using multi-jet signatures and the all-hadronic decay of the weak bosons, Higgs bosons and top quarks can be selected in high transverse momentum ('boosted') topologies using large-radius jets. Searches for physics beyond the SM, such as high-mass dijet resonances, supersymmetry or large extra dimensions, often utilise single-jet and multijet unprescaled triggers with a high transverse momentum threshold.Jet triggers are used for signal selection in a wide variety of physics measurements and detector performance studies. Precision measurements of inclusive jet, dijet and multi-jet topologies rely on the events selected with the single-jet and multi-jet triggers. Events selected by the single-jet triggers are also used for the calibration of the calorimeter jet energy scale and resolution. All-hadronic decays of t t events can be studied using multi-jet signatures and the all-hadronic decay of the weak bosons, Higgs bosons and top quarks can be selected in high transverse momentum ('boosted') topologies using large-radius jets. Searches for physics beyond the SM, such as high-mass dijet resonances, supersymmetry or large extra dimensions, often utilise single-jet and multijet unprescaled triggers with a high transverse momentum threshold.</p>
        <p>A detailed description of the jet triggers used during Run 1 can be found in Ref. [5]. Jets are reconstructed in the HLT using the anti-k t jet algorithm [43] with a radius parameter of R = 0.4 or R = 1.0. The inputs to the algorithm are calorimeter topo-clusters that are reconstructed from the full set of calorimeter cell information calibrated by default at the EM scale. The jets are calibrated in a procedure similar to that adopted for offline physics analyses [44]. First, contributions to the jet energy from pile-up collisions are subtracted on an event-by-event basis using the calculated area of each jet and the measured energy density within |η| &lt; 2. Second, the response of the calorimeter is corrected using a series of p Tand η-dependent calibration factors derived from simulation.A detailed description of the jet triggers used during Run 1 can be found in Ref. [5]. Jets are reconstructed in the HLT using the anti-k t jet algorithm [43] with a radius parameter of R = 0.4 or R = 1.0. The inputs to the algorithm are calorimeter topo-clusters that are reconstructed from the full set of calorimeter cell information calibrated by default at the EM scale. The jets are calibrated in a procedure similar to that adopted for offline physics analyses [44]. First, contributions to the jet energy from pile-up collisions are subtracted on an event-by-event basis using the calculated area of each jet and the measured energy density within |η| &lt; 2. Second, the response of the calorimeter is corrected using a series of p Tand η-dependent calibration factors derived from simulation.</p>
        <p>The jet reconstruction in the HLT is highly flexible and some triggers use non-standard inputs or a calibration procedure that differs from the default outlined above. For example, the clusters can be reconstructed using cells from a restricted region in the calorimeter defined using the RoIs identified by the L1 trigger. The clusters can also be calibrated using local calibration weights that are applied after classifying each cluster as electromagnetic or hadronic in origin. Furthermore, the jet calibration can be applied in four ways: no jet calibration, pile-up subtraction only, jet response correction only, or both pile-up subtraction and jet response corrections (default). Finally, the jet reconstruction can be run twice to produce reclustered jets [45], in which the input to the second jet-finding is the output from the first, e.g. to build large-R jets from small-R jets.The jet reconstruction in the HLT is highly flexible and some triggers use non-standard inputs or a calibration procedure that differs from the default outlined above. For example, the clusters can be reconstructed using cells from a restricted region in the calorimeter defined using the RoIs identified by the L1 trigger. The clusters can also be calibrated using local calibration weights that are applied after classifying each cluster as electromagnetic or hadronic in origin. Furthermore, the jet calibration can be applied in four ways: no jet calibration, pile-up subtraction only, jet response correction only, or both pile-up subtraction and jet response corrections (default). Finally, the jet reconstruction can be run twice to produce reclustered jets [45], in which the input to the second jet-finding is the output from the first, e.g. to build large-R jets from small-R jets.</p>
        <p>The jet trigger menu consists of single-jet triggers, which require at least one jet above a given transverse energy threshold, multi-jet triggers, which require at least N jets above a given transverse energy threshold, H T triggers, which require the scalar sum of the transverse energy of all jets in the event, H T , above a given threshold, and analysis-specific triggers for specific topologies of interest. The jet triggers use at L1 either a random trigger (on colliding bunches) or an L1 jet algorithm. The random trigger is typically used for triggers that select events with offline jet p T &lt; 45 GeV to avoid bias due to inefficiencies of the L1 jet algorithm for lowp T jets. In the following, only the most commonly used jet triggers are discussed.The jet trigger menu consists of single-jet triggers, which require at least one jet above a given transverse energy threshold, multi-jet triggers, which require at least N jets above a given transverse energy threshold, H T triggers, which require the scalar sum of the transverse energy of all jets in the event, H T , above a given threshold, and analysis-specific triggers for specific topologies of interest. The jet triggers use at L1 either a random trigger (on colliding bunches) or an L1 jet algorithm. The random trigger is typically used for triggers that select events with offline jet p T &lt; 45 GeV to avoid bias due to inefficiencies of the L1 jet algorithm for lowp T jets. In the following, only the most commonly used jet triggers are discussed.</p>
        <p>The lowest-threshold unprescaled single-jet trigger for standard jets (R = 0.4) selects events that contain a jet at L1 with transverse energy above 100 GeV (L1_J100) and a jet in the HLT with transverse energy above 360 GeV (j360). This trigger has a rate of 18 Hz at a luminosity of 5 × 10 33 cm -2 s -1 . The lowest-threshold unprescaled multi-jet triggers are 3j175, 4j85, 5j60 and 6j45, which have rates of 6, 20, 15 and 12 Hz, respectively. The lowestthreshold unprescaled H T trigger used in 2015 is ht850 with a rate of 12 Hz where one jet with transverse energy above 100 GeV is required at L1 and H T is required to be above 850 GeV at HLT.The lowest-threshold unprescaled single-jet trigger for standard jets (R = 0.4) selects events that contain a jet at L1 with transverse energy above 100 GeV (L1_J100) and a jet in the HLT with transverse energy above 360 GeV (j360). This trigger has a rate of 18 Hz at a luminosity of 5 × 10 33 cm -2 s -1 . The lowest-threshold unprescaled multi-jet triggers are 3j175, 4j85, 5j60 and 6j45, which have rates of 6, 20, 15 and 12 Hz, respectively. The lowestthreshold unprescaled H T trigger used in 2015 is ht850 with a rate of 12 Hz where one jet with transverse energy above 100 GeV is required at L1 and H T is required to be above 850 GeV at HLT.</p>
        <p>In addition to the unprescaled triggers, a set of lowerthreshold triggers select events that contain jets with lower transverse momentum and are typically prescaled to give an event rate of 1 Hz each. The lowest-threshold single-jet trigger in 2015 is j15, which uses a random trigger at L1. Multiple thresholds for single jets exist between j15 and j360 to cover the entire p T spectrum.In addition to the unprescaled triggers, a set of lowerthreshold triggers select events that contain jets with lower transverse momentum and are typically prescaled to give an event rate of 1 Hz each. The lowest-threshold single-jet trigger in 2015 is j15, which uses a random trigger at L1. Multiple thresholds for single jets exist between j15 and j360 to cover the entire p T spectrum.</p>
        <p>Jet trigger efficiencies are determined using the bootstrap method with respect to the p T of the jet. The single-jet trigger efficiencies for L1 and the HLT are shown in Fig. 31 for both the central and forward regions of the calorimeter. The ranges in |η| are chosen to ensure that the probe jet is fully contained within the |η| region of study. Good agreement is observed between simulation and data. The sharp HLT efficiency turnon curves in Fig. 31 are due to good agreement between the energy scale of jets in the HLT and offline, as shown in Fig. 32.Jet trigger efficiencies are determined using the bootstrap method with respect to the p T of the jet. The single-jet trigger efficiencies for L1 and the HLT are shown in Fig. 31 for both the central and forward regions of the calorimeter. The ranges in |η| are chosen to ensure that the probe jet is fully contained within the |η| region of study. Good agreement is observed between simulation and data. The sharp HLT efficiency turnon curves in Fig. 31 are due to good agreement between the energy scale of jets in the HLT and offline, as shown in Fig. 32.</p>
        <p>The multi-jet trigger efficiencies are dominated by the trigger efficiency of the N th leading jet and are shown in Fig. 33 for (a) L1 and (b) HLT as a function of the N th leading jet transverse momentum. Good agreement is found for the efficiency as a function of the N th jet for different jet multiplicities with the same threshold (e.g. L1_6J15, L1_4J15 and 4j45, 5j45) and between data and simulation for the HLT.The multi-jet trigger efficiencies are dominated by the trigger efficiency of the N th leading jet and are shown in Fig. 33 for (a) L1 and (b) HLT as a function of the N th leading jet transverse momentum. Good agreement is found for the efficiency as a function of the N th jet for different jet multiplicities with the same threshold (e.g. L1_6J15, L1_4J15 and 4j45, 5j45) and between data and simulation for the HLT.</p>
        <p>Finally, the efficiency of the H T and large-R (R = 1.0) triggers are shown in Fig. 34. The H T trigger efficiencies are measured with respect to the HLT_j150_L1J40 trigger. There is a small offset in the efficiency curves for data and simulation for both thresholds. For the large-R trig- gers, the HLT threshold is set to 360 GeV and the efficiency curves are shown for three different calibrations and jet input options: jets built from topo-clusters at the EM scale with a pile-up subtraction applied (a10_sub), jets built from topoclusters with local calibration weights and pile-up subtraction applied (a10_lcw_sub) and reclustered jets built from R = 0.4 jets using both pile-up subtraction and local calibration weights (a10r).Finally, the efficiency of the H T and large-R (R = 1.0) triggers are shown in Fig. 34. The H T trigger efficiencies are measured with respect to the HLT_j150_L1J40 trigger. There is a small offset in the efficiency curves for data and simulation for both thresholds. For the large-R trig- gers, the HLT threshold is set to 360 GeV and the efficiency curves are shown for three different calibrations and jet input options: jets built from topo-clusters at the EM scale with a pile-up subtraction applied (a10_sub), jets built from topoclusters with local calibration weights and pile-up subtraction applied (a10_lcw_sub) and reclustered jets built from R = 0.4 jets using both pile-up subtraction and local calibration weights (a10r).</p>
        <p>Searches for dijet resonances with sub-TeV masses are statistically limited by the bandwidth allocated to inclusive singlejet triggers. Due to large SM multi-jet backgrounds, these triggers must be prescaled in order to fit within the total physics trigger output rate of 1 kHz. However, as the properties of jets reconstructed at the HLT are comparable to that of jets reconstructed offline, one can avoid this rate limitation by using Trigger-Level Analysis (TLA) triggers that record partial events, containing only relevant HLT jet objects needed for the search, to a dedicated stream. Using Trigger-Level Analysis triggers allows a factor of 100 increase in the event recording rates, and results in a significant increase in the number of lowp T jets as shown in Fig. 35. Dedicated calibration and jet identification procedures are applied to these partially built events, accounting for differences between offline jets and trigger jets as well as for the lack of detector data other than from the calorimeters. These procedures are described in detail in Ref. [46].Searches for dijet resonances with sub-TeV masses are statistically limited by the bandwidth allocated to inclusive singlejet triggers. Due to large SM multi-jet backgrounds, these triggers must be prescaled in order to fit within the total physics trigger output rate of 1 kHz. However, as the properties of jets reconstructed at the HLT are comparable to that of jets reconstructed offline, one can avoid this rate limitation by using Trigger-Level Analysis (TLA) triggers that record partial events, containing only relevant HLT jet objects needed for the search, to a dedicated stream. Using Trigger-Level Analysis triggers allows a factor of 100 increase in the event recording rates, and results in a significant increase in the number of lowp T jets as shown in Fig. 35. Dedicated calibration and jet identification procedures are applied to these partially built events, accounting for differences between offline jets and trigger jets as well as for the lack of detector data other than from the calorimeters. These procedures are described in detail in Ref. [46].</p>
        <p>Tau leptons are a key signature in many SM measurements and searches for new physics. The decay into tau lepton pairs provides the strongest signal for measurements of the SM Higgs boson coupling to fermions. Final states containing tau leptons are also often favoured by heavier Higgs bosons or other new resonances in many scenarios beyond the SM. Most (about 65%) of tau leptons decay hadronically. Hence an efficient trigger on hadronic tau decays is crucial for many analyses using tau leptons.Tau leptons are a key signature in many SM measurements and searches for new physics. The decay into tau lepton pairs provides the strongest signal for measurements of the SM Higgs boson coupling to fermions. Final states containing tau leptons are also often favoured by heavier Higgs bosons or other new resonances in many scenarios beyond the SM. Most (about 65%) of tau leptons decay hadronically. Hence an efficient trigger on hadronic tau decays is crucial for many analyses using tau leptons.</p>
        <p>Dedicated tau trigger algorithms were designed and implemented based on the main features of hadronic tau decays: narrow calorimeter energy deposits and a small number of associated tracks. Due to the high production rate of jets with features very similar to hadronic tau decays, keeping the rate of tau triggers under control is particularly challenging.Dedicated tau trigger algorithms were designed and implemented based on the main features of hadronic tau decays: narrow calorimeter energy deposits and a small number of associated tracks. Due to the high production rate of jets with features very similar to hadronic tau decays, keeping the rate of tau triggers under control is particularly challenging.</p>
        <p>At L1 the tau trigger uses the algorithms described in Sect. 3.1. The isolation requirement was tuned with 13 TeV simulation to yield an efficiency of 98% and is not applied for tau candidates with a transverse energy above 60 GeV.At L1 the tau trigger uses the algorithms described in Sect. 3.1. The isolation requirement was tuned with 13 TeV simulation to yield an efficiency of 98% and is not applied for tau candidates with a transverse energy above 60 GeV.</p>
        <p>At the HLT three sequential selections are made. First, a minimum requirement is applied to the transverse energy of the tau candidate. The energy is calculated using the locally calibrated topo-clusters of calorimeter cells contained in a cone of size R = 0.2 around the L1 tau RoI direction taken from the L1 cluster. A dedicated tau energy calibration scheme is used. Second, two-stage fast tracking (Sect. 5.1.3) is used to select tau candidates with low track multiplicity. A leading track is sought within a narrow cone ( R = 0.1) around the tau direction followed by a second fast tracking step using a larger cone ( R = 0.4) but with the tracks required to originate from within a fixed interval along the beam line around the leading track. Tracks with p T &gt; 1 GeV are counted in the core cone region R &lt; 0.2 and in the isolation annulus 0.2 &lt; R &lt; 0.4 around the tau candidate direction. A track multiplicity requirement selects tau candidates with 1 ≤ N trk R&lt;0.2 ≤ 3 and N trk 0.2&lt; R&lt;0.4 ≤ 1. Finally, the HLT precision tracking is run, and a collection of variables built from calorimeter and track variables are input to a Boosted Decision Tree (BDT), which produces a score used for the final tau identification. The implementation of those variables follows closely their offline counterparts as described in Ref. [47]. In addition, the same BDT training is used offline and online to ensure a maximal correlation between online and offline identification criteria. The performance of the offline training was found to be comparable to a dedicated online training. To ensure a robust response under differing pile-up conditions, corrections as a function of the average number of interactions per bunch-crossing are applied to the discriminating variables. Working points of the BDT are tuned separately for 1-prong and 3-prong candidates. The baseline medium working point operates with an efficiency of 95% (70%) for true 1-prong (3-prong) taus.At the HLT three sequential selections are made. First, a minimum requirement is applied to the transverse energy of the tau candidate. The energy is calculated using the locally calibrated topo-clusters of calorimeter cells contained in a cone of size R = 0.2 around the L1 tau RoI direction taken from the L1 cluster. A dedicated tau energy calibration scheme is used. Second, two-stage fast tracking (Sect. 5.1.3) is used to select tau candidates with low track multiplicity. A leading track is sought within a narrow cone ( R = 0.1) around the tau direction followed by a second fast tracking step using a larger cone ( R = 0.4) but with the tracks required to originate from within a fixed interval along the beam line around the leading track. Tracks with p T &gt; 1 GeV are counted in the core cone region R &lt; 0.2 and in the isolation annulus 0.2 &lt; R &lt; 0.4 around the tau candidate direction. A track multiplicity requirement selects tau candidates with 1 ≤ N trk R&lt;0.2 ≤ 3 and N trk 0.2&lt; R&lt;0.4 ≤ 1. Finally, the HLT precision tracking is run, and a collection of variables built from calorimeter and track variables are input to a Boosted Decision Tree (BDT), which produces a score used for the final tau identification. The implementation of those variables follows closely their offline counterparts as described in Ref. [47]. In addition, the same BDT training is used offline and online to ensure a maximal correlation between online and offline identification criteria. The performance of the offline training was found to be comparable to a dedicated online training. To ensure a robust response under differing pile-up conditions, corrections as a function of the average number of interactions per bunch-crossing are applied to the discriminating variables. Working points of the BDT are tuned separately for 1-prong and 3-prong candidates. The baseline medium working point operates with an efficiency of 95% (70%) for true 1-prong (3-prong) taus.</p>
        <p>The primary tau triggers consist of triggers for single high transverse momentum taus, and combined τ + X triggers, where X stands for an electron, muon, a second tau or E miss T . The transverse momentum thresholds used in the single-tau and ditau triggers in 2015 are indicated in Table 1. For all tau triggers the L1 isolation, HLT track multiplicity and online medium identification requirements are applied to the tau candidates.The primary tau triggers consist of triggers for single high transverse momentum taus, and combined τ + X triggers, where X stands for an electron, muon, a second tau or E miss T . The transverse momentum thresholds used in the single-tau and ditau triggers in 2015 are indicated in Table 1. For all tau triggers the L1 isolation, HLT track multiplicity and online medium identification requirements are applied to the tau candidates.</p>
        <p>Due to L1 rate limitations, the combined triggers τ +(e, μ) and τ +E miss T require the presence of an additional jet candidate at L1 with transverse momentum above 25 and 20 GeV, respectively. Variants of these triggers with higher thresholds for the tau transverse momentum and without the L1 jet requirement are also included in the trigger menu. Figure 36 shows the L1 and HLT output rates as function of the instantaneous luminosity for the primary single-tau, ditau, τ + e, τ + μ and τ +E miss T triggers.Due to L1 rate limitations, the combined triggers τ +(e, μ) and τ +E miss T require the presence of an additional jet candidate at L1 with transverse momentum above 25 and 20 GeV, respectively. Variants of these triggers with higher thresholds for the tau transverse momentum and without the L1 jet requirement are also included in the trigger menu. Figure 36 shows the L1 and HLT output rates as function of the instantaneous luminosity for the primary single-tau, ditau, τ + e, τ + μ and τ +E miss T triggers.</p>
        <p>The efficiency of the tau trigger was measured using a tagand-probe (T&amp;P) method in an enriched sample of Z → τ μ τ had → μ + 2ν + τ had events, where τ μ is a tau lepton decaying to μνν and τ had is a tau lepton decaying hadronically. Events are selected by the lowest unprescaled singlemuon trigger and are tagged by an offline reconstructed and isolated muon with transverse momentum above 22 GeV. The presence of an offline reconstructed tau candidate with transverse momentum above 25 GeV, one or three tracks, fulfilling the medium identification criteria and with electric charge opposite to the muon charge is also required. This reconstructed tau candidate is the probe with respect to which the tau trigger efficiency is measured. The event selection used to enhance the sample with Z → τ μ τ had events and therefore the purity of the probe tau candidate is similar to the one described in Ref. [47]: to reject Z (→ μμ) + jets and W (→ μν) + jets events, the invariant mass of the muon and the offline tau candidate is required to be between 45 and 80 GeV, the transverse mass, m T , composed of the muon p T andThe efficiency of the tau trigger was measured using a tagand-probe (T&amp;P) method in an enriched sample of Z → τ μ τ had → μ + 2ν + τ had events, where τ μ is a tau lepton decaying to μνν and τ had is a tau lepton decaying hadronically. Events are selected by the lowest unprescaled singlemuon trigger and are tagged by an offline reconstructed and isolated muon with transverse momentum above 22 GeV. The presence of an offline reconstructed tau candidate with transverse momentum above 25 GeV, one or three tracks, fulfilling the medium identification criteria and with electric charge opposite to the muon charge is also required. This reconstructed tau candidate is the probe with respect to which the tau trigger efficiency is measured. The event selection used to enhance the sample with Z → τ μ τ had events and therefore the purity of the probe tau candidate is similar to the one described in Ref. [47]: to reject Z (→ μμ) + jets and W (→ μν) + jets events, the invariant mass of the muon and the offline tau candidate is required to be between 45 and 80 GeV, the transverse mass, m T , composed of the muon p T and</p>
        <p>) is required to be smaller than 50 GeV, and the variable built from the difference in azimuth between the muon and E miss T and between the offline tau candidate and E miss T (cos φ(μ, E miss T ) + cos φ(τ, E miss T )) is required to be above -0.5. The dominant sources of background events in the resulting sample are W (→ μν)+jets and multi-jet events and their contributions are determined in data as described in Ref. [47]. The multi-jet contribution is estimated from events where the offline tau candidate and the muon have the same electric charge. The W (→ μν) + jets contribution is estimated from events with high m T .) is required to be smaller than 50 GeV, and the variable built from the difference in azimuth between the muon and E miss T and between the offline tau candidate and E miss T (cos φ(μ, E miss T ) + cos φ(τ, E miss T )) is required to be above -0.5. The dominant sources of background events in the resulting sample are W (→ μν)+jets and multi-jet events and their contributions are determined in data as described in Ref. [47]. The multi-jet contribution is estimated from events where the offline tau candidate and the muon have the same electric charge. The W (→ μν) + jets contribution is estimated from events with high m T .</p>
        <p>Distributions of the transverse momentum, pseudorapidity, track multiplicity and BDT discriminant score for the HLT tau candidates matched to the offline probe tau candidates are shown in Fig. 37. The HLT tau candidates pass the tau25_medium trigger, which requires an isolated L1 RoI with transverse momentum above 12 GeV and a tau candidate at the HLT with transverse momentum above 25 GeV satisfying the track multiplicity and the online medium identification criteria. The observed distributions in data are in good agreement with simulation.Distributions of the transverse momentum, pseudorapidity, track multiplicity and BDT discriminant score for the HLT tau candidates matched to the offline probe tau candidates are shown in Fig. 37. The HLT tau candidates pass the tau25_medium trigger, which requires an isolated L1 RoI with transverse momentum above 12 GeV and a tau candidate at the HLT with transverse momentum above 25 GeV satisfying the track multiplicity and the online medium identification criteria. The observed distributions in data are in good agreement with simulation.</p>
        <p>The estimated background is subtracted from data and the uncertainty in this subtraction is considered as a systematic uncertainty in the measured efficiency. This systematic uncertainty includes uncertainties in the background contri-butions estimated from both simulation and data. Figure 38a shows the measured efficiency for the tau25_medium trigger as a function of the transverse momentum of the offline tau candidate. The efficiency loss of the HLT with respect to L1 is mainly due to the HLT's track multiplicity selection and its BDT selection, which uses slightly different input variables online and offline. In Fig. 38b this efficiency is compared with simulation. The statistical uncertainties in data and simulation are shown together with the systematic uncertainties associated with the background subtraction procedure in data.The estimated background is subtracted from data and the uncertainty in this subtraction is considered as a systematic uncertainty in the measured efficiency. This systematic uncertainty includes uncertainties in the background contri-butions estimated from both simulation and data. Figure 38a shows the measured efficiency for the tau25_medium trigger as a function of the transverse momentum of the offline tau candidate. The efficiency loss of the HLT with respect to L1 is mainly due to the HLT's track multiplicity selection and its BDT selection, which uses slightly different input variables online and offline. In Fig. 38b this efficiency is compared with simulation. The statistical uncertainties in data and simulation are shown together with the systematic uncertainties associated with the background subtraction procedure in data.</p>
        <p>The E miss T trigger is used in searches where the final state contains only jets and large E miss T . The E miss T trigger can also be the most efficient trigger for selecting final states that contain highly energetic muons. An example is searches for supersymmetric particle production where jets, leptons and invisible particles are produced. Another major use is for multi-particle final states where the combination of E miss T with other trigger objects such as jets, electrons, or photons enables lower thresholds to be used for these other objects than would otherwise be possible. Finally, the E miss T trigger collects data samples used for detector performance studies. For example, the data set used for electron efficiency calculations in events consistent with a W boson is selected with an E miss T trigger.The E miss T trigger is used in searches where the final state contains only jets and large E miss T . The E miss T trigger can also be the most efficient trigger for selecting final states that contain highly energetic muons. An example is searches for supersymmetric particle production where jets, leptons and invisible particles are produced. Another major use is for multi-particle final states where the combination of E miss T with other trigger objects such as jets, electrons, or photons enables lower thresholds to be used for these other objects than would otherwise be possible. Finally, the E miss T trigger collects data samples used for detector performance studies. For example, the data set used for electron efficiency calculations in events consistent with a W boson is selected with an E miss T trigger.</p>
        <p>The very large rate of hadronic jet production means that, even with reasonably good calorimeter resolution, jet energy mismeasurement can lead to an unaffordably large E miss The improvements in the L1 E miss T determination, including the L1 dynamic pedestal correction described in Sect. 3.1, have been important in maintaining L1 performance. In par-ticular they have permitted the L1_XE50 trigger to be used without prescale throughout 2015.The very large rate of hadronic jet production means that, even with reasonably good calorimeter resolution, jet energy mismeasurement can lead to an unaffordably large E miss The improvements in the L1 E miss T determination, including the L1 dynamic pedestal correction described in Sect. 3.1, have been important in maintaining L1 performance. In par-ticular they have permitted the L1_XE50 trigger to be used without prescale throughout 2015.</p>
        <p>To fulfil the desired broad E miss T -based physics programme, different HLT algorithmic strategies based on cells, jets or topo-clusters in addition to two methods for correcting the effects of pile-up were developed during LS1 and deployed during 2015 data-taking. While the offline algo- • Cell algorithm (xe): The measured energy in each LAr and Tile calorimeter cell, labelled i, and the position of the cell in the detector are used to obtain the components of the cell measured momentum in the massless approximation, i.e. p x,i = E i sin θ i cos φ i and p y,i = E i sin θ i sin φ i . To suppress noise and cells with large negative energy, only those cells with energy satisfying |E i | &gt; 2σ i and E i &gt; -5σ i , are considered further, where σ i is the noise in the cell energy measurement, including the noise-like effects from pile-up. 4 Non-functioning calorimeter cells are masked out and do not contribute to the calculation. The total missing transverse momentum two-vector p miss T =i ( p x,i , p y,i ) is found from For each topocluster j, the momentum components ( p x, j , p y, j ) are calculated in the approximation that the particles contributing energy to the cluster are massless, and, in a manner similar to the cell algorithm, the missing transverse momentum is calculated from the negative vector sum of these components.To fulfil the desired broad E miss T -based physics programme, different HLT algorithmic strategies based on cells, jets or topo-clusters in addition to two methods for correcting the effects of pile-up were developed during LS1 and deployed during 2015 data-taking. While the offline algo- • Cell algorithm (xe): The measured energy in each LAr and Tile calorimeter cell, labelled i, and the position of the cell in the detector are used to obtain the components of the cell measured momentum in the massless approximation, i.e. p x,i = E i sin θ i cos φ i and p y,i = E i sin θ i sin φ i . To suppress noise and cells with large negative energy, only those cells with energy satisfying |E i | &gt; 2σ i and E i &gt; -5σ i , are considered further, where σ i is the noise in the cell energy measurement, including the noise-like effects from pile-up. 4 Non-functioning calorimeter cells are masked out and do not contribute to the calculation. The total missing transverse momentum two-vector p miss T =i ( p x,i , p y,i ) is found from For each topocluster j, the momentum components ( p x, j , p y, j ) are calculated in the approximation that the particles contributing energy to the cluster are massless, and, in a manner similar to the cell algorithm, the missing transverse momentum is calculated from the negative vector sum of these components.</p>
        <p>• Pile-up suppression algorithm (xe_tc_pueta): This algorithm is based on the topo-cluster E miss T algorithm described above, but includes a further pile-up suppression method that is intended to limit the degradation of the E miss T resolution at very high pile-up. The method starts by calculating the average topo-cluster energy and standard deviation in ten regions of pseudorapidity covering, in equal steps, -5.0 &lt; η &lt; 5.0 in the calorimeter. In each pseudorapidity region, known as a ring, the topo-clusters of energy above 2σ are omitted and the average energy of the residual topo-clusters is calculated. This average represents an estimate of the energy contribution from pile-up in that ring. The pile-up energy density in each ring is obtained by dividing the average energy by the solid angle of the ring. This energy density is then multiplied by the solid angle of each topo-cluster and then subtracted from the energy of that topo-cluster to obtain a topo-cluster energy measurement corrected for pile-up. The E miss T is recalculated as described above using the ( p x, j , p y, j ) of topo-clusters after the pile-up subtraction.• Pile-up suppression algorithm (xe_tc_pueta): This algorithm is based on the topo-cluster E miss T algorithm described above, but includes a further pile-up suppression method that is intended to limit the degradation of the E miss T resolution at very high pile-up. The method starts by calculating the average topo-cluster energy and standard deviation in ten regions of pseudorapidity covering, in equal steps, -5.0 &lt; η &lt; 5.0 in the calorimeter. In each pseudorapidity region, known as a ring, the topo-clusters of energy above 2σ are omitted and the average energy of the residual topo-clusters is calculated. This average represents an estimate of the energy contribution from pile-up in that ring. The pile-up energy density in each ring is obtained by dividing the average energy by the solid angle of the ring. This energy density is then multiplied by the solid angle of each topo-cluster and then subtracted from the energy of that topo-cluster to obtain a topo-cluster energy measurement corrected for pile-up. The E miss T is recalculated as described above using the ( p x, j , p y, j ) of topo-clusters after the pile-up subtraction.</p>
        <p>• Pile-up fit algorithm (xe_tc_pufit): Starting again from the topo-cluster E miss T described above, a different pile-up suppression method is used in this algorithm. The calorimeter is partitioned into 112 towers each of size η × φ ≈ 0.71 × 0.79. For each tower, the p x and p y components of all the topo-clusters with centres in that tower are summed to obtain the transverse momentum p T,k of that kth tower. The transverse energy sum of the tower E T,k is also calculated from the scalar sum of the p T of the individual clusters. If E T,k &lt; 45 GeV, the tower is determined to be below threshold and its energy assumed to be due to pile-up. The average pile-up E T density is calculated from k E T,k / k A k of all the towers below threshold, where A k is the total area in (η, φ) coordinates of those towers. A fit estimates the E T contributed by pile-up in each tower above threshold using the average pile-up E T density and constraining the event-wide E miss T from pile-up to be zero within resolution. These estimated pile-up contributions are subtracted from the corresponding E T measurements for towers above threshold, and these corrected E T values are used to calculate E miss T .• Pile-up fit algorithm (xe_tc_pufit): Starting again from the topo-cluster E miss T described above, a different pile-up suppression method is used in this algorithm. The calorimeter is partitioned into 112 towers each of size η × φ ≈ 0.71 × 0.79. For each tower, the p x and p y components of all the topo-clusters with centres in that tower are summed to obtain the transverse momentum p T,k of that kth tower. The transverse energy sum of the tower E T,k is also calculated from the scalar sum of the p T of the individual clusters. If E T,k &lt; 45 GeV, the tower is determined to be below threshold and its energy assumed to be due to pile-up. The average pile-up E T density is calculated from k E T,k / k A k of all the towers below threshold, where A k is the total area in (η, φ) coordinates of those towers. A fit estimates the E T contributed by pile-up in each tower above threshold using the average pile-up E T density and constraining the event-wide E miss T from pile-up to be zero within resolution. These estimated pile-up contributions are subtracted from the corresponding E T measurements for towers above threshold, and these corrected E T values are used to calculate E miss T .</p>
        <p>Figure 39 shows the E miss T distribution of the various HLT algorithms for events accepted into the Main physics stream. The differences observed between the cell-based and the topo-cluster-based E miss T distributions are caused in part by different calibration; the cell-based algorithm is calibrated at the EM scale, while algorithms based on topo-clusters generally have larger values of E miss T as they include a correction for the calorimeter response to hadrons (hadronic scale). Differences between the E miss T distributions for the various pile-up correction schemes are small, since these algorithms were optimised to improve the resolution at large pile-up values of 80 overlapping interactions that will only be achieved in future LHC runs.Figure 39 shows the E miss T distribution of the various HLT algorithms for events accepted into the Main physics stream. The differences observed between the cell-based and the topo-cluster-based E miss T distributions are caused in part by different calibration; the cell-based algorithm is calibrated at the EM scale, while algorithms based on topo-clusters generally have larger values of E miss T as they include a correction for the calorimeter response to hadrons (hadronic scale). Differences between the E miss T distributions for the various pile-up correction schemes are small, since these algorithms were optimised to improve the resolution at large pile-up values of 80 overlapping interactions that will only be achieved in future LHC runs.</p>
        <p>All the primary HLT E miss T algorithms used in 2015 were seeded by the L1_XE50 trigger with a nominal threshold, calibrated at the EM scale, of 50 GeV. The L1_XE50 output rate was approximately 700 Hz at an instantaneous luminosity of 5 × 10 33 cm -2 s -1 as shown in Fig. 40a. The HLT xe trigger with a threshold of 70 GeV remained unprescaled throughout the 2015 data-taking period. The typical output rate for this trigger was approximately 50 Hz at the same luminosity as seen in Fig. 40b. The topo-cluster-based algorithms, all of which are calibrated at the hadronic scale, had rates of approximately 110 Hz at the equivalent nominal threshold of 70 GeV. The output rate from these algorithms is larger for the same nominal threshold due in part to the different calibration methods. Prescaled triggers at a set of lower L1 and HLT thresholds, with HLT output rates of order 1 Hz each, were included in the menu to record a sample of data from which the efficiency of the unprescaled, primary physics triggers could be calculated. Further triggers based on the significance of the observed E miss T , known as xs triggers [48] were used to select W → eν events for electron reconstruction performance studies. Triggers used during Run 1 for selecting events based on the scalar sum of the transverse energy of all calorimeter cells E T were found to have a high sensitivity to pile-up [48], and so were not used during the proton-proton run in 2016. 5All the primary HLT E miss T algorithms used in 2015 were seeded by the L1_XE50 trigger with a nominal threshold, calibrated at the EM scale, of 50 GeV. The L1_XE50 output rate was approximately 700 Hz at an instantaneous luminosity of 5 × 10 33 cm -2 s -1 as shown in Fig. 40a. The HLT xe trigger with a threshold of 70 GeV remained unprescaled throughout the 2015 data-taking period. The typical output rate for this trigger was approximately 50 Hz at the same luminosity as seen in Fig. 40b. The topo-cluster-based algorithms, all of which are calibrated at the hadronic scale, had rates of approximately 110 Hz at the equivalent nominal threshold of 70 GeV. The output rate from these algorithms is larger for the same nominal threshold due in part to the different calibration methods. Prescaled triggers at a set of lower L1 and HLT thresholds, with HLT output rates of order 1 Hz each, were included in the menu to record a sample of data from which the efficiency of the unprescaled, primary physics triggers could be calculated. Further triggers based on the significance of the observed E miss T , known as xs triggers [48] were used to select W → eν events for electron reconstruction performance studies. Triggers used during Run 1 for selecting events based on the scalar sum of the transverse energy of all calorimeter cells E T were found to have a high sensitivity to pile-up [48], and so were not used during the proton-proton run in 2016. 5</p>
        <p>Since E miss T is a global observable calculated from many contributions, each of which has its own detector resolution, the efficiency of the E miss T trigger for any particular analysis inevitably depends on the event selection used in that analysis. The efficiency turn-on curves of the various E miss where the triggers approach full efficiency, the topo-cluster-based HLT algorithms show good linearity at values close to unity. The L1 and the xe HLT algorithms also show stable linearity in the trigger efficiency plateau, but at a lower value, reflecting their calibration at the EM scale rather than the hadronic scale.Since E miss T is a global observable calculated from many contributions, each of which has its own detector resolution, the efficiency of the E miss T trigger for any particular analysis inevitably depends on the event selection used in that analysis. The efficiency turn-on curves of the various E miss where the triggers approach full efficiency, the topo-cluster-based HLT algorithms show good linearity at values close to unity. The L1 and the xe HLT algorithms also show stable linearity in the trigger efficiency plateau, but at a lower value, reflecting their calibration at the EM scale rather than the hadronic scale.</p>
        <p>The E miss T resolution is defined as the RMS of the xcomponent of the core of the p miss T distribution. Since the resolution is dominated by the stochastic fluctuations in calorimeter energy measurements, it is shown in Fig. 42b as a function of the offline value of E T (reconstructed offline without muon corrections). The expected approximate scaling of E miss T with √ E T can be observed. The stochastic contribution to the resolution can be seen to be accompanied by an offset that varies from algorithm to algorithm and that is lower in the cell-based, electromagnetically calibrated L1 and xe algorithms. Such differences are expected because different noise suppression schemes are used to define calorimeter cells and topological clusters.The E miss T resolution is defined as the RMS of the xcomponent of the core of the p miss T distribution. Since the resolution is dominated by the stochastic fluctuations in calorimeter energy measurements, it is shown in Fig. 42b as a function of the offline value of E T (reconstructed offline without muon corrections). The expected approximate scaling of E miss T with √ E T can be observed. The stochastic contribution to the resolution can be seen to be accompanied by an offset that varies from algorithm to algorithm and that is lower in the cell-based, electromagnetically calibrated L1 and xe algorithms. Such differences are expected because different noise suppression schemes are used to define calorimeter cells and topological clusters.</p>
        <p>Figure 43 shows the efficiency of the trigger-level E miss T algorithm for W → μν events for several ranges of the number of reconstructed vertices. The effect of pile-up on the E miss T turn-on curves can be seen in this figure for the topocluster algorithm (xe_tc_lcw), which does not employ any pile-up correction methods. Some degradation of efficiency is observed for larger numbers of proton-proton vertices N vtx . The larger pile-up both increases the trigger rate, through increasing the probability to pass the trigger at lower E miss T , and degrades the efficiency in the turn-on region. significance is defined as the impact parameter divided by the associated uncertainty. The impact parameters are signed such that track displacements in the direction of the jet have positive values, while tracks with displacements opposite of the jet direction are negativeFigure 43 shows the efficiency of the trigger-level E miss T algorithm for W → μν events for several ranges of the number of reconstructed vertices. The effect of pile-up on the E miss T turn-on curves can be seen in this figure for the topocluster algorithm (xe_tc_lcw), which does not employ any pile-up correction methods. Some degradation of efficiency is observed for larger numbers of proton-proton vertices N vtx . The larger pile-up both increases the trigger rate, through increasing the probability to pass the trigger at lower E miss T , and degrades the efficiency in the turn-on region. significance is defined as the impact parameter divided by the associated uncertainty. The impact parameters are signed such that track displacements in the direction of the jet have positive values, while tracks with displacements opposite of the jet direction are negative</p>
        <p>Several b-hadron properties are exploited to identify (tag) bjets. The b-hadrons have a mean lifetime of ∼1.5 ps and often travel several millimetres before decaying. Consequently, a secondary vertex (SV) displaced from a primary interaction point characterises the decay. Reconstructed tracks associated with this SV have large transverse and longitudinal (z 0 ) impact parameters with respect to the primary vertex. In addition, b-hadrons go through hard fragmentation and have a relatively high mass of about 5 GeV. Thus, in addition to the decay length, b-jets can be distinguished from light-quark jets by having a large invariant mass, a large fraction of jet energy carried by tracks and a large track multiplicity.Several b-hadron properties are exploited to identify (tag) bjets. The b-hadrons have a mean lifetime of ∼1.5 ps and often travel several millimetres before decaying. Consequently, a secondary vertex (SV) displaced from a primary interaction point characterises the decay. Reconstructed tracks associated with this SV have large transverse and longitudinal (z 0 ) impact parameters with respect to the primary vertex. In addition, b-hadrons go through hard fragmentation and have a relatively high mass of about 5 GeV. Thus, in addition to the decay length, b-jets can be distinguished from light-quark jets by having a large invariant mass, a large fraction of jet energy carried by tracks and a large track multiplicity.</p>
        <p>As track and vertex reconstruction are crucial for the identification of b-jets, the b-jet trigger relies heavily on the performance of the ID tracking described in Sect. 5.1. Several improvements in the ID tracking made for Run 2 have directly benefited the b-jet trigger. The new IBL improves the impact parameter resolution of reconstructed tracks, leading to better b-jet identification and overall performance of the b-jet triggers [7]. Another improvement for Run 2 is the multiple-stage tracking described in Sect. 5.1.3. This new approach provides improved primary vertex finding and mitigates CPU requirements in the face of increased pile-up.As track and vertex reconstruction are crucial for the identification of b-jets, the b-jet trigger relies heavily on the performance of the ID tracking described in Sect. 5.1. Several improvements in the ID tracking made for Run 2 have directly benefited the b-jet trigger. The new IBL improves the impact parameter resolution of reconstructed tracks, leading to better b-jet identification and overall performance of the b-jet triggers [7]. Another improvement for Run 2 is the multiple-stage tracking described in Sect. 5.1.3. This new approach provides improved primary vertex finding and mitigates CPU requirements in the face of increased pile-up.</p>
        <p>The basic inputs to b-tagging are reconstructed jets, reconstructed tracks and the position of the primary vertex. The jet reconstruction used in the trigger is described in Sect. 6.4.1. The b-jet trigger uses tracks from the precision stage of the ID trigger reconstruction. The beam-spot location is used for the position of the primary vertex in the plane transverse to the beam line. Dedicated algorithms are run online to recon-struct and monitor the position of the beam spot in real time. The position of the primary vertex along the beam line is taken from the z position of the primary vertex reconstructed as described in Sect. 5.1.3. Distributions of the transverse and longitudinal impact parameter significances for lightflavour and b-quark jets are shown in Fig. 44 for a sample of simulated t t events. Tracks used in the online b-tagging are compared to the corresponding tracks used offline.The basic inputs to b-tagging are reconstructed jets, reconstructed tracks and the position of the primary vertex. The jet reconstruction used in the trigger is described in Sect. 6.4.1. The b-jet trigger uses tracks from the precision stage of the ID trigger reconstruction. The beam-spot location is used for the position of the primary vertex in the plane transverse to the beam line. Dedicated algorithms are run online to recon-struct and monitor the position of the beam spot in real time. The position of the primary vertex along the beam line is taken from the z position of the primary vertex reconstructed as described in Sect. 5.1.3. Distributions of the transverse and longitudinal impact parameter significances for lightflavour and b-quark jets are shown in Fig. 44 for a sample of simulated t t events. Tracks used in the online b-tagging are compared to the corresponding tracks used offline.</p>
        <p>During Run 1, the b-jet triggers used a combination of two likelihood-based algorithms, IP3D and SV1 [49]. The IP3D algorithm discriminates between b-and light-jets using the two-dimensional distribution of the longitudinal and transverse impact parameter significances. The SV1 algorithm exploits properties of the secondary vertex such as the invariant mass of tracks matched to the vertex, the fraction of the jet energy associated with the secondary vertex and the number of two-track vertices. These Run 1 algorithms, optimised for Run 2 conditions, were used during 2015 data-taking. Three operating points, loose, medium and tight, are defined to correspond to b-jet identification efficiencies obtained from simulated t t events of 79, 72 and 62%, respectively.During Run 1, the b-jet triggers used a combination of two likelihood-based algorithms, IP3D and SV1 [49]. The IP3D algorithm discriminates between b-and light-jets using the two-dimensional distribution of the longitudinal and transverse impact parameter significances. The SV1 algorithm exploits properties of the secondary vertex such as the invariant mass of tracks matched to the vertex, the fraction of the jet energy associated with the secondary vertex and the number of two-track vertices. These Run 1 algorithms, optimised for Run 2 conditions, were used during 2015 data-taking. Three operating points, loose, medium and tight, are defined to correspond to b-jet identification efficiencies obtained from simulated t t events of 79, 72 and 62%, respectively.</p>
        <p>Another major development in the b-jet trigger for Run 2 is the adaptation of the offline b-tagging algorithms [50] for use in the trigger. The use of the offline MV2 multivariate b-tagging algorithm provides better online b-jet identification and leads to a higher level of coherence between the online and offline b-tagging decisions. The MV2 algorithm uses inputs from the IP3D, SV1 and JetFitter algorithms. The JetFitter algorithm exploits the topological structure of weak b-and c-hadron decays inside the jet. The MV2 algorithm used in the trigger was optimised to identify b-jets using a training sample with a background composition of 80% Figure 46 shows the efficiency of the online b-tagging as a function of jet p T for the three operating points. The efficiencies are calculated in a pure sample of b-jets from fully leptonic t t decays and are computed with respect to jets identified by the 70% working point of the MV2c20 algorithm. Events used in the efficiency calculation require an online jet with p T greater than 40 GeV. A significant gain in trigger efficiency is seen when moving to the MV2 b-tagging algorithms.Another major development in the b-jet trigger for Run 2 is the adaptation of the offline b-tagging algorithms [50] for use in the trigger. The use of the offline MV2 multivariate b-tagging algorithm provides better online b-jet identification and leads to a higher level of coherence between the online and offline b-tagging decisions. The MV2 algorithm uses inputs from the IP3D, SV1 and JetFitter algorithms. The JetFitter algorithm exploits the topological structure of weak b-and c-hadron decays inside the jet. The MV2 algorithm used in the trigger was optimised to identify b-jets using a training sample with a background composition of 80% Figure 46 shows the efficiency of the online b-tagging as a function of jet p T for the three operating points. The efficiencies are calculated in a pure sample of b-jets from fully leptonic t t decays and are computed with respect to jets identified by the 70% working point of the MV2c20 algorithm. Events used in the efficiency calculation require an online jet with p T greater than 40 GeV. A significant gain in trigger efficiency is seen when moving to the MV2 b-tagging algorithms.</p>
        <p>Several b-jet triggers have been implemented with different combinations of jets and b-tagged jets, using different p T thresholds and b-tagging operating points. The operating points, thresholds and multiplicities, for several of the primary b-jet triggers are listed in Table 1. The jet multiplicities vary between one and four, with up to two b-tagged jets. The b-jet triggers are typically seeded at L1 using either a single jet with E T &gt; 100 GeV or three jets with E T &gt; 25 GeV and pseudorapidity |η| &lt; 2.5. Rates of various b-jet triggers as a function of luminosity are shown in Fig. 47. (s) , ϒ(1, 2, 3S)) or forming a B → μμX candidate after combination with additional tracks found in ID (open markers). As L1_2MU4 was prescaled at luminosities above 4 × 10 33 cm -2 s -1 , the rate of 2mu4_bJpsimumu seeded from this L1 trigger drops above that luminositySeveral b-jet triggers have been implemented with different combinations of jets and b-tagged jets, using different p T thresholds and b-tagging operating points. The operating points, thresholds and multiplicities, for several of the primary b-jet triggers are listed in Table 1. The jet multiplicities vary between one and four, with up to two b-tagged jets. The b-jet triggers are typically seeded at L1 using either a single jet with E T &gt; 100 GeV or three jets with E T &gt; 25 GeV and pseudorapidity |η| &lt; 2.5. Rates of various b-jet triggers as a function of luminosity are shown in Fig. 47. (s) , ϒ(1, 2, 3S)) or forming a B → μμX candidate after combination with additional tracks found in ID (open markers). As L1_2MU4 was prescaled at luminosities above 4 × 10 33 cm -2 s -1 , the rate of 2mu4_bJpsimumu seeded from this L1 trigger drops above that luminosity</p>
        <p>The trigger selection of events for B-physics analyses is primarily based on the identification of b-hadrons through decays including a muon pair in the final state. Examples are decays with charmonium, B → J/ψ(→ μμ)X , rare decays B 0 (s) → μμ, and semileptonic B → μμX . Decays of prompt charmonium and bottomonium are also identified through their dimuon decays, and are therefore similar to b-hadron decays, apart from the lack of measurable displacement from the pp interaction point.The trigger selection of events for B-physics analyses is primarily based on the identification of b-hadrons through decays including a muon pair in the final state. Examples are decays with charmonium, B → J/ψ(→ μμ)X , rare decays B 0 (s) → μμ, and semileptonic B → μμX . Decays of prompt charmonium and bottomonium are also identified through their dimuon decays, and are therefore similar to b-hadron decays, apart from the lack of measurable displacement from the pp interaction point.</p>
        <p>The primary suite of triggers require two muons at L1. Their rate is substantially reduced compared to single-muon L1 triggers. However, this results in inefficiencies at high transverse momentum, where the opening angle of the two muons becomes small for low-mass resonances, and the granularity at L1 is not sufficient to form separate RoIs. At the HLT, muons are reconstructed using the same algorithms as described in Sect. 5.3 with the additional requirement that the two muons should have opposite charges and form a good vertex (where the fit is performed using the ID track parameters) within a certain invariant mass window. The primary triggers use three dimuon mass windows: 2.5 to 4.3 GeV intended for the selection of J/ψ and ψ(2S) decays into muon pairs (including charmonia produced in b-hadron decays), 4.0 to 8.5 GeV for B 0 (s) → μμ decays, and 8 to 12 GeV for ϒ(1, 2, 3S) → μμ decays. These invariant mass selections are indicated by the bJpsimumu, bBmumu and bUpsimumu suffixes in the trigger names, respectively. Additional primary and supporting triggers are also implemented. Triggers using a single L1 muon RoI with an additional track found at the HLT do not have similar opening angle issues, but suffer from high rates and run with high prescale factors. These combined muon triggers are, however, essential components in data-driven estimates of the dimuon trigger efficiencies. Triggers requiring three muons at L1 help to maintain the lowest muon p T thresholds for certain event signatures with a likely presence of a third muon. Finally, for selecting semileptonic decays, such as B 0 → μμK * 0 (→ K + π -), searches for additional ID tracks and a combined vertex fit are performed assuming a few exclusive decay hypotheses. This reduces the rate with respect to a simple dimuon vertex selection thus allowing the dimuon mass window to be widened to the full kinematically allowed range. The corresponding trigger names use the bBmumuxv2 suffix.The primary suite of triggers require two muons at L1. Their rate is substantially reduced compared to single-muon L1 triggers. However, this results in inefficiencies at high transverse momentum, where the opening angle of the two muons becomes small for low-mass resonances, and the granularity at L1 is not sufficient to form separate RoIs. At the HLT, muons are reconstructed using the same algorithms as described in Sect. 5.3 with the additional requirement that the two muons should have opposite charges and form a good vertex (where the fit is performed using the ID track parameters) within a certain invariant mass window. The primary triggers use three dimuon mass windows: 2.5 to 4.3 GeV intended for the selection of J/ψ and ψ(2S) decays into muon pairs (including charmonia produced in b-hadron decays), 4.0 to 8.5 GeV for B 0 (s) → μμ decays, and 8 to 12 GeV for ϒ(1, 2, 3S) → μμ decays. These invariant mass selections are indicated by the bJpsimumu, bBmumu and bUpsimumu suffixes in the trigger names, respectively. Additional primary and supporting triggers are also implemented. Triggers using a single L1 muon RoI with an additional track found at the HLT do not have similar opening angle issues, but suffer from high rates and run with high prescale factors. These combined muon triggers are, however, essential components in data-driven estimates of the dimuon trigger efficiencies. Triggers requiring three muons at L1 help to maintain the lowest muon p T thresholds for certain event signatures with a likely presence of a third muon. Finally, for selecting semileptonic decays, such as B 0 → μμK * 0 (→ K + π -), searches for additional ID tracks and a combined vertex fit are performed assuming a few exclusive decay hypotheses. This reduces the rate with respect to a simple dimuon vertex selection thus allowing the dimuon mass window to be widened to the full kinematically allowed range. The corresponding trigger names use the bBmumuxv2 suffix.</p>
        <p>Dimuon trigger rate restrictions at L1 define the lowest muon transverse momentum thresholds for primary B-physics triggers in 2015 data-taking. HLT triggers using L1_2MU4 were unprescaled up to a luminosity of 4 × 10 33 cm -2 s -1 . Above this, triggers seeded from L1_MU6_2MU4, 6 which requires No accounting for overlaps between triggers is made, and the distributions are shown overlaid, and not stacked. For comparison, the number of candidates passing the lowest unprescaled single-muon trigger and supporting dimuon trigger is also shown two muons with p T above 4 and 6 GeV, were unprescaled. The overall loss of events collected with the former amounts to 15%. Higher-threshold triggers seeded from L1_2MU6 and L1_2MU10 were also active. Figure 48 shows the L1 rates for lowp T dimuon triggers as well as the HLT rates for various primary triggers seeded from them, as a function of the instantaneous luminosity.Dimuon trigger rate restrictions at L1 define the lowest muon transverse momentum thresholds for primary B-physics triggers in 2015 data-taking. HLT triggers using L1_2MU4 were unprescaled up to a luminosity of 4 × 10 33 cm -2 s -1 . Above this, triggers seeded from L1_MU6_2MU4, 6 which requires No accounting for overlaps between triggers is made, and the distributions are shown overlaid, and not stacked. For comparison, the number of candidates passing the lowest unprescaled single-muon trigger and supporting dimuon trigger is also shown two muons with p T above 4 and 6 GeV, were unprescaled. The overall loss of events collected with the former amounts to 15%. Higher-threshold triggers seeded from L1_2MU6 and L1_2MU10 were also active. Figure 48 shows the L1 rates for lowp T dimuon triggers as well as the HLT rates for various primary triggers seeded from them, as a function of the instantaneous luminosity.</p>
        <p>The invariant mass distribution of offline reconstructed dimuon candidates passing the suite of primary triggers is shown in Fig. 49. For comparison, the number of candidates passing the lowest unprescaled single-muon trigger is also shown, as well as the supporting dimuon trigger with wide invariant mass range.The invariant mass distribution of offline reconstructed dimuon candidates passing the suite of primary triggers is shown in Fig. 49. For comparison, the number of candidates passing the lowest unprescaled single-muon trigger is also shown, as well as the supporting dimuon trigger with wide invariant mass range.</p>
        <p>To evaluate the efficiency of the B-physics selection at the HLT, two supporting triggers with and without the oppositesign and vertex criteria are used. The first trigger requires that the events contain two opposite-sign muons and form a good fit to a common vertex, using the ID track parameters of the identified muons with a χ 2 &lt; 20 for the one degreeof-freedom. This selection is the same as used in primary dimuon triggers but has a wider invariant mass window. The second trigger differs by the absence of the muon charge selection and vertex fit. The efficiency is calculated using a sample collected by these triggers.To evaluate the efficiency of the B-physics selection at the HLT, two supporting triggers with and without the oppositesign and vertex criteria are used. The first trigger requires that the events contain two opposite-sign muons and form a good fit to a common vertex, using the ID track parameters of the identified muons with a χ 2 &lt; 20 for the one degreeof-freedom. This selection is the same as used in primary dimuon triggers but has a wider invariant mass window. The second trigger differs by the absence of the muon charge selection and vertex fit. The efficiency is calculated using a sample collected by these triggers.</p>
        <p>For the efficiency measurement, events are selected by requiring two offline reconstructed combined muons satisfying the tight quality selection criteria and p T (μ) &gt; 4 GeV, Fig. 50 The efficiency of the opposite-sign muon requirement and vertex quality selection applied for dimuon B-physics triggers as a function of p T (μμ) for three rapidity regions. Supporting dimuon triggers with and without the selection criteria applied are used to determine the efficiency. The integrated luminosity shown takes into account the high prescale factors applied to the supporting triggers |η(μ)| &lt; 2.3. The offline muons are fit to a common vertex, using their ID track parameters, with a fit quality of χ 2 /dof &lt; 10 and invariant mass |m(μμ)m J/ψ | &lt; 0.3 GeV. The number of J/ψ candidates is determined from a fit to the offline dimuon invariant mass distribution. The efficiency of the opposite-sign muon requirement and vertex quality selection is shown in Fig. 50 as a function of the offline dimuon transverse momentum p T (μμ) calculated using the track parameters extracted after the vertex fit, for three slices of J/ψ rapidity. The observed small drop in efficiency at high p T (μμ) is due to the increasing collinearity of the two muons.For the efficiency measurement, events are selected by requiring two offline reconstructed combined muons satisfying the tight quality selection criteria and p T (μ) &gt; 4 GeV, Fig. 50 The efficiency of the opposite-sign muon requirement and vertex quality selection applied for dimuon B-physics triggers as a function of p T (μμ) for three rapidity regions. Supporting dimuon triggers with and without the selection criteria applied are used to determine the efficiency. The integrated luminosity shown takes into account the high prescale factors applied to the supporting triggers |η(μ)| &lt; 2.3. The offline muons are fit to a common vertex, using their ID track parameters, with a fit quality of χ 2 /dof &lt; 10 and invariant mass |m(μμ)m J/ψ | &lt; 0.3 GeV. The number of J/ψ candidates is determined from a fit to the offline dimuon invariant mass distribution. The efficiency of the opposite-sign muon requirement and vertex quality selection is shown in Fig. 50 as a function of the offline dimuon transverse momentum p T (μμ) calculated using the track parameters extracted after the vertex fit, for three slices of J/ψ rapidity. The observed small drop in efficiency at high p T (μμ) is due to the increasing collinearity of the two muons.</p>
        <p>A large number of trigger upgrades and developments for the ATLAS experiment were made during the first long shutdown of the LHC in preparation for the Run 2 data-taking. A summary of the various updates as well as the first Run 2 performance studies can be found in this paper.A large number of trigger upgrades and developments for the ATLAS experiment were made during the first long shutdown of the LHC in preparation for the Run 2 data-taking. A summary of the various updates as well as the first Run 2 performance studies can be found in this paper.</p>
        <p>Many improvements in the L1 trigger were implemented including the addition of completely new systems. Upgrades in the L1 calorimeter trigger included the implementation of a dynamic pedestal correction to mitigate pile-up effects. In the L1 muon trigger, a new coincidence logic between the muon end-cap trigger and the innermost muon chamber has been used since 2015, and it is being extended with the hadronic calorimeter, to suppress the fake-muon rate. New chambers were also installed to increase the trigger coverage. In addition, the new central trigger processor doubles the number of L1 trigger thresholds and the L1 output rate limit has increased from 70 to 100 kHz. Furthermore, a new topological processor was installed and is being commissioned. A new HLT architecture was developed to unify the Level-2 and Event Filter scheme used in Run 1, improving the flexibility of the system. The HLT software was also upgraded, making the algorithms and selections closer to the offline reconstruction to maximise the efficiency, and making use of the newly installed systems such as the innermost pixel layer IBL.Many improvements in the L1 trigger were implemented including the addition of completely new systems. Upgrades in the L1 calorimeter trigger included the implementation of a dynamic pedestal correction to mitigate pile-up effects. In the L1 muon trigger, a new coincidence logic between the muon end-cap trigger and the innermost muon chamber has been used since 2015, and it is being extended with the hadronic calorimeter, to suppress the fake-muon rate. New chambers were also installed to increase the trigger coverage. In addition, the new central trigger processor doubles the number of L1 trigger thresholds and the L1 output rate limit has increased from 70 to 100 kHz. Furthermore, a new topological processor was installed and is being commissioned. A new HLT architecture was developed to unify the Level-2 and Event Filter scheme used in Run 1, improving the flexibility of the system. The HLT software was also upgraded, making the algorithms and selections closer to the offline reconstruction to maximise the efficiency, and making use of the newly installed systems such as the innermost pixel layer IBL.</p>
        <p>The trigger menu was revisited and redesigned to cope with the greater rates due to the higher centre-of-mass energy and increasing instantaneous luminosity. The different trigger signatures were set up according to the physics needs, considering different luminosity scenarios. The ATLAS trigger system was successfully commissioned with the first data acquired at 13 TeV. First performance studies of the different trigger signatures and trigger efficiencies with respect to the offline quantities are presented using the 13 TeV protonproton collision data with a 25 ns bunch separation collected during 2015.The trigger menu was revisited and redesigned to cope with the greater rates due to the higher centre-of-mass energy and increasing instantaneous luminosity. The different trigger signatures were set up according to the physics needs, considering different luminosity scenarios. The ATLAS trigger system was successfully commissioned with the first data acquired at 13 TeV. First performance studies of the different trigger signatures and trigger efficiencies with respect to the offline quantities are presented using the 13 TeV protonproton collision data with a 25 ns bunch separation collected during 2015.</p>
        <p>The four complex dead-time settings were 15/370, 42/381, 9/351 and 7/350, where the first number specifies the number of triggers and the second number specifies the number of bunch-crossings, e.g. 7 triggers inThe four complex dead-time settings were 15/370, 42/381, 9/351 and 7/350, where the first number specifies the number of triggers and the second number specifies the number of bunch-crossings, e.g. 7 triggers in</p>
        <p>bunch-crossings.bunch-crossings.</p>
        <p>-8 -6 -4 -2-8 -6 -4 -2</p>
        <p>-8 -6 -4 -2 --8 -6 -4 -2 -</p>
        <p>-10 5 -10 4 -10 3 -10 2 --10 5 -10 4 -10 3 -10 2 -</p>
        <p>A E T trigger was used during heavy-ion collisions at L1.A E T trigger was used during heavy-ion collisions at L1.</p>
        <p>L1 muon thresholds are inclusive, i.e. L1_MU6_2MU4 is a dimuon trigger.L1 muon thresholds are inclusive, i.e. L1_MU6_2MU4 is a dimuon trigger.</p>
        <p>We thank CERN for the very successful oper-We thank CERN for the very successful oper-</p>
        <p>ation of the LHC, as well as the support staff from our institutions without whom ATLAS could not be operated efficiently. We acknowledge the support of ANPCyT, Argentina; YerPhI, Armenia; ARC, Australia; BMWFW and FWF, Austria; the Royal Society and Leverhulme Trust, United Kingdom. The crucial computing support from all WLCG partners is acknowledged gratefully, in particular from CERN, the ATLAS Tier-1 facilities at TRIUMF (Canada), NDGF (Denmark, Norway, Sweden), CC-IN2P3 (France), KIT/GridKA (Germany), INFN-CNAF (Italy), NL-T1 (Netherlands), PIC (Spain), ASGC (Taiwan), RAL (UK) and BNL (USA), the Tier-2 facilities worldwide and large non-WLCG resource providers. Major contributors of computing resources are listed in Ref. [51].ation of the LHC, as well as the support staff from our institutions without whom ATLAS could not be operated efficiently. We acknowledge the support of ANPCyT, Argentina; YerPhI, Armenia; ARC, Australia; BMWFW and FWF, Austria; the Royal Society and Leverhulme Trust, United Kingdom. The crucial computing support from all WLCG partners is acknowledged gratefully, in particular from CERN, the ATLAS Tier-1 facilities at TRIUMF (Canada), NDGF (Denmark, Norway, Sweden), CC-IN2P3 (France), KIT/GridKA (Germany), INFN-CNAF (Italy), NL-T1 (Netherlands), PIC (Spain), ASGC (Taiwan), RAL (UK) and BNL (USA), the Tier-2 facilities worldwide and large non-WLCG resource providers. Major contributors of computing resources are listed in Ref. [51].</p>
        <p>Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecomm ons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. Funded by SCOAP 3 .Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecomm ons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. Funded by SCOAP 3 .</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:26+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in terms of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in detail. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.Over the past two decades, complex network theory provided the ideal framework for investigating the intimate relationships between the topological properties characterizing the wiring of connections among a system's unitary components and its emergent synchronized functioning. An increased number of setups from the real world found therefore a representation in terms of graphs, while more and more sophisticated methods were developed with the aim of furnishing a realistic description of the connectivity patterns under study. In particular, a significant number of systems in physics, biology and social science features a time-varying nature of the interactions among their units. We here give a comprehensive review of the major results obtained by contemporary studies on the emergence of synchronization in time-varying networks. In particular, two paradigmatic frameworks will be described in detail. The first encompasses those systems where the time dependence of the nodes' connections is due to adaptation, external forces, or any other process affecting each of the links of the network. The second framework, instead, corresponds to the case in which the structural evolution of the graph is due to the movement of the nodes, or agents, in physical spaces and to the fact that interactions may be ruled by space-dependent laws in a way that connections are continuously switched on and off in the course of the time. Finally, our report ends with a short discussion on promising directions and open problems for future studies.</p>
        <p>Complex networks are nothing but collections of nodes (or vertices) connected by links (or edges) that form a connec-3 tivity wiring featuring specific and rich topological properties. Such mathematical objects provide actually representations 4Complex networks are nothing but collections of nodes (or vertices) connected by links (or edges) that form a connec-3 tivity wiring featuring specific and rich topological properties. Such mathematical objects provide actually representations 4</p>
        <p>for modeling many real-world, distributed, systems [1][2][3][4][5]. At the same time, they also yield a comprehensive framework 5 to investigate the rise of collective behaviors emerging from the interaction of a large number of dynamical units, in 6 systems whose functioning occurs at microscopic scales (like, for instance, metabolic and genetic networks), or in systems 7for modeling many real-world, distributed, systems [1][2][3][4][5]. At the same time, they also yield a comprehensive framework 5 to investigate the rise of collective behaviors emerging from the interaction of a large number of dynamical units, in 6 systems whose functioning occurs at microscopic scales (like, for instance, metabolic and genetic networks), or in systems 7</p>
        <p>working at mesoscales (as the human brain), or even in systems, such as human societies or infrastructure networks, which 8 organize at global scales. 9working at mesoscales (as the human brain), or even in systems, such as human societies or infrastructure networks, which 8 organize at global scales. 9</p>
        <p>One of the most prominent features of real-world networks is that the interactions among the components are not 10 fixed in time, but they have an explicit temporal nature. They may be adaptive (and, therefore, their strength changes 11 in time), or they may even be suppressed in some moments and activated in others. Collective dynamics emerging in time-varying systems, such as consensus [6,7], disease spreading [8], process of chemotaxis [9], and many others, are found, on the other hand, in various areas and sectors, including functional brain networks [10], power transmission systems [11], person-to-person communication [12], wireless sensor networks [13], as well as many biological networks like metabolic, protein-protein interaction networks, and gene-regulatory systems [14][15][16].One of the most prominent features of real-world networks is that the interactions among the components are not 10 fixed in time, but they have an explicit temporal nature. They may be adaptive (and, therefore, their strength changes 11 in time), or they may even be suppressed in some moments and activated in others. Collective dynamics emerging in time-varying systems, such as consensus [6,7], disease spreading [8], process of chemotaxis [9], and many others, are found, on the other hand, in various areas and sectors, including functional brain networks [10], power transmission systems [11], person-to-person communication [12], wireless sensor networks [13], as well as many biological networks like metabolic, protein-protein interaction networks, and gene-regulatory systems [14][15][16].</p>
        <p>Properly modeling processes such as mutation in biological systems [17], synaptic plasticity in neuronal networks [18], or adaptation in social or financial market dynamics [19] would then require accounting for time-varying networks whose evolution may take place over characteristic time scales that are even commensurate with those of the node dynamics. In neuronal systems, existing neuronal interactions may not be active for all time, and new links may form over time. This makes the framework of temporal networks most suitable for modeling neuronal communication, as the passage of chemical molecules and electrical signals between neurons can be mimicked by a temporary edge between them that is switched on when these flows are active [20]. There have been few studies incorporating the time-varying character of connections in this context [10,21,22]. For instance, Ref. [21] studied persistent patterns of interconnection in time-varying cortical networks in humans during a simple motor act extracted from a set of high-resolution electroencephalography (EEG); Ref. [10] analyzed the dynamical evolution of functional brain networks in time-frequency space; and Ref. [22] identified significant modular structure in human brain function during learning over a range of temporal scales. So, a shift from a static to a dynamic neuronal interaction scenario is essential for further understanding neuronal communication.Properly modeling processes such as mutation in biological systems [17], synaptic plasticity in neuronal networks [18], or adaptation in social or financial market dynamics [19] would then require accounting for time-varying networks whose evolution may take place over characteristic time scales that are even commensurate with those of the node dynamics. In neuronal systems, existing neuronal interactions may not be active for all time, and new links may form over time. This makes the framework of temporal networks most suitable for modeling neuronal communication, as the passage of chemical molecules and electrical signals between neurons can be mimicked by a temporary edge between them that is switched on when these flows are active [20]. There have been few studies incorporating the time-varying character of connections in this context [10,21,22]. For instance, Ref. [21] studied persistent patterns of interconnection in time-varying cortical networks in humans during a simple motor act extracted from a set of high-resolution electroencephalography (EEG); Ref. [10] analyzed the dynamical evolution of functional brain networks in time-frequency space; and Ref. [22] identified significant modular structure in human brain function during learning over a range of temporal scales. So, a shift from a static to a dynamic neuronal interaction scenario is essential for further understanding neuronal communication.</p>
        <p>On the other hand, complex networks are the prominent candidates to describe the occurrence of synchronization, in many areas of science [1,3]. Such a collective state was first observed by Huygens in weakly coupled clock pendula [23], and later described in a variety of systems, ranging from fireflies in the forest [24], animal gaits [25], descriptions of the heart [26,27], improved understanding of brain seizures [28], nonlinear optics [29,30], and meteorology [31]. In particular, it has been shown that even chaotic oscillators can synchronize under suitable coupling functions and/or network architectures [32,33]. Recent investigations have sought to characterize how oscillatory elements coupled according to a large scale network architecture are impacted by the choice of the interaction topology and the corresponding spectral properties of the network [34][35][36][37][38]. A recent review on various aspects of synchronization in coupled systems and networks is available in Ref. [39].On the other hand, complex networks are the prominent candidates to describe the occurrence of synchronization, in many areas of science [1,3]. Such a collective state was first observed by Huygens in weakly coupled clock pendula [23], and later described in a variety of systems, ranging from fireflies in the forest [24], animal gaits [25], descriptions of the heart [26,27], improved understanding of brain seizures [28], nonlinear optics [29,30], and meteorology [31]. In particular, it has been shown that even chaotic oscillators can synchronize under suitable coupling functions and/or network architectures [32,33]. Recent investigations have sought to characterize how oscillatory elements coupled according to a large scale network architecture are impacted by the choice of the interaction topology and the corresponding spectral properties of the network [34][35][36][37][38]. A recent review on various aspects of synchronization in coupled systems and networks is available in Ref. [39].</p>
        <p>Synchronization of populations of dynamical units has attracted researchers from diverse fields such as physics, mathematics, biology, ecology and engineering [40][41][42][43][44]. Synchronization processes indeed are at the basis for the emergence of coherent global behaviors in both normal and abnormal brain functions [45], and play a crucial role in determining the food web dynamics in ecological systems [46]. So far, synchronized behaviors [33] have been mostly studied in the limit of static networks, e.g., networks whose wiring of connections is fixed with the emphasis focusing on how the complexity in the overall topology influences the propensity of the coupled units to synchronize [35,[47][48][49]. In particular, it has been established that proper weighting procedures in static complex networks are able to greatly enhance the appearance of synchronized behavior [50][51][52]. However, lately there have been efforts to incorporate a timevarying nature of the interactions leading to evolving networks. In one way such time variations represent the evolution of interactions over time. In another way they can be helpful in representing discontinuities in interactions, where the nodes interact only for limited time. Such time-varying interactions are commonly found in social networks, communication, biological systems, spread of epidemics, computer networks, world wide web, engineering systems, etc., and have been shown to result in significantly different emergent phenomena [10,21,[53][54][55][56][57][58][59][60][61][62].Synchronization of populations of dynamical units has attracted researchers from diverse fields such as physics, mathematics, biology, ecology and engineering [40][41][42][43][44]. Synchronization processes indeed are at the basis for the emergence of coherent global behaviors in both normal and abnormal brain functions [45], and play a crucial role in determining the food web dynamics in ecological systems [46]. So far, synchronized behaviors [33] have been mostly studied in the limit of static networks, e.g., networks whose wiring of connections is fixed with the emphasis focusing on how the complexity in the overall topology influences the propensity of the coupled units to synchronize [35,[47][48][49]. In particular, it has been established that proper weighting procedures in static complex networks are able to greatly enhance the appearance of synchronized behavior [50][51][52]. However, lately there have been efforts to incorporate a timevarying nature of the interactions leading to evolving networks. In one way such time variations represent the evolution of interactions over time. In another way they can be helpful in representing discontinuities in interactions, where the nodes interact only for limited time. Such time-varying interactions are commonly found in social networks, communication, biological systems, spread of epidemics, computer networks, world wide web, engineering systems, etc., and have been shown to result in significantly different emergent phenomena [10,21,[53][54][55][56][57][58][59][60][61][62].</p>
        <p>In various works, the case of time-varying networks has been taken into account [53,55,[63][64][65][66], among which most of the researches are prone to the fast switching case, i.e., the time scale of the variation in networks is much shorter than that of the oscillator dynamics. Systems under different time scales of network variation may exhibit very different synchronous behaviors, where the role of time scales for network synchronization could be of crucial importance. However, the case of a network evolution which takes place over characteristic time scales that commensurate with those of the node dynamics characterizes many situations, such as synaptic plasticity in neuronal networks [18], social or financial market adaptation dynamics [19,67,68], or mutation processes in biological systems [17]. In these situations, the time scale competition between local dynamics and network evolution becomes of utmost importance and, for this reason, is thus focused in this review as well.In various works, the case of time-varying networks has been taken into account [53,55,[63][64][65][66], among which most of the researches are prone to the fast switching case, i.e., the time scale of the variation in networks is much shorter than that of the oscillator dynamics. Systems under different time scales of network variation may exhibit very different synchronous behaviors, where the role of time scales for network synchronization could be of crucial importance. However, the case of a network evolution which takes place over characteristic time scales that commensurate with those of the node dynamics characterizes many situations, such as synaptic plasticity in neuronal networks [18], social or financial market adaptation dynamics [19,67,68], or mutation processes in biological systems [17]. In these situations, the time scale competition between local dynamics and network evolution becomes of utmost importance and, for this reason, is thus focused in this review as well.</p>
        <p>A temporal progression of links is an inherent feature also of several natural and artificial networks [69], and a static approximation to such systems is valid only when the changes in links occur over extremely long time scales. For instance, Ref. [70] describes the case in which a so-called function dynamics gives rise to networks that evolve according to a dynamical system. Major advances have been made in the analysis of such time-varying networks. In social interaction networks [71], the social relationship or communication between pairs of individuals changes continuously, and so links are continuously created or terminated or changed over time. There are applications where the coupling strengths and even the network topology can evolve in time. A large volume of literature has focused on temporal networks whose connectivity and coupling strengths vary over time [72][73][74]. In this context, recent researches have also focused on the emergence of synchronization in a time-varying complex network [59,[75][76][77][78][79][80].A temporal progression of links is an inherent feature also of several natural and artificial networks [69], and a static approximation to such systems is valid only when the changes in links occur over extremely long time scales. For instance, Ref. [70] describes the case in which a so-called function dynamics gives rise to networks that evolve according to a dynamical system. Major advances have been made in the analysis of such time-varying networks. In social interaction networks [71], the social relationship or communication between pairs of individuals changes continuously, and so links are continuously created or terminated or changed over time. There are applications where the coupling strengths and even the network topology can evolve in time. A large volume of literature has focused on temporal networks whose connectivity and coupling strengths vary over time [72][73][74]. In this context, recent researches have also focused on the emergence of synchronization in a time-varying complex network [59,[75][76][77][78][79][80].</p>
        <p>Many earlier approaches have studied the stability of the synchronized state in time-invariant networks by linearizing the dynamical equations. The master stability function approach [49] relates the spectral properties of the graph Laplacian of the network to synchrony of supported oscillators. It is shown that the spectrum of the graph Laplacian can be used to assess stability of the controlled system. This technique has been used in the study of synchronization stability on arbitrary network architectures [35]. Considering certain time-varying coupled network architectures, Stilwell et al. specifically built a novel concept of fast switching stability criterion [55]. They adopted a mathematical machinery from the field of switched systems which was not typically used in the synchronization community, and extracted stability criteria. Such approaches have enabled the analysis of stability of large class of synchronized oscillators. These local stability results can only be valid for small perturbations, and here small could actually be infinitesimal in some cases. It has been shown that if the connections change quite rapidly, then the network can be essentially modeled as the aggregate of the interactions over time [53,81]. However, if the Laplacian matrices at different times do not commute, the spread of transverse Lyapunov exponents decreases and for coupled Rössler oscillators the stability range of the time-varying underlying network is larger than that of for the time-average underlying network [54]. Long lasting interactions slowed down diffusion in such networks and the slow eigenmodes of the effective Laplacian matrix were shown to be affected more as compared to fast eigenmodes [60].Many earlier approaches have studied the stability of the synchronized state in time-invariant networks by linearizing the dynamical equations. The master stability function approach [49] relates the spectral properties of the graph Laplacian of the network to synchrony of supported oscillators. It is shown that the spectrum of the graph Laplacian can be used to assess stability of the controlled system. This technique has been used in the study of synchronization stability on arbitrary network architectures [35]. Considering certain time-varying coupled network architectures, Stilwell et al. specifically built a novel concept of fast switching stability criterion [55]. They adopted a mathematical machinery from the field of switched systems which was not typically used in the synchronization community, and extracted stability criteria. Such approaches have enabled the analysis of stability of large class of synchronized oscillators. These local stability results can only be valid for small perturbations, and here small could actually be infinitesimal in some cases. It has been shown that if the connections change quite rapidly, then the network can be essentially modeled as the aggregate of the interactions over time [53,81]. However, if the Laplacian matrices at different times do not commute, the spread of transverse Lyapunov exponents decreases and for coupled Rössler oscillators the stability range of the time-varying underlying network is larger than that of for the time-average underlying network [54]. Long lasting interactions slowed down diffusion in such networks and the slow eigenmodes of the effective Laplacian matrix were shown to be affected more as compared to fast eigenmodes [60].</p>
        <p>In order to describe all the above mentioned cases (and many more for which the emergence of an organized dynamics is essentially ruled by time-dependent interactions among the system's units), we have decided to organize this report as follows.In order to describe all the above mentioned cases (and many more for which the emergence of an organized dynamics is essentially ruled by time-dependent interactions among the system's units), we have decided to organize this report as follows.</p>
        <p>In Section 2 we give a short account of the fundamental definitions and mathematical concepts that will be then used along the entire paper. Far from offering an exhaustive discussion on the formal and mathematical frame for temporal networks, the section intends to cover some basic aspects and key notions of graph theory which needed proper extensions for being applicable to temporal networks, and to introduce the formalism that will be then adopted along all other sections.In Section 2 we give a short account of the fundamental definitions and mathematical concepts that will be then used along the entire paper. Far from offering an exhaustive discussion on the formal and mathematical frame for temporal networks, the section intends to cover some basic aspects and key notions of graph theory which needed proper extensions for being applicable to temporal networks, and to introduce the formalism that will be then adopted along all other sections.</p>
        <p>In Section 3 we focus on the case in which synchronization emerges among static network nodes. This latter means that either the nodes are not embedded in a physical space or, if they are, their coordinates remain fixed in time. In any case, the nodes represent dynamical systems, and the attribute static should be referred to the way in which they influence the mechanisms of link evolution. In this context, the wiring of connections may experience temporal changes of any type, as a response to specific mechanisms or processes (such as plasticity and adaptation, or external modulations and control, or forcing of any kind), which are taking place in parallel with the node intrinsic dynamics. We will describe both the case of monolayer networks and that of multilayer and hypernetwork structures.In Section 3 we focus on the case in which synchronization emerges among static network nodes. This latter means that either the nodes are not embedded in a physical space or, if they are, their coordinates remain fixed in time. In any case, the nodes represent dynamical systems, and the attribute static should be referred to the way in which they influence the mechanisms of link evolution. In this context, the wiring of connections may experience temporal changes of any type, as a response to specific mechanisms or processes (such as plasticity and adaptation, or external modulations and control, or forcing of any kind), which are taking place in parallel with the node intrinsic dynamics. We will describe both the case of monolayer networks and that of multilayer and hypernetwork structures.</p>
        <p>In Section 4, we deal with the complimentary case of synchronization in systems of mobile agents. A typical example is a temporal proximity graph describing a population of agents, each one of them being equipped with a communication system with limited range. During their motion, agents then communicate only if they are located at a physical distance shorter than the so-called sensing radius, so that the network of interactions is determined also by the characteristics of the agent motion.In Section 4, we deal with the complimentary case of synchronization in systems of mobile agents. A typical example is a temporal proximity graph describing a population of agents, each one of them being equipped with a communication system with limited range. During their motion, agents then communicate only if they are located at a physical distance shorter than the so-called sensing radius, so that the network of interactions is determined also by the characteristics of the agent motion.</p>
        <p>Finally, in Section 5, we give our short conclusive viewpoint on the subject, delineate open problems, and offer hints and ideas for perspective future studies in the field.Finally, in Section 5, we give our short conclusive viewpoint on the subject, delineate open problems, and offer hints and ideas for perspective future studies in the field.</p>
        <p>In this Section, we give some fundamental definitions and concepts that are essential for our entire review. They cover some basic aspects of graph theory and some key notions on temporal networks.In this Section, we give some fundamental definitions and concepts that are essential for our entire review. They cover some basic aspects of graph theory and some key notions on temporal networks.</p>
        <p>Preliminarily, let us notice that the term 'network' is often used to refer to the physical system, whereas the term 'graph' to refer to the mathematical representation of a network. In the following we will not consider this distinction and use the two terms equivalently.Preliminarily, let us notice that the term 'network' is often used to refer to the physical system, whereas the term 'graph' to refer to the mathematical representation of a network. In the following we will not consider this distinction and use the two terms equivalently.</p>
        <p>A network G is mathematically represented by a pair of two sets, V = {v 1 , v 2 , . . . , v N } as the set of N vertices/nodes, and E as the set of edges/links among nodes. In short, it can be denoted as G(V, E). The set of edges is formed by ordered pairs of nodes, such that (v i , v j ) ∈ E indicates the existence of a link from node v i ∈ V (or, shortly, i) to node v j ∈ V (or, shortly, j).A network G is mathematically represented by a pair of two sets, V = {v 1 , v 2 , . . . , v N } as the set of N vertices/nodes, and E as the set of edges/links among nodes. In short, it can be denoted as G(V, E). The set of edges is formed by ordered pairs of nodes, such that (v i , v j ) ∈ E indicates the existence of a link from node v i ∈ V (or, shortly, i) to node v j ∈ V (or, shortly, j).</p>
        <p>The cardinality of the set V is usually indicated with N and represents the number of nodes in the network. Instead, the cardinality of E is usually indicated as L and represents the number of links in the structure.The cardinality of the set V is usually indicated with N and represents the number of nodes in the network. Instead, the cardinality of E is usually indicated as L and represents the number of links in the structure.</p>
        <p>The network is said to be undirected if (v i , v j ) ∈ E implies that also (v j , v i ) ∈ E. Otherwise, the network is directed. The network is said simple, if there are no multiple edges connecting the same pair of nodes and there are no self-loops, i.e., links starting and ending in the same node. Unless otherwise specified, in the following we will always refer to simple networks.The network is said to be undirected if (v i , v j ) ∈ E implies that also (v j , v i ) ∈ E. Otherwise, the network is directed. The network is said simple, if there are no multiple edges connecting the same pair of nodes and there are no self-loops, i.e., links starting and ending in the same node. Unless otherwise specified, in the following we will always refer to simple networks.</p>
        <p>Networks may also be weighted and, in this case, they are represented by a triplet of sets, i.e., G(V, E, W), where W is a set of weights, one associated to each link of the network.Networks may also be weighted and, in this case, they are represented by a triplet of sets, i.e., G(V, E, W), where W is a set of weights, one associated to each link of the network.</p>
        <p>According to the notation introduced, the description of an unweighted network G(V, E) is tantamount to specifying the two sets V and E, which can be equivalently done by providing a list of nodes and a list of edges. A temporal network corresponding to the following event-based representation: {(2, 3, t 1 ), (1, 2, t 2 ), (3, 4, t 2 ), (1, 5, t 3 ), (2, 4, t 3 ), (3,4, t 3 ), (1, 3, t 4 ), (2, 3, t 4 ), (4, 5, t 5 ), (1, 2, t 6 ), (2, 3, t 6 ), (4, 5, t 7 ), (1, 4, t 8 )}. An alternative representation makes use of the adjacency matrix A, that is, an N × N matrix where the generic element A ij is given by: A ij = 1 if (v i , v j ) ∈ E, and A ij = 0 otherwise. Since we are considering simple graphs, then A ii = 0, ∀i = 1, 2, . . . , N. From the adjacency matrix, it is often convenient to define the Laplacian matrix L with elements given by: L ij = -A ij for i ̸ = j, and L ii = ∑ N j=1 A ij . As the focus of this review is on networks that evolve in time, we have to preliminarily notice that these networks have been referred to with different names such as temporal networks, time-dependent networks, time-varying networks, and so on. These networks are structures where the set of nodes, or the set of edges, or both, depend on time, i.e., V = V(t), E = E(t) or both V = V(t) and E = E(t). Here t ∈ [0, t max ], where t max is the lifetime or observation period. Without lack of generality, we will restrict the attention to the case where only the set of edges depend on time (note, in fact, that the case of a time-dependent set of nodes can be re-conducted to a static scenario by properly defining an augmented set of nodes, including all the nodes existing at same time in the observation period of the network).According to the notation introduced, the description of an unweighted network G(V, E) is tantamount to specifying the two sets V and E, which can be equivalently done by providing a list of nodes and a list of edges. A temporal network corresponding to the following event-based representation: {(2, 3, t 1 ), (1, 2, t 2 ), (3, 4, t 2 ), (1, 5, t 3 ), (2, 4, t 3 ), (3,4, t 3 ), (1, 3, t 4 ), (2, 3, t 4 ), (4, 5, t 5 ), (1, 2, t 6 ), (2, 3, t 6 ), (4, 5, t 7 ), (1, 4, t 8 )}. An alternative representation makes use of the adjacency matrix A, that is, an N × N matrix where the generic element A ij is given by: A ij = 1 if (v i , v j ) ∈ E, and A ij = 0 otherwise. Since we are considering simple graphs, then A ii = 0, ∀i = 1, 2, . . . , N. From the adjacency matrix, it is often convenient to define the Laplacian matrix L with elements given by: L ij = -A ij for i ̸ = j, and L ii = ∑ N j=1 A ij . As the focus of this review is on networks that evolve in time, we have to preliminarily notice that these networks have been referred to with different names such as temporal networks, time-dependent networks, time-varying networks, and so on. These networks are structures where the set of nodes, or the set of edges, or both, depend on time, i.e., V = V(t), E = E(t) or both V = V(t) and E = E(t). Here t ∈ [0, t max ], where t max is the lifetime or observation period. Without lack of generality, we will restrict the attention to the case where only the set of edges depend on time (note, in fact, that the case of a time-dependent set of nodes can be re-conducted to a static scenario by properly defining an augmented set of nodes, including all the nodes existing at same time in the observation period of the network).</p>
        <p>Let us, then, consider a time-dependent network G(V, E(t)) and discuss how one can represent it. Although there are many ways to represent a time-dependent network, two widely adopted descriptions are the event-based and the snapshot representation [82].Let us, then, consider a time-dependent network G(V, E(t)) and discuss how one can represent it. Although there are many ways to represent a time-dependent network, two widely adopted descriptions are the event-based and the snapshot representation [82].</p>
        <p>In the event-based representation, the temporal network is described by giving the time-ordered list of events. Let us index the events with h = 1, 2, . . . , n ev , where n ev is the number of events in the observation window [0, t max ]. Each event consists of an ordered triplet (v (h) i , v (h) j , t h ), where v (h) i is the starting node and v (h) j the ending node of a link (v (h) i , v (h) j ) that is created at time t h and ceases to exist at time t h+1 . The temporal network is fully characterized when the list of all events is provided, i.e., {(v (h) i , v (h) j , t h ); (h = 1, 2, . . . , n ev )}. The event-based representation can be viewed as an extension, to the time-varying case, of the representation of a classical network in terms of a list of edges. Here, being the links dependent on time, also the time at which the link occurs has to be specified. An example of a temporal network described by an event-based representation is shown in Fig. 1.In the event-based representation, the temporal network is described by giving the time-ordered list of events. Let us index the events with h = 1, 2, . . . , n ev , where n ev is the number of events in the observation window [0, t max ]. Each event consists of an ordered triplet (v (h) i , v (h) j , t h ), where v (h) i is the starting node and v (h) j the ending node of a link (v (h) i , v (h) j ) that is created at time t h and ceases to exist at time t h+1 . The temporal network is fully characterized when the list of all events is provided, i.e., {(v (h) i , v (h) j , t h ); (h = 1, 2, . . . , n ev )}. The event-based representation can be viewed as an extension, to the time-varying case, of the representation of a classical network in terms of a list of edges. Here, being the links dependent on time, also the time at which the link occurs has to be specified. An example of a temporal network described by an event-based representation is shown in Fig. 1.</p>
        <p>In the snapshot representation, the temporal network is described as a discrete-time sequence of networks: G = {G(t 1 ), G(t 2 ), . . . , G(t max )}. This corresponds to give a sequence of adjacency matrices A = A(t) = {A(t 1 ), A(t 2 ), . . . , A(t max )}, where the generic element A ij (t) of this time-dependent adjacency matrix is equal to one, when nodes i and j are connected by a link at time t. An example of the snapshot representation is shown in Fig. 2. Equivalently, a set of Laplacian matrices L(t) = {L(t 1 ), L(t 2 ), . . . , L(t max )} can be used to characterize the evolution of connectivity over time.In the snapshot representation, the temporal network is described as a discrete-time sequence of networks: G = {G(t 1 ), G(t 2 ), . . . , G(t max )}. This corresponds to give a sequence of adjacency matrices A = A(t) = {A(t 1 ), A(t 2 ), . . . , A(t max )}, where the generic element A ij (t) of this time-dependent adjacency matrix is equal to one, when nodes i and j are connected by a link at time t. An example of the snapshot representation is shown in Fig. 2. Equivalently, a set of Laplacian matrices L(t) = {L(t 1 ), L(t 2 ), . . . , L(t max )} can be used to characterize the evolution of connectivity over time.</p>
        <p>If a temporal network is described with an event-based representation and the time is discretized, then an equivalent snapshot representation can be given. Notice, however, that the snapshot representation is sometimes constructed in such a way that it provides a coarse-grained description of the event-based representation. This is the case when a time window of length T w is defined and A ij (t h ) is set equal to one if one or more links from i to j occurred at any time t ∈ [t h , t h + T w ) (alternatively, if a weighted network representation is adopted, A ij (t h ) can be set equal to the number of events linking nodes i and j in the time window [t h , t h + T w )). An example of a coarse-grained description of the temporal network of Fig. 1 with T w = 4 is shown in Fig. 3.If a temporal network is described with an event-based representation and the time is discretized, then an equivalent snapshot representation can be given. Notice, however, that the snapshot representation is sometimes constructed in such a way that it provides a coarse-grained description of the event-based representation. This is the case when a time window of length T w is defined and A ij (t h ) is set equal to one if one or more links from i to j occurred at any time t ∈ [t h , t h + T w ) (alternatively, if a weighted network representation is adopted, A ij (t h ) can be set equal to the number of events linking nodes i and j in the time window [t h , t h + T w )). An example of a coarse-grained description of the temporal network of Fig. 1 with T w = 4 is shown in Fig. 3.</p>
        <p>As the focus of this review is on collective dynamical behavior, in the following we will mostly resort to use the snapshot representation. In fact, this seems a more convenient representation when coupled dynamical units are dealt with, as the coupling terms are often written with reference to the adjacency matrix (or analogously to the Laplacian matrix). In the next section, we will show some examples of how the network, and so the adjacency matrix, may depend on time. We will see that different types of processes may regulate this time-dependence, thus generating diverse models of temporal networks that are particularly useful in the study of collective dynamical behaviors emerging from time-varying interaction mechanisms.As the focus of this review is on collective dynamical behavior, in the following we will mostly resort to use the snapshot representation. In fact, this seems a more convenient representation when coupled dynamical units are dealt with, as the coupling terms are often written with reference to the adjacency matrix (or analogously to the Laplacian matrix). In the next section, we will show some examples of how the network, and so the adjacency matrix, may depend on time. We will see that different types of processes may regulate this time-dependence, thus generating diverse models of temporal networks that are particularly useful in the study of collective dynamical behaviors emerging from time-varying interaction mechanisms.</p>
        <p>In the previous section, we have seen that in the snapshot representation a temporal network may be described in terms of a time-varying adjacency matrix A = A(t). However, links may depend on time in different ways. Although they can be a direct function of time, it is often the case that they are function of another process that in turn explicitly depends on time. Notice that different models of temporal networks are obtained as soon as the time-dependence of A is better characterized. With a slight abuse of notation, in such cases we will indicate A = A(σ(t)) where σ(t) is a generic function of time used to model how A evolves in time. To make a few examples, in the case of blinking networks A = A(s(t)), where s(t) is a sequence of binary values generated by a stochastic process; for activity driven networks, instead, A = A(a(t)), where a(t) is the stack vector of the activities at the nodes; in temporal proximity graphs and in metapopulation models A = A(y(t)), where y is the stack vector of the agent positions in a continuous space in the case of temporal proximity graphs or discrete in the case of metapopulation models, where the agents are located in the nodes of a backbone network modeling interconnections among subpopulations. Another possibility is represented by adaptive networks, where the evolution of links depends on the node states. In this case, the coefficients of A are dynamical variables themselves, whose specific rule of evolution contributes to determine the behavior of the entire system. All these models of temporal networks will be briefly described in the following.In the previous section, we have seen that in the snapshot representation a temporal network may be described in terms of a time-varying adjacency matrix A = A(t). However, links may depend on time in different ways. Although they can be a direct function of time, it is often the case that they are function of another process that in turn explicitly depends on time. Notice that different models of temporal networks are obtained as soon as the time-dependence of A is better characterized. With a slight abuse of notation, in such cases we will indicate A = A(σ(t)) where σ(t) is a generic function of time used to model how A evolves in time. To make a few examples, in the case of blinking networks A = A(s(t)), where s(t) is a sequence of binary values generated by a stochastic process; for activity driven networks, instead, A = A(a(t)), where a(t) is the stack vector of the activities at the nodes; in temporal proximity graphs and in metapopulation models A = A(y(t)), where y is the stack vector of the agent positions in a continuous space in the case of temporal proximity graphs or discrete in the case of metapopulation models, where the agents are located in the nodes of a backbone network modeling interconnections among subpopulations. Another possibility is represented by adaptive networks, where the evolution of links depends on the node states. In this case, the coefficients of A are dynamical variables themselves, whose specific rule of evolution contributes to determine the behavior of the entire system. All these models of temporal networks will be briefly described in the following.</p>
        <p>We begin with a general model of temporal networks where the presence of a link between two nodes of the network at each time t is the result of a stochastic process. In this model, known as blinking network, at each time t each pair of nodes has a given probability to be connected (in general, different from pair to pair), and, as time evolves, links will be randomly activated and de-activated.We begin with a general model of temporal networks where the presence of a link between two nodes of the network at each time t is the result of a stochastic process. In this model, known as blinking network, at each time t each pair of nodes has a given probability to be connected (in general, different from pair to pair), and, as time evolves, links will be randomly activated and de-activated.</p>
        <p>The model is formally described as follows [83,84]. Let s(t) : [0, ∞) ∈ {0, 1} L be a piecewise constant function that takes the constant binary vector value s hThe model is formally described as follows [83,84]. Let s(t) : [0, ∞) ∈ {0, 1} L be a piecewise constant function that takes the constant binary vector value s h</p>
        <p>The sequence of binary vectors s h with h = 1, 2, . . . represents the switching sequence, determining at each time t h which links exist in the network, or equivalently, which links are switched on. In particular, when s h i = 1 then link i ∈ E(t h ), whereas when s h i = 0 then link i / ∈ E(t h ). The switching sequences are considered instances of a stochastic process S h , h = 1, 2, . . ., where the random vectors S h are independent and identically distributed, with p s being the probability that S h assumes the value s ∈ {0, 1} L . According to this model, hence, the temporal network has a number L of links which are independently switched on and off. Consequently, the adjacency matrix can be expressed as function of the switching sequence, i.e., A = A(s(t)). Synchronization in blinking networks has been studied with particular attention to the behavior that is obtained when the time scale of the stochastic process is faster than that of the dynamics of the units, a scenario which clarifies the origin of the name 'blinking networks' [83,84]. This model includes several specific cases which are particularly important in the study of synchronization. The first case we discuss is the on-off coupling. In this case, the temporal network has L links which are simultaneously turned on or off. In the blinking network model this corresponds to have only two binary vector values with non-zero probability, i.e., s on = [1, . . . , 1] T with probability p on , and s off = [0, . . . , 0] T with probability p off = 1 -p on . In this specific case, the adjacency matrix can be rewritten as A(s(t)) = ε(t)A b , where A b represents the backbone structure whose L links are simultaneously turned on or off, and ε(t) = {0, 1} with ε(t) = 0 with probability p on and ε(t) = 1 with probability p off . The second interesting case is a temporal network where at each time t links are generated according to the Erdös-Rényi model for (static) random networks. Each snapshot of the temporal network, therefore, represents a structure that can be modeled as an Erdös-Rényi network. This model corresponds to a blinking network where L = N(N -1)/2 (all possible pairs are considered) and the probability that a component of the vector S h is equal to one, indicated as Π(S h i = 1), is independent from i and h, i.e., Π(S h i = 1) = p, where p is the wiring probability of the Erdös-Rényi model. Finally, we notice that the blinking model incorporates also temporal networks where there exists a fixed backbone that does not change in time, whereas the other links depend on time. In this case, Π(S h i = 1) = 1, ∀i ∈ E b , where E b ⊂ E is the set of the edges of the backbone structure.The sequence of binary vectors s h with h = 1, 2, . . . represents the switching sequence, determining at each time t h which links exist in the network, or equivalently, which links are switched on. In particular, when s h i = 1 then link i ∈ E(t h ), whereas when s h i = 0 then link i / ∈ E(t h ). The switching sequences are considered instances of a stochastic process S h , h = 1, 2, . . ., where the random vectors S h are independent and identically distributed, with p s being the probability that S h assumes the value s ∈ {0, 1} L . According to this model, hence, the temporal network has a number L of links which are independently switched on and off. Consequently, the adjacency matrix can be expressed as function of the switching sequence, i.e., A = A(s(t)). Synchronization in blinking networks has been studied with particular attention to the behavior that is obtained when the time scale of the stochastic process is faster than that of the dynamics of the units, a scenario which clarifies the origin of the name 'blinking networks' [83,84]. This model includes several specific cases which are particularly important in the study of synchronization. The first case we discuss is the on-off coupling. In this case, the temporal network has L links which are simultaneously turned on or off. In the blinking network model this corresponds to have only two binary vector values with non-zero probability, i.e., s on = [1, . . . , 1] T with probability p on , and s off = [0, . . . , 0] T with probability p off = 1 -p on . In this specific case, the adjacency matrix can be rewritten as A(s(t)) = ε(t)A b , where A b represents the backbone structure whose L links are simultaneously turned on or off, and ε(t) = {0, 1} with ε(t) = 0 with probability p on and ε(t) = 1 with probability p off . The second interesting case is a temporal network where at each time t links are generated according to the Erdös-Rényi model for (static) random networks. Each snapshot of the temporal network, therefore, represents a structure that can be modeled as an Erdös-Rényi network. This model corresponds to a blinking network where L = N(N -1)/2 (all possible pairs are considered) and the probability that a component of the vector S h is equal to one, indicated as Π(S h i = 1), is independent from i and h, i.e., Π(S h i = 1) = p, where p is the wiring probability of the Erdös-Rényi model. Finally, we notice that the blinking model incorporates also temporal networks where there exists a fixed backbone that does not change in time, whereas the other links depend on time. In this case, Π(S h i = 1) = 1, ∀i ∈ E b , where E b ⊂ E is the set of the edges of the backbone structure.</p>
        <p>Activity driven networks (ADNs) have been introduced in 2012 by Perra et al. [85] to model the concurrent evolution, at comparable time scales, of link formation and node dynamics. This regime is typical of many real-world phenomena. For example, in the contemporary, hyper-connected world, humans can travel around the world at the same speed at which epidemics incubate and spread, favoring the inception of pandemics [86].Activity driven networks (ADNs) have been introduced in 2012 by Perra et al. [85] to model the concurrent evolution, at comparable time scales, of link formation and node dynamics. This regime is typical of many real-world phenomena. For example, in the contemporary, hyper-connected world, humans can travel around the world at the same speed at which epidemics incubate and spread, favoring the inception of pandemics [86].</p>
        <p>Activity driven models constitute a parsimonious alternative to connectivity-driven models, where interactions are based on spatial proximity [64,78,87,88] and a motion and interaction model should be coupled to the node dynamics model. In ADNs, in fact, the temporal interaction pattern of each node is dictated by a single parameter, called activity potential (sometimes shortened in just activity), which quantifies the attitude of the node to generate connections over time. More precisely, the activity potential of a node is defined as the ratio between the number of interactions made by the node and the total number of interactions occurring in the network during a given time interval. In its original incarnations, activity potentials of nodes are constant and are obtained as independent and identically distributed realizations of stochastic variables. The study of several temporal networks representative of socio-technical systems of different nature have led to conjecture that nodes potentials are often distributed as power laws [85].Activity driven models constitute a parsimonious alternative to connectivity-driven models, where interactions are based on spatial proximity [64,78,87,88] and a motion and interaction model should be coupled to the node dynamics model. In ADNs, in fact, the temporal interaction pattern of each node is dictated by a single parameter, called activity potential (sometimes shortened in just activity), which quantifies the attitude of the node to generate connections over time. More precisely, the activity potential of a node is defined as the ratio between the number of interactions made by the node and the total number of interactions occurring in the network during a given time interval. In its original incarnations, activity potentials of nodes are constant and are obtained as independent and identically distributed realizations of stochastic variables. The study of several temporal networks representative of socio-technical systems of different nature have led to conjecture that nodes potentials are often distributed as power laws [85].</p>
        <p>Considering a network of N nodes labeled with i = (1, 2, . . . , n), where each node has constant activity potential a i , the link formation dynamics of an ADN over a discrete time interval [t, t + ∆t] is exemplified as follows:Considering a network of N nodes labeled with i = (1, 2, . . . , n), where each node has constant activity potential a i , the link formation dynamics of an ADN over a discrete time interval [t, t + ∆t] is exemplified as follows:</p>
        <p>1. the ADN is initialized to be fully disconnected; 2. each node i becomes active with probability a i ∆t. An active node forms m undirected links (with m constant integer)1. the ADN is initialized to be fully disconnected; 2. each node i becomes active with probability a i ∆t. An active node forms m undirected links (with m constant integer)</p>
        <p>with nodes drawn at random from a uniform distribution; 3. all the links are removed, discrete time is updated, and the process is resumed from the first step.with nodes drawn at random from a uniform distribution; 3. all the links are removed, discrete time is updated, and the process is resumed from the first step.</p>
        <p>While the instantaneous instances of an ADN consist of mostly disconnected networks, the union of all such instances has a degree distribution that asymptotically scales like the distribution of the activity potentials of nodes, for large network sizes and times [85,89].While the instantaneous instances of an ADN consist of mostly disconnected networks, the union of all such instances has a degree distribution that asymptotically scales like the distribution of the activity potentials of nodes, for large network sizes and times [85,89].</p>
        <p>Besides the link formation process, a dynamical system located at each node can coevolve at a comparable time scale, either according to a continuous-time formalism, or to a discrete-time one. Such a dynamics can be dictated, moreover, by either a deterministic or a stochastic process. While the dynamical systems located on nodes can independently evolve in time, the temporary formation of a link provides to connected process the opportunity to communicate, for instance exchanging information, or through diffusion mechanisms. Hence, such a communication may occur between steps 2 and 3 of the link formation process described above. To highlight such co-evolution mechanism without introducing a cumbersome mathematical formalism, we illustrate the evolution of an ADN with N = 5 nodes where a Susceptible-Infected-Susceptible epidemic process (thus, evolving in discrete time through a stochastic process) coevolves with the network formation. In this example, illustrated in Fig. 4, nodes can transit from a susceptible state to an infected one with a certain probability only upon the formation of a contact with an infected node, whereas an infected node can reverse its state to susceptible autonomously, with a certain probability, without the occurrence of a contact.Besides the link formation process, a dynamical system located at each node can coevolve at a comparable time scale, either according to a continuous-time formalism, or to a discrete-time one. Such a dynamics can be dictated, moreover, by either a deterministic or a stochastic process. While the dynamical systems located on nodes can independently evolve in time, the temporary formation of a link provides to connected process the opportunity to communicate, for instance exchanging information, or through diffusion mechanisms. Hence, such a communication may occur between steps 2 and 3 of the link formation process described above. To highlight such co-evolution mechanism without introducing a cumbersome mathematical formalism, we illustrate the evolution of an ADN with N = 5 nodes where a Susceptible-Infected-Susceptible epidemic process (thus, evolving in discrete time through a stochastic process) coevolves with the network formation. In this example, illustrated in Fig. 4, nodes can transit from a susceptible state to an infected one with a certain probability only upon the formation of a contact with an infected node, whereas an infected node can reverse its state to susceptible autonomously, with a certain probability, without the occurrence of a contact.</p>
        <p>The ADN formalism has been extended along different directions in the last decade. Behavioral traits in node dynamics, depending on global observables of the process unfolding upon the network have been considered to account for the effects of individual behavior on epidemic spreading [91]. This model has then been used to tackle real epidemiological models, such as the Ebola Virus Disease in West Africa [90], or the COVID-19 diffusion in Italy [92] and in the U.S. [93]. Further studies concentrated on a continuous-time, discrete-distribution approach to deal with the possibility of analytical treatment and avoid the confounds related with the choice of the sampling time [94,95] and the introduction of memory effects toward the study of self-excitement dynamics [96,97]. Further theoretical studies deal with the analysis of consensus [98,[98][99][100][101], collective motion [102], diffusion of innovation [103], voter models [104], and synchronization of chaotic dynamics [105].The ADN formalism has been extended along different directions in the last decade. Behavioral traits in node dynamics, depending on global observables of the process unfolding upon the network have been considered to account for the effects of individual behavior on epidemic spreading [91]. This model has then been used to tackle real epidemiological models, such as the Ebola Virus Disease in West Africa [90], or the COVID-19 diffusion in Italy [92] and in the U.S. [93]. Further studies concentrated on a continuous-time, discrete-distribution approach to deal with the possibility of analytical treatment and avoid the confounds related with the choice of the sampling time [94,95] and the introduction of memory effects toward the study of self-excitement dynamics [96,97]. Further theoretical studies deal with the analysis of consensus [98,[98][99][100][101], collective motion [102], diffusion of innovation [103], voter models [104], and synchronization of chaotic dynamics [105].</p>
        <p>Another interesting class of synthetic temporal network models derives from the generalization to the time-varying case of spatial graphs. Spatial/geometric graphs are characterized by nodes located in a space equipped with a metric. An example is the random geometric graph that is obtained by considering nodes distributed uniformly in a random way in a two-dimensional Euclidean space and connecting two nodes if their relative distance is smaller than a given threshold, usually defined as the interaction/neighborhood radius. Once nodes are allowed to move according to a motion law, then, the resulting graph is a time-varying one. © 2016 with permission from Elsevier. This approach is clearly general and can start from other types of spatial graphs, so that it yields a class of synthetic temporal networks, rather than a single model. Each member of this class of models is fully specified once the metric used in the space, the rule to set the links among the nodes, and the motion law are given. In this context, nodes/vertices of the network are often referred to as agents to represent their capability to move in the space. Typical applications of these temporal networks arise in the context of transportation and mobility systems, mobile phone networks, multi-agent robotics, and epidemic modeling.Another interesting class of synthetic temporal network models derives from the generalization to the time-varying case of spatial graphs. Spatial/geometric graphs are characterized by nodes located in a space equipped with a metric. An example is the random geometric graph that is obtained by considering nodes distributed uniformly in a random way in a two-dimensional Euclidean space and connecting two nodes if their relative distance is smaller than a given threshold, usually defined as the interaction/neighborhood radius. Once nodes are allowed to move according to a motion law, then, the resulting graph is a time-varying one. © 2016 with permission from Elsevier. This approach is clearly general and can start from other types of spatial graphs, so that it yields a class of synthetic temporal networks, rather than a single model. Each member of this class of models is fully specified once the metric used in the space, the rule to set the links among the nodes, and the motion law are given. In this context, nodes/vertices of the network are often referred to as agents to represent their capability to move in the space. Typical applications of these temporal networks arise in the context of transportation and mobility systems, mobile phone networks, multi-agent robotics, and epidemic modeling.</p>
        <p>To stem our discussion to a specific example, which has been proved to be particularly effective in the study of synchronization (as we will see in Section 4.3), we now describe, in some more detail, temporal proximity graphs.To stem our discussion to a specific example, which has been proved to be particularly effective in the study of synchronization (as we will see in Section 4.3), we now describe, in some more detail, temporal proximity graphs.</p>
        <p>Let us consider N agents located in a space, without lack of generality assumed to be a two-dimensional L × L square with periodic boundary conditions. We indicate the position of agent i at time t as y i (t) = [y i,1 (t), y i,2 (t)] T and consider two agents to be connected at time t if their distance is less than the interaction radius R (Fig. 5). At each time t, hence, the generic ij element of the adjacency matrix A ij (t) encoding the network connectivity is defined by:Let us consider N agents located in a space, without lack of generality assumed to be a two-dimensional L × L square with periodic boundary conditions. We indicate the position of agent i at time t as y i (t) = [y i,1 (t), y i,2 (t)] T and consider two agents to be connected at time t if their distance is less than the interaction radius R (Fig. 5). At each time t, hence, the generic ij element of the adjacency matrix A ij (t) encoding the network connectivity is defined by:</p>
        <p>Typically, the Euclidean norm is used, such that, in the two-dimensional case under analysis, we have:Typically, the Euclidean norm is used, such that, in the two-dimensional case under analysis, we have:</p>
        <p>((</p>
        <p>PLREP: 2197 This model accounts for agents equipped with limited sensing/communication capabilities, as typically occurs in multiagent systems [106]. One can think to agents as disks of radius R that communicate, and hence interact, with each other, only if they overlap at some time. For this reason, the model is also known as (temporal) R-disk proximity graph.PLREP: 2197 This model accounts for agents equipped with limited sensing/communication capabilities, as typically occurs in multiagent systems [106]. One can think to agents as disks of radius R that communicate, and hence interact, with each other, only if they overlap at some time. For this reason, the model is also known as (temporal) R-disk proximity graph.</p>
        <p>To fully characterize the model, the motion law needs to be also specified. The selection of the motion law is strictly related to the application considered. For instance, if the multi-agent system needs to be coordinated into a formation, then a specific control law to rule agent motion is required. Here, we consider a generic setup where agents move independently of the other units of the system as random walkers that eventually perform long distance jumps.To fully characterize the model, the motion law needs to be also specified. The selection of the motion law is strictly related to the application considered. For instance, if the multi-agent system needs to be coordinated into a formation, then a specific control law to rule agent motion is required. Here, we consider a generic setup where agents move independently of the other units of the system as random walkers that eventually perform long distance jumps.</p>
        <p>In more detail, one defines a jump probability p j ∈ [0, 1], and considers the following rules for updating the agent positions when they perform a jump or when they do not. Let us start with the second case. In this case, the ith agent moves with velocity v i (t), having constant modulus v and variable heading θ i , such that v i (t) = ve jθ i (t) . The heading of agent i is updated randomly at discrete time steps t k , with t k -t k-1 = τ M , such that one hasIn more detail, one defines a jump probability p j ∈ [0, 1], and considers the following rules for updating the agent positions when they perform a jump or when they do not. Let us start with the second case. In this case, the ith agent moves with velocity v i (t), having constant modulus v and variable heading θ i , such that v i (t) = ve jθ i (t) . The heading of agent i is updated randomly at discrete time steps t k , with t k -t k-1 = τ M , such that one has</p>
        <p>where η i (t k ) is an independent random variable chosen at each time t k with uniform probability in the interval [-π, π].where η i (t k ) is an independent random variable chosen at each time t k with uniform probability in the interval [-π, π].</p>
        <p>As periodic boundary conditions are assumed, the agent positions are considered modulus L.As periodic boundary conditions are assumed, the agent positions are considered modulus L.</p>
        <p>In addition, the model includes the possibility that agents perform long-distance jumps with probability p j . When such an event occurs, then the position of the agent i performing this jump is updated as follows:In addition, the model includes the possibility that agents perform long-distance jumps with probability p j . When such an event occurs, then the position of the agent i performing this jump is updated as follows:</p>
        <p>where ξ i (t k ) is a vector of two independent random variable chosen at each time t k with uniform probability in the interval [0, L]. In summary, each agent with probability 1 -p j moves as a random walker performing a step of length vτ M in an arbitrary direction, and with probability p j it jumps in an arbitrary position of the plane, thus performing a step of random length.where ξ i (t k ) is a vector of two independent random variable chosen at each time t k with uniform probability in the interval [0, L]. In summary, each agent with probability 1 -p j moves as a random walker performing a step of length vτ M in an arbitrary direction, and with probability p j it jumps in an arbitrary position of the plane, thus performing a step of random length.</p>
        <p>The jumping probability p j represents a control parameter for the system that tunes the type of motion and, consequently, the properties of the temporal network. If p j is set to one, then each snapshot of the temporal network exactly corresponds to an instance of the random geometric graph in the given plane. On the contrary, for p j ̸ = 1, the model exhibits correlations among agent positions at successive time steps which, in turn, generate correlations among the links of temporal network snapshots.The jumping probability p j represents a control parameter for the system that tunes the type of motion and, consequently, the properties of the temporal network. If p j is set to one, then each snapshot of the temporal network exactly corresponds to an instance of the random geometric graph in the given plane. On the contrary, for p j ̸ = 1, the model exhibits correlations among agent positions at successive time steps which, in turn, generate correlations among the links of temporal network snapshots.</p>
        <p>The effect of p j on the structure of the temporal network can be unveiled by using different ways to extend classical measures for static networks to time-varying scenarios, as considered in Refs. [56,107]. Quite interestingly, these different approaches point towards the same result, a small-world behavior emerging as a function of the parameter p j . More in general, the problem of defining the proper measures to capture the topological characteristics of a temporal network is far from trivial and often open to many different solutions. This aspect, although very important, goes beyond the purpose of this report and we refer the reader to the book [82] for a detailed discussion on the topic.The effect of p j on the structure of the temporal network can be unveiled by using different ways to extend classical measures for static networks to time-varying scenarios, as considered in Refs. [56,107]. Quite interestingly, these different approaches point towards the same result, a small-world behavior emerging as a function of the parameter p j . More in general, the problem of defining the proper measures to capture the topological characteristics of a temporal network is far from trivial and often open to many different solutions. This aspect, although very important, goes beyond the purpose of this report and we refer the reader to the book [82] for a detailed discussion on the topic.</p>
        <p>To hallmark the small-world effect in the temporal proximity graph, following the approach presented in Ref. [107], let us define a new time-varying adjacency matrix, indicated as A τ (t), averaging the properties of the snapshot in a moving time window of length τ . The generic ij element of this matrix is defined as follows:To hallmark the small-world effect in the temporal proximity graph, following the approach presented in Ref. [107], let us define a new time-varying adjacency matrix, indicated as A τ (t), averaging the properties of the snapshot in a moving time window of length τ . The generic ij element of this matrix is defined as follows:</p>
        <p>Then, the time-average value of the characteristic path length, L G , and the clustering coefficient, C G , as a function of p j are calculated for A τ (t). These parameters decrease with increasing p j (Fig. 6), but quite interestingly there is an interval of values of p j where the clustering coefficient is still large and the characteristic path length is already small, indicating the presence of a small-world effect. The same conclusion is obtained by inspecting other network measures as done in Ref. [56], where the Authors consider the average topological overlap of the neighbor set of a node between two successive graphs in the sequence, which provides a measure of the local connectivity of the nodes, and the characteristic temporal path length, which, on the contrary, provides an indication of the average distance between two nodes. These two parameters again decrease with p j with a region clearly indicating the presence of a small-world property in the temporal network.Then, the time-average value of the characteristic path length, L G , and the clustering coefficient, C G , as a function of p j are calculated for A τ (t). These parameters decrease with increasing p j (Fig. 6), but quite interestingly there is an interval of values of p j where the clustering coefficient is still large and the characteristic path length is already small, indicating the presence of a small-world effect. The same conclusion is obtained by inspecting other network measures as done in Ref. [56], where the Authors consider the average topological overlap of the neighbor set of a node between two successive graphs in the sequence, which provides a measure of the local connectivity of the nodes, and the characteristic temporal path length, which, on the contrary, provides an indication of the average distance between two nodes. These two parameters again decrease with p j with a region clearly indicating the presence of a small-world property in the temporal network.</p>
        <p>Another particularly relevant example of temporal spatial graphs is obtained when a topological rather than metric criterion is used to set the connections among agents at each time t. In this scenario, a fixed number M of agents is defined and, at each time t, each agent links with exactly other M agents, selecting, in particular, the M agents at the closest distance from its actual position, that are sometimes named the topological neighbors.Another particularly relevant example of temporal spatial graphs is obtained when a topological rather than metric criterion is used to set the connections among agents at each time t. In this scenario, a fixed number M of agents is defined and, at each time t, each agent links with exactly other M agents, selecting, in particular, the M agents at the closest distance from its actual position, that are sometimes named the topological neighbors.</p>
        <p>This rule produces adjacency matrices A(t) that, in the general case, are not symmetric, whereas those in the temporal proximity graphs discussed in the previous section are symmetric. An example with M = 1 is illustrated in Fig. 7, which shows that agent k is the nearest neighbor of agent j, but not vice-versa. In fact, the nearest neighbor of agent j is agent i, such that i and j are connected by a bidirectional link, whereas j and k by a directed one. With the topological interaction rule, each agent always interacts with a fixed number of other units, regardless of their geometric distances. In multi-agent systems, this has the benefit of generating snapshots that are always connected, but, in general, requires a more powerful communication system to reach units at an arbitrary distance. Quite interestingly, in biological systems, both examples of use of the metric and the topological interaction rules are found. Models of animal flocking, which often have been the source of inspiration for engineers to design control protocols for multi-agent systems [108][109][110], have, in fact, shown that the metric interaction scheme is likely to be adopted in collective motions by groups with a high density such as in locust swarming [111] and fish schooling [112], whereas the topological interaction scheme is used when the flock-mates can be perceived even at a large distance such as in bird flocks [113].This rule produces adjacency matrices A(t) that, in the general case, are not symmetric, whereas those in the temporal proximity graphs discussed in the previous section are symmetric. An example with M = 1 is illustrated in Fig. 7, which shows that agent k is the nearest neighbor of agent j, but not vice-versa. In fact, the nearest neighbor of agent j is agent i, such that i and j are connected by a bidirectional link, whereas j and k by a directed one. With the topological interaction rule, each agent always interacts with a fixed number of other units, regardless of their geometric distances. In multi-agent systems, this has the benefit of generating snapshots that are always connected, but, in general, requires a more powerful communication system to reach units at an arbitrary distance. Quite interestingly, in biological systems, both examples of use of the metric and the topological interaction rules are found. Models of animal flocking, which often have been the source of inspiration for engineers to design control protocols for multi-agent systems [108][109][110], have, in fact, shown that the metric interaction scheme is likely to be adopted in collective motions by groups with a high density such as in locust swarming [111] and fish schooling [112], whereas the topological interaction scheme is used when the flock-mates can be perceived even at a large distance such as in bird flocks [113].</p>
        <p>Social networks constitute a salient example of time-varying networks, due to their continuous evolution, often concurrently with dynamical processes occurring on their nodes and exchanging information during interactions [71]. Notably, these systems are characterized by intermittent and rewiring links, multiple characteristic time scales, burstiness, formation of communities, and other complex behaviors, which question the adequacy of traditional analyses relying on the strong hypothesis of Poisson distributed processes. Due to advances in technologies, several data collection strategies have been put forward to characterize these systems, making available diverse data sets on urban and long-range mobility cell phone calls, online interactions, and human proximity [114]. In particular, the latter has been pursued since 2008 by the SocioPatterns collaboration [115] through the realization of low-cost sensors able to record mutual proximity of their wearers by exchanging low-power radio packets. Sensors have been distributed to attendees at gatherings such as schools, museums, or conferences, revealing common statistical properties and the coexistence of heterogeneous time scales, ranging from 20 seconds to several hours, which entails bursty patterns of interaction. Moreover the presence of super-connectors is observed, a concept equivalent to hubs in static networks [116]. Fig. 8 illustrates the probability distribution of the duration of contacts between any two given persons in three different deployments of the SocioPatterns experiment, denoting the lack of a characteristic time scale and a striking similarity among the experiments. Different models have been proposed to reproduce salient characteristics of face-to-face interactions. An agent-based model is proposed in Refs. [58,114]. The model is constructed upon a population with a fixed number of individuals confined in a bounded two-dimensional space, where well-mixing conditions can be assumed. At any time instant, agents can be either isolated or belonging to a group; thus, the contact network is formed by disconnected cliques of different size. The contact networks evolve by letting each agent decide whether to join a group, if they are isolated, or leaving the group to which they belong. Probabilities of switching state (from isolated to grouped, and vice versa) obey to a memory effect, whereby they depend on the state of the agent and on the time an agent spent in its current state. A reinforcement mechanism is put forward such that agents that interact for long times are less likely to leave their group and, conversely, agents that are isolated for long times are less likely to join a group. This concept is similar to that of preferential attachment in complex networks [117], and may have connections with Hebbian-like mechanisms at the underlying cognitive level. The proposed model is amenable to some analytical treatment and is able to qualitatively reproduce empirical results derived from experimental data [116,118].Social networks constitute a salient example of time-varying networks, due to their continuous evolution, often concurrently with dynamical processes occurring on their nodes and exchanging information during interactions [71]. Notably, these systems are characterized by intermittent and rewiring links, multiple characteristic time scales, burstiness, formation of communities, and other complex behaviors, which question the adequacy of traditional analyses relying on the strong hypothesis of Poisson distributed processes. Due to advances in technologies, several data collection strategies have been put forward to characterize these systems, making available diverse data sets on urban and long-range mobility cell phone calls, online interactions, and human proximity [114]. In particular, the latter has been pursued since 2008 by the SocioPatterns collaboration [115] through the realization of low-cost sensors able to record mutual proximity of their wearers by exchanging low-power radio packets. Sensors have been distributed to attendees at gatherings such as schools, museums, or conferences, revealing common statistical properties and the coexistence of heterogeneous time scales, ranging from 20 seconds to several hours, which entails bursty patterns of interaction. Moreover the presence of super-connectors is observed, a concept equivalent to hubs in static networks [116]. Fig. 8 illustrates the probability distribution of the duration of contacts between any two given persons in three different deployments of the SocioPatterns experiment, denoting the lack of a characteristic time scale and a striking similarity among the experiments. Different models have been proposed to reproduce salient characteristics of face-to-face interactions. An agent-based model is proposed in Refs. [58,114]. The model is constructed upon a population with a fixed number of individuals confined in a bounded two-dimensional space, where well-mixing conditions can be assumed. At any time instant, agents can be either isolated or belonging to a group; thus, the contact network is formed by disconnected cliques of different size. The contact networks evolve by letting each agent decide whether to join a group, if they are isolated, or leaving the group to which they belong. Probabilities of switching state (from isolated to grouped, and vice versa) obey to a memory effect, whereby they depend on the state of the agent and on the time an agent spent in its current state. A reinforcement mechanism is put forward such that agents that interact for long times are less likely to leave their group and, conversely, agents that are isolated for long times are less likely to join a group. This concept is similar to that of preferential attachment in complex networks [117], and may have connections with Hebbian-like mechanisms at the underlying cognitive level. The proposed model is amenable to some analytical treatment and is able to qualitatively reproduce empirical results derived from experimental data [116,118].</p>
        <p>A further salient agent-based model of face-to-face interactions has been proposed in Ref. [119]. Therein, agents perform a biased random-walk and interact according to spatial proximity. Agents are supposed to have a heterogeneous level of attractiveness, which biases the random-walk of agents toward the most attractive ones. This is a typical phenomenon occurring in social, economic, and natural communities, where some individuals are able to attract most of the attention of the entire community. Differently from Refs. [58,114], agents can enter or exit an active state, where they are enabled to move and make connections. The resulting model is Markovian, and even in its simplicity, is able to capture many features of empirical and experimental data. Fig. 9 illustrates the distribution of the contact duration (top) and the distribution of the time interval between consecutive contacts (bottom) for various datasets [115] and the proposed model, and for different population densities, ρ, denoting a great agreement. Further investigations are carried out on the correlation between the number of different contacts and the temporal duration of such contacts, yielding to the observation of a super-linear ''hub-like'' behavior, whereby nodes with high degrees tend to spend more time in interactions with others than individuals with a lower number of connections, a phenomenon empirically observed in Ref. [116]. The model is also able to reproduce an empirical phenomenon observed in human mobility, whereby the tendency of agents to interact with new agents decreases in time. This phenomenon manifests itself through a sub-linear increase of the number of different contacts of single individuals [120].A further salient agent-based model of face-to-face interactions has been proposed in Ref. [119]. Therein, agents perform a biased random-walk and interact according to spatial proximity. Agents are supposed to have a heterogeneous level of attractiveness, which biases the random-walk of agents toward the most attractive ones. This is a typical phenomenon occurring in social, economic, and natural communities, where some individuals are able to attract most of the attention of the entire community. Differently from Refs. [58,114], agents can enter or exit an active state, where they are enabled to move and make connections. The resulting model is Markovian, and even in its simplicity, is able to capture many features of empirical and experimental data. Fig. 9 illustrates the distribution of the contact duration (top) and the distribution of the time interval between consecutive contacts (bottom) for various datasets [115] and the proposed model, and for different population densities, ρ, denoting a great agreement. Further investigations are carried out on the correlation between the number of different contacts and the temporal duration of such contacts, yielding to the observation of a super-linear ''hub-like'' behavior, whereby nodes with high degrees tend to spend more time in interactions with others than individuals with a lower number of connections, a phenomenon empirically observed in Ref. [116]. The model is also able to reproduce an empirical phenomenon observed in human mobility, whereby the tendency of agents to interact with new agents decreases in time. This phenomenon manifests itself through a sub-linear increase of the number of different contacts of single individuals [120].</p>
        <p>Metapopulation models consider an ensemble of individuals that are distributed in local populations (modeling, for instance, neighborhoods, cities, urban areas, or ecological habitats), and may migrate from one population to another. This system can be, hence, described by a network where the nodes are the local populations, within which all the agents/individuals interact each other, and the edges represent the migration routes that the individuals may follow. The system may equivalently described as a larger network where nodes now represent the individuals and links account for the interactions within the local populations. As the composition of these local populations change in time because of migration, then the network is effectively time-varying.Metapopulation models consider an ensemble of individuals that are distributed in local populations (modeling, for instance, neighborhoods, cities, urban areas, or ecological habitats), and may migrate from one population to another. This system can be, hence, described by a network where the nodes are the local populations, within which all the agents/individuals interact each other, and the edges represent the migration routes that the individuals may follow. The system may equivalently described as a larger network where nodes now represent the individuals and links account for the interactions within the local populations. As the composition of these local populations change in time because of migration, then the network is effectively time-varying.</p>
        <p>Metapopulation models are widely used in mathematical epidemiology [121,122], where they allow to capture the different spatial and temporal scales of epidemic spreading in a system of interconnected populations, in ecology [123, the weights as A ij (t) = w ij (t) and providing the equations for their dynamics that, considering, for instance, again the scenario where they depend on the states at the nodes i and j, read as ẇij = f (x i (t), x j (t)).Metapopulation models are widely used in mathematical epidemiology [121,122], where they allow to capture the different spatial and temporal scales of epidemic spreading in a system of interconnected populations, in ecology [123, the weights as A ij (t) = w ij (t) and providing the equations for their dynamics that, considering, for instance, again the scenario where they depend on the states at the nodes i and j, read as ẇij = f (x i (t), x j (t)).</p>
        <p>The idea of adaptive networks was pioneered by the concept of dynamic graphs introduced by Siljak [131] and characterized by weights that vary in time and obey to a differential equation. Dynamic graphs do no include the possibility for networks to grow, that is instead incorporated in the Holland's concept of complex adaptive systems [132]. Recently, the adaptation and growth mechanisms of networks have been framed into the more general formalism of evolving dynamical networks that also incorporates the evolution of the component dynamics [133,134].The idea of adaptive networks was pioneered by the concept of dynamic graphs introduced by Siljak [131] and characterized by weights that vary in time and obey to a differential equation. Dynamic graphs do no include the possibility for networks to grow, that is instead incorporated in the Holland's concept of complex adaptive systems [132]. Recently, the adaptation and growth mechanisms of networks have been framed into the more general formalism of evolving dynamical networks that also incorporates the evolution of the component dynamics [133,134].</p>
        <p>Examples of adaptive networks are commonly found in nature. Flocks of birds or schools of fishes possess the ability of reshaping the structure of interactions by forming or suppressing interconnections among the individuals and adjusting their strength [134]. Further examples of real-world systems that can be modeled as adaptive networks include social systems, neural networks and other biological networks [130].Examples of adaptive networks are commonly found in nature. Flocks of birds or schools of fishes possess the ability of reshaping the structure of interactions by forming or suppressing interconnections among the individuals and adjusting their strength [134]. Further examples of real-world systems that can be modeled as adaptive networks include social systems, neural networks and other biological networks [130].</p>
        <p>Adaptive networks are also of utmost importance in control engineering where they represent a way to embed a control law able to re-adapt the weight of a link or modulate the structure of interactions in order to achieve a desired state or collective behavior for the system [134]. For instance, one can consider a network starting from a configuration that does not synchronize and use adaptation mechanisms for the links that evolve the system towards a structure supporting synchronization. We will discuss this example and several others in Section 3.Adaptive networks are also of utmost importance in control engineering where they represent a way to embed a control law able to re-adapt the weight of a link or modulate the structure of interactions in order to achieve a desired state or collective behavior for the system [134]. For instance, one can consider a network starting from a configuration that does not synchronize and use adaptation mechanisms for the links that evolve the system towards a structure supporting synchronization. We will discuss this example and several others in Section 3.</p>
        <p>Consider a family of networksConsider a family of networks</p>
        <p>) , α = 1, 2, . . . , M, where V is a fixed set of nodes for each α, and) , α = 1, 2, . . . , M, where V is a fixed set of nodes for each α, and</p>
        <p>is a family of edges, which represents various interaction types, then a hypernetwork is a pair H = (V , E). Here each E [α] corresponds to a different mode of interaction.is a family of edges, which represents various interaction types, then a hypernetwork is a pair H = (V , E). Here each E [α] corresponds to a different mode of interaction.</p>
        <p>We call each of these as a tier. Here, M is the total number of tiers in the hypernetwork. For a time-varying hypernetwork, each tier E [α] (t) of the network is a function of time. The hypernetwork H (t) is said to be jointly connected, if the union of its frozen-time projected networksWe call each of these as a tier. Here, M is the total number of tiers in the hypernetwork. For a time-varying hypernetwork, each tier E [α] (t) of the network is a function of time. The hypernetwork H (t) is said to be jointly connected, if the union of its frozen-time projected networks</p>
        <p>))</p>
        <p>constitutes a connected graph. In a frozen-time, any tierconstitutes a connected graph. In a frozen-time, any tier</p>
        <p>may have one or more disconnected components, but the frozen-time projected network should be connected. A schematic diagram illustrating time-varying interactions in a hypernetwork of 10 nodes is shown in Fig. 10.may have one or more disconnected components, but the frozen-time projected network should be connected. A schematic diagram illustrating time-varying interactions in a hypernetwork of 10 nodes is shown in Fig. 10.</p>
        <p>is a family of graphs each representing a layer andis a family of graphs each representing a layer and</p>
        <p>is the set of interconnections between the nodes of non-identical layers G β 1 and G β 2 . The elements of E β are called intra-layer connections, while the elements of C are called crossed layers, where each element of E β 1 β 2 stands for an inter-layer connection. If a multilayer network is formed by the same number of vertices in each layer, and each node is only connected to its counterpart node in the rest of the layers, then it is known as multiplex network. Therefore, for multiplex networks,is the set of interconnections between the nodes of non-identical layers G β 1 and G β 2 . The elements of E β are called intra-layer connections, while the elements of C are called crossed layers, where each element of E β 1 β 2 stands for an inter-layer connection. If a multilayer network is formed by the same number of vertices in each layer, and each node is only connected to its counterpart node in the rest of the layers, then it is known as multiplex network. Therefore, for multiplex networks,</p>
        <p>, where |V | denotes the cardinality of the vertex set V ., where |V | denotes the cardinality of the vertex set V .</p>
        <p>is the set of inter-layer connections between the nodes of non-identical layers G β 1 and G β 2 . A time-varying hypernetwork is obtained when each tier Eis the set of inter-layer connections between the nodes of non-identical layers G β 1 and G β 2 . A time-varying hypernetwork is obtained when each tier E</p>
        <p>[α] β (t) is a function of time, but the inter-layer connections C are time-invariant. The network M H (t) is said to be jointly connected if the union of its frozen-time projected network[α] β (t) is a function of time, but the inter-layer connections C are time-invariant. The network M H (t) is said to be jointly connected if the union of its frozen-time projected network</p>
        <p>constitutes a connected graph. At any frozen-time, any tierconstitutes a connected graph. At any frozen-time, any tier</p>
        <p>may have one or more disconnected components, but the necessary condition for achieving complete synchronization inside each layer is that the frozen-time projected network be jointly connected.may have one or more disconnected components, but the necessary condition for achieving complete synchronization inside each layer is that the frozen-time projected network be jointly connected.</p>
        <p>The schematic diagram in Fig. 11 represents a time-varying hypernetwork with a multiplex structure of two layers consisting of N = 8 nodes and M = 2 interaction types in each layer. Two different types of interacting tiers are shown at two particular instances of times t = t 1 and t = t 2 in Figs. 11(a) and 11(b), respectively. The links of one tier are denoted by the green dashed lines while the links of the other tiers are depicted with magenta dotted lines, whereas the connections between the layers are time-independent and are represented by black solid lines.The schematic diagram in Fig. 11 represents a time-varying hypernetwork with a multiplex structure of two layers consisting of N = 8 nodes and M = 2 interaction types in each layer. Two different types of interacting tiers are shown at two particular instances of times t = t 1 and t = t 2 in Figs. 11(a) and 11(b), respectively. The links of one tier are denoted by the green dashed lines while the links of the other tiers are depicted with magenta dotted lines, whereas the connections between the layers are time-independent and are represented by black solid lines.</p>
        <p>Switched systems are a class of systems whose coefficients undergo abrupt changes. Consider the linear state equationSwitched systems are a class of systems whose coefficients undergo abrupt changes. Consider the linear state equation</p>
        <p>where ρ : R → Z + is a switching sequence that selects elements from a family of matrix valued coefficients {A 1 , A 2 , . . . , A n , . . . } and dot stands for temporal derivative. When all of these elements are Hurwitz, stability of Eq. ( 5) is guaranteed if ρ(t) switches at a sufficiently high rate [137]. Further restrictions on these elements, such as the existence of a common Lyapunov function, can guarantee stability for arbitrary switching functions, even for slow switching. Source: Reprinted with permission from Ref. [136].where ρ : R → Z + is a switching sequence that selects elements from a family of matrix valued coefficients {A 1 , A 2 , . . . , A n , . . . } and dot stands for temporal derivative. When all of these elements are Hurwitz, stability of Eq. ( 5) is guaranteed if ρ(t) switches at a sufficiently high rate [137]. Further restrictions on these elements, such as the existence of a common Lyapunov function, can guarantee stability for arbitrary switching functions, even for slow switching. Source: Reprinted with permission from Ref. [136].</p>
        <p>When either the elements A i 's are not all Hurwitz or none of them is Hurwitz, the stability of Eq. ( 5) is yet possible, although the class of switching functions is further restricted. For such a case, stability can be guaranteed if the switching sequence is sufficiently fast. Then it can be shown that Eq. ( 5) is asymptotically stable if there exists a constant T such that the time-average matrix 1 T ∫ t+T t A ρ(τ ) dτ is Hurwitz for all t when ϵ is sufficiently small [138,139]. For each case, stability of the specific time-average system implies stability of the original system, and requires the existence of a time-independent Lyapunov function of the certain average system.When either the elements A i 's are not all Hurwitz or none of them is Hurwitz, the stability of Eq. ( 5) is yet possible, although the class of switching functions is further restricted. For such a case, stability can be guaranteed if the switching sequence is sufficiently fast. Then it can be shown that Eq. ( 5) is asymptotically stable if there exists a constant T such that the time-average matrix 1 T ∫ t+T t A ρ(τ ) dτ is Hurwitz for all t when ϵ is sufficiently small [138,139]. For each case, stability of the specific time-average system implies stability of the original system, and requires the existence of a time-independent Lyapunov function of the certain average system.</p>
        <p>This Section deals with temporal networks where links change as the result of diverse mechanisms such as deterministic or stochastic processes, external driving forces, or intrinsic adaptation capabilities.This Section deals with temporal networks where links change as the result of diverse mechanisms such as deterministic or stochastic processes, external driving forces, or intrinsic adaptation capabilities.</p>
        <p>In the first part of the Section, we review a series of fundamental theoretical tools developed to study the onset and stability of synchronization in temporal networks. These methods are developed in the framework of continuous-time nonlinear oscillators (including periodic and chaotic systems), but their analysis provides general notions and concepts (such as the role of the different time scales at work in a temporal network) that are also useful to study the synchronous behavior of purely phase oscillators. For this reason these methods are reviewed in Section 3.1, whereas examples of synchronization in temporal networks of phase oscillators are dealt with in Section 3.2 and limit cycle and chaotic systems in Section 3.3. At the end of the Section, we will also present examples of synchronous behavior emerging in multilayer networks and hypernetworks.In the first part of the Section, we review a series of fundamental theoretical tools developed to study the onset and stability of synchronization in temporal networks. These methods are developed in the framework of continuous-time nonlinear oscillators (including periodic and chaotic systems), but their analysis provides general notions and concepts (such as the role of the different time scales at work in a temporal network) that are also useful to study the synchronous behavior of purely phase oscillators. For this reason these methods are reviewed in Section 3.1, whereas examples of synchronization in temporal networks of phase oscillators are dealt with in Section 3.2 and limit cycle and chaotic systems in Section 3.3. At the end of the Section, we will also present examples of synchronous behavior emerging in multilayer networks and hypernetworks.</p>
        <p>Let us begin with briefly reviewing a few fundamental notions on synchronization in classical networks, i.e., with time-independent links, that prove to be particularly important also for the analysis of temporal networks. The general model to study synchronization in ensembles of N identical units which are networking with a complex connectivity structure is described by the following equations:Let us begin with briefly reviewing a few fundamental notions on synchronization in classical networks, i.e., with time-independent links, that prove to be particularly important also for the analysis of temporal networks. The general model to study synchronization in ensembles of N identical units which are networking with a complex connectivity structure is described by the following equations:</p>
        <p>with i = 1, . . . , N. Here, x i (t) ∈ R n represents the state vector of the oscillator at node i, f : R n → R n the vector field describing the uncoupled node dynamics, h : R n → R n the coupling function, ϵ the coupling strength, and L = {L ij } the Laplacian matrix mapping the interactions among the network units.with i = 1, . . . , N. Here, x i (t) ∈ R n represents the state vector of the oscillator at node i, f : R n → R n the vector field describing the uncoupled node dynamics, h : R n → R n the coupling function, ϵ the coupling strength, and L = {L ij } the Laplacian matrix mapping the interactions among the network units.</p>
        <p>As the Laplacian matrix is a zero-row sum matrix, then Eqs. (6) always admit a solution of the typeAs the Laplacian matrix is a zero-row sum matrix, then Eqs. (6) always admit a solution of the type</p>
        <p>with x s such that ẋs = f(x s ). These conditions define the so-called synchronization manifold and the synchronous solution x s . However, the mere existence of this solution does not suffice to guarantee that the system will converge towards it: to observe synchronization, the solution needs to be stable.with x s such that ẋs = f(x s ). These conditions define the so-called synchronization manifold and the synchronous solution x s . However, the mere existence of this solution does not suffice to guarantee that the system will converge towards it: to observe synchronization, the solution needs to be stable.</p>
        <p>A widely used technique to study the stability of the synchronous solution is based on the derivation of a Master Stability Function (MSF) [49,140,141]. The technique consists in linearizing Eqs. (6) around x s , and then applying a proper transformation of variables that leads to a generic variational block of this type: where ξ ∈ R n and Jf and Jh represent the Jacobian matrices of f and h, computed around x s and α is a generic complex parameter. For simplicity, let us focus here on the case of undirected connected networks where α is a real parameter.A widely used technique to study the stability of the synchronous solution is based on the derivation of a Master Stability Function (MSF) [49,140,141]. The technique consists in linearizing Eqs. (6) around x s , and then applying a proper transformation of variables that leads to a generic variational block of this type: where ξ ∈ R n and Jf and Jh represent the Jacobian matrices of f and h, computed around x s and α is a generic complex parameter. For simplicity, let us focus here on the case of undirected connected networks where α is a real parameter.</p>
        <p>From Eqs. (7) the maximum conditional Lyapunov exponent Λ max can be computed as a function of the independent parameter α, thus obtaining the MSF Λ max = Λ max (α). This function fully characterizes the stability of the synchronous state, as this requires that the maximum conditional exponent is negative for α = {ϵλ 2 , . . . , ϵλ N }, where 0 = λ 1 &lt; λ 2 ≤ λ N are the eigenvalues of the Laplacian matrix of the network assumed to be undirected and connected (such that its eigenvalues are all positive, except the first which is zero).From Eqs. (7) the maximum conditional Lyapunov exponent Λ max can be computed as a function of the independent parameter α, thus obtaining the MSF Λ max = Λ max (α). This function fully characterizes the stability of the synchronous state, as this requires that the maximum conditional exponent is negative for α = {ϵλ 2 , . . . , ϵλ N }, where 0 = λ 1 &lt; λ 2 ≤ λ N are the eigenvalues of the Laplacian matrix of the network assumed to be undirected and connected (such that its eigenvalues are all positive, except the first which is zero).</p>
        <p>The way in which the maximum conditional Lyapunov exponent Λ max depends on α yields three different classes of systems: (i) type I systems where Λ max (α) &gt; 0 ∀α, which includes dynamical systems that will never synchronize, no matter which network's structure of interactions is considered; (ii) type II systems where Λ max (α) &lt; 0 for α ∈ [α 1 , +∞), such that by tuning the coupling strength the synchronization condition ϵλ 2 &gt; α 1 can be met; (iii) type III systems where Λ max (α) &lt; 0 for α ∈ [α 1 , α 2 ], resulting in a non-trivial condition for synchronization requiring simultaneously that ϵλ 2 &gt; α 1 and ϵλ N &lt; α 2 [3].The way in which the maximum conditional Lyapunov exponent Λ max depends on α yields three different classes of systems: (i) type I systems where Λ max (α) &gt; 0 ∀α, which includes dynamical systems that will never synchronize, no matter which network's structure of interactions is considered; (ii) type II systems where Λ max (α) &lt; 0 for α ∈ [α 1 , +∞), such that by tuning the coupling strength the synchronization condition ϵλ 2 &gt; α 1 can be met; (iii) type III systems where Λ max (α) &lt; 0 for α ∈ [α 1 , α 2 ], resulting in a non-trivial condition for synchronization requiring simultaneously that ϵλ 2 &gt; α 1 and ϵλ N &lt; α 2 [3].</p>
        <p>A few fundamental aspects are here worth to remark. First, the criterion only provides a necessary (and local) condition for synchronization, but in many works it proved to accurately predict the onset of it. Second, the MSF Λ max = Λ max (α) solely depends on the node dynamics and the coupling function and therefore can be calculated independently from the specific network's structure. Third, the topology of the network plays a fundamental role in determining the stability of synchronization as the condition Λ max (α) &lt; 0 is checked in N -1 points that depend on the eigenvalues of the Laplacian matrix. This matrix is, therefore, of crucial relevance for synchronization stability. Finally, we notice that Eqs. ( 7) are derived through the transformation that uses the eigenbasis which diagonalizes the Laplacian matrix. These observations are particularly important when we move to consider the case of temporal networks. In a timevarying structure, the Laplacian matrix changes with time, and therefore its eigenvalues and corresponding eigenvectors are functions of time as well. Consequently, the basis furnished by the eigenvectors also changes whenever the Laplacian matrix does. As we will see in detail in this section, this is a crucial issue as, in the general case, it hampers the derivation of an equation similar to Eqs. (7). However, there are particular cases when the structure of the temporal network simplifies so that the eigenvector basis is constant in time and only the eigenvalues are time-varying. In other circumstances, i.e., when the time scale at which the temporal network changes significantly differs from that of the process taking place in the units of the system, the analysis of the temporal network can be reconducted to the static case, enabling the extension of techniques based on the MSF. This section is devoted to discuss the techniques for studying synchronization stability in temporal networks that can be developed starting from these considerations.A few fundamental aspects are here worth to remark. First, the criterion only provides a necessary (and local) condition for synchronization, but in many works it proved to accurately predict the onset of it. Second, the MSF Λ max = Λ max (α) solely depends on the node dynamics and the coupling function and therefore can be calculated independently from the specific network's structure. Third, the topology of the network plays a fundamental role in determining the stability of synchronization as the condition Λ max (α) &lt; 0 is checked in N -1 points that depend on the eigenvalues of the Laplacian matrix. This matrix is, therefore, of crucial relevance for synchronization stability. Finally, we notice that Eqs. ( 7) are derived through the transformation that uses the eigenbasis which diagonalizes the Laplacian matrix. These observations are particularly important when we move to consider the case of temporal networks. In a timevarying structure, the Laplacian matrix changes with time, and therefore its eigenvalues and corresponding eigenvectors are functions of time as well. Consequently, the basis furnished by the eigenvectors also changes whenever the Laplacian matrix does. As we will see in detail in this section, this is a crucial issue as, in the general case, it hampers the derivation of an equation similar to Eqs. (7). However, there are particular cases when the structure of the temporal network simplifies so that the eigenvector basis is constant in time and only the eigenvalues are time-varying. In other circumstances, i.e., when the time scale at which the temporal network changes significantly differs from that of the process taking place in the units of the system, the analysis of the temporal network can be reconducted to the static case, enabling the extension of techniques based on the MSF. This section is devoted to discuss the techniques for studying synchronization stability in temporal networks that can be developed starting from these considerations.</p>
        <p>Stillwell et al. [55] introduced a fast switching stability criterion, in which the time-scale of the network evolution is faster than the time-scale of the coupled oscillators. Before describing this technique, we need the following preliminary lemma.Stillwell et al. [55] introduced a fast switching stability criterion, in which the time-scale of the network evolution is faster than the time-scale of the coupled oscillators. Before describing this technique, we need the following preliminary lemma.</p>
        <p>Lemma 1 (Ref. [55]). Suppose that there exists a time-average matrix Ē of the matrix valued function E(t), such that for all t ∈ R + and for some constant T , 1 T ∫ t+T t E(τ ) dτ = Ē. Then, for sufficiently fast switching, the following systemLemma 1 (Ref. [55]). Suppose that there exists a time-average matrix Ē of the matrix valued function E(t), such that for all t ∈ R + and for some constant T , 1 T ∫ t+T t E(τ ) dτ = Ē. Then, for sufficiently fast switching, the following system</p>
        <p>will be uniformly asymptotically stable whenever the time-average systemwill be uniformly asymptotically stable whenever the time-average system</p>
        <p>is also uniformly asymptotically stable.is also uniformly asymptotically stable.</p>
        <p>Here z 0 and x 0 are two different initial conditions from the basin of attraction of the asymptotically stable state of systems ( 8) and ( 9) respectively. The lemma is valid for any constant time t, and for sufficiently large T , it depends on A(t) and how fast E(t) is switching. Stability of the frozen-time system does not guarantee the stability of the switched system, but this lemma shows that the switched time-varying system can be asymptotically stable if the time-average system is asymptotically stable for sufficient fast switching. Now consider a temporal network of N identical coupled oscillatorsHere z 0 and x 0 are two different initial conditions from the basin of attraction of the asymptotically stable state of systems ( 8) and ( 9) respectively. The lemma is valid for any constant time t, and for sufficiently large T , it depends on A(t) and how fast E(t) is switching. Stability of the frozen-time system does not guarantee the stability of the switched system, but this lemma shows that the switched time-varying system can be asymptotically stable if the time-average system is asymptotically stable for sufficient fast switching. Now consider a temporal network of N identical coupled oscillators</p>
        <p>where i = 1, . . . , N, x i ∈ R n is the state variable of the ith node and B ∈ R n×n is the inner coupling matrix. The scalar ϵ is a control parameter that sets the coupling strength between the oscillators. A(t) is the N × N time-varying graph adjacency matrix, which describes the interconnections between the oscillators. When complete synchronization emerges in system (10), all the oscillators evolve in unison. Then, there exists awhere i = 1, . . . , N, x i ∈ R n is the state variable of the ith node and B ∈ R n×n is the inner coupling matrix. The scalar ϵ is a control parameter that sets the coupling strength between the oscillators. A(t) is the N × N time-varying graph adjacency matrix, which describes the interconnections between the oscillators. When complete synchronization emerges in system (10), all the oscillators evolve in unison. Then, there exists a</p>
        <p>Consequently, the complete synchronization manifold can be defined asConsequently, the complete synchronization manifold can be defined as</p>
        <p>PLREP: 2197PLREP: 2197</p>
        <p>Owing to the diffusive nature of the coupling, the complete synchronization solution x s (t) is an invariant state for all the coupling strengths ϵ and all choices of the inner coupling matrix B.Owing to the diffusive nature of the coupling, the complete synchronization solution x s (t) is an invariant state for all the coupling strengths ϵ and all choices of the inner coupling matrix B.</p>
        <p>For sufficiently fast switching, the time-average Laplacian matrix L satisfies L = 1 T ∫ t+T t L(τ ) dτ , for some constant T .For sufficiently fast switching, the time-average Laplacian matrix L satisfies L = 1 T ∫ t+T t L(τ ) dτ , for some constant T .</p>
        <p>The matrix L has the same inherent zero-row sum property as the parent Laplacian L(t). But L is not actually describing a particular network, rather it is just the term by term time-average of the time-varying graph Laplacian L(t). However, the real square matrix L can be unitarily triangularizable. Then, there exists a unitary matrix P each column of which is made of the orthonormal eigenvectors of L, such thatThe matrix L has the same inherent zero-row sum property as the parent Laplacian L(t). But L is not actually describing a particular network, rather it is just the term by term time-average of the time-varying graph Laplacian L(t). However, the real square matrix L can be unitarily triangularizable. Then, there exists a unitary matrix P each column of which is made of the orthonormal eigenvectors of L, such that</p>
        <p>is the Schur transformation of L. Here Ū2 ∈ C (N-1)×(N-1) is an upper triangular matrix containing in the main diagonal elements the N -1 eigenvalues of L excluding 0. The equations of motion of the coupled system incorporating the above average Laplacian are obtained from Eq. ( 10) just by replacing L(t) by L.is the Schur transformation of L. Here Ū2 ∈ C (N-1)×(N-1) is an upper triangular matrix containing in the main diagonal elements the N -1 eigenvalues of L excluding 0. The equations of motion of the coupled system incorporating the above average Laplacian are obtained from Eq. ( 10) just by replacing L(t) by L.</p>
        <p>Considering the Schur transformation and using the unitary matrix P, the equation for the error η transverse to the synchronization manifold can be written asConsidering the Schur transformation and using the unitary matrix P, the equation for the error η transverse to the synchronization manifold can be written as</p>
        <p>By considering the same Schur transformation applied to Eq. ( 10), the equation of motion of the transverse error system becomesBy considering the same Schur transformation applied to Eq. ( 10), the equation of motion of the transverse error system becomes</p>
        <p>wherewhere</p>
        <p>] ,] ,</p>
        <p>is the Schur transformation of L(t).is the Schur transformation of L(t).</p>
        <p>Now, it is easy to derive that Ū2 = 1 T ∫ t+T t U 2 (τ ) dτ . Thus, Lemma 1 yields that, if the time-average system has an asymptotically stable synchronization manifold, then the time-varying network also has asymptotically stable synchronization for sufficient fast switching. This fast switching stability criterion is a fundamental tool to assess the local stability of several temporal networks.Now, it is easy to derive that Ū2 = 1 T ∫ t+T t U 2 (τ ) dτ . Thus, Lemma 1 yields that, if the time-average system has an asymptotically stable synchronization manifold, then the time-varying network also has asymptotically stable synchronization for sufficient fast switching. This fast switching stability criterion is a fundamental tool to assess the local stability of several temporal networks.</p>
        <p>Quite interestingly, using the MSF approach, one can derive the conditions for synchronization also when the network dynamics is much slower than that of the nodes, as discussed in detail in Ref. [142]. In Section 3.3.2 we will see an application of this case.Quite interestingly, using the MSF approach, one can derive the conditions for synchronization also when the network dynamics is much slower than that of the nodes, as discussed in detail in Ref. [142]. In Section 3.3.2 we will see an application of this case.</p>
        <p>As opposed to the limit of fast switching discussed above, Ref. [65] considers explicitly the case where the time scale at which the network changes is commensurable with that of the dynamics taking place in each unit. The same study has shown for the first time that the synchronizability of a network can be significantly improved by evolving the graph along a time-dependent connectivity matrix. In Ref. [65], the Authors consider a network of N coupled identical systems, whose evolution is described byAs opposed to the limit of fast switching discussed above, Ref. [65] considers explicitly the case where the time scale at which the network changes is commensurable with that of the dynamics taking place in each unit. The same study has shown for the first time that the synchronizability of a network can be significantly improved by evolving the graph along a time-dependent connectivity matrix. In Ref. [65], the Authors consider a network of N coupled identical systems, whose evolution is described by</p>
        <p>Once again, x(t) ∈ R n is the n-dimensional vector describing the state of the ith node, f : R n → R n governs the local dynamics of the nodes, h : R n → R n is a vectorial output function, ϵ is the coupling strength, and L(t) is the timevarying zero row sum N × N symmetric Laplacian matrix. L(t) specifies the evolution in strength and topology of the underlying connection wiring. Being symmetric, L(t) admits at all times a set (λ i (t), v i (t)) of real eigenpairs such thatOnce again, x(t) ∈ R n is the n-dimensional vector describing the state of the ith node, f : R n → R n governs the local dynamics of the nodes, h : R n → R n is a vectorial output function, ϵ is the coupling strength, and L(t) is the timevarying zero row sum N × N symmetric Laplacian matrix. L(t) specifies the evolution in strength and topology of the underlying connection wiring. Being symmetric, L(t) admits at all times a set (λ i (t), v i (t)) of real eigenpairs such that</p>
        <p>It is worth noticing that the zero row sum condition imposed on L(t) ensures that the spectrum is (at each time) entirely non-negative, i.e., λ i (t) ≥ 0 for all i and t. Moreover, λ 1 (t) = 0 with associated eigenvector v 1 (t) =It is worth noticing that the zero row sum condition imposed on L(t) ensures that the spectrum is (at each time) entirely non-negative, i.e., λ i (t) ≥ 0 for all i and t. Moreover, λ 1 (t) = 0 with associated eigenvector v 1 (t) =</p>
        <p>One can consider, for instance, δx i (t) = x i (t) -x s (t) to be the deviation of the ith state vector from the synchronization manifold, and focus on the column vector δx(t) = ( δx 1 (t), δx 2 (t), . . . , δx N (t) ) T . Then in linear order of δx(t), the evolution equation reads asOne can consider, for instance, δx i (t) = x i (t) -x s (t) to be the deviation of the ith state vector from the synchronization manifold, and focus on the column vector δx(t) = ( δx 1 (t), δx 2 (t), . . . , δx N (t) ) T . Then in linear order of δx(t), the evolution equation reads as</p>
        <p>where ⊗ stands for the matrix direct product and J denotes the Jacobian operator. Now, the arbitrary state δx(t) can be written, at each time, as δx(t) = ∑ N j=1 v j (t) ⊗ η j (t). Then applying v j (t) T to the 48 left side of each term in Eq. ( 15), one finally obtainswhere ⊗ stands for the matrix direct product and J denotes the Jacobian operator. Now, the arbitrary state δx(t) can be written, at each time, as δx(t) = ∑ N j=1 v j (t) ⊗ η j (t). Then applying v j (t) T to the 48 left side of each term in Eq. ( 15), one finally obtains</p>
        <p>If compared with the classic Master Stability Function approach, Eq. ( 16) contains an extra termIf compared with the classic Master Stability Function approach, Eq. ( 16) contains an extra term</p>
        <p>) which accounts for the projection of the new basis of eigenvectors into the old one. Such a term is of paramount importance, as it can completely change the stability properties of the synchronous solution (as we will shortly see). Now notice that Eq. ( 16) transforms into a set of N variational equations of the form) which accounts for the projection of the new basis of eigenvectors into the old one. Such a term is of paramount importance, as it can completely change the stability properties of the synchronous solution (as we will shortly see). Now notice that Eq. ( 16) transforms into a set of N variational equations of the form</p>
        <p>as soon as all eigenvectors are fixed in time, i.e.,as soon as all eigenvectors are fixed in time, i.e.,</p>
        <p>∑ N j=1 v i (t) T dv j (t) dt η j (t) = 0. The above condition can be realized in two different ways. Namely, either the coupling matrix L(t) is constant (and therefore one recovers the classical case of Master Stability Function), or when starting from an initial wiring condition L(t = t 0 ), the coupling matrix L(t) commutes at any time with L(t = t 0 ).∑ N j=1 v i (t) T dv j (t) dt η j (t) = 0. The above condition can be realized in two different ways. Namely, either the coupling matrix L(t) is constant (and therefore one recovers the classical case of Master Stability Function), or when starting from an initial wiring condition L(t = t 0 ), the coupling matrix L(t) commutes at any time with L(t = t 0 ).</p>
        <p>Ref. [65] considers the case of an evolution along commutative graphs, and demonstrates that (also under such a rather restrictive hypothesis) synchronization can be greatly enhanced. Notice, indeed, that the initial Laplacian can be written asRef. [65] considers the case of an evolution along commutative graphs, and demonstrates that (also under such a rather restrictive hypothesis) synchronization can be greatly enhanced. Notice, indeed, that the initial Laplacian can be written as</p>
        <p>] is an orthogonal matrix whose columns are the eigenvectors of L(t 0 ) and Λ 0 = diag(0, λ 2 (0), λ 3 (0), . . . , λ N (0)) is the diagonal matrix consisting of the eigenvalues of L(t 0 ). At any time t, a commuting matrix L(t) can be constructed as L(t) = V Λ(t)V T . Here, Λ(t) = diag(0, λ 2 (t), λ 3 (t), . . . , λ N (t)) and, for all i &gt; 1, λ i (t) are positive real numbers. Therefore, L(t) is positive semi-definite and zero row-sum. Since the standard orthogonal basis vectors are not collinear with the eigenvectors v 1 , therefore L ii (t) &gt; 0 for all i.] is an orthogonal matrix whose columns are the eigenvectors of L(t 0 ) and Λ 0 = diag(0, λ 2 (0), λ 3 (0), . . . , λ N (0)) is the diagonal matrix consisting of the eigenvalues of L(t 0 ). At any time t, a commuting matrix L(t) can be constructed as L(t) = V Λ(t)V T . Here, Λ(t) = diag(0, λ 2 (t), λ 3 (t), . . . , λ N (t)) and, for all i &gt; 1, λ i (t) are positive real numbers. Therefore, L(t) is positive semi-definite and zero row-sum. Since the standard orthogonal basis vectors are not collinear with the eigenvectors v 1 , therefore L ii (t) &gt; 0 for all i.</p>
        <p>Because of the commuting properties of L(t), Eq. ( 16) becomes ηi (t) = K i η i (t) for all i = 2, 3, . . . , N. Replacing ϵλ i (t) by ν in the kernel K i , the problem of stability of the synchronization manifold is tantamount to study the n-dimensionalBecause of the commuting properties of L(t), Eq. ( 16) becomes ηi (t) = K i η i (t) for all i = 2, 3, . . . , N. Replacing ϵλ i (t) by ν in the kernel K i , the problem of stability of the synchronization manifold is tantamount to study the n-dimensional</p>
        <p>Then the stability region corresponds to the region where the curve of Λ max , the largest Lyapunov exponents with respect to ν, is negative.Then the stability region corresponds to the region where the curve of Λ max , the largest Lyapunov exponents with respect to ν, is negative.</p>
        <p>Later on, Ref. [143] provided a rigorous solution to the problem of constructing a structural evolution that switches between topologies without constraints on their commutativity, thus providing the most general framework for studies of synchronization of identical units under smooth changes in time of their connectivity, independently on the particular topologies visited, and also on the time scale of the evolution, which can be faster than, comparable to, or even secular with respect to the dynamics of the units.Later on, Ref. [143] provided a rigorous solution to the problem of constructing a structural evolution that switches between topologies without constraints on their commutativity, thus providing the most general framework for studies of synchronization of identical units under smooth changes in time of their connectivity, independently on the particular topologies visited, and also on the time scale of the evolution, which can be faster than, comparable to, or even secular with respect to the dynamics of the units.</p>
        <p>The methods discussed so far refer to linear stability analysis, and therefore their predictions hold only for small perturbations from a desired state. However, perturbations are not always infinitesimal and, in these latter cases, other techniques should be used. In order to understand the dynamical response to any kind of perturbation, one should have a picture of the complete landscape of the coupled system, that is, one should know the size of basin of attraction [144,145] for all attractors of the system. Recently, the concept of a measure of basin stability has been proposed to quantify how stable a synchronization state is against large perturbations [146][147][148]. It is a nonlinear and nonlocal approach that relies on the volume of the basin of attraction rather than the traditional linearization based approach. It is applicable even to higher dimensional systems and is a robust measure for characterizing multi-stable states.The methods discussed so far refer to linear stability analysis, and therefore their predictions hold only for small perturbations from a desired state. However, perturbations are not always infinitesimal and, in these latter cases, other techniques should be used. In order to understand the dynamical response to any kind of perturbation, one should have a picture of the complete landscape of the coupled system, that is, one should know the size of basin of attraction [144,145] for all attractors of the system. Recently, the concept of a measure of basin stability has been proposed to quantify how stable a synchronization state is against large perturbations [146][147][148]. It is a nonlinear and nonlocal approach that relies on the volume of the basin of attraction rather than the traditional linearization based approach. It is applicable even to higher dimensional systems and is a robust measure for characterizing multi-stable states.</p>
        <p>The basin stability paradigm is particularly useful in case of time-varying networks, as it can be applied to a very large class of systems, whereas the linear stability analysis can be done exactly only in some specific cases. As we have seen, derivation of analytical or semi-analytical conditions for stability often needs to make special hypotheses on the coupling scheme, such as fast rewiring [54,81], on-off coupling [149] or a particular class of local dynamics [150]. The basin stability measure is a general numerical technique that can be used to analyze the stability of high-dimensional systems and to quantify different stable steady states in coupled delayed [148] and non-delayed systems [151], synchronized states [76] and chimera states [152].The basin stability paradigm is particularly useful in case of time-varying networks, as it can be applied to a very large class of systems, whereas the linear stability analysis can be done exactly only in some specific cases. As we have seen, derivation of analytical or semi-analytical conditions for stability often needs to make special hypotheses on the coupling scheme, such as fast rewiring [54,81], on-off coupling [149] or a particular class of local dynamics [150]. The basin stability measure is a general numerical technique that can be used to analyze the stability of high-dimensional systems and to quantify different stable steady states in coupled delayed [148] and non-delayed systems [151], synchronized states [76] and chimera states [152].</p>
        <p>The basin stability measure is defined as BS = ∫ B χ(x)ρ(x)dx, where B is the set of all possible perturbations x and χ(x) is equal to 1 if the system converges to synchronization after the perturbation x, and zero otherwise. ρ(x) is the density of the perturbed states withThe basin stability measure is defined as BS = ∫ B χ(x)ρ(x)dx, where B is the set of all possible perturbations x and χ(x) is equal to 1 if the system converges to synchronization after the perturbation x, and zero otherwise. ρ(x) is the density of the perturbed states with</p>
        <p>To compute BS, the coupled system is integrated for a number Q (sufficiently large) of different states distributed randomly over a prescribed phase space volume, then the evolution of the system from these different initial states is computed. Let M be the number of states that reach the synchronous state, then the BS for the synchronous state is estimated as M Q . BS takes values in [0, 1], with BS = 0 implying that for all random initial conditions the synchronized state is unstable, and BS = 1 indicating that it is globally stable for any perturbation. Intermediate values of BS represent the probability to find a synchronous state starting from an initial condition that lies in the prescribed phase space volume. Very often this measure complements the information obtained through linear stability analysis. Examples of applications of the concept of BS to temporal networks are found in Ref. [76].To compute BS, the coupled system is integrated for a number Q (sufficiently large) of different states distributed randomly over a prescribed phase space volume, then the evolution of the system from these different initial states is computed. Let M be the number of states that reach the synchronous state, then the BS for the synchronous state is estimated as M Q . BS takes values in [0, 1], with BS = 0 implying that for all random initial conditions the synchronized state is unstable, and BS = 1 indicating that it is globally stable for any perturbation. Intermediate values of BS represent the probability to find a synchronous state starting from an initial condition that lies in the prescribed phase space volume. Very often this measure complements the information obtained through linear stability analysis. Examples of applications of the concept of BS to temporal networks are found in Ref. [76].</p>
        <p>Another very important class of methods for the analysis of synchronization stability is based on Lyapunov functions. These methods may provide local or global conditions for synchronization stability, but, especially for the case of temporal networks, often need to be tailored on the specific assumptions on the network evolution. They have been proved to be particularly effective for the study of adaptive networks of limit cycle and chaotic oscillators, where also the law of evolution for the network is deterministic. For this reason, they will be discussed later on in Section 3.3.3 with specific reference to this type of temporal networks.Another very important class of methods for the analysis of synchronization stability is based on Lyapunov functions. These methods may provide local or global conditions for synchronization stability, but, especially for the case of temporal networks, often need to be tailored on the specific assumptions on the network evolution. They have been proved to be particularly effective for the study of adaptive networks of limit cycle and chaotic oscillators, where also the law of evolution for the network is deterministic. For this reason, they will be discussed later on in Section 3.3.3 with specific reference to this type of temporal networks.</p>
        <p>In his pioneer work, Kuramoto introduced a simple model to study synchronization in interacting systems. He considered the scenario where the coupling is weak compared to the attraction force towards the limit cycle that represents the natural tendency of the unit to oscillate at its own frequency of oscillation (i.e., the uncoupled dynamics).In his pioneer work, Kuramoto introduced a simple model to study synchronization in interacting systems. He considered the scenario where the coupling is weak compared to the attraction force towards the limit cycle that represents the natural tendency of the unit to oscillate at its own frequency of oscillation (i.e., the uncoupled dynamics).</p>
        <p>Under these circumstances, each oscillator dynamics can be fully described by a single state variable, φ i , representing the phase of the ith oscillator. In his work, Kuramoto considered the case where each oscillator is coupled to all the others, a scenario which, in the framework of complex networks, corresponds to consider interactions that are fixed in time and global and that is referred to as global coupling, fully connectivity or all-to-all coupling. The model was then extended to account for general topologies of interactions [153][154][155], as well as higher-order coupling mechanisms and time-varying intrinsic parameters (e.g., the natural frequency) [156][157][158]. Here, we are focused on Kuramoto oscillators, also known as pure phase oscillators, interacting via time-varying edges. In particular, we start with blinking networks.Under these circumstances, each oscillator dynamics can be fully described by a single state variable, φ i , representing the phase of the ith oscillator. In his work, Kuramoto considered the case where each oscillator is coupled to all the others, a scenario which, in the framework of complex networks, corresponds to consider interactions that are fixed in time and global and that is referred to as global coupling, fully connectivity or all-to-all coupling. The model was then extended to account for general topologies of interactions [153][154][155], as well as higher-order coupling mechanisms and time-varying intrinsic parameters (e.g., the natural frequency) [156][157][158]. Here, we are focused on Kuramoto oscillators, also known as pure phase oscillators, interacting via time-varying edges. In particular, we start with blinking networks.</p>
        <p>A first interesting study of blinking networks of Kuramoto oscillators concerns a rather special setting of the timevarying interactions [81]. The Authors have considered a system composed by two large heterogeneous populations of phase oscillators interacting according to two fixed arrangements switched at a given blinking frequency. In agreement with the key findings of the fast switching approach, they demonstrate that, at sufficiently high blinking frequencies, the two populations of interacting phase oscillators behave as if their connectivity were static and equal to the time-average of the temporal structure of coupling. This blinking mechanism was proved to be capable of inducing synchronization, even when the switching occurs between topologies that individually do not support a coherent state.A first interesting study of blinking networks of Kuramoto oscillators concerns a rather special setting of the timevarying interactions [81]. The Authors have considered a system composed by two large heterogeneous populations of phase oscillators interacting according to two fixed arrangements switched at a given blinking frequency. In agreement with the key findings of the fast switching approach, they demonstrate that, at sufficiently high blinking frequencies, the two populations of interacting phase oscillators behave as if their connectivity were static and equal to the time-average of the temporal structure of coupling. This blinking mechanism was proved to be capable of inducing synchronization, even when the switching occurs between topologies that individually do not support a coherent state.</p>
        <p>To delve into the analysis of blinking networks of phase oscillators, let us illustrate in more detail the model and the results discussed in Ref. [159], where, in particular, two interesting findings emerge. First, the Authors are able to show that, if the coupling is sufficiently strong and the rewiring sufficiently fast, then partial synchronization can be reached even in the presence of extremely low instantaneous connectivity. Second, the Authors provide approximate analytical arguments that are able to predict the transition to synchronization beyond the limit conditions for fast switching.To delve into the analysis of blinking networks of phase oscillators, let us illustrate in more detail the model and the results discussed in Ref. [159], where, in particular, two interesting findings emerge. First, the Authors are able to show that, if the coupling is sufficiently strong and the rewiring sufficiently fast, then partial synchronization can be reached even in the presence of extremely low instantaneous connectivity. Second, the Authors provide approximate analytical arguments that are able to predict the transition to synchronization beyond the limit conditions for fast switching.</p>
        <p>To begin our discussion, let us describe the dynamical equations governing the model considered in Ref. [159]. The system consists of N phase oscillators interacting through a temporal network with adjacency matrix A ij (t):To begin our discussion, let us describe the dynamical equations governing the model considered in Ref. [159]. The system consists of N phase oscillators interacting through a temporal network with adjacency matrix A ij (t):</p>
        <p>with i = 1, . . . , N. Here, φ i ∈ [0, 2π ] represents the instantaneous phase of the ith oscillator and ω i is the natural frequency chosen from a zero-mean Gaussian distribution with standard deviation σ (notice that, in line with the Kuramoto model, the system is heterogeneous, with each oscillator characterized by its own natural frequency, in general different from that of the other units). The parameter ϵ is the coupling strength and d i (t) represents the instantaneous degree of the ith node at time t, that is,with i = 1, . . . , N. Here, φ i ∈ [0, 2π ] represents the instantaneous phase of the ith oscillator and ω i is the natural frequency chosen from a zero-mean Gaussian distribution with standard deviation σ (notice that, in line with the Kuramoto model, the system is heterogeneous, with each oscillator characterized by its own natural frequency, in general different from that of the other units). The parameter ϵ is the coupling strength and d i (t) represents the instantaneous degree of the ith node at time t, that is,</p>
        <p>The connectivity of the temporal network is encoded in the adjacency matrix A ij (t): at each time step, links are generated according to the ER model [160] with wiring probability indicated as p; in addition, a Poissonian process for random rewiring of the edges is employed, with each individual node rewiring synchronously all its incident edges with probability rate 1 T , with T being the rewiring time period. To implement the Poissonian process in numerical simulations, at each integration time step, each node goes through a rewiring event with Poissonian probability given byThe connectivity of the temporal network is encoded in the adjacency matrix A ij (t): at each time step, links are generated according to the ER model [160] with wiring probability indicated as p; in addition, a Poissonian process for random rewiring of the edges is employed, with each individual node rewiring synchronously all its incident edges with probability rate 1 T , with T being the rewiring time period. To implement the Poissonian process in numerical simulations, at each integration time step, each node goes through a rewiring event with Poissonian probability given by</p>
        <p>where ∆t is the integration step size considered. Following Ref. [159], one also defines q = pN as the mean degree connectivity.where ∆t is the integration step size considered. Following Ref. [159], one also defines q = pN as the mean degree connectivity.</p>
        <p>To monitor the level of synchronization, one can consider the classical Kuramoto order parameter:To monitor the level of synchronization, one can consider the classical Kuramoto order parameter:</p>
        <p>with ι = √ -1, and R is defined as the steady-state value taken by this parameter:with ι = √ -1, and R is defined as the steady-state value taken by this parameter:</p>
        <p>where T w denotes a sufficiently large window of time over which the system dynamics is calculated. The order parameter r (and consequently R) varies in the interval [0, 1], with r = 1 indicating complete synchronization and r ∼ 0 for the incoherent state.where T w denotes a sufficiently large window of time over which the system dynamics is calculated. The order parameter r (and consequently R) varies in the interval [0, 1], with r = 1 indicating complete synchronization and r ∼ 0 for the incoherent state.</p>
        <p>In order to gain some insights on the system behavior, it is instructive to inspect the time evolution of the Kuramoto order parameter r(t) for selected values of the rewiring period T . Fig. 12 illustrates it for a network with N = 10 4 nodes, connectivity parameter q = 0.8, coupling strength ϵ = 8, and standard deviation of the natural frequency distribution set to σ = 1. We can notice that synchronization does not occur for high T, e.g. T = 6.28, but, as the value of T is Source: Reprinted figure with permission from Ref. [159].In order to gain some insights on the system behavior, it is instructive to inspect the time evolution of the Kuramoto order parameter r(t) for selected values of the rewiring period T . Fig. 12 illustrates it for a network with N = 10 4 nodes, connectivity parameter q = 0.8, coupling strength ϵ = 8, and standard deviation of the natural frequency distribution set to σ = 1. We can notice that synchronization does not occur for high T, e.g. T = 6.28, but, as the value of T is Source: Reprinted figure with permission from Ref. [159].</p>
        <p>lowered to T = 0.63 or T = 0.31, synchronization emerges although with different levels of coherence. The highest 51 level of synchrony, in particular, is observed for the smallest value of the rewiring period T , indicating that not only fast switching enables synchronization, but also improves it.lowered to T = 0.63 or T = 0.31, synchronization emerges although with different levels of coherence. The highest 51 level of synchrony, in particular, is observed for the smallest value of the rewiring period T , indicating that not only fast switching enables synchronization, but also improves it.</p>
        <p>A more systematic analysis of the network dynamics can be carried out by considering how R depends on two important control parameters, the connectivity q and the blinking period T (Fig. 13(a)). For sufficiently large connectivity, i.e., q &gt; q, the system reaches synchronization irrespective of the value of T . Notice that the threshold q is higher than that for the emergence of a giant connected component in ER networks (i.e., q = 1). This suggests that this structural condition promotes the observed dynamical regime. Instead, for small q, a sufficiently fast rewiring (associated to low values of T ) is required to attain synchronization, at least for q &gt; 0.5. It is also interesting to observe that, if we indicate as T c (q) the value of T delimitating the boundary between partial synchrony and incoherence in the q -T plane, it appears to be linear with q (Fig. 13(b)) before entering in the region where the giant connected component emerges and synchronization becomes independent of T .A more systematic analysis of the network dynamics can be carried out by considering how R depends on two important control parameters, the connectivity q and the blinking period T (Fig. 13(a)). For sufficiently large connectivity, i.e., q &gt; q, the system reaches synchronization irrespective of the value of T . Notice that the threshold q is higher than that for the emergence of a giant connected component in ER networks (i.e., q = 1). This suggests that this structural condition promotes the observed dynamical regime. Instead, for small q, a sufficiently fast rewiring (associated to low values of T ) is required to attain synchronization, at least for q &gt; 0.5. It is also interesting to observe that, if we indicate as T c (q) the value of T delimitating the boundary between partial synchrony and incoherence in the q -T plane, it appears to be linear with q (Fig. 13(b)) before entering in the region where the giant connected component emerges and synchronization becomes independent of T .</p>
        <p>The Authors of Ref. [159] propose an analytical argument to explain this behavior, based on the following considerations. There are three time scales at work in this system. The first one rules local synchronization and is measured by τ LS defined as the time needed by a pair of coupled oscillators to synchronize. This parameter is particularly relevant in the case of low connectivity. The second time scale is the local desynchronization time τ LD , indicating the time to lose synchronization by a pair after being disconnected. The third time scale is the one ruling network blinking and is measured by the effective rewiring time τ ER . Now, suppose that τ LS is much smaller than the other time scales, a regime where the coupled oscillators quickly synchronize. Then, if two coupled oscillators become disconnected, two events may occur: either the oscillators become linked to other units of the system, in a time shorter than τ LD , thus effectively propagating the information to synchronize the whole system, or they lose synchrony before being linked to other units, thus effectively stopping the propagation of the information for synchronization. The first event will likely occur when τ LD &lt; τ ER , the second in the opposite scenario, i.e., then τ ER &lt; τ LD , such that the boundary between synchrony and incoherence could be identified by the condition τ ER ≈ τ LD . Taking into account that the relevant time scales can be approximated as τ LD ≈ π 2 √ 2σ and τ ER ≈ T q (see Ref. [159] for a detailed discussion), then one derives that:The Authors of Ref. [159] propose an analytical argument to explain this behavior, based on the following considerations. There are three time scales at work in this system. The first one rules local synchronization and is measured by τ LS defined as the time needed by a pair of coupled oscillators to synchronize. This parameter is particularly relevant in the case of low connectivity. The second time scale is the local desynchronization time τ LD , indicating the time to lose synchronization by a pair after being disconnected. The third time scale is the one ruling network blinking and is measured by the effective rewiring time τ ER . Now, suppose that τ LS is much smaller than the other time scales, a regime where the coupled oscillators quickly synchronize. Then, if two coupled oscillators become disconnected, two events may occur: either the oscillators become linked to other units of the system, in a time shorter than τ LD , thus effectively propagating the information to synchronize the whole system, or they lose synchrony before being linked to other units, thus effectively stopping the propagation of the information for synchronization. The first event will likely occur when τ LD &lt; τ ER , the second in the opposite scenario, i.e., then τ ER &lt; τ LD , such that the boundary between synchrony and incoherence could be identified by the condition τ ER ≈ τ LD . Taking into account that the relevant time scales can be approximated as τ LD ≈ π 2 √ 2σ and τ ER ≈ T q (see Ref. [159] for a detailed discussion), then one derives that:</p>
        <p>that represents a linear relationship between T c and q as expected.that represents a linear relationship between T c and q as expected.</p>
        <p>Far from being limited to this specific example, the interplay between the diverse time scales that regulate the dynamics of coupled oscillators in temporal networks plays a crucial role in the emergence of synchronization. Along this review, we will see many other examples where the way in which these time scales interact is a fundamental determinant for the emerging dynamics.Far from being limited to this specific example, the interplay between the diverse time scales that regulate the dynamics of coupled oscillators in temporal networks plays a crucial role in the emergence of synchronization. Along this review, we will see many other examples where the way in which these time scales interact is a fundamental determinant for the emerging dynamics.</p>
        <p>Let us now continue the analysis of the model focusing on the limit of fast rewiring, i.e., T → 0. Under this hypothesis the blinking network dynamics is equivalent to that of a system of oscillators coupled via an adjacency matrix fixed in time and obtained as the time-average of the temporal adjacency matrix. The generic ij element of this matrix is computed as follows:Let us now continue the analysis of the model focusing on the limit of fast rewiring, i.e., T → 0. Under this hypothesis the blinking network dynamics is equivalent to that of a system of oscillators coupled via an adjacency matrix fixed in time and obtained as the time-average of the temporal adjacency matrix. The generic ij element of this matrix is computed as follows:</p>
        <p>1 where T av is the time window where the average is calculated. This has to be sufficiently large in order to include many 36 rewiring events. Notice that the term p l (1 -p) N-1-l (N-2)! (l-1)!(N-1-l)! represents the probability that the ith node is interacting with the jth node having exactly l -1 other connections.1 where T av is the time window where the average is calculated. This has to be sufficiently large in order to include many 36 rewiring events. Notice that the term p l (1 -p) N-1-l (N-2)! (l-1)!(N-1-l)! represents the probability that the ith node is interacting with the jth node having exactly l -1 other connections.</p>
        <p>Plugging p = q N in Eq. ( 22) and considering the limit for large N, one can derive the following approximation:Plugging p = q N in Eq. ( 22) and considering the limit for large N, one can derive the following approximation:</p>
        <p>11</p>
        <p>Consequently, under the hypothesis of fast switching, the blinking network in Eq. ( 17) will exhibit the same behavior of the time-invariant network described by the following dynamics:Consequently, under the hypothesis of fast switching, the blinking network in Eq. ( 17) will exhibit the same behavior of the time-invariant network described by the following dynamics:</p>
        <p>with i = 1, . . . , N. Here, it is very important to hallmark that the average network is characterized by an all-to-all topology. We will see the same results arising in several other setups of temporal networks under the fast switching approximation.with i = 1, . . . , N. Here, it is very important to hallmark that the average network is characterized by an all-to-all topology. We will see the same results arising in several other setups of temporal networks under the fast switching approximation.</p>
        <p>An interesting study has later pointed out that properly designing the temporal network can also favor synchronization without necessarily requiring to operate at a fast rewiring rate, see Ref. [161]. In particular, the results of this work leverage the technique of optimal control to consider either the case of minimizing the connectivity cost of a network of phase oscillators with prescribed synchrony or maximizing the synchrony of a network with bounded connectivity cost. The snapshots of the temporal network obtained with this method will no more have random topologies, but will be given by the result of an optimization procedure. The physical mechanism induced by this technique is to preserve synchrony by shortening the duration of low synchrony states and lengthening the duration of high synchrony states.An interesting study has later pointed out that properly designing the temporal network can also favor synchronization without necessarily requiring to operate at a fast rewiring rate, see Ref. [161]. In particular, the results of this work leverage the technique of optimal control to consider either the case of minimizing the connectivity cost of a network of phase oscillators with prescribed synchrony or maximizing the synchrony of a network with bounded connectivity cost. The snapshots of the temporal network obtained with this method will no more have random topologies, but will be given by the result of an optimization procedure. The physical mechanism induced by this technique is to preserve synchrony by shortening the duration of low synchrony states and lengthening the duration of high synchrony states.</p>
        <p>As introduced in Section 2.2.7, adaptive networks are time-varying graphs where the node dynamics and the network topology evolve together. In particular, here we consider the case where the collective dynamics influencing (and influenced by) the link evolution is synchronization, and the nodes are described by phase oscillators. Particularly relevant applications, where synchronization develops through links which continuously adapt in time, are biological 21 networks, and, more specifically, networks of neurons. Here, time-dependent synaptic plasticity and Hebbian learning play a fundamental role in the emergence of high-level computational tasks such as learning and memory. As a first example of an adaptive network, let us consider the model introduced in Ref. [162], that consists of N phase oscillators described by:As introduced in Section 2.2.7, adaptive networks are time-varying graphs where the node dynamics and the network topology evolve together. In particular, here we consider the case where the collective dynamics influencing (and influenced by) the link evolution is synchronization, and the nodes are described by phase oscillators. Particularly relevant applications, where synchronization develops through links which continuously adapt in time, are biological 21 networks, and, more specifically, networks of neurons. Here, time-dependent synaptic plasticity and Hebbian learning play a fundamental role in the emergence of high-level computational tasks such as learning and memory. As a first example of an adaptive network, let us consider the model introduced in Ref. [162], that consists of N phase oscillators described by:</p>
        <p>where the coupling weights w ij (t) evolve in time with a dynamics given by: ẇij = -ϵ sin(φ i -φ j + β), (26) with the additional constraint that |w ij (t)| ≤ 1, so that to avoid an indefinite growth of the weights. In this model, all the oscillators have the same natural frequency, for convenience set to one, i.e., ω i = 1. Furthermore, the time scale of the network evolution, which is given by ϵ -1 , is set to be much larger than that of the oscillator dynamics, which implies ϵ ≪ 1. Here, α and β are two control parameters of the model; by varying them as in the phase diagram shown in Fig. 14(a), the model displays three types of asymptotic behavior: a two cluster state, a coherent state, and a chaotic state.where the coupling weights w ij (t) evolve in time with a dynamics given by: ẇij = -ϵ sin(φ i -φ j + β), (26) with the additional constraint that |w ij (t)| ≤ 1, so that to avoid an indefinite growth of the weights. In this model, all the oscillators have the same natural frequency, for convenience set to one, i.e., ω i = 1. Furthermore, the time scale of the network evolution, which is given by ϵ -1 , is set to be much larger than that of the oscillator dynamics, which implies ϵ ≪ 1. Here, α and β are two control parameters of the model; by varying them as in the phase diagram shown in Fig. 14(a), the model displays three types of asymptotic behavior: a two cluster state, a coherent state, and a chaotic state.</p>
        <p>In more detail, for β ∈ (-π , 0) and α ∈ (0, π/2) a two cluster state may be observed. In this regime (Fig. 14(b)), the oscillators split in two groups with anti-phase synchronization, being the ratio of the two populations generally function of the initial conditions of the system (if the initial phases are chosen uniformly in the interval [0, 2π ), then the two clusters have almost the same size, as in the middle panel of Fig. 14(b)). This is a stable state as the rate of change of the total weight converges to zero (left panel of Fig. 14(b)). The weights thus reach a stationary value such that if two nodes i and j belong to the same cluster then w ij = 1, and otherwise w ij = -1 (right panel of Fig. 14(b)). For β ∈ (-π /2, π/2) and α ∈ (0, π /2), the system also displays a coherent state with the phases of the oscillators almost uniformly distributed (Fig. 14(c)). Furthermore this state is stable as the rate of total weight change approaches zero. Finally, for β ∈ (0, π) and α ∈ (0, π /2) a chaotic state with frustration is also found (Fig. 14(d)). In this case, the rate of total weight change does not vanish such that the network does not reach a stationary state. Instead, the weights continue to evolve in time. In this state, phases are not synchronized but evolve in a chaotic way. In all the scenarios illustrated in Fig. 14, the parameter ϵ has been set to ϵ = 0.005, and the initial values for the weights, i.e., w ij (0), have been set to random values selected from a uniform distribution in [-1, 1].In more detail, for β ∈ (-π , 0) and α ∈ (0, π/2) a two cluster state may be observed. In this regime (Fig. 14(b)), the oscillators split in two groups with anti-phase synchronization, being the ratio of the two populations generally function of the initial conditions of the system (if the initial phases are chosen uniformly in the interval [0, 2π ), then the two clusters have almost the same size, as in the middle panel of Fig. 14(b)). This is a stable state as the rate of change of the total weight converges to zero (left panel of Fig. 14(b)). The weights thus reach a stationary value such that if two nodes i and j belong to the same cluster then w ij = 1, and otherwise w ij = -1 (right panel of Fig. 14(b)). For β ∈ (-π /2, π/2) and α ∈ (0, π /2), the system also displays a coherent state with the phases of the oscillators almost uniformly distributed (Fig. 14(c)). Furthermore this state is stable as the rate of total weight change approaches zero. Finally, for β ∈ (0, π) and α ∈ (0, π /2) a chaotic state with frustration is also found (Fig. 14(d)). In this case, the rate of total weight change does not vanish such that the network does not reach a stationary state. Instead, the weights continue to evolve in time. In this state, phases are not synchronized but evolve in a chaotic way. In all the scenarios illustrated in Fig. 14, the parameter ϵ has been set to ϵ = 0.005, and the initial values for the weights, i.e., w ij (0), have been set to random values selected from a uniform distribution in [-1, 1].</p>
        <p>The effect of heterogeneous natural frequencies in this system has been studied in Ref. [163], where it is shown that the same qualitative states also emerge when the oscillators are non-identical. In particular, a larger heterogeneity is reflected into a slower convergence to the two-cluster state.The effect of heterogeneous natural frequencies in this system has been studied in Ref. [163], where it is shown that the same qualitative states also emerge when the oscillators are non-identical. In particular, a larger heterogeneity is reflected into a slower convergence to the two-cluster state.</p>
        <p>As we have already seen in our first example of adaptive network of Kuramoto oscillators, the coevolving dynamics that is obtained depends on the specific rule used to model weight adaptivity. In this example, in particular, changing a parameter (β) was sufficient to elicit different regimes both with respect to the collective behavior observed and to the structure of the emerging network. The rule to update the weights plays therefore a crucial role in shaping the behavior of an adaptive network. Other models for the weight dynamics, in fact, lead to networks exhibiting other self-organizing features. For instance, in Ref. [164] the Authors study a network described by the following equations:As we have already seen in our first example of adaptive network of Kuramoto oscillators, the coevolving dynamics that is obtained depends on the specific rule used to model weight adaptivity. In this example, in particular, changing a parameter (β) was sufficient to elicit different regimes both with respect to the collective behavior observed and to the structure of the emerging network. The rule to update the weights plays therefore a crucial role in shaping the behavior of an adaptive network. Other models for the weight dynamics, in fact, lead to networks exhibiting other self-organizing features. For instance, in Ref. [164] the Authors study a network described by the following equations:</p>
        <p>with weight dynamics given by:with weight dynamics given by:</p>
        <p>This rule consists of two factors. The first factor, α| sin(β(φ i -φ j ))|, is a growth term that becomes stronger for oscillators having a larger phase difference, such that weights of links between oscillators with a large phase mismatch are updated by a larger quantity compared to oscillators with similar phases. The second factor, -w ij , represents a forgetting/discharge term guaranteeing a continuous adaptation of the weights. With this choice of the coupling adaptation rule, the ability of the network to achieve synchronization is enhanced, since the weight of each link is adapted to the mismatch in the intrinsic frequencies of the oscillators at its extremes. In fact, the Authors of Ref. [164] show that the steady-state distribution of the weights is w ij ∝ |ω i -ω j | µ with 0.85 &lt; µ &lt; 0.95, which is very close to the criterion of optimal coupling law, in the sense of least average coupling cost, that corresponds to µ = 1. In addition, with such an adaptive coupling way, the system self-organizes such that larger coupling strengths are used when needed, i.e., during the transient, to quickly converge towards the synchronization regime, and then smaller weights are used to maintain the synchronization state.This rule consists of two factors. The first factor, α| sin(β(φ i -φ j ))|, is a growth term that becomes stronger for oscillators having a larger phase difference, such that weights of links between oscillators with a large phase mismatch are updated by a larger quantity compared to oscillators with similar phases. The second factor, -w ij , represents a forgetting/discharge term guaranteeing a continuous adaptation of the weights. With this choice of the coupling adaptation rule, the ability of the network to achieve synchronization is enhanced, since the weight of each link is adapted to the mismatch in the intrinsic frequencies of the oscillators at its extremes. In fact, the Authors of Ref. [164] show that the steady-state distribution of the weights is w ij ∝ |ω i -ω j | µ with 0.85 &lt; µ &lt; 0.95, which is very close to the criterion of optimal coupling law, in the sense of least average coupling cost, that corresponds to µ = 1. In addition, with such an adaptive coupling way, the system self-organizes such that larger coupling strengths are used when needed, i.e., during the transient, to quickly converge towards the synchronization regime, and then smaller weights are used to maintain the synchronization state.</p>
        <p>Another very interesting model of adaptive network of Kuramoto oscillators investigates the structures that emerge in the presence of a constraint in the resources available to a node to establish links with the other units of the system [165,166]. The model incorporates two fundamental ingredients: homophily, taken into account by defining a weight update law that enhances connections between units with similar phases, and homeostasis, by incorporating a 53 mechanism that preserves the sum of the weights of incoming connections at each node. In more detail, the dynamics of the adaptive network [166] is described by:Another very interesting model of adaptive network of Kuramoto oscillators investigates the structures that emerge in the presence of a constraint in the resources available to a node to establish links with the other units of the system [165,166]. The model incorporates two fundamental ingredients: homophily, taken into account by defining a weight update law that enhances connections between units with similar phases, and homeostasis, by incorporating a 53 mechanism that preserves the sum of the weights of incoming connections at each node. In more detail, the dynamics of the adaptive network [166] is described by:</p>
        <p>where p ij is the average phase correlation between oscillators i and j in the time interval T :where p ij is the average phase correlation between oscillators i and j in the time interval T :</p>
        <p>From Eq. ( 30) it follows that ∑ j∈N i w ij = 1, such that the resources available for a node to establish/reinforce connections with other units are maintained constant in time. The competition between homophily (modeled by the first term on the right-hand side of Eq. ( 30)) and homeostasis (modeled by the second term on the right-hand side of the same equation) in this network leads to an enhancement in synchronization and to the emergence of a mesoscale of communities and a scale-free distribution in the connection weights. The enhancement in synchronization can be monitored by calculating the Kuramoto order parameter r(t) at different values of the coupling strength ϵ (Fig. 15(a)). The network is initialized by randomly selecting a number K of neighbors for each node and setting the initial value of each weight to 1/K ; the initial conditions for the phases are selected randomly from a uniform distribution in [0, 2π ); the frequencies ω i are also randomly assigned, from a uniform distribution in (-π , π]. Eqs. ( 29) and ( 30) are integrated without adaptation for 200 time units and, then, at t = 0, the adaptation mechanism is activated, such that the adaptive case can be compared with the corresponding behavior in the non-adaptive counterpart of the temporal network. The time evolution of the Kuramoto order parameter r(t) shows that synchronization is generally enhanced by the adaptive mechanism ruling weight evolution (Fig. 15(a)). This is particularly evident when the coupling strength is below the critical value to attain synchronization in a non-adaptive network, whereas for large ϵ the enhancement in synchronization is smaller.From Eq. ( 30) it follows that ∑ j∈N i w ij = 1, such that the resources available for a node to establish/reinforce connections with other units are maintained constant in time. The competition between homophily (modeled by the first term on the right-hand side of Eq. ( 30)) and homeostasis (modeled by the second term on the right-hand side of the same equation) in this network leads to an enhancement in synchronization and to the emergence of a mesoscale of communities and a scale-free distribution in the connection weights. The enhancement in synchronization can be monitored by calculating the Kuramoto order parameter r(t) at different values of the coupling strength ϵ (Fig. 15(a)). The network is initialized by randomly selecting a number K of neighbors for each node and setting the initial value of each weight to 1/K ; the initial conditions for the phases are selected randomly from a uniform distribution in [0, 2π ); the frequencies ω i are also randomly assigned, from a uniform distribution in (-π , π]. Eqs. ( 29) and ( 30) are integrated without adaptation for 200 time units and, then, at t = 0, the adaptation mechanism is activated, such that the adaptive case can be compared with the corresponding behavior in the non-adaptive counterpart of the temporal network. The time evolution of the Kuramoto order parameter r(t) shows that synchronization is generally enhanced by the adaptive mechanism ruling weight evolution (Fig. 15(a)). This is particularly evident when the coupling strength is below the critical value to attain synchronization in a non-adaptive network, whereas for large ϵ the enhancement in synchronization is smaller.</p>
        <p>However, the coupling strength influences not only the extent of enhancement in synchronization but also the structures emerging during adaptation (Fig. 15(b)). For small ϵ, the topology resulting from the competitive adaptive Source: Reprinted figure with permission from Ref. [166].However, the coupling strength influences not only the extent of enhancement in synchronization but also the structures emerging during adaptation (Fig. 15(b)). For small ϵ, the topology resulting from the competitive adaptive Source: Reprinted figure with permission from Ref. [166].</p>
        <p>© 2011 by the American Physical Society.© 2011 by the American Physical Society.</p>
        <p>mechanism is highly heterogeneous, characterized by a weight distribution close to a power-law, indicating that in 25 this case a complete redistribution of the weights occurred. As the coupling strength increases, the weight distribution exhibits a local maximum and the network is segregated into distinct modules, each characterized by its own frequency of oscillation. For instance, at ϵ = 2 three communities appear, whereas at ϵ = 3 the network splits into two communities. Finally, when the coupling strength is larger than the synchronization threshold in the non-adaptive counterpart of the temporal network, then the weight distribution becomes peaked at 1/K (this behavior is exemplified in Fig. 15(b), at ϵ = 4.2). In these conditions, as the network is able to reach synchronization even in the absence of adaptation, the weights do not significantly change from their initial value 1/K . In the last model we illustrate here, the rule of connectivity is stochastic and based on a fitness. At variance with what discussed so far, this means that the weights are all equal and constant in time, and only the topology is instantaneously updated following the so-called fitness or hidden variable network model [167,168]. According to this model, each pair of nodes is coupled with a probability given by a fitness function. In the adaptive network of Kuramoto oscillators, this fitness depends on the phases of the oscillators, i.e., f (φ i , φ j ), where φ i and φ j are the phases of the oscillators i and j that evolve as in Eqs. (29). In Ref. [169], the Authors select the function f (φ i , φ j ) to embed a homophily mechanism in the adaptive network:mechanism is highly heterogeneous, characterized by a weight distribution close to a power-law, indicating that in 25 this case a complete redistribution of the weights occurred. As the coupling strength increases, the weight distribution exhibits a local maximum and the network is segregated into distinct modules, each characterized by its own frequency of oscillation. For instance, at ϵ = 2 three communities appear, whereas at ϵ = 3 the network splits into two communities. Finally, when the coupling strength is larger than the synchronization threshold in the non-adaptive counterpart of the temporal network, then the weight distribution becomes peaked at 1/K (this behavior is exemplified in Fig. 15(b), at ϵ = 4.2). In these conditions, as the network is able to reach synchronization even in the absence of adaptation, the weights do not significantly change from their initial value 1/K . In the last model we illustrate here, the rule of connectivity is stochastic and based on a fitness. At variance with what discussed so far, this means that the weights are all equal and constant in time, and only the topology is instantaneously updated following the so-called fitness or hidden variable network model [167,168]. According to this model, each pair of nodes is coupled with a probability given by a fitness function. In the adaptive network of Kuramoto oscillators, this fitness depends on the phases of the oscillators, i.e., f (φ i , φ j ), where φ i and φ j are the phases of the oscillators i and j that evolve as in Eqs. (29). In Ref. [169], the Authors select the function f (φ i , φ j ) to embed a homophily mechanism in the adaptive network:</p>
        <p>where z is a positive parameter. In this way, the wiring probability for nodes with similar phases is higher than that for nodes having a larger phase difference. For oscillators with the same phase, i.e., φ i = φ j , the fitness becomes:where z is a positive parameter. In this way, the wiring probability for nodes with similar phases is higher than that for nodes having a larger phase difference. For oscillators with the same phase, i.e., φ i = φ j , the fitness becomes:</p>
        <p>whereas for oscillators with a phase lag equal to π, i.e., |φ i -φ j | = π , f (φ i , φ j ) = 0. Therefore, high values of z yield a more connected structure. The parameter ϵ instead control the coherence of the oscillators, with higher values leading to a more coherent regime. Numerical simulations of this model reveal interesting features, in particular how in this structure percolation and synchronization are simultaneously enhanced. Percolation, that is one of the most important phenomena occurring in networks [170,171], is the critical transition observed when the addition of a small number of links makes a substantial number of components of the network to become connected [172][173][174][175][176][177][178][179]. Here, it is monitored by measuring the size of the largest connected component s(t) and, in particular, its steady state value s ∞ . Synchronization is instead monitored by the classical Kuramoto order parameter as in Eq. ( 19) and its steady-state value r ∞ . Following Ref. [169], let us consider a network with N = 150 nodes and natural frequencies assigned from a uniform distribution in the range [-1, 1]. Fig. 16 shows how percolation and synchronization varies as a function of the two parameters ϵ and z, ruling the model, for both the non-adaptive (fixed weights) and the adaptive case (fitness-based weight update). The transition to percolation in the non-adaptive case only depends on z (Fig. 16(a)). In particular, the typical regimes are observed, with the subcritical regime (s ∞ ∼ 0.0) appearing for z &lt; 1, the critical regime for z ∼ 1, the supercritical regime (0 &lt; s ∞ &lt; 1) for 1 &lt; z &lt; 3, and the connected regime (s ∞ ∼ 1.0) for z &gt; 3. Correspondingly, diverse levels of synchronization are found, depending on the percolation state where the network is (Fig. 16(b)): incoherent states r ∞ &lt; 0.05 in the sub-critical and critical regimes (z &lt; 1, ∀ϵ); partial synchronization states (0.1 &lt; r ∞ &lt; 0.9) in the supercritical regime; and highly synchronized states in the connected regime z &gt; 3. A different scenario emerges in the adaptive case, with a significant enhancement of both percolation and synchronization (Figs. 16(c) and 16(d)). Now, percolation becomes function of both z and ϵ, with larger values of ϵ favoring the emergence of a giant component at smaller values of z and a significant enhancement of percolation observed in particular in the region z &lt; 3. Similarly, also synchronization benefits from the presence of the adaptation mechanism, and, again, this is particularly evident in the region z &lt; 3 where the connectivity is low. Overall, these results lead to the conclusion that the adaptivity instaurates a positive feedback loop between network structure and dynamics that enables the emergence of synchronization and connected components even in regions when the resources for interactions are limited. Quite 42 interestingly, recent results [180] have shown that adaptive mechanisms can also induce regime beyond that of global synchronization, leading, for instance, to the formation of partial synchronization patterns. To show this, the Authors of Ref. [180] actually consider a network organized in more than one layer and demonstrate how multiplexing can induce various stable phase cluster states that are not stable or do not exist in the monolayer structure.whereas for oscillators with a phase lag equal to π, i.e., |φ i -φ j | = π , f (φ i , φ j ) = 0. Therefore, high values of z yield a more connected structure. The parameter ϵ instead control the coherence of the oscillators, with higher values leading to a more coherent regime. Numerical simulations of this model reveal interesting features, in particular how in this structure percolation and synchronization are simultaneously enhanced. Percolation, that is one of the most important phenomena occurring in networks [170,171], is the critical transition observed when the addition of a small number of links makes a substantial number of components of the network to become connected [172][173][174][175][176][177][178][179]. Here, it is monitored by measuring the size of the largest connected component s(t) and, in particular, its steady state value s ∞ . Synchronization is instead monitored by the classical Kuramoto order parameter as in Eq. ( 19) and its steady-state value r ∞ . Following Ref. [169], let us consider a network with N = 150 nodes and natural frequencies assigned from a uniform distribution in the range [-1, 1]. Fig. 16 shows how percolation and synchronization varies as a function of the two parameters ϵ and z, ruling the model, for both the non-adaptive (fixed weights) and the adaptive case (fitness-based weight update). The transition to percolation in the non-adaptive case only depends on z (Fig. 16(a)). In particular, the typical regimes are observed, with the subcritical regime (s ∞ ∼ 0.0) appearing for z &lt; 1, the critical regime for z ∼ 1, the supercritical regime (0 &lt; s ∞ &lt; 1) for 1 &lt; z &lt; 3, and the connected regime (s ∞ ∼ 1.0) for z &gt; 3. Correspondingly, diverse levels of synchronization are found, depending on the percolation state where the network is (Fig. 16(b)): incoherent states r ∞ &lt; 0.05 in the sub-critical and critical regimes (z &lt; 1, ∀ϵ); partial synchronization states (0.1 &lt; r ∞ &lt; 0.9) in the supercritical regime; and highly synchronized states in the connected regime z &gt; 3. A different scenario emerges in the adaptive case, with a significant enhancement of both percolation and synchronization (Figs. 16(c) and 16(d)). Now, percolation becomes function of both z and ϵ, with larger values of ϵ favoring the emergence of a giant component at smaller values of z and a significant enhancement of percolation observed in particular in the region z &lt; 3. Similarly, also synchronization benefits from the presence of the adaptation mechanism, and, again, this is particularly evident in the region z &lt; 3 where the connectivity is low. Overall, these results lead to the conclusion that the adaptivity instaurates a positive feedback loop between network structure and dynamics that enables the emergence of synchronization and connected components even in regions when the resources for interactions are limited. Quite 42 interestingly, recent results [180] have shown that adaptive mechanisms can also induce regime beyond that of global synchronization, leading, for instance, to the formation of partial synchronization patterns. To show this, the Authors of Ref. [180] actually consider a network organized in more than one layer and demonstrate how multiplexing can induce various stable phase cluster states that are not stable or do not exist in the monolayer structure.</p>
        <p>Our first example of blinking networks of chaotic oscillators is devoted to illustrate the fast switching stability criterion. In particular, as in Ref. [55], we focus on a set of N coupled identical Rössler oscillators.Our first example of blinking networks of chaotic oscillators is devoted to illustrate the fast switching stability criterion. In particular, as in Ref. [55], we focus on a set of N coupled identical Rössler oscillators.</p>
        <p>The dynamics of the system is described by the following equations:The dynamics of the system is described by the following equations:</p>
        <p>where i = 1, . . . , N and L(t) is the time-varying Laplacian describing the blinking network. The system parameters are chosen as a = 0.2, b = 0.2 and c = 5.7 so that the uncoupled dynamics is chaotic.where i = 1, . . . , N and L(t) is the time-varying Laplacian describing the blinking network. The system parameters are chosen as a = 0.2, b = 0.2 and c = 5.7 so that the uncoupled dynamics is chaotic.</p>
        <p>Here, we consider a blinking network with the following characteristics. At each snapshot the connectivity is given by an Erdös-Rényi network model with edge joining probability set to p = 0.1. At each time step the underlying network is rewired with probability p r .Therefore, at each time instance, L (t) changes with the characteristics rewiring probability p r . The network comprises N = 200 identical Rössler oscillators.Here, we consider a blinking network with the following characteristics. At each snapshot the connectivity is given by an Erdös-Rényi network model with edge joining probability set to p = 0.1. At each time step the underlying network is rewired with probability p r .Therefore, at each time instance, L (t) changes with the characteristics rewiring probability p r . The network comprises N = 200 identical Rössler oscillators.</p>
        <p>To detect the emergence of complete synchronization, the synchronization errorTo detect the emergence of complete synchronization, the synchronization error</p>
        <p>dτ is monitored. In Fig. 17 or ϵ ≥ 0.008 for p r = 10 0 . The results point out that a higher rewiring probability enhances synchrony, as it widens the interval of values of the coupling strength for which synchronization takes place, as compared to the case of a static network. To further illustrate the effect of the rewiring probability p r on synchronization, it is instructive to inspect the behavior of the synchronization 27 error E in the plane (ϵ, p r ) (Fig. 17(b)), where the enhancement in synchronization due to an increase in the rate of switching among the different link configurations is evident. Quite remarkably, this trend continues up to a certain limit, after which further increases of p r do not lead to changes in E.dτ is monitored. In Fig. 17 or ϵ ≥ 0.008 for p r = 10 0 . The results point out that a higher rewiring probability enhances synchrony, as it widens the interval of values of the coupling strength for which synchronization takes place, as compared to the case of a static network. To further illustrate the effect of the rewiring probability p r on synchronization, it is instructive to inspect the behavior of the synchronization 27 error E in the plane (ϵ, p r ) (Fig. 17(b)), where the enhancement in synchronization due to an increase in the rate of switching among the different link configurations is evident. Quite remarkably, this trend continues up to a certain limit, after which further increases of p r do not lead to changes in E.</p>
        <p>The onset of synchronization, when p r is large, can be predicted by the fast switching stability criterion. In fact, one can calculate the time-average of the temporal network and use it to compute the maximum transverse Lyapunov exponent as a function of ϵ, according to Eqs. (12). The result is illustrated in Fig. 17(c) which clearly shows that the maximum transverse Lyapunov exponent becomes negative in correspondence of the values of ϵ for which the synchronization error E for the blinking network with high p r becomes zero. Hence, we conclude that, as predicted by the fast switching stability criterion, if the static network of oscillators with topology given by the time-average of the temporal connectivity synchronizes, then the temporal network will also synchronize, provided that the switching occurs at a sufficiently enough high rate.The onset of synchronization, when p r is large, can be predicted by the fast switching stability criterion. In fact, one can calculate the time-average of the temporal network and use it to compute the maximum transverse Lyapunov exponent as a function of ϵ, according to Eqs. (12). The result is illustrated in Fig. 17(c) which clearly shows that the maximum transverse Lyapunov exponent becomes negative in correspondence of the values of ϵ for which the synchronization error E for the blinking network with high p r becomes zero. Hence, we conclude that, as predicted by the fast switching stability criterion, if the static network of oscillators with topology given by the time-average of the temporal connectivity synchronizes, then the temporal network will also synchronize, provided that the switching occurs at a sufficiently enough high rate.</p>
        <p>As a second example of synchronization in blinking networks, we now discuss oscillators interacting via on-off coupling strategy as in Ref. [149]. In particular, the Authors of Ref. [149] have found that switching the on-off coupling at a rate comparable to that of the node dynamics can be advantageous for synchronization, independently of the specific features of the network topology adopted to set the interactions during the 'on' state. In more detail, the Authors have varied the on-off time scale considering a large interval of variations and studied the effects of this parameter on synchronization. When the on-off time scale is very small compared to that of the associated coupled node dynamics, the synchronization stability can be predicted by applying the fast switching stability criterion, and, hence, considering the static Laplacian matrix accounting for time-average couplings. As the on-off time scale is increased to become comparable to the time scale of node dynamics, several interesting features are identified, among which the most exciting finding is that one of the traditional bounds for synchronization, due to short-wavelength bifurcations (SWBs) [181], nearly disappears, yielding a neat advantage in allowing fast synchronization in large networks.As a second example of synchronization in blinking networks, we now discuss oscillators interacting via on-off coupling strategy as in Ref. [149]. In particular, the Authors of Ref. [149] have found that switching the on-off coupling at a rate comparable to that of the node dynamics can be advantageous for synchronization, independently of the specific features of the network topology adopted to set the interactions during the 'on' state. In more detail, the Authors have varied the on-off time scale considering a large interval of variations and studied the effects of this parameter on synchronization. When the on-off time scale is very small compared to that of the associated coupled node dynamics, the synchronization stability can be predicted by applying the fast switching stability criterion, and, hence, considering the static Laplacian matrix accounting for time-average couplings. As the on-off time scale is increased to become comparable to the time scale of node dynamics, several interesting features are identified, among which the most exciting finding is that one of the traditional bounds for synchronization, due to short-wavelength bifurcations (SWBs) [181], nearly disappears, yielding a neat advantage in allowing fast synchronization in large networks.</p>
        <p>Taking into account that in on-off blinking networks a single, fixed set of interconnections is switched on and off, the dynamics of each node can be described as follows:Taking into account that in on-off blinking networks a single, fixed set of interconnections is switched on and off, the dynamics of each node can be described as follows:</p>
        <p>where ϵ(t) is the on-off coupling strength. Two further parameters are needed to describe the network dynamics: T , representing the on-off period, and θ ∈ [0, 1] representing the on-off rate. Connections among the oscillators are switched on for a fraction of the on-off period T equal to θ, such that for hT &lt; t &lt; (h + θ)T (with h = 0, 1, 2, . . . ) they are active and the coupling strength is set to ϵ(t) = ϵ, whereas for the remaining time (n + θ)T &lt; t &lt; (n + 1)T they are turned off and ϵ(t) = 0. When θ = 0 the oscillators are isolated for all time, whereas for θ = 1 the connections are always active and the network is no more time-dependent.where ϵ(t) is the on-off coupling strength. Two further parameters are needed to describe the network dynamics: T , representing the on-off period, and θ ∈ [0, 1] representing the on-off rate. Connections among the oscillators are switched on for a fraction of the on-off period T equal to θ, such that for hT &lt; t &lt; (h + θ)T (with h = 0, 1, 2, . . . ) they are active and the coupling strength is set to ϵ(t) = ϵ, whereas for the remaining time (n + θ)T &lt; t &lt; (n + 1)T they are turned off and ϵ(t) = 0. When θ = 0 the oscillators are isolated for all time, whereas for θ = 1 the connections are always active and the network is no more time-dependent.</p>
        <p>For numerical illustration, we resort again to Rössler oscillators, this time setting the parameters as a = b = 0.2 and c = 7, for which another chaotic attractor characterizes the uncoupled dynamics. In this case the time scale of the unit dynamics, estimated from the time evolution of the variable x(t), is T typical = 5.89. © 2009 by the American Physical Society.For numerical illustration, we resort again to Rössler oscillators, this time setting the parameters as a = b = 0.2 and c = 7, for which another chaotic attractor characterizes the uncoupled dynamics. In this case the time scale of the unit dynamics, estimated from the time evolution of the variable x(t), is T typical = 5.89. © 2009 by the American Physical Society.</p>
        <p>The specific way in which, in on-off blinking networks, the coupling depends on time prompts for a particularly 35 convenient form of the linearized dynamics. To show this, let us consider again linearization around the synchronization manifold x s and write the equations for the deviation variables δx i (t) = x i (t) -x s (t) as:The specific way in which, in on-off blinking networks, the coupling depends on time prompts for a particularly 35 convenient form of the linearized dynamics. To show this, let us consider again linearization around the synchronization manifold x s and write the equations for the deviation variables δx i (t) = x i (t) -x s (t) as:</p>
        <p>At this point, one can apply the transformation ηAt this point, one can apply the transformation η</p>
        <p>and hence study the generic variational equationand hence study the generic variational equation</p>
        <p>where ε(t) = ϵ(t) ϵ is the normalized on-off coupling and Eq. ( 37) is called the coupling-dependent master stability equation (CMSE). The MSF then can be obtained by studying the largest Lyapunov exponent of the CMSE as a function of α and β, i.e., λ max (α + iβ). At this point, the sign of λ max at the points α + iβ = ϵγ k for the transverse modes k = 2, 3, . . . , N is studied. It is here important to remark that γ k are the eigenvalues of the Laplacian matrix of the static configuration which is switched on and off in the blinking process. Only when all the transverse modes are located in the negative region of the MSF, the synchronous state is stable.where ε(t) = ϵ(t) ϵ is the normalized on-off coupling and Eq. ( 37) is called the coupling-dependent master stability equation (CMSE). The MSF then can be obtained by studying the largest Lyapunov exponent of the CMSE as a function of α and β, i.e., λ max (α + iβ). At this point, the sign of λ max at the points α + iβ = ϵγ k for the transverse modes k = 2, 3, . . . , N is studied. It is here important to remark that γ k are the eigenvalues of the Laplacian matrix of the static configuration which is switched on and off in the blinking process. Only when all the transverse modes are located in the negative region of the MSF, the synchronous state is stable.</p>
        <p>To illustrate some numerical results, following Ref. [149], let us fix the coupling function h(x) as h(x) = [x 0 0] T , that produces in the case of time-independent links a type III MSF. We also consider that the interaction structure to switch is bidirectional, so that the Laplacian matrix L is symmetric and Eq. ( 37) can be studied for real eigenvalues, i.e., β = 0.To illustrate some numerical results, following Ref. [149], let us fix the coupling function h(x) as h(x) = [x 0 0] T , that produces in the case of time-independent links a type III MSF. We also consider that the interaction structure to switch is bidirectional, so that the Laplacian matrix L is symmetric and Eq. ( 37) can be studied for real eigenvalues, i.e., β = 0.</p>
        <p>The maximum Lyapunov exponent Λ max = Λ max (α, θ) is shown for several values of the on-off period T in Fig. 18. For T = 0.1 ≪ T typical (Fig. 18(a)) the fast switching stability criterion applies. Let α 1 and α 2 be the thresholds for stability obtained for continuous coupling, i.e., for θ = 1 (for the given parameters, we have α 1 ≃ 0.14 and α 2 ≃ 4.48) such that Λ max (α, 1)The maximum Lyapunov exponent Λ max = Λ max (α, θ) is shown for several values of the on-off period T in Fig. 18. For T = 0.1 ≪ T typical (Fig. 18(a)) the fast switching stability criterion applies. Let α 1 and α 2 be the thresholds for stability obtained for continuous coupling, i.e., for θ = 1 (for the given parameters, we have α 1 ≃ 0.14 and α 2 ≃ 4.48) such that Λ max (α, 1)</p>
        <p>Then, the region of negative Λ max (α, θ), that is the region where the synchronization manifold is stable, is bounded by two hyperbola, given by ϵθγ 2 = α 1 and ϵθγ N = α 2 . Notice that the prediction by the fast switching stability criterion is accurate as the theoretical boundaries of the region correspond with those derived from numerical simulations of the on-off blinking network (Fig. 18(a)).Then, the region of negative Λ max (α, θ), that is the region where the synchronization manifold is stable, is bounded by two hyperbola, given by ϵθγ 2 = α 1 and ϵθγ N = α 2 . Notice that the prediction by the fast switching stability criterion is accurate as the theoretical boundaries of the region correspond with those derived from numerical simulations of the on-off blinking network (Fig. 18(a)).</p>
        <p>Consider now the scenario where the time scale of network blinking is of the same order of that of the node dynamics.Consider now the scenario where the time scale of network blinking is of the same order of that of the node dynamics.</p>
        <p>Two examples are shown in Fig. 18(b) for T = 3 and in Fig. 18(c) for T = 6. Here, the synchronization region is no more bounded by the two hyperbolic boundaries, that on the contrary gradually become parallel to each other for large α.Two examples are shown in Fig. 18(b) for T = 3 and in Fig. 18(c) for T = 6. Here, the synchronization region is no more bounded by the two hyperbolic boundaries, that on the contrary gradually become parallel to each other for large α.</p>
        <p>Noticeably, in Fig. 18(b) the stability region is larger than in the fast switching regime, whereas in Fig. 18(c) it appears intertwined by another region where synchronization is impossible to attain. In this intermediate time scale regime the striking feature that emerges is that in an interval of values of θ the second threshold disappears, denoting a transition from a type III to a type II MSF behavior.Noticeably, in Fig. 18(b) the stability region is larger than in the fast switching regime, whereas in Fig. 18(c) it appears intertwined by another region where synchronization is impossible to attain. In this intermediate time scale regime the striking feature that emerges is that in an interval of values of θ the second threshold disappears, denoting a transition from a type III to a type II MSF behavior.</p>
        <p>The last scenario we study occurs for a time scale of the on-off blinking much larger than the time scale of the unit 32 dynamics. The Authors of Ref. [149] observe that, starting from the scenario of comparable time scales, further increasing the time scale of blinking reduce the region of synchronization stability up to the case illustrated in Fig. 18(d) where this region looks very different from that of Fig. 18(a)-(b). To account for the behavior in this regime, one can consider that during each period T , in the first fraction of time, θT , the coupling is turned on, and the divergence of nearby trajectory is dominated by the maximum Lyapunov exponent Λ ⊥ (which thus correspond to the continuous coupling case, when θ is always zero), whereas, in the second fraction of time (1 -θ)T , the coupling is off and the divergence is dominated by the maximum Lyapunov exponent of the uncoupled dynamics, here indicated as Λ 0 max . Taking into account these considerations, the maximum Lyapunov exponent Λ max for a given θ can be approximated asThe last scenario we study occurs for a time scale of the on-off blinking much larger than the time scale of the unit 32 dynamics. The Authors of Ref. [149] observe that, starting from the scenario of comparable time scales, further increasing the time scale of blinking reduce the region of synchronization stability up to the case illustrated in Fig. 18(d) where this region looks very different from that of Fig. 18(a)-(b). To account for the behavior in this regime, one can consider that during each period T , in the first fraction of time, θT , the coupling is turned on, and the divergence of nearby trajectory is dominated by the maximum Lyapunov exponent Λ ⊥ (which thus correspond to the continuous coupling case, when θ is always zero), whereas, in the second fraction of time (1 -θ)T , the coupling is off and the divergence is dominated by the maximum Lyapunov exponent of the uncoupled dynamics, here indicated as Λ 0 max . Taking into account these considerations, the maximum Lyapunov exponent Λ max for a given θ can be approximated as</p>
        <p>Fig. 18(d) shows that this approximation proves to accurately predict the behavior when the blinking occurs at a rate very low compared to the evolution of the unit dynamics.Fig. 18(d) shows that this approximation proves to accurately predict the behavior when the blinking occurs at a rate very low compared to the evolution of the unit dynamics.</p>
        <p>Reference [182] provides further examples of networks with on-off coupling (in particular, of Rössler and Duffing oscillators) that, for intermediate switching frequencies, exhibit windows (in the study called windows of opportunity), where synchronization is stable in the temporal network, but unstable in the time-average structure, and so in the fast switching regime.Reference [182] provides further examples of networks with on-off coupling (in particular, of Rössler and Duffing oscillators) that, for intermediate switching frequencies, exhibit windows (in the study called windows of opportunity), where synchronization is stable in the temporal network, but unstable in the time-average structure, and so in the fast switching regime.</p>
        <p>Finally, it is interesting to note that the main features observed in the numerical simulations of the on-off blinking network illustrated above also appear in real experiments with physical systems, such as nonlinear electronic circuits [149]. Earlier experiments conducted on two on-off coupled Chua's circuits [183] show the typical behavior arising for type II system, with synchronization achievable for switching frequencies greater than a certain threshold. A complex pattern of intertwined regions of synchronization or desynchronization in real systems can be also observed when nonlinear electronic oscillators are switched among two different configurations as in [105].Finally, it is interesting to note that the main features observed in the numerical simulations of the on-off blinking network illustrated above also appear in real experiments with physical systems, such as nonlinear electronic circuits [149]. Earlier experiments conducted on two on-off coupled Chua's circuits [183] show the typical behavior arising for type II system, with synchronization achievable for switching frequencies greater than a certain threshold. A complex pattern of intertwined regions of synchronization or desynchronization in real systems can be also observed when nonlinear electronic oscillators are switched among two different configurations as in [105].</p>
        <p>Let us now consider an activity driven network of chaotic oscillators as in Ref. [105]. As discussed in Section 2.2.2, in this network links are set according to the node activity rate so that a sequence of snapshots switched with a time scale here fixed at τ ∆t (where ∆t is the integration step size of the unit dynamics) is obtained. τ represents an important control parameter for the system as it modulates the rate of switching among the possible configurations. In particular, for small τ (e.g., τ = 1) the system is close to fast switching regime, whereas increasing τ the dynamics of the network evolution becomes first comparable and then slower than that of the units at the nodes.Let us now consider an activity driven network of chaotic oscillators as in Ref. [105]. As discussed in Section 2.2.2, in this network links are set according to the node activity rate so that a sequence of snapshots switched with a time scale here fixed at τ ∆t (where ∆t is the integration step size of the unit dynamics) is obtained. τ represents an important control parameter for the system as it modulates the rate of switching among the possible configurations. In particular, for small τ (e.g., τ = 1) the system is close to fast switching regime, whereas increasing τ the dynamics of the network evolution becomes first comparable and then slower than that of the units at the nodes.</p>
        <p>We consider first the network behavior in the fast switching regime. Under this hypothesis, we can consider the time-average Laplacian matrix L and rewrite it as L = 1We consider first the network behavior in the fast switching regime. Under this hypothesis, we can consider the time-average Laplacian matrix L and rewrite it as L = 1</p>
        <p>∑ Nc k=1 Lt k where N c is the number of possible instantaneous configurations of the network (each being equally probable). If the switching is sufficiently fast, the generic ij element of this matrix is given by the probability p ADN ij that nodes i and j are connected at a given time. As this probability [184] can be written as:∑ Nc k=1 Lt k where N c is the number of possible instantaneous configurations of the network (each being equally probable). If the switching is sufficiently fast, the generic ij element of this matrix is given by the probability p ADN ij that nodes i and j are connected at a given time. As this probability [184] can be written as:</p>
        <p>so that for i ̸ = j we can approximate Lij asso that for i ̸ = j we can approximate Lij as</p>
        <p>and finally Lii = -∑ j Lij . The availability of an approximated expression for L makes possible, on the one hand, to predict the boundaries for the synchronization stability region and, on the other hand, to gain some insights on the effect of the parameters underlying the architecture of the temporal network such as m and γ . This analysis carried out in Ref. [105] yields the conclusion that in both type II and type III systems increasing m and γ generally favors synchronization, even if the eigenvalues of the time-average Laplacian, and so network synchronization, strongly depend on the specific instance of the power-law distribution of the node activity rates.and finally Lii = -∑ j Lij . The availability of an approximated expression for L makes possible, on the one hand, to predict the boundaries for the synchronization stability region and, on the other hand, to gain some insights on the effect of the parameters underlying the architecture of the temporal network such as m and γ . This analysis carried out in Ref. [105] yields the conclusion that in both type II and type III systems increasing m and γ generally favors synchronization, even if the eigenvalues of the time-average Laplacian, and so network synchronization, strongly depend on the specific instance of the power-law distribution of the node activity rates.</p>
        <p>The other extreme scenario is represented by the regime when the network evolves with a time scale large compared to that of the system dynamics. In this case, the approach described in Ref. [142] can be applied. Given that there are N c possible configurations for the network, then one has to verify that the condition ∑ Nc k=1 Λ max (ϵλ j k k ) &lt; 0 is true for each combination of the transverse modes of the N c configurations (j 1 , . . . , j Nc ). Notice, however, that, for activity driven networks with a power-law distribution of the node activity rates and small m, as it typically occurs, each snapshot will very likely include at least one node not connected to any other unit of the network, such that each configuration will include at least one zero eigenvalue. In this case, the condition for synchronization would require that N c Λ max (0) &lt; 0 which is impossible to obtain for chaotic dynamics where Λ max (0) (the maximum Lyapunov exponent of the isolated dynamics) is positive. This yields the conclusion that activity driven networks with a power-law distribution of the node activity rates and small m are difficult to synchronize in the regime of slow switching. This analysis also informs about possible mechanisms to promote synchronization under this regime: they have to be directed towards reducing the likelihood of finding isolated nodes, for instance increasing m or changing the shape of the activity rate distribution. The numerical results discussed in Ref. [105] confirm the suitability of the fast switching stability criterion to predict 55 the region of synchronization stability as well as the analysis under the slow switching regime pointing out the difficulty to synchronize activity driven networks with power-law distribution of the activity rates and small m. The Authors of Ref. [105] also investigate the behavior for intermediate values of τ . Here, they find that increasing τ first the region of synchronization stability widens and then if becomes unbounded. We can conclude that in activity driven networks the same qualitative behavior observed in the examples of on-off blinking networks discussed above as well as in other temporal networks (see for instance Ref. [76]) arises, suggesting that this behavior does not depend on the specific features of how the time evolution shapes the link formation. The overall picture that emerges from these studies is that, in temporal networks, synchronization is generally promoted by fast network evolution, but more favorable conditions for synchronization can occur at switching rates slightly lower than the fast switching regime.The other extreme scenario is represented by the regime when the network evolves with a time scale large compared to that of the system dynamics. In this case, the approach described in Ref. [142] can be applied. Given that there are N c possible configurations for the network, then one has to verify that the condition ∑ Nc k=1 Λ max (ϵλ j k k ) &lt; 0 is true for each combination of the transverse modes of the N c configurations (j 1 , . . . , j Nc ). Notice, however, that, for activity driven networks with a power-law distribution of the node activity rates and small m, as it typically occurs, each snapshot will very likely include at least one node not connected to any other unit of the network, such that each configuration will include at least one zero eigenvalue. In this case, the condition for synchronization would require that N c Λ max (0) &lt; 0 which is impossible to obtain for chaotic dynamics where Λ max (0) (the maximum Lyapunov exponent of the isolated dynamics) is positive. This yields the conclusion that activity driven networks with a power-law distribution of the node activity rates and small m are difficult to synchronize in the regime of slow switching. This analysis also informs about possible mechanisms to promote synchronization under this regime: they have to be directed towards reducing the likelihood of finding isolated nodes, for instance increasing m or changing the shape of the activity rate distribution. The numerical results discussed in Ref. [105] confirm the suitability of the fast switching stability criterion to predict 55 the region of synchronization stability as well as the analysis under the slow switching regime pointing out the difficulty to synchronize activity driven networks with power-law distribution of the activity rates and small m. The Authors of Ref. [105] also investigate the behavior for intermediate values of τ . Here, they find that increasing τ first the region of synchronization stability widens and then if becomes unbounded. We can conclude that in activity driven networks the same qualitative behavior observed in the examples of on-off blinking networks discussed above as well as in other temporal networks (see for instance Ref. [76]) arises, suggesting that this behavior does not depend on the specific features of how the time evolution shapes the link formation. The overall picture that emerges from these studies is that, in temporal networks, synchronization is generally promoted by fast network evolution, but more favorable conditions for synchronization can occur at switching rates slightly lower than the fast switching regime.</p>
        <p>As the analysis based on the MSF formalism demonstrates, for unweighted networks of limit cycle and chaotic oscillators the problem of synchronization stability is not trivial. In particular, for systems with type III MSF the coupling strength needs to be properly selected and we can encounter situations where it is not possible to achieve synchronization. In this case, one can resort to weighted networks where links do not anymore have the same strength, but independent coupling coefficients. How can the values of such coefficients be found? Is it possible to use the self-organizing properties of temporal networks to let the network spontaneously evolve towards a set of suitable values? Adaptive networks provide an affirmative answer to these questions by furnishing a paradigm where the link weights change in time as a function of the network dynamical state. In addition, as we have already seen while illustrating the case of Kuramoto oscillators, in adaptive networks the structure of interconnections may be evolved as well, such that not only the values of the links, but also the presence or not of an interaction between two nodes is the result of the coevolution of network and dynamics.As the analysis based on the MSF formalism demonstrates, for unweighted networks of limit cycle and chaotic oscillators the problem of synchronization stability is not trivial. In particular, for systems with type III MSF the coupling strength needs to be properly selected and we can encounter situations where it is not possible to achieve synchronization. In this case, one can resort to weighted networks where links do not anymore have the same strength, but independent coupling coefficients. How can the values of such coefficients be found? Is it possible to use the self-organizing properties of temporal networks to let the network spontaneously evolve towards a set of suitable values? Adaptive networks provide an affirmative answer to these questions by furnishing a paradigm where the link weights change in time as a function of the network dynamical state. In addition, as we have already seen while illustrating the case of Kuramoto oscillators, in adaptive networks the structure of interconnections may be evolved as well, such that not only the values of the links, but also the presence or not of an interaction between two nodes is the result of the coevolution of network and dynamics.</p>
        <p>There are several ways in which adaptation can be embedded in a network [134]. The adaptation strategy may be global (or centralized) if the same time-varying coupling ϵ = ϵ(t) is used for all network links, or local (or decentralized), when an adaptive coupling coefficient is associated to each node, i.e., w i (t), or link, w ij (t). In addition, the network backbone can be kept constant, such that adaptation involves only a change of the weight of the interactions, or may be evolved, such that the adaptation mechanism is also able to cancel existing edges or form new links in the structure. When a global adaptation strategy is used, the coupling strength is the same for all network nodes and is adapted taking into account the global status of the network, i.e., an information regarding all the oscillators of the system. An example of this strategy is provided in Ref. [185] where the following model is considered:There are several ways in which adaptation can be embedded in a network [134]. The adaptation strategy may be global (or centralized) if the same time-varying coupling ϵ = ϵ(t) is used for all network links, or local (or decentralized), when an adaptive coupling coefficient is associated to each node, i.e., w i (t), or link, w ij (t). In addition, the network backbone can be kept constant, such that adaptation involves only a change of the weight of the interactions, or may be evolved, such that the adaptation mechanism is also able to cancel existing edges or form new links in the structure. When a global adaptation strategy is used, the coupling strength is the same for all network nodes and is adapted taking into account the global status of the network, i.e., an information regarding all the oscillators of the system. An example of this strategy is provided in Ref. [185] where the following model is considered:</p>
        <p>where the right hand side is function of the status of all network nodes. This strategy is able to drive the network towards synchronization. In correspondence of it, the coupling gain ϵ(t) reaches a stationary value. As further demonstrated in Ref. [186], where, however, other update laws are used, the technique may be applied to diverse systems, such as networks of linearly or nonlinearly coupled oscillators, networks where the structure of interactions is not known, and networks where the backbone changes in time.where the right hand side is function of the status of all network nodes. This strategy is able to drive the network towards synchronization. In correspondence of it, the coupling gain ϵ(t) reaches a stationary value. As further demonstrated in Ref. [186], where, however, other update laws are used, the technique may be applied to diverse systems, such as networks of linearly or nonlinearly coupled oscillators, networks where the structure of interactions is not known, and networks where the backbone changes in time.</p>
        <p>Notice that the adaptive mechanism of Eq. ( 42), as well as many others discussed in this section, is based on increasing the coupling strength proportionally to a measure of the synchronization error. However, this adaptive mechanism fails when the units have a type III MSF and the initial weight is beyond the second threshold for synchronization. Similar cases require the use of a mechanism able to either increase or decrease the coupling strength. A suitable solution is the adaptive strategy introduced in Ref. [187] and successfully applied to synchronize a master-slave configuration of two Lorenz systems.Notice that the adaptive mechanism of Eq. ( 42), as well as many others discussed in this section, is based on increasing the coupling strength proportionally to a measure of the synchronization error. However, this adaptive mechanism fails when the units have a type III MSF and the initial weight is beyond the second threshold for synchronization. Similar cases require the use of a mechanism able to either increase or decrease the coupling strength. A suitable solution is the adaptive strategy introduced in Ref. [187] and successfully applied to synchronize a master-slave configuration of two Lorenz systems.</p>
        <p>In local adaptive strategies, instead, each node makes use of information that can be gathered from its neighbors. To discuss these methods, let us begin with the so-called vertex-based approach where an adaptive coupling coefficient is associated to each node. In particular, let us consider a system of coupled oscillators described by the following equations:In local adaptive strategies, instead, each node makes use of information that can be gathered from its neighbors. To discuss these methods, let us begin with the so-called vertex-based approach where an adaptive coupling coefficient is associated to each node. In particular, let us consider a system of coupled oscillators described by the following equations:</p>
        <p>where w i (t) is the time-varying coupling coefficient of node i with i = 1, . . . , N. Several adaptive laws can be used for updating w i (t) to achieve synchronization. For instance, in Ref. [188] the law is given by: ẇi =where w i (t) is the time-varying coupling coefficient of node i with i = 1, . . . , N. Several adaptive laws can be used for updating w i (t) to achieve synchronization. For instance, in Ref. [188] the law is given by: ẇi =</p>
        <p>Another possibility is to select the update law as in Ref. [185]:Another possibility is to select the update law as in Ref. [185]:</p>
        <p>In both cases, the network structure is kept constant, i.e., the terms A ij are time-invariant. The two strategies also share the idea that the nodes negotiate the coupling strength with their neighbors by comparing their own output with the average output of their neighbors. Both strategies yield synchronization in many large networks where synchronization cannot be obtained if all the weights are equal [185,188]. This is a particularly relevant result, as these adaptive strategies not only demonstrate the possibility to achieve synchronization, but also provide a self-tuning approach to obtain the suitable values of the weights which, otherwise, would be difficult to obtain. When synchronization is reached, the weights stop to update as the right-hand term of Eqs. (45) or (46) becomes zero. In correspondence of synchronization, therefore, the weights reach stationary values. Note, however, that, when the oscillators are non-identical, complete synchronization cannot be achieved, and the weights will increase indefinitely. For this reason, under these circumstances, the update laws have to be modified to incorporate some mechanism of weight saturation.In both cases, the network structure is kept constant, i.e., the terms A ij are time-invariant. The two strategies also share the idea that the nodes negotiate the coupling strength with their neighbors by comparing their own output with the average output of their neighbors. Both strategies yield synchronization in many large networks where synchronization cannot be obtained if all the weights are equal [185,188]. This is a particularly relevant result, as these adaptive strategies not only demonstrate the possibility to achieve synchronization, but also provide a self-tuning approach to obtain the suitable values of the weights which, otherwise, would be difficult to obtain. When synchronization is reached, the weights stop to update as the right-hand term of Eqs. (45) or (46) becomes zero. In correspondence of synchronization, therefore, the weights reach stationary values. Note, however, that, when the oscillators are non-identical, complete synchronization cannot be achieved, and the weights will increase indefinitely. For this reason, under these circumstances, the update laws have to be modified to incorporate some mechanism of weight saturation.</p>
        <p>A related problem is that of using an adaptive strategy to track synchronization in a time-varying network. In particular, the Authors of Ref. [66] consider a slowly changing network and assume that each node only knows a single coupling signal received from the other nodes of the network, i.e., s i (t) = ∑ j A ij h(x j (t)). The Authors show that, under these conditions, synchronization can be achieved and maintained by properly setting the coupling gains (one for each node) with an adaptive law. Crucially, this requires coupling coefficients that also evolve in time to track the characteristics of the network that changes in time.A related problem is that of using an adaptive strategy to track synchronization in a time-varying network. In particular, the Authors of Ref. [66] consider a slowly changing network and assume that each node only knows a single coupling signal received from the other nodes of the network, i.e., s i (t) = ∑ j A ij h(x j (t)). The Authors show that, under these conditions, synchronization can be achieved and maintained by properly setting the coupling gains (one for each node) with an adaptive law. Crucially, this requires coupling coefficients that also evolve in time to track the characteristics of the network that changes in time.</p>
        <p>At variance with the vertex-based approach, in the edge-based approach the negotiation occurs between each pair of nodes. To implement this approach, the following model can be used:At variance with the vertex-based approach, in the edge-based approach the negotiation occurs between each pair of nodes. To implement this approach, the following model can be used:</p>
        <p>where i and j are such that (i, j) ∈ E. In this way, the backbone of the network is kept constant in time, but its weights evolve in time.where i and j are such that (i, j) ∈ E. In this way, the backbone of the network is kept constant in time, but its weights evolve in time.</p>
        <p>For the special case where h(x i ) = x i , that is, when coupling occurs through all the state variables, the global asymptotic stability of both vertex-based and edge-based adaptive strategies can be proved via Lyapunov-based techniques [189]. To illustrate this result, the notion of QUAD functions is required [186]. A function f :For the special case where h(x i ) = x i , that is, when coupling occurs through all the state variables, the global asymptotic stability of both vertex-based and edge-based adaptive strategies can be proved via Lyapunov-based techniques [189]. To illustrate this result, the notion of QUAD functions is required [186]. A function f :</p>
        <p>with ∆ a diagonal matrix and ω a positive constant. Following Ref. [189], we also need to introduce several matrices. Let L be the Laplacian matrix describing the network backbone (undirected and unweighted) and let v 1 be the normalized left eigenvector of L. Define V as the diagonal matrix that contains the elements of v 1 and let U = Vv 1 v. Finally, let L w a matrix with coefficientswith ∆ a diagonal matrix and ω a positive constant. Following Ref. [189], we also need to introduce several matrices. Let L be the Laplacian matrix describing the network backbone (undirected and unweighted) and let v 1 be the normalized left eigenvector of L. Define V as the diagonal matrix that contains the elements of v 1 and let U = Vv 1 v. Finally, let L w a matrix with coefficients</p>
        <p>, and L w ij = 0 otherwise. At this point, we can summarize the sufficient conditions for the vertex-based and edge-based strategies to achieve synchronization (Theorem 1 of Ref. [189]). If f is QUAD and the matrix (, and L w ij = 0 otherwise. At this point, we can summarize the sufficient conditions for the vertex-based and edge-based strategies to achieve synchronization (Theorem 1 of Ref. [189]). If f is QUAD and the matrix (</p>
        <p>semi-definite ∀t ≥ 0, then the vertex-based adaptive strategy of Eq. ( 46) guarantees synchronization. If f is QUAD andsemi-definite ∀t ≥ 0, then the vertex-based adaptive strategy of Eq. ( 46) guarantees synchronization. If f is QUAD and</p>
        <p>) is negative semi-definite ∀t ≥ 0, then the edge-based adaptive strategy of Eq. ( 48) guarantees synchronization. These conditions take a particularly convenient form when ∆ = 0 (Corollary 1 of) is negative semi-definite ∀t ≥ 0, then the edge-based adaptive strategy of Eq. ( 48) guarantees synchronization. These conditions take a particularly convenient form when ∆ = 0 (Corollary 1 of</p>
        <p>Ref. [189]), in this case it suffices to check that f is QUAD with ∆ = 0 and that the network is connected, to conclude that the two strategies guarantee synchronization.Ref. [189]), in this case it suffices to check that f is QUAD with ∆ = 0 and that the network is connected, to conclude that the two strategies guarantee synchronization.</p>
        <p>As a numerical example we refer to a network of Chua's circuits as in Ref. [189]. The Chua's circuit is QUAD with ∆ = 0; in addition, the network considered is connected (in particular, its topology is scale-free), such that the conditions mentioned above (Corollary 1 of Ref. [189]) hold and the two strategies yield stable synchronization (Fig. 19, top). As expected, the weights of the network links converge to stationary values as shown in Fig. 19, bottom, where, in particular, the fast dynamics of the link evolution compared to that of the chaotic oscillators can be appreciated. It suffices that f is QUAD to guarantee that, for any number of pinned nodes, the network reaches synchronization 35 with all nodes variables asymptotically converging to the desired reference trajectory and the coupling weights and the control gains to finite steady-state values. Extensions derived from this technique are the non-identical reference hybrid pinning strategy [197], the network chaos control hybrid pinning strategy [197], and the node-to-node fully adaptive decentralized pinning strategy [198].As a numerical example we refer to a network of Chua's circuits as in Ref. [189]. The Chua's circuit is QUAD with ∆ = 0; in addition, the network considered is connected (in particular, its topology is scale-free), such that the conditions mentioned above (Corollary 1 of Ref. [189]) hold and the two strategies yield stable synchronization (Fig. 19, top). As expected, the weights of the network links converge to stationary values as shown in Fig. 19, bottom, where, in particular, the fast dynamics of the link evolution compared to that of the chaotic oscillators can be appreciated. It suffices that f is QUAD to guarantee that, for any number of pinned nodes, the network reaches synchronization 35 with all nodes variables asymptotically converging to the desired reference trajectory and the coupling weights and the control gains to finite steady-state values. Extensions derived from this technique are the non-identical reference hybrid pinning strategy [197], the network chaos control hybrid pinning strategy [197], and the node-to-node fully adaptive decentralized pinning strategy [198].</p>
        <p>Let us now consider adaptation techniques where the network structure itself is evolved and, more specifically, the edge snapping adaptation strategy [200]. We refer to a network of oscillators coupled as follows:Let us now consider adaptation techniques where the network structure itself is evolved and, more specifically, the edge snapping adaptation strategy [200]. We refer to a network of oscillators coupled as follows:</p>
        <p>which correspond to Eqs. (55) with the inclusion of a further global (and constant in time) gain ϵ. In addition, let us consider weights that, rather than obeying to an update law as in Eq. ( 50), undergo a second order dynamicswhich correspond to Eqs. (55) with the inclusion of a further global (and constant in time) gain ϵ. In addition, let us consider weights that, rather than obeying to an update law as in Eq. ( 50), undergo a second order dynamics</p>
        <p>where γ is the damping coefficient, V (w ij ) a potential function, and g(e ij ) a function of the error e ij = x j -x i . Suppose to select the potential function V such that it has two local minima, one at w ij = 1 and one at w ij = 0. Correspondingly, the link (i, j) will be considered as active, when w ij = 1, or non-active when w ij = 0. Now, let start the network from an initial condition w ij (0) = 0, i.e., where all link weights are zero. Then, due to the presence in Eq. ( 56) of the forcing term g(e ij ), some of the variables w ij (t) can eventually leave the initial equilibrium point and reach the other equilibrium (atwhere γ is the damping coefficient, V (w ij ) a potential function, and g(e ij ) a function of the error e ij = x j -x i . Suppose to select the potential function V such that it has two local minima, one at w ij = 1 and one at w ij = 0. Correspondingly, the link (i, j) will be considered as active, when w ij = 1, or non-active when w ij = 0. Now, let start the network from an initial condition w ij (0) = 0, i.e., where all link weights are zero. Then, due to the presence in Eq. ( 56) of the forcing term g(e ij ), some of the variables w ij (t) can eventually leave the initial equilibrium point and reach the other equilibrium (at</p>
        <p>. In this way, the network will evolve towards a state where some of the links are adaptively activated to support synchronization. As an example [200], let us consider a system of N = 100 coupled Lorenz systems, with g(e ij ) = ∥x j -x i ∥ 2 ,. In this way, the network will evolve towards a state where some of the links are adaptively activated to support synchronization. As an example [200], let us consider a system of N = 100 coupled Lorenz systems, with g(e ij ) = ∥x j -x i ∥ 2 ,</p>
        <p>The time evolution of one representative state variable for each oscillator and the weights w ij (t) are illustrated in Fig. 9 of Ref. [200]. There the Authors notice that the network reaches synchronization in short time and, correspondingly, the weights asymptotically approach one of the two equilibrium points of the potential. In this way, the network is embedded with an adaptive mechanism that makes it able to self-determine its structure in order to support synchronization. The characteristics of the emerging topology depend on the dynamical evolution of the complex system and, thus, ultimately, by its initial conditions. Quite interestingly, the Authors of Ref. [200] observe strong correlation between the initial conditions of the nodes and their degree in the final structure and between the initial conditions of the network and the maximum eigenvector of the topology obtained.The time evolution of one representative state variable for each oscillator and the weights w ij (t) are illustrated in Fig. 9 of Ref. [200]. There the Authors notice that the network reaches synchronization in short time and, correspondingly, the weights asymptotically approach one of the two equilibrium points of the potential. In this way, the network is embedded with an adaptive mechanism that makes it able to self-determine its structure in order to support synchronization. The characteristics of the emerging topology depend on the dynamical evolution of the complex system and, thus, ultimately, by its initial conditions. Quite interestingly, the Authors of Ref. [200] observe strong correlation between the initial conditions of the nodes and their degree in the final structure and between the initial conditions of the network and the maximum eigenvector of the topology obtained.</p>
        <p>Adaptive mechanisms based on edge-snapping may be also applied to the problem of pinning control, in particular to select which nodes to pin [201]. In this case, the values of δ i are not decided a priori, but are the result of the coevolution of the dynamics and the structure (that now includes the links from the reference system to the nodes that can be potentially pinned). To this purpose, the variables δ i can be set as δ i = b i with b i obeying second-order dynamics similar to Eq. ( 56). Finally, we mention that the weights of a network can be adapted not only to reach synchronization, but also to optimize other quantities such as: (i) the network synchronizability as measured either by the smallest non-zero eigenvalue of the Laplacian matrix or the ratio between the largest and the smallest non-zero eigenvalue [202]; (ii) the robustness of consensus [203]; (iii) the selection of the control gains used in pinning control laws [204]. Remarkably, these objectives can be reached with a fully decentralized approach by designing a multi-layer structure where the layer devoted to weight optimization works in parallel with the layers estimating the actual value of the synchronizability measure.Adaptive mechanisms based on edge-snapping may be also applied to the problem of pinning control, in particular to select which nodes to pin [201]. In this case, the values of δ i are not decided a priori, but are the result of the coevolution of the dynamics and the structure (that now includes the links from the reference system to the nodes that can be potentially pinned). To this purpose, the variables δ i can be set as δ i = b i with b i obeying second-order dynamics similar to Eq. ( 56). Finally, we mention that the weights of a network can be adapted not only to reach synchronization, but also to optimize other quantities such as: (i) the network synchronizability as measured either by the smallest non-zero eigenvalue of the Laplacian matrix or the ratio between the largest and the smallest non-zero eigenvalue [202]; (ii) the robustness of consensus [203]; (iii) the selection of the control gains used in pinning control laws [204]. Remarkably, these objectives can be reached with a fully decentralized approach by designing a multi-layer structure where the layer devoted to weight optimization works in parallel with the layers estimating the actual value of the synchronizability measure.</p>
        <p>To conclude this section on adaptive networks, we remark that most of the techniques illustrated also apply to consensus problems. Furthermore, with the use of memristors, electrical schemes realizing the key mechanisms underlying these techniques can be conceived [205,206]; the memristor technology is still at the early stage of its development, such that there is a considerable gap to be filled between the theoretical schemes designed and their practical counterparts, but the use of this technology is a promising approach to implement adaptive mechanisms in efficient ways requiring a small number of components.To conclude this section on adaptive networks, we remark that most of the techniques illustrated also apply to consensus problems. Furthermore, with the use of memristors, electrical schemes realizing the key mechanisms underlying these techniques can be conceived [205,206]; the memristor technology is still at the early stage of its development, such that there is a considerable gap to be filled between the theoretical schemes designed and their practical counterparts, but the use of this technology is a promising approach to implement adaptive mechanisms in efficient ways requiring a small number of components.</p>
        <p>Up to now, we have discussed the time-varying effect on single layer networks. However, a large class of real-world and engineered systems are represented as networks whose architecture is multilayer [207,208], i.e., it is made of two or more interaction layers. In these systems, one has to distinguish between intra-layer and inter-layer interactions: all nodes within a layer interact among them via intra-layer links, whereas the interaction between nodes that belong to different layers take the name of inter-layer interactions (Fig. 11). There are many examples of such multilayer networks, like 51 social networks [209], mobility transport networks [210,211], air transportation networks [212], subway networks [213], and neural networks [214]. Furthermore, the multilayered network structure greatly affects collective phenomena in such systems [215,216], such as the processes of epidemic spreading [217][218][219], diffusion [220], controllability [221], percolation [175,222,223], and evolutionary game dynamics [224]. On the other hand, due to the interplay between the local dynamics of the nodes and the network topology, different types of global states emerge, like intra-layer and inter-layer synchronization [225][226][227], explosive synchronization [228], cluster synchronization [229] and chimera states [230,231]. We here show an example of a multiplex network [232] where the intra-layer network architecture experiences processes of stochastic rewiring with a characteristic switching frequency, while the connections between the layers are static. In such evolving network, two types of synchronization patterns has been investigated, intra-layer synchronization and inter-layer synchronization. The effect of a time-varying layer in a chain of static layers gives an enhancing effect on inter-layer synchronization [233].Up to now, we have discussed the time-varying effect on single layer networks. However, a large class of real-world and engineered systems are represented as networks whose architecture is multilayer [207,208], i.e., it is made of two or more interaction layers. In these systems, one has to distinguish between intra-layer and inter-layer interactions: all nodes within a layer interact among them via intra-layer links, whereas the interaction between nodes that belong to different layers take the name of inter-layer interactions (Fig. 11). There are many examples of such multilayer networks, like 51 social networks [209], mobility transport networks [210,211], air transportation networks [212], subway networks [213], and neural networks [214]. Furthermore, the multilayered network structure greatly affects collective phenomena in such systems [215,216], such as the processes of epidemic spreading [217][218][219], diffusion [220], controllability [221], percolation [175,222,223], and evolutionary game dynamics [224]. On the other hand, due to the interplay between the local dynamics of the nodes and the network topology, different types of global states emerge, like intra-layer and inter-layer synchronization [225][226][227], explosive synchronization [228], cluster synchronization [229] and chimera states [230,231]. We here show an example of a multiplex network [232] where the intra-layer network architecture experiences processes of stochastic rewiring with a characteristic switching frequency, while the connections between the layers are static. In such evolving network, two types of synchronization patterns has been investigated, intra-layer synchronization and inter-layer synchronization. The effect of a time-varying layer in a chain of static layers gives an enhancing effect on inter-layer synchronization [233].</p>
        <p>Then we will consider a temporal single layer network where more than one intra-layer connections exist between the nodes, and necessary and sufficient conditions are discussed for such a time-varying hypernetwork [135]. Finally, we review a series of works on the invariance and stability conditions for synchronization in time-varying multiplex hypernetworks [136,234].Then we will consider a temporal single layer network where more than one intra-layer connections exist between the nodes, and necessary and sufficient conditions are discussed for such a time-varying hypernetwork [135]. Finally, we review a series of works on the invariance and stability conditions for synchronization in time-varying multiplex hypernetworks [136,234].</p>
        <p>We begin with a bilayer multiplex network, where each layer contains N nodes and the local dynamics in each node is a d-dimensional identical oscillator. The evolution of the ith node is described asWe begin with a bilayer multiplex network, where each layer contains N nodes and the local dynamics in each node is a d-dimensional identical oscillator. The evolution of the ith node is described as</p>
        <p>where x i,1 and x i,2 are the ith node's state variables in the two layers. Here, F : R d → R d represents the vector field of the individual oscillators, G : R d → R d and H : R d → R d stand respectively for the output vectorial functions within the layers and between the layers. The parameters ϵ and λ are the intra-layer and inter-layer coupling strengths, respectively.where x i,1 and x i,2 are the ith node's state variables in the two layers. Here, F : R d → R d represents the vector field of the individual oscillators, G : R d → R d and H : R d → R d stand respectively for the output vectorial functions within the layers and between the layers. The parameters ϵ and λ are the intra-layer and inter-layer coupling strengths, respectively.</p>
        <p>Here, the architectures of the two layers are encoded by the Laplacian matrices L 1 (t) and L 2 (t). The intra-layer network topologies in both layers are small-world temporal networks, constructed by the method introduced by Watts and Strogatz in their seminal paper Ref. [235]. These small-world networks evolve over time by rewiring every edges stochastically and independently with a characteristics frequency f . However, all the inter-layer links are fixed in time. In particular, each layer is rewired with probability p r = fdt by constructing a new small-world network, where dt is a given time step. Due to the fixed choice of parameters p WS (small world probability) and k (average degree of the nodes), these successively rewired small-world networks will be statistically equivalent throughout the procedure. Here, p r ∼ 1 implies that the two layers evolve rapidly due to a fast switching of links. However, when p r ∼ 0, the network edges have very small probability of change, hence the intra-layer networks are almost static. In these networks it is interesting to study intra-layer synchronization, which occurs when all the units in each layer are synchronized each other, but not necessarily with their counterparts in other layers, i.e., x 1,1 = • • • = x N,1 , as well as inter-layer synchronization, which occurs when all the nodes are synchronized with their counterparts in the other layers, but not necessarily with the other nodes in the same layers, i.e.,Here, the architectures of the two layers are encoded by the Laplacian matrices L 1 (t) and L 2 (t). The intra-layer network topologies in both layers are small-world temporal networks, constructed by the method introduced by Watts and Strogatz in their seminal paper Ref. [235]. These small-world networks evolve over time by rewiring every edges stochastically and independently with a characteristics frequency f . However, all the inter-layer links are fixed in time. In particular, each layer is rewired with probability p r = fdt by constructing a new small-world network, where dt is a given time step. Due to the fixed choice of parameters p WS (small world probability) and k (average degree of the nodes), these successively rewired small-world networks will be statistically equivalent throughout the procedure. Here, p r ∼ 1 implies that the two layers evolve rapidly due to a fast switching of links. However, when p r ∼ 0, the network edges have very small probability of change, hence the intra-layer networks are almost static. In these networks it is interesting to study intra-layer synchronization, which occurs when all the units in each layer are synchronized each other, but not necessarily with their counterparts in other layers, i.e., x 1,1 = • • • = x N,1 , as well as inter-layer synchronization, which occurs when all the nodes are synchronized with their counterparts in the other layers, but not necessarily with the other nodes in the same layers, i.e.,</p>
        <p>, where M is the number of layers (in, where M is the number of layers (in</p>
        <p>To numerically explore the emergent behavior of system (58), the nodal dynamics is taken to be the three-dimensionalTo numerically explore the emergent behavior of system (58), the nodal dynamics is taken to be the three-dimensional</p>
        <p>Rössler oscillators ẋ = -y -z, ẏ = x + 0.1y, ż = 0.1 + z(x -14). A diffusive coupling function is considered for both intra-layer and inter-layer couplings through the variable y, i.e., G(x) = [0 y 0] T and H(x) = [0 y 0] T . To explore the effect of the intra-layer rewiring frequency f on the synchronization states, one defines the errors asRössler oscillators ẋ = -y -z, ẏ = x + 0.1y, ż = 0.1 + z(x -14). A diffusive coupling function is considered for both intra-layer and inter-layer couplings through the variable y, i.e., G(x) = [0 y 0] T and H(x) = [0 y 0] T . To explore the effect of the intra-layer rewiring frequency f on the synchronization states, one defines the errors as</p>
        <p>andand</p>
        <p>where T t is a sufficiently large positive constant. The final state is considered as a stable synchronization state if the corresponding synchronization error is bounded by 10 -4 , otherwise the multiplex network Eq. ( 58) is said to be in an asynchronous state. Here, E inter first decreases as ϵ gradually increases, and becomes zero as it crosses a certain threshold value, indicating the emergence of the inter-layer synchronization state. Although, in this case, the critical values of ϵ only mildly depend on the rewiring frequency f . Further, increasing the intra-layer coupling strength beyond certain values, the multiplex network loses both types of synchronization patterns. Such a synchronization to desynchronization critical transition point of ϵ depends on f . Larger values of the switching frequency f yield higher persistence of intra-layer and inter-layer synchronization states. Notice that both synchronization features occur for ϵ ≥ 0.5 irrespectively of the values of f , hence the whole network exhibits a global synchronization state.where T t is a sufficiently large positive constant. The final state is considered as a stable synchronization state if the corresponding synchronization error is bounded by 10 -4 , otherwise the multiplex network Eq. ( 58) is said to be in an asynchronous state. Here, E inter first decreases as ϵ gradually increases, and becomes zero as it crosses a certain threshold value, indicating the emergence of the inter-layer synchronization state. Although, in this case, the critical values of ϵ only mildly depend on the rewiring frequency f . Further, increasing the intra-layer coupling strength beyond certain values, the multiplex network loses both types of synchronization patterns. Such a synchronization to desynchronization critical transition point of ϵ depends on f . Larger values of the switching frequency f yield higher persistence of intra-layer and inter-layer synchronization states. Notice that both synchronization features occur for ϵ ≥ 0.5 irrespectively of the values of f , hence the whole network exhibits a global synchronization state.</p>
        <p>Next, we discuss the conditions for the appearance of intra-layer and inter-layer synchronization states. For intra-layer synchrony, let the layer-1 and layer-2 evolve synchronously with the respective synchronization manifolds x 0,1 and x 0,2 .Next, we discuss the conditions for the appearance of intra-layer and inter-layer synchronization states. For intra-layer synchrony, let the layer-1 and layer-2 evolve synchronously with the respective synchronization manifolds x 0,1 and x 0,2 .</p>
        <p>Let δx i,1 and δx i,2 be the perturbations from such manifolds, i.e., x i,1 = x 0,1 + δx i,1 and x i,2 = x 0,2 + δx i,2 . Then the linearized equations can be written asLet δx i,1 and δx i,2 be the perturbations from such manifolds, i.e., x i,1 = x 0,1 + δx i,1 and x i,2 = x 0,2 + δx i,2 . Then the linearized equations can be written as</p>
        <p>where J represents the Jacobian operator and the intra-layer synchronization solution (x 0,1 , x 0,2 ) satisfies the following evolution equation,where J represents the Jacobian operator and the intra-layer synchronization solution (x 0,1 , x 0,2 ) satisfies the following evolution equation,</p>
        <p>If the largest Lyapunov exponent Λ intra max of the coupled linearized systems (61) is negative, then the intra-layer synchronization state is stable. In the linearized error system (61), the dependency of the switching link frequency f is incorporated through the intra-layer Laplacian matrices L 1 (t) and L 2 (t). In Ref. [232] the Authors report that the variation of Λ intra max by changing the coupling strength ϵ and rewiring frequency 22 f becomes negative where E intra turns out to be zero (see Fig. 1(a) and 2(a) in Ref. [232]). Further increasing the values of ϵ, Λ intra max assumes positive values, as a result of a short-wavelength bifurcation [181]. These transition points accurately match where Λ intra max becomes again non-zero. For larger values of f , Λ intra max becomes negative faster, and persists to be negative at higher values of ϵ.If the largest Lyapunov exponent Λ intra max of the coupled linearized systems (61) is negative, then the intra-layer synchronization state is stable. In the linearized error system (61), the dependency of the switching link frequency f is incorporated through the intra-layer Laplacian matrices L 1 (t) and L 2 (t). In Ref. [232] the Authors report that the variation of Λ intra max by changing the coupling strength ϵ and rewiring frequency 22 f becomes negative where E intra turns out to be zero (see Fig. 1(a) and 2(a) in Ref. [232]). Further increasing the values of ϵ, Λ intra max assumes positive values, as a result of a short-wavelength bifurcation [181]. These transition points accurately match where Λ intra max becomes again non-zero. For larger values of f , Λ intra max becomes negative faster, and persists to be negative at higher values of ϵ.</p>
        <p>As for inter-layer synchronization, let us consider a small difference δz i = x i,2 -x i,1 between the state of the ith replica node and that corresponding to complete synchronization. Then, around the inter-layer synchronization solution, the linearized equation isAs for inter-layer synchronization, let us consider a small difference δz i = x i,2 -x i,1 between the state of the ith replica node and that corresponding to complete synchronization. Then, around the inter-layer synchronization solution, the linearized equation is</p>
        <p>where ∆L(t) is defined as ∆L(t) = L 1 (t) -L 2 (t). Here, xi denotes the state variable of the ith node at the inter-layer synchronization solution, and its dynamics can be written as,where ∆L(t) is defined as ∆L(t) = L 1 (t) -L 2 (t). Here, xi denotes the state variable of the ith node at the inter-layer synchronization solution, and its dynamics can be written as,</p>
        <p>The numerical results show that the quantity Λ inter max experiences, as a function of ϵ, a first transition from the asynchronous to the synchronous state which is independent on the rewiring frequency f . This scenario changes for higher values of ϵ, where inter-layer synchronization becomes unstable, and Λ inter max becomes again positive, with a transition point which now strongly depend on f [232].The numerical results show that the quantity Λ inter max experiences, as a function of ϵ, a first transition from the asynchronous to the synchronous state which is independent on the rewiring frequency f . This scenario changes for higher values of ϵ, where inter-layer synchronization becomes unstable, and Λ inter max becomes again positive, with a transition point which now strongly depend on f [232].</p>
        <p>In this subsection, we will discuss the effects of the rewiring frequency f on the global stability of intra-layer and inter-layer synchronization states. The global stability of these states is assessed by means of the basin stability (BS) measurement (as introduced in Section 3.1.3). To calculate BS numerically, the entire system is integrated for 1,000 distinct initial conditions chosen from the three-dimensional space [-20, 20] × [-20, 20] × [0, 35] in which the isolated attractor resides. The bottom panel of Fig. 20(c) and 20(d) depicts the variation of basin stability for intra-layer (BS intra ) and inter-layer (BS inter ) synchronization states in the (f , ϵ) parameter plane, respectively. As ϵ increases, the values of BS intra gradually increase from zero, and approach the maximum value one. As the values of f increases, BS intra varies more sharply. Irrespective of the intra-layer rewiring frequencies and up to a certain value of ϵ, unit values of BS intra persist. Moreover a transition toward the intra-layer desynchronization is observed, no matter how promptly the intra-layer networks vary. Here, BS intra progressively goes to zero, as ϵ systematically increases. Different BS intra values for distinct values of f demonstrate the influence of temporal intra-layer network on this synchronization pattern. As the intra-layer coupling strength increases, the value of BS inter grows abruptly, and do not significantly rely on the intra-layer rewiring frequencies. This result is in agreement with the aptitude of the variation of E inter already observed in Fig. 20(b). Similar to the variation of BS intra , unit value of BS inter persists up to a certain value of ϵ. Then, the inter-layer synchronization state gradually disappears, with a tendency that significantly depends on f . At the critical transition point of ϵ for which synchronization patterns emerge, the corresponding basin stability values stay in the open interval (0, 1). This implies that a fraction of the initial conditions supports the respective synchronization patterns.In this subsection, we will discuss the effects of the rewiring frequency f on the global stability of intra-layer and inter-layer synchronization states. The global stability of these states is assessed by means of the basin stability (BS) measurement (as introduced in Section 3.1.3). To calculate BS numerically, the entire system is integrated for 1,000 distinct initial conditions chosen from the three-dimensional space [-20, 20] × [-20, 20] × [0, 35] in which the isolated attractor resides. The bottom panel of Fig. 20(c) and 20(d) depicts the variation of basin stability for intra-layer (BS intra ) and inter-layer (BS inter ) synchronization states in the (f , ϵ) parameter plane, respectively. As ϵ increases, the values of BS intra gradually increase from zero, and approach the maximum value one. As the values of f increases, BS intra varies more sharply. Irrespective of the intra-layer rewiring frequencies and up to a certain value of ϵ, unit values of BS intra persist. Moreover a transition toward the intra-layer desynchronization is observed, no matter how promptly the intra-layer networks vary. Here, BS intra progressively goes to zero, as ϵ systematically increases. Different BS intra values for distinct values of f demonstrate the influence of temporal intra-layer network on this synchronization pattern. As the intra-layer coupling strength increases, the value of BS inter grows abruptly, and do not significantly rely on the intra-layer rewiring frequencies. This result is in agreement with the aptitude of the variation of E inter already observed in Fig. 20(b). Similar to the variation of BS intra , unit value of BS inter persists up to a certain value of ϵ. Then, the inter-layer synchronization state gradually disappears, with a tendency that significantly depends on f . At the critical transition point of ϵ for which synchronization patterns emerge, the corresponding basin stability values stay in the open interval (0, 1). This implies that a fraction of the initial conditions supports the respective synchronization patterns.</p>
        <p>In multiplex networks, one of the issues that needs to be addressed is the robustness of inter-layer synchronization against a progressive demultiplexing of the replica connections: from a given multiplex structure, one has to progressively take off inter-layer links until both layers become isolated, and monitor the resilience of inter-layer synchronization. The number of demultiplexed replicas is denoted by ν and varied between 0 and N = 200. Here, ν = 0 means a complete multiplex network, i.e., all the replicas are present. When ν = N, the two layers become completely decoupled. The influence of ϵ on the resilience of inter-layer synchronization is illustrated in Fig. 21. Here, E inter continuously changes in the (ν, ϵ) parameter plane, and inter-layer synchronization persists up to a certain value of ν. By increasing the values of ϵ, the number of demultiplexed replicas that preserve the inter-layer synchrony grows as well. When ϵ = 2, the inter-layer synchronization can be observed even when nearly 170 replica connections are removed. However, this tendency holds up to a given value of ϵ: increasing the intra-layer coupling strength above this threshold reduces the critical value of demultiplexed replicas.In multiplex networks, one of the issues that needs to be addressed is the robustness of inter-layer synchronization against a progressive demultiplexing of the replica connections: from a given multiplex structure, one has to progressively take off inter-layer links until both layers become isolated, and monitor the resilience of inter-layer synchronization. The number of demultiplexed replicas is denoted by ν and varied between 0 and N = 200. Here, ν = 0 means a complete multiplex network, i.e., all the replicas are present. When ν = N, the two layers become completely decoupled. The influence of ϵ on the resilience of inter-layer synchronization is illustrated in Fig. 21. Here, E inter continuously changes in the (ν, ϵ) parameter plane, and inter-layer synchronization persists up to a certain value of ν. By increasing the values of ϵ, the number of demultiplexed replicas that preserve the inter-layer synchrony grows as well. When ϵ = 2, the inter-layer synchronization can be observed even when nearly 170 replica connections are removed. However, this tendency holds up to a given value of ϵ: increasing the intra-layer coupling strength above this threshold reduces the critical value of demultiplexed replicas.</p>
        <p>The study has been later extended in Ref. [233], which explored intra-layer synchronization in presence of static as well as of multiple time-varying layers, and reveals how already a single time-varying layer is able to enhance intra-layer synchronization.The study has been later extended in Ref. [233], which explored intra-layer synchronization in presence of static as well as of multiple time-varying layers, and reveals how already a single time-varying layer is able to enhance intra-layer synchronization.</p>
        <p>Synchronization of coupled Hindmarsh-Rose neurons with hypernetwork architecture has been studied in Ref. [135], with two different kinds of neuronal communications, namely electric gap junctional and chemical synaptic interactions. The simultaneous occurrence of these two kinds of network architectures in a single network gives rise to a hypernetwork architecture. For inter-neuronal communication, both kinds of synaptic interactions coexist and also work independently over time in several neuronal network. Mimicking a realistic neurobiological scenario, every links of this hypernetwork are permitted to switch over time, and are varied with a switching frequency f . Various kinds of spatiotemporal dynamics are explored by tuning the comparative gap junctional strength ϵ, the synaptic interaction strength g c , the characteristics rewiring frequency f , and the small-world probability p SW , which accounts for the randomness of the network. For instance, if p SW = 0, the network is fully non-local, while p SW = 1 indicates it is completely a random network.Synchronization of coupled Hindmarsh-Rose neurons with hypernetwork architecture has been studied in Ref. [135], with two different kinds of neuronal communications, namely electric gap junctional and chemical synaptic interactions. The simultaneous occurrence of these two kinds of network architectures in a single network gives rise to a hypernetwork architecture. For inter-neuronal communication, both kinds of synaptic interactions coexist and also work independently over time in several neuronal network. Mimicking a realistic neurobiological scenario, every links of this hypernetwork are permitted to switch over time, and are varied with a switching frequency f . Various kinds of spatiotemporal dynamics are explored by tuning the comparative gap junctional strength ϵ, the synaptic interaction strength g c , the characteristics rewiring frequency f , and the small-world probability p SW , which accounts for the randomness of the network. For instance, if p SW = 0, the network is fully non-local, while p SW = 1 indicates it is completely a random network.</p>
        <p>Consider N = 200 Hindmarsh-Rose (HR) model neurons, coupled concurrently through linear electric gap junction and non-linear chemical synaptic interactions. Then, the evolution equation of such neuronal hypernetwork is described byConsider N = 200 Hindmarsh-Rose (HR) model neurons, coupled concurrently through linear electric gap junction and non-linear chemical synaptic interactions. Then, the evolution equation of such neuronal hypernetwork is described by</p>
        <p>where i = 1, 2, . . . , N is the oscillator index, the membrane potential of neuron i is represented by the state variable x i (t), y i (t) and corresponds to the ions transportation of the ith neuron across the membrane through the fast current associated with Na + and K + ions. The variable z i (t) is the transportation of ions for the ith neuron across the membrane potential through the slow current associated with Ca 2+ ions, and the speed of such current is controlled by the system parameter r. g c represents the chemical synaptic strength, while ϵ is the electrical gap junctional strength. These two synaptic strengths regulate how information will be dispensed among the neurons mediated by the interaction channels. The value of the system parameters are chosen as a = 1, b = 3, c = 1, d = 5, r = 0.005, s = 4, x 0 = -1.6, and I = 3.25. For this set of parameter values, the membrane potential of the individual HR system demonstrates multi-time scale chaotic dynamics, known as spiking bursting pattern. Moreover, the chemical coupling parameters are fixed at Θ s = -0.25, v s = 2 and λ = 10.where i = 1, 2, . . . , N is the oscillator index, the membrane potential of neuron i is represented by the state variable x i (t), y i (t) and corresponds to the ions transportation of the ith neuron across the membrane through the fast current associated with Na + and K + ions. The variable z i (t) is the transportation of ions for the ith neuron across the membrane potential through the slow current associated with Ca 2+ ions, and the speed of such current is controlled by the system parameter r. g c represents the chemical synaptic strength, while ϵ is the electrical gap junctional strength. These two synaptic strengths regulate how information will be dispensed among the neurons mediated by the interaction channels. The value of the system parameters are chosen as a = 1, b = 3, c = 1, d = 5, r = 0.005, s = 4, x 0 = -1.6, and I = 3.25. For this set of parameter values, the membrane potential of the individual HR system demonstrates multi-time scale chaotic dynamics, known as spiking bursting pattern. Moreover, the chemical coupling parameters are fixed at Θ s = -0.25, v s = 2 and λ = 10.</p>
        <p>In neuronal system, the electrical interaction is bidirectional in nature, however, the chemical coupling is unidirectional type [236]. The former type of coupling has been considered to display a small-world network architecture [237][238][239], formulated through the construction algorithm proposed by Watts-Strogatz [235]. The corresponding Laplacian is the square matrix L (e) of order N, with average degree ⟨k e ⟩ and edge rewiring probability p SW . However, the second type of interaction is represented by a unidirectional random network, encoded by the adjacency A (c) and the matrix L (c) denotes 28 the associated Laplacian. The in-degrees of all the vertices are considered here as identical and equal to k c . Complete neuronal synchrony indicates that all the neurons evolve in unison with identical trajectory in the phase-space. For such case, the associated synchronization error becomes zero, i.e., E = 0. Here, stability of the neuronal synchronization solution against any arbitrary perturbations is characterized through the basin stability measurement. The phase-space volume has been sampled from [-1.5, 2.0] × [-7.0, 1.0] × [2.9, 3.4], and if E &lt; 10 -5 , the entire coupled system is assumed to be synchronized.In neuronal system, the electrical interaction is bidirectional in nature, however, the chemical coupling is unidirectional type [236]. The former type of coupling has been considered to display a small-world network architecture [237][238][239], formulated through the construction algorithm proposed by Watts-Strogatz [235]. The corresponding Laplacian is the square matrix L (e) of order N, with average degree ⟨k e ⟩ and edge rewiring probability p SW . However, the second type of interaction is represented by a unidirectional random network, encoded by the adjacency A (c) and the matrix L (c) denotes 28 the associated Laplacian. The in-degrees of all the vertices are considered here as identical and equal to k c . Complete neuronal synchrony indicates that all the neurons evolve in unison with identical trajectory in the phase-space. For such case, the associated synchronization error becomes zero, i.e., E = 0. Here, stability of the neuronal synchronization solution against any arbitrary perturbations is characterized through the basin stability measurement. The phase-space volume has been sampled from [-1.5, 2.0] × [-7.0, 1.0] × [2.9, 3.4], and if E &lt; 10 -5 , the entire coupled system is assumed to be synchronized.</p>
        <p>The synchronization error and the corresponding basin stability the (ϵ, f ) parameter plane are reported in Fig. 4 of Ref. [135], respectively in the upper and lower panels. Color bar in the upper panel depicts the variation of E. On the other hand, the color bar in lower panel represents the variation of basin stability and hence characterizes global robustness of neuronal synchronization. The left, middle and right panels are respectively drawn for k e = 4, k e = 6 and k e = 8. Here, the chemical synaptic interaction strength systematically varies as g c = ϵ 2 , and the in-degree of the underlying network is fixed at k c = 5. The upper panel shows that the region of neuronal synchrony monotonically enlarges as the average degree of the SW network gradually increases. Likewise, a similar tendency occurs for the corresponding BS diagram in lower panel.The synchronization error and the corresponding basin stability the (ϵ, f ) parameter plane are reported in Fig. 4 of Ref. [135], respectively in the upper and lower panels. Color bar in the upper panel depicts the variation of E. On the other hand, the color bar in lower panel represents the variation of basin stability and hence characterizes global robustness of neuronal synchronization. The left, middle and right panels are respectively drawn for k e = 4, k e = 6 and k e = 8. Here, the chemical synaptic interaction strength systematically varies as g c = ϵ 2 , and the in-degree of the underlying network is fixed at k c = 5. The upper panel shows that the region of neuronal synchrony monotonically enlarges as the average degree of the SW network gradually increases. Likewise, a similar tendency occurs for the corresponding BS diagram in lower panel.</p>
        <p>To gather more information on neuronal synchrony, the synchronization time, i.e., the mean time to attain that solution is plotted in Fig. 8 of Ref. [135] for five exemplary values of switching frequency f . Here, left and right panels depict the variation of the synchronization time with respect to ϵ for k e = 6, and k e = 8 respectively. The synchronization time gradually reduces as the electrical interaction strength increases.To gather more information on neuronal synchrony, the synchronization time, i.e., the mean time to attain that solution is plotted in Fig. 8 of Ref. [135] for five exemplary values of switching frequency f . Here, left and right panels depict the variation of the synchronization time with respect to ϵ for k e = 6, and k e = 8 respectively. The synchronization time gradually reduces as the electrical interaction strength increases.</p>
        <p>Let L(e) and L(c) denote the time-average Laplacians for the electrical gap junctional network and chemical synaptic network, respectively. Additionally, {0, γ (e) 2 , γ (e) 3 , . . . , γ (e) N } and {0, γ (c) 2 , γ (c) 3 , . . . , γ (c) N } are the set of eigenvalues of the zero-row sum matrices L(e) and L(c) , respectively. For sufficiently rapid rewiring, the time-average weighted network warrants a synchronization transition to occur [55]. Then Master Stability Equation (MSE) transverse to the complete synchronization manifold reads as follows, ξiLet L(e) and L(c) denote the time-average Laplacians for the electrical gap junctional network and chemical synaptic network, respectively. Additionally, {0, γ (e) 2 , γ (e) 3 , . . . , γ (e) N } and {0, γ (c) 2 , γ (c) 3 , . . . , γ (c) N } are the set of eigenvalues of the zero-row sum matrices L(e) and L(c) , respectively. For sufficiently rapid rewiring, the time-average weighted network warrants a synchronization transition to occur [55]. Then Master Stability Equation (MSE) transverse to the complete synchronization manifold reads as follows, ξi</p>
        <p>where i = 2, 3, . . . , N, and (x(t), y(t), z(t)) is the state vector for the neuronal synchronization manifold, which dominates the following evolution equation,where i = 2, 3, . . . , N, and (x(t), y(t), z(t)) is the state vector for the neuronal synchronization manifold, which dominates the following evolution equation,</p>
        <p>The variation of Λ max of Eq. ( 66) is drawn in Fig. 11 of Ref. [135] by simultaneously varying the coupling strengths ϵ and g c , where the color bar depicts the variation of Λ max . Here, the largest transverse Lyapunov exponent turn out to be negative value exactly where E becomes zero for f ∈ [10,100], i.e., for enough rapid switching.The variation of Λ max of Eq. ( 66) is drawn in Fig. 11 of Ref. [135] by simultaneously varying the coupling strengths ϵ and g c , where the color bar depicts the variation of Λ max . Here, the largest transverse Lyapunov exponent turn out to be negative value exactly where E becomes zero for f ∈ [10,100], i.e., for enough rapid switching.</p>
        <p>According to the Wu-Chua conjecture [240], the critical coupling threshold for ϵ can be derived asAccording to the Wu-Chua conjecture [240], the critical coupling threshold for ϵ can be derived as</p>
        <p>For given values of chemical synaptic strength g c , ϵ * 2 (g c ) and ϵ * N (g c ) are respectively the electrical coupling threshold for network size 2 and N. For several chemical synaptic strength g c , the threshold value of electrical interaction strength ϵ * N (g c ) has been obtained. In that figure, the dashed black curve represents the analytically obtained critical coupling strength. In the (ϵ, g c ) parameter plane, the regions below and above the critical curve respectively correspond to the desynchronization and synchronization solution. From this figure, it is evident that the analytical curve obtained from the Wu-Chua conjecture agrees very well with the transition of maximum transverse Lyapunov exponent.For given values of chemical synaptic strength g c , ϵ * 2 (g c ) and ϵ * N (g c ) are respectively the electrical coupling threshold for network size 2 and N. For several chemical synaptic strength g c , the threshold value of electrical interaction strength ϵ * N (g c ) has been obtained. In that figure, the dashed black curve represents the analytically obtained critical coupling strength. In the (ϵ, g c ) parameter plane, the regions below and above the critical curve respectively correspond to the desynchronization and synchronization solution. From this figure, it is evident that the analytical curve obtained from the Wu-Chua conjecture agrees very well with the transition of maximum transverse Lyapunov exponent.</p>
        <p>Recently, studies on intra-layer synchronization have been extended to time-varying multiplex hypernetworks [136], and inter-layer synchronization has also been described in stochastic multiplex hypernetworks [234].Recently, studies on intra-layer synchronization have been extended to time-varying multiplex hypernetworks [136], and inter-layer synchronization has also been described in stochastic multiplex hypernetworks [234].</p>
        <p>Global stability analysis of synchronization in time-varying multiplex networks will be discussed in this section via basin stability measurement (discussed in Section 3.1.3) [146,148]. Recently, the Authors of Ref. [241] went through such a study for a multiplex time-varying network of mobile oscillators based on the idea of targeted dynamical attacks of a specific node in the network, termed as ''single-node basin stability (SNBS)". This SNBS scheme is mainly used to understand the probability of return to the synchronization manifold when a particular node experiences an arbitrary nonlocal perturbation. Notice that the problem of global stability and resilience of networked dynamical systems in response to small perturbations simultaneously affecting multiple nodes can be studied using the multiple-node basin 48 stability approach introduced in Ref. [242].Global stability analysis of synchronization in time-varying multiplex networks will be discussed in this section via basin stability measurement (discussed in Section 3.1.3) [146,148]. Recently, the Authors of Ref. [241] went through such a study for a multiplex time-varying network of mobile oscillators based on the idea of targeted dynamical attacks of a specific node in the network, termed as ''single-node basin stability (SNBS)". This SNBS scheme is mainly used to understand the probability of return to the synchronization manifold when a particular node experiences an arbitrary nonlocal perturbation. Notice that the problem of global stability and resilience of networked dynamical systems in response to small perturbations simultaneously affecting multiple nodes can be studied using the multiple-node basin 48 stability approach introduced in Ref. [242].</p>
        <p>Reference [241] considered a bilayer multiplex network with N mobile multi-agent systems, which are performing a 2D-lattice random walk in each layer. The local dynamics of each node i (i = 1, 2, . . . , N) is associated with a dynamical system Ẋi = F (X i ), where X i is a d-dimensional vector of dynamical variables and F (X i ) is a vector field characterizing the dynamical units. Mathematically the entire dynamical network is described asReference [241] considered a bilayer multiplex network with N mobile multi-agent systems, which are performing a 2D-lattice random walk in each layer. The local dynamics of each node i (i = 1, 2, . . . , N) is associated with a dynamical system Ẋi = F (X i ), where X i is a d-dimensional vector of dynamical variables and F (X i ) is a vector field characterizing the dynamical units. Mathematically the entire dynamical network is described as</p>
        <p>where k 1 and k 2 are the intra-layer coupling strengths among the mobile multi-agent in layer-1 and layer-2, respectively, and ϵ is the inter-layer coupling strength. The time-varying intra-layer connections are governed by the zero-row sum Laplacian matrices of order N aswhere k 1 and k 2 are the intra-layer coupling strengths among the mobile multi-agent in layer-1 and layer-2, respectively, and ϵ is the inter-layer coupling strength. The time-varying intra-layer connections are governed by the zero-row sum Laplacian matrices of order N as</p>
        <p>agent lies in the vision size of the ith agent and zero otherwise. Also,agent lies in the vision size of the ith agent and zero otherwise. Also,</p>
        <p>) and H : R d → R d are the intra-layer and inter-layer output vectorial functions, respectively.) and H : R d → R d are the intra-layer and inter-layer output vectorial functions, respectively.</p>
        <p>Paradigmatic chaotic Rössler oscillators [243] are then chosen to represent the dynamics of the nodes, with F (X k,i ) equal toParadigmatic chaotic Rössler oscillators [243] are then chosen to represent the dynamics of the nodes, with F (X k,i ) equal to</p>
        <p>))</p>
        <p>..</p>
        <p>((</p>
        <p>The intra-layer and inter-layer coupling functions E k (X k ) = (0, y k , 0) T and H(X k ) = (0, y k , 0) T with k = 1, 2 are taken to act at the y variable. Further, the movement algorithm of the multi-agent systems is on 2D-lattice of M 1 × M 2 mesh points.The intra-layer and inter-layer coupling functions E k (X k ) = (0, y k , 0) T and H(X k ) = (0, y k , 0) T with k = 1, 2 are taken to act at the y variable. Further, the movement algorithm of the multi-agent systems is on 2D-lattice of M 1 × M 2 mesh points.</p>
        <p>During movement, each agent creates a much smaller square shape region of φ 2 area (calling them vision size) than the physical space M 1 × M 2 . The ith agent interacts with other agents if they belong to the ith agent's vision range. The vision range of a particular agent creates in the direction of the agent's motion and so the multi-agent generates a random geometric graph [244,245] which is a simple and interesting model in spatial networks [246]. Now, one can calculate the SNBS Eqs. ( 69), and to this purpose one takes M number of different points from the attractor far away from the synchronization manifold S(t) = (X 1 , X 2 , . . . , X N ). The mean SNBS of the ith agent is writtenDuring movement, each agent creates a much smaller square shape region of φ 2 area (calling them vision size) than the physical space M 1 × M 2 . The ith agent interacts with other agents if they belong to the ith agent's vision range. The vision range of a particular agent creates in the direction of the agent's motion and so the multi-agent generates a random geometric graph [244,245] which is a simple and interesting model in spatial networks [246]. Now, one can calculate the SNBS Eqs. ( 69), and to this purpose one takes M number of different points from the attractor far away from the synchronization manifold S(t) = (X 1 , X 2 , . . . , X N ). The mean SNBS of the ith agent is written</p>
        <p>where BS(i, m) = J I is the SNBS of the ith agent starting from the mth point on the attractor, and J is the number of initial conditions that return to the synchronized manifold among I number of initial conditions.where BS(i, m) = J I is the SNBS of the ith agent starting from the mth point on the attractor, and J is the number of initial conditions that return to the synchronized manifold among I number of initial conditions.</p>
        <p>For inter-layer synchronization, one calculates ⟨BS(i)⟩ of all the pairs of replica nodes (i = 1, 2, . . . , N). The value of the vision range φ of each agents play an important role for the time-varying nature of the network. Increasing φ indicates the possibility of more interaction among mobile agents. For this reason, one changes the vision range φ to study the global stability. Fig. 22 shows the results on SNBS of each replica nodes and their corresponding histograms. For the vision range φ = 10, the values of SNBS of the replica nodes i = 1, 2, . . . , N are shown in Fig. 22(a). Here, the SNBS lies in the range [0.75, 0.79]. With increase of the vision range φ = 20, the values of SNBS lie in the range [0.90, 0.92] as evident from Fig. 22(c). With further increase φ = 30, almost all the initial conditions eventually lead to the synchronized state with ⟨BS(i)⟩ ∈ [0.99, 1.0] (i = 1, 2, . . . , N) and no matter which pairs of nodes are perturbed (see Fig. 22(e)). The corresponding frequencies of ⟨BS(i)⟩ for all these three values of φ ∈ {10, 20, 30} are respectively shown in Figs. 22(b,d,f). Interestingly, one sees note that ⟨BS(i)⟩ increases monotonically with φ, while its dispersion has the opposite trend. Thus, a sufficient vision range can substantiate an optimal response of the nodes to perturbations.For inter-layer synchronization, one calculates ⟨BS(i)⟩ of all the pairs of replica nodes (i = 1, 2, . . . , N). The value of the vision range φ of each agents play an important role for the time-varying nature of the network. Increasing φ indicates the possibility of more interaction among mobile agents. For this reason, one changes the vision range φ to study the global stability. Fig. 22 shows the results on SNBS of each replica nodes and their corresponding histograms. For the vision range φ = 10, the values of SNBS of the replica nodes i = 1, 2, . . . , N are shown in Fig. 22(a). Here, the SNBS lies in the range [0.75, 0.79]. With increase of the vision range φ = 20, the values of SNBS lie in the range [0.90, 0.92] as evident from Fig. 22(c). With further increase φ = 30, almost all the initial conditions eventually lead to the synchronized state with ⟨BS(i)⟩ ∈ [0.99, 1.0] (i = 1, 2, . . . , N) and no matter which pairs of nodes are perturbed (see Fig. 22(e)). The corresponding frequencies of ⟨BS(i)⟩ for all these three values of φ ∈ {10, 20, 30} are respectively shown in Figs. 22(b,d,f). Interestingly, one sees note that ⟨BS(i)⟩ increases monotonically with φ, while its dispersion has the opposite trend. Thus, a sufficient vision range can substantiate an optimal response of the nodes to perturbations.</p>
        <p>Multi-agent systems are composed of many agents interconnected by a communication network and capable to deal with problems that are difficult (or even impossible) to solve by a single agent [247]. Such agents are autonomous and can in fact take decisions by their own, but it is only due to their interactions that they (as a whole system) can perform a task in an efficient way. Agents are usually equipped with limited knowledge, such that their control occurs in a decentralized and distributed way.Multi-agent systems are composed of many agents interconnected by a communication network and capable to deal with problems that are difficult (or even impossible) to solve by a single agent [247]. Such agents are autonomous and can in fact take decisions by their own, but it is only due to their interactions that they (as a whole system) can perform a task in an efficient way. Agents are usually equipped with limited knowledge, such that their control occurs in a decentralized and distributed way.</p>
        <p>Synchronization and consensus are two central mechanisms in such systems [248,249] and several works have dealt with issues like robustness [250,251], resilience [252], observer-based methods [253,254], time-delayed communications [255,256], event-triggered control [257], adaptive coupling [205,258,259], and many other aspects (a comprehensive list of all of them is beyond the scope of this work).Synchronization and consensus are two central mechanisms in such systems [248,249] and several works have dealt with issues like robustness [250,251], resilience [252], observer-based methods [253,254], time-delayed communications [255,256], event-triggered control [257], adaptive coupling [205,258,259], and many other aspects (a comprehensive list of all of them is beyond the scope of this work).</p>
        <p>In this Section we review the relevant literature on synchronization in systems of mobile agents. These systems form, indeed, intrinsically time-dependent networks, as the communication network depends on the agent position and, hence, is not fixed in time. A typical example is given by a temporal proximity graph, where agents are assumed to be equipped with a communication system with limited range and to be able to communicate only with those agents that are at a distance shorter than the so-called interacting or sensing radius. Therefore, the network of interactions is determined by the way in which connections are established between agents and by the characteristics of the agent motion. The third key ingredient of the models discussed in this section is the dynamics associated to the agents. We start with what is probably the most simple representation of oscillator dynamics, i.e., phase oscillators (Section 4.1), then move to pulse-coupled oscillators (Section 4.2), and, finally, investigate the case of limit cycle and chaotic systems (Section 4.3).In this Section we review the relevant literature on synchronization in systems of mobile agents. These systems form, indeed, intrinsically time-dependent networks, as the communication network depends on the agent position and, hence, is not fixed in time. A typical example is given by a temporal proximity graph, where agents are assumed to be equipped with a communication system with limited range and to be able to communicate only with those agents that are at a distance shorter than the so-called interacting or sensing radius. Therefore, the network of interactions is determined by the way in which connections are established between agents and by the characteristics of the agent motion. The third key ingredient of the models discussed in this section is the dynamics associated to the agents. We start with what is probably the most simple representation of oscillator dynamics, i.e., phase oscillators (Section 4.1), then move to pulse-coupled oscillators (Section 4.2), and, finally, investigate the case of limit cycle and chaotic systems (Section 4.3).</p>
        <p>We start to elucidate a simple case of agents' motion and dynamics, namely the motion over a lattice and phase oscillators in continuous time [260].We start to elucidate a simple case of agents' motion and dynamics, namely the motion over a lattice and phase oscillators in continuous time [260].</p>
        <p>We then consider N phase oscillators, indexed by i = 1, . . . , N, each occupying the position of a node in a linear lattice of size N. Nodes can interact with neighbors within a discrete radius R, and mobility is dictated by position exchanges with only one of the node's immediate neighbors at random times, generated as identical and independently distributed random variables with Poisson distribution of rate λ. The system can be described in a rotating reference frame in terms of locally coupled oscillating dynamics, asWe then consider N phase oscillators, indexed by i = 1, . . . , N, each occupying the position of a node in a linear lattice of size N. Nodes can interact with neighbors within a discrete radius R, and mobility is dictated by position exchanges with only one of the node's immediate neighbors at random times, generated as identical and independently distributed random variables with Poisson distribution of rate λ. The system can be described in a rotating reference frame in terms of locally coupled oscillating dynamics, as</p>
        <p>where φ i is the phase of the ith oscillator, R is the interaction radius, n i is the size of the interaction neighborhood, and ϵ is the coupling strength, considered as uniform along the whole lattice. Initial phases φ i (0) are uniformly distributed at random in the interval [0, 2π ]. Boundary conditions are non-periodic, whereby an oscillator at the border can interact and exchange positions only with neighbors on one side.where φ i is the phase of the ith oscillator, R is the interaction radius, n i is the size of the interaction neighborhood, and ϵ is the coupling strength, considered as uniform along the whole lattice. Initial phases φ i (0) are uniformly distributed at random in the interval [0, 2π ]. Boundary conditions are non-periodic, whereby an oscillator at the border can interact and exchange positions only with neighbors on one side.</p>
        <p>The model comprises two time scales: 1/ϵ accounts for the coupled phase dynamics, and 1/λ is representative of the motion dynamics. The parameter λ/ϵ is therefore representative of the interplay between the two time scales [260] and, without loss of generality, we can set ϵ = 1 to elucidate salient properties of system (72). Source: Reprinted figure with permission from Ref. [260].The model comprises two time scales: 1/ϵ accounts for the coupled phase dynamics, and 1/λ is representative of the motion dynamics. The parameter λ/ϵ is therefore representative of the interplay between the two time scales [260] and, without loss of generality, we can set ϵ = 1 to elucidate salient properties of system (72). Source: Reprinted figure with permission from Ref. [260].</p>
        <p>© 2013 by the American Physical Society.© 2013 by the American Physical Society.</p>
        <p>The first set of experiments of Ref. [260] is conducted with unitary mobility range and connection radius, that is, R = 1.The first set of experiments of Ref. [260] is conducted with unitary mobility range and connection radius, that is, R = 1.</p>
        <p>Due also to the selected boundary condition, the oscillators tend toward complete phase synchronization, passing from the initial random distribution of the phases, through a snaky phase pattern that comprises several spatial modes with heterogeneous wavelengths, eventually attaining complete synchronization. For non-mobile nodes, that is, λ/ϵ = 0, the transition to complete synchronization is extremely slow. When dealing with mobile oscillators, increasing the value of λ/ϵ, the pattern is dominated by longer wavelength modes. For high values of the coefficient λ/ϵ, the snaky pattern does not appear and the oscillators rapidly converge toward complete synchronization. Intuitively, as long as the value of the parameter λ/ϵ increases, neighboring oscillators spend a shorter and shorter time coupled together before exchanging places. On the one hand, this prevents neighboring oscillators to settle toward a common phase before moving elsewhere; however, mobility globally speeds up complete synchronization, making all the phases rapidly converge to the mean phase of the population. Results are summarized in Fig. 23.Due also to the selected boundary condition, the oscillators tend toward complete phase synchronization, passing from the initial random distribution of the phases, through a snaky phase pattern that comprises several spatial modes with heterogeneous wavelengths, eventually attaining complete synchronization. For non-mobile nodes, that is, λ/ϵ = 0, the transition to complete synchronization is extremely slow. When dealing with mobile oscillators, increasing the value of λ/ϵ, the pattern is dominated by longer wavelength modes. For high values of the coefficient λ/ϵ, the snaky pattern does not appear and the oscillators rapidly converge toward complete synchronization. Intuitively, as long as the value of the parameter λ/ϵ increases, neighboring oscillators spend a shorter and shorter time coupled together before exchanging places. On the one hand, this prevents neighboring oscillators to settle toward a common phase before moving elsewhere; however, mobility globally speeds up complete synchronization, making all the phases rapidly converge to the mean phase of the population. Results are summarized in Fig. 23.</p>
        <p>To characterize the transient dynamics, one can introduce a parameter quantifying the correlation between two lattice sitesTo characterize the transient dynamics, one can introduce a parameter quantifying the correlation between two lattice sites</p>
        <p>where φ k (t) and φ k+d (t) are the phases at time t in nodes k and k + d, respectively, d is the distance between two nodes and the average ⟨•⟩ k is executed for variable k = 1, . . . , Nd. The parameter quantifies the average instantaneous phase correlation at a distance d in the lattice. Namely, if ρ ≈ 0, phases are uncorrelated at the given distance, whereas if ρ ≈ ±1, phases are totally correlated at the given distance. Value -1 denotes correlation with opposite sign. The time evolution of ρ(t, d) elucidates the convergence dynamics of the oscillator network toward the common phase.where φ k (t) and φ k+d (t) are the phases at time t in nodes k and k + d, respectively, d is the distance between two nodes and the average ⟨•⟩ k is executed for variable k = 1, . . . , Nd. The parameter quantifies the average instantaneous phase correlation at a distance d in the lattice. Namely, if ρ ≈ 0, phases are uncorrelated at the given distance, whereas if ρ ≈ ±1, phases are totally correlated at the given distance. Value -1 denotes correlation with opposite sign. The time evolution of ρ(t, d) elucidates the convergence dynamics of the oscillator network toward the common phase.</p>
        <p>The dynamics of parameter ρ follows an exponential relaxation, as 1 -ρ(d) ≈ e -t/Tc , where T c is a relaxation time that depends on the interplay between mobility and dynamics. It has been shown that, in a wide mobility range between λ/ϵ = 1 and the onset of the mean field condition (very high λ/ϵ), the following relationship holds:The dynamics of parameter ρ follows an exponential relaxation, as 1 -ρ(d) ≈ e -t/Tc , where T c is a relaxation time that depends on the interplay between mobility and dynamics. It has been shown that, in a wide mobility range between λ/ϵ = 1 and the onset of the mean field condition (very high λ/ϵ), the following relationship holds:</p>
        <p>At the extremes of the mobility range, the relaxation time T c does not depend on the ratio λ/ϵ. For very low mobility, T c strongly depends on the system size. For very high mobility, the collective behavior can be interpreted through a mean-field analysis, yielding T c ≈ 0.5. In such a mean-field regime, mobility has an effect equivalent to extending the coupling range to an effective value R e , which is estimated as R e = (1/4)(-3 + √ 49 + 48λ/ϵ), in the regime where the longest spatial mode keeps its sinusoidal shape, that is, for (λ/ϵ)/N 2 ≪ 1.At the extremes of the mobility range, the relaxation time T c does not depend on the ratio λ/ϵ. For very low mobility, T c strongly depends on the system size. For very high mobility, the collective behavior can be interpreted through a mean-field analysis, yielding T c ≈ 0.5. In such a mean-field regime, mobility has an effect equivalent to extending the coupling range to an effective value R e , which is estimated as R e = (1/4)(-3 + √ 49 + 48λ/ϵ), in the regime where the longest spatial mode keeps its sinusoidal shape, that is, for (λ/ϵ)/N 2 ≪ 1.</p>
        <p>Considering a long-range coupling (R &gt; 1) yields a qualitatively similar, yet richer, spectrum of phenomena. A meanfield behavior, similar to that observed for R = 1, emerges also in this case, and for R ≫ 1 and r/N ≪ 1, becomes apparent for λ/ϵ &gt; R 2 /3. Casting the latter inequality as (R 2 /λ)/(1/ϵ) &lt; 3, we can observe that mobility plays an active role in affecting synchronization when the time scale for each oscillator to explore the coupling range R is comparable to that of the phase dynamics. Also in the case of long-range coupling, a widening of the effective coupling range R is observed under mean-field condition, indicating an effective role of mobility on the overall system dynamics. © 2013 by the American Physical Society.Considering a long-range coupling (R &gt; 1) yields a qualitatively similar, yet richer, spectrum of phenomena. A meanfield behavior, similar to that observed for R = 1, emerges also in this case, and for R ≫ 1 and r/N ≪ 1, becomes apparent for λ/ϵ &gt; R 2 /3. Casting the latter inequality as (R 2 /λ)/(1/ϵ) &lt; 3, we can observe that mobility plays an active role in affecting synchronization when the time scale for each oscillator to explore the coupling range R is comparable to that of the phase dynamics. Also in the case of long-range coupling, a widening of the effective coupling range R is observed under mean-field condition, indicating an effective role of mobility on the overall system dynamics. © 2013 by the American Physical Society.</p>
        <p>In the previous section we have discussed a model of phase oscillators moving on a lattice structure where, at each time, each node is occupied by a single agent and interactions occur among adjacent nodes. We now move to consider a more general, metapopulation model where (i) motion occurs along an arbitrary network structure; (ii) each node can host a population of agents; and (iii) interactions occur within each node of the network (i.e., among the agents that occupy the same node). The model, first introduced in Ref. [127], consists of a set of W agents distributed along the N nodes of a network with adjacency matrix A. The agents of the system are labeled with the subscript i (i = 1, . . . , W ), while the nodes of the network with subscript I (I = 1, . . . , N).In the previous section we have discussed a model of phase oscillators moving on a lattice structure where, at each time, each node is occupied by a single agent and interactions occur among adjacent nodes. We now move to consider a more general, metapopulation model where (i) motion occurs along an arbitrary network structure; (ii) each node can host a population of agents; and (iii) interactions occur within each node of the network (i.e., among the agents that occupy the same node). The model, first introduced in Ref. [127], consists of a set of W agents distributed along the N nodes of a network with adjacency matrix A. The agents of the system are labeled with the subscript i (i = 1, . . . , W ), while the nodes of the network with subscript I (I = 1, . . . , N).</p>
        <p>At any time t, each agent is located in one of the nodes of the network where it interacts with all the other agents located in that node. Then, after a time interval equal to τ M , the agent moves to one of the adjacent nodes where it will interact with other agents of the system and so on. Therefore, at each node of the network an all-to-all interaction among the agents of the node takes place, such that, assuming that agent i at time t is in node I, the dynamics of the phase variables is given by:At any time t, each agent is located in one of the nodes of the network where it interacts with all the other agents located in that node. Then, after a time interval equal to τ M , the agent moves to one of the adjacent nodes where it will interact with other agents of the system and so on. Therefore, at each node of the network an all-to-all interaction among the agents of the node takes place, such that, assuming that agent i at time t is in node I, the dynamics of the phase variables is given by:</p>
        <p>where ω i are the natural frequencies of the oscillators, drawn from a distribution g(ω), and ϵ is the coupling strength. Mobility of the agents, which corresponds to migration from one population to the other of the metapopulation model, consists of a degree-biased random walk on the network [261][262][263]. According to this mobility rule, the probability ∏ I→J that an agent at node I moves to an adjacent node J depends on the degree k J of the destination node as follows:where ω i are the natural frequencies of the oscillators, drawn from a distribution g(ω), and ϵ is the coupling strength. Mobility of the agents, which corresponds to migration from one population to the other of the metapopulation model, consists of a degree-biased random walk on the network [261][262][263]. According to this mobility rule, the probability ∏ I→J that an agent at node I moves to an adjacent node J depends on the degree k J of the destination node as follows:</p>
        <p>where α represents a parameter tuning the bias of agent motion towards low-degree nodes (for negative α) or highdegree ones (for positive α), whereas α = 0 implements the unbiased random walk. The model hence has three control parameters: the coupling strength ϵ, acting on the dynamical interactions among the phase variables; the bias parameter α, determining the type of motion; and the interval τ M between subsequent steps of motion, which fixes the ratio between the time scales of interaction and motion.where α represents a parameter tuning the bias of agent motion towards low-degree nodes (for negative α) or highdegree ones (for positive α), whereas α = 0 implements the unbiased random walk. The model hence has three control parameters: the coupling strength ϵ, acting on the dynamical interactions among the phase variables; the bias parameter α, determining the type of motion; and the interval τ M between subsequent steps of motion, which fixes the ratio between the time scales of interaction and motion.</p>
        <p>It is instructive to show how the order parameter r = lim T →∞It is instructive to show how the order parameter r = lim T →∞</p>
        <p>⏐ ⏐ ⏐ varies with α. An example, obtained for a fixed value of ϵ (ϵ = 0.08) in a metapopulation model with W = 5000 agents moving on a scale-free network with N = 500 nodes, is shown in Fig. 24. Suppose to start from a value of α = -1, where oscillations are incoherent as r ≃ 0, then, a fully coherent state can be reached either increasing or decreasing α. This clearly demonstrates that it is possible to tune the level of synchronization by acting on the motion parameter, i.e., changing the type of motion of the walkers on the network. However, the microscopic mechanisms underlying synchronization in the case of large negative α and of small positive α are quite different. The Authors of Ref. [127], in fact, observe two microscopic paths to synchronization: one, arising for α &lt; -1, driven by low-degree nodes, and one, arising for α &gt; -1, by the hubs.⏐ ⏐ ⏐ varies with α. An example, obtained for a fixed value of ϵ (ϵ = 0.08) in a metapopulation model with W = 5000 agents moving on a scale-free network with N = 500 nodes, is shown in Fig. 24. Suppose to start from a value of α = -1, where oscillations are incoherent as r ≃ 0, then, a fully coherent state can be reached either increasing or decreasing α. This clearly demonstrates that it is possible to tune the level of synchronization by acting on the motion parameter, i.e., changing the type of motion of the walkers on the network. However, the microscopic mechanisms underlying synchronization in the case of large negative α and of small positive α are quite different. The Authors of Ref. [127], in fact, observe two microscopic paths to synchronization: one, arising for α &lt; -1, driven by low-degree nodes, and one, arising for α &gt; -1, by the hubs.</p>
        <p>Another interesting result illustrated in Ref. [127] is the behavior with τ M , the time constant of motion. In the limit case that τ M is infinite, the metapopulation model becomes equivalent to a set of N independent globally coupled populations of Kuramoto oscillators. Under these conditions, synchronization can be attained locally, but not globally, as there are no interactions among the different populations. This consideration also explains the behavior for finite, but large τ M , where rare motion events yield to poor mixing, preventing the emergence of global order. On the contrary, for small τ M , the time spent by agents at each node is so small to hinder local synchronization, but the fast motion favors the onset of global synchronization.Another interesting result illustrated in Ref. [127] is the behavior with τ M , the time constant of motion. In the limit case that τ M is infinite, the metapopulation model becomes equivalent to a set of N independent globally coupled populations of Kuramoto oscillators. Under these conditions, synchronization can be attained locally, but not globally, as there are no interactions among the different populations. This consideration also explains the behavior for finite, but large τ M , where rare motion events yield to poor mixing, preventing the emergence of global order. On the contrary, for small τ M , the time spent by agents at each node is so small to hinder local synchronization, but the fast motion favors the onset of global synchronization.</p>
        <p>To illustrate the case where the oscillators are carried by agents moving in a continuous space, we begin with the model investigated in Ref. [264]. According to this study, agents are assumed to perform random walks in a two-dimensional plane, in particular a square of size L and periodic boundary conditions. In addition, agents are assumed to communicate 42 with each other through short-range devices, such that interactions among them can be modeled by a proximity graph that changes in time as the result of the agent motion.To illustrate the case where the oscillators are carried by agents moving in a continuous space, we begin with the model investigated in Ref. [264]. According to this study, agents are assumed to perform random walks in a two-dimensional plane, in particular a square of size L and periodic boundary conditions. In addition, agents are assumed to communicate 42 with each other through short-range devices, such that interactions among them can be modeled by a proximity graph that changes in time as the result of the agent motion.</p>
        <p>The ith agent moves with velocity v i (t), having constant modulus v and variable heading θ i , such that v i (t) = ve ιθ i (t) .The ith agent moves with velocity v i (t), having constant modulus v and variable heading θ i , such that v i (t) = ve ιθ i (t) .</p>
        <p>The headings are updated randomly at discrete time steps t k , with t k -t k-1 = τ M , such that, indicating with y i (t) ∈ R 2 the position of the ith agent in the plane at time t, we have that motion is governed by Eqs. (3), here repeated for convenienceThe headings are updated randomly at discrete time steps t k , with t k -t k-1 = τ M , such that, indicating with y i (t) ∈ R 2 the position of the ith agent in the plane at time t, we have that motion is governed by Eqs. (3), here repeated for convenience</p>
        <p>where η i (t k ) (i = 1, . . . , N) are N independent random variables chosen at each time t k with uniform probability in the interval [-π , π].where η i (t k ) (i = 1, . . . , N) are N independent random variables chosen at each time t k with uniform probability in the interval [-π , π].</p>
        <p>The Authors of Ref. [264] consider that the oscillator associated with each agent is described by a single phase variable, φ i (t) with i = 1, . . . , N, as in the Kuramoto model [154,155]. Here, however, unlike the classical Kuramoto model, all the agents have the same natural frequency, that without lack of generality is set to zero, i.e., ω i = 0, and interact according to the temporal network induced by the motion scheme discussed above. Accordingly, the dynamics of the oscillators are described by:The Authors of Ref. [264] consider that the oscillator associated with each agent is described by a single phase variable, φ i (t) with i = 1, . . . , N, as in the Kuramoto model [154,155]. Here, however, unlike the classical Kuramoto model, all the agents have the same natural frequency, that without lack of generality is set to zero, i.e., ω i = 0, and interact according to the temporal network induced by the motion scheme discussed above. Accordingly, the dynamics of the oscillators are described by:</p>
        <p>where the phases are updated at discrete time intervals, here indicated with τ P . In Eq. ( 78), the time-varying pattern of interactions among agents is encoded in the coefficients A ij (t) of the adjacency matrix A(t) that is function of time t. Interactions are mutual, so that at each time instant the adjacency matrix is symmetric. The coefficients are, therefore, defined as A ij (t) = A ji (t) = 1 if i and j are connected by a link at time t, while A ij (t) = A ji (t) = 0 otherwise. In addition, A ii (t) = 0, as there are no self-loops. At each time instant, links are established based on the mutual distance between agents, such that A(t) is the adjacency matrix of the proximity graph induced by the positions y i (t), i.e., A ij (t) = 1 if ∥y j (t) -y i (t)∥ &lt; R, where R is the sensing radius.where the phases are updated at discrete time intervals, here indicated with τ P . In Eq. ( 78), the time-varying pattern of interactions among agents is encoded in the coefficients A ij (t) of the adjacency matrix A(t) that is function of time t. Interactions are mutual, so that at each time instant the adjacency matrix is symmetric. The coefficients are, therefore, defined as A ij (t) = A ji (t) = 1 if i and j are connected by a link at time t, while A ij (t) = A ji (t) = 0 otherwise. In addition, A ii (t) = 0, as there are no self-loops. At each time instant, links are established based on the mutual distance between agents, such that A(t) is the adjacency matrix of the proximity graph induced by the positions y i (t), i.e., A ij (t) = 1 if ∥y j (t) -y i (t)∥ &lt; R, where R is the sensing radius.</p>
        <p>Before discussing how synchronization arises in this system, it is important to note that, similar to continuum percolation, when R grows the interaction topology has a transition from a regime characterized by small-size, disconnected components to a single giant component including all the agents. The transition occurs at (N -1)π R 2 c /L 2 ≈ 4.51, and, accordingly, different synchronization behaviors are observed for R &lt; R c or R &gt; R c . To characterize the system behavior, the Authors of Ref. [264] introduce the average phase differenceBefore discussing how synchronization arises in this system, it is important to note that, similar to continuum percolation, when R grows the interaction topology has a transition from a regime characterized by small-size, disconnected components to a single giant component including all the agents. The transition occurs at (N -1)π R 2 c /L 2 ≈ 4.51, and, accordingly, different synchronization behaviors are observed for R &lt; R c or R &gt; R c . To characterize the system behavior, the Authors of Ref. [264] introduce the average phase difference</p>
        <p>and observe that, after an initial transient, it decays exponentially such that it is possible to define the synchronization characteristic time T s by the relationship E φ ∝ e -t/Ts . In turn, this allows the definition of the number of phase updates needed by the system to reach synchronization as n T = T s /τ P . The smaller is this parameter the more efficient is the system, as it can reach synchronization in a shorter time. To illustrate the system behavior, let us start considering the scenario where agent motion is fast compared to the oscillator dynamics. Under this assumption, the topology changes fast enough such that the system behavior can be described by taking into account the average interaction matrix from which one can provide the following fast-switching estimate T FS of the synchronization characteristic time T s :and observe that, after an initial transient, it decays exponentially such that it is possible to define the synchronization characteristic time T s by the relationship E φ ∝ e -t/Ts . In turn, this allows the definition of the number of phase updates needed by the system to reach synchronization as n T = T s /τ P . The smaller is this parameter the more efficient is the system, as it can reach synchronization in a shorter time. To illustrate the system behavior, let us start considering the scenario where agent motion is fast compared to the oscillator dynamics. Under this assumption, the topology changes fast enough such that the system behavior can be described by taking into account the average interaction matrix from which one can provide the following fast-switching estimate T FS of the synchronization characteristic time T s :</p>
        <p>where p = π R 2 /L 2 represents the probability that two agents are interacting each other under the fast-switching hypothesis. In fact, in this scenario the updated position becomes uncorrelated with the previous one, and the probability that two agents interact becomes equal to the probability that an agent lies in the sensing area of another agent. The latter is given by the ratio between the sensing area πR 2 and the overall area where agents move, i.e., L 2 .where p = π R 2 /L 2 represents the probability that two agents are interacting each other under the fast-switching hypothesis. In fact, in this scenario the updated position becomes uncorrelated with the previous one, and the probability that two agents interact becomes equal to the probability that an agent lies in the sensing area of another agent. The latter is given by the ratio between the sensing area πR 2 and the overall area where agents move, i.e., L 2 .</p>
        <p>The behavior of n T as a function of R is illustrated in Fig. 25 for several settings of τ P along with the prediction of the fast-switching assumption (represented by the continuous line). We notice that n T decreases with increasing R, as a larger interaction radius favors synchronization. In addition, for large τ P the fast-switching prediction is accurate in the whole interval of R, whereas for small τ P the behavior of n T deviates from the prediction, specially close to the percolation transition R ≃ R c . In fact, a large τ P indicates that the oscillator dynamics is slow, a scenario where the fast-switching approximation well reproduces the system behavior.The behavior of n T as a function of R is illustrated in Fig. 25 for several settings of τ P along with the prediction of the fast-switching assumption (represented by the continuous line). We notice that n T decreases with increasing R, as a larger interaction radius favors synchronization. In addition, for large τ P the fast-switching prediction is accurate in the whole interval of R, whereas for small τ P the behavior of n T deviates from the prediction, specially close to the percolation transition R ≃ R c . In fact, a large τ P indicates that the oscillator dynamics is slow, a scenario where the fast-switching approximation well reproduces the system behavior.</p>
        <p>Altogether, the analysis of the model reveals three diverse asymptotic behaviors that can be explained taking into account that there are two characteristic time scales, one that accounts for the typical time for clusters to synchronize and the other one for leaving the cluster. The first regime takes place when R is small and τ P is large. In this case, the network connectivity changes very fast, before the time required by the agents to synchronize with their neighbors, such that all © 2011 by the American Physical Society.Altogether, the analysis of the model reveals three diverse asymptotic behaviors that can be explained taking into account that there are two characteristic time scales, one that accounts for the typical time for clusters to synchronize and the other one for leaving the cluster. The first regime takes place when R is small and τ P is large. In this case, the network connectivity changes very fast, before the time required by the agents to synchronize with their neighbors, such that all © 2011 by the American Physical Society.</p>
        <p>units reach synchronization at approximately the same rate. In Ref. [264] this regime is called global synchronization. The 52 fast-switching assumption accurately describes this mechanism of synchronization. The second regime occurs when τ P is decreased or R is increased, still remaining below the threshold for percolation, i.e., R &lt; R c . In this scenario, the formation of clusters of synchronous nodes is favored by the fact that the topology changes slowly compared to the time scale of the synchronization process. This mechanism is called multiple cluster local synchronization and is not well reproduced by the fast switching approximation that neglects the correlation between consecutive positions of agents and system dynamics. Finally, for R &gt; R c a third regime is observed. Here, all the units are connected into a single giant component and motion becomes unnecessary for synchronization, but still contributes to the evolution towards the final state of the system. When R is large enough that the connectivity becomes that of a complete graph, the fast switching assumption again predicts the system behavior.units reach synchronization at approximately the same rate. In Ref. [264] this regime is called global synchronization. The 52 fast-switching assumption accurately describes this mechanism of synchronization. The second regime occurs when τ P is decreased or R is increased, still remaining below the threshold for percolation, i.e., R &lt; R c . In this scenario, the formation of clusters of synchronous nodes is favored by the fact that the topology changes slowly compared to the time scale of the synchronization process. This mechanism is called multiple cluster local synchronization and is not well reproduced by the fast switching approximation that neglects the correlation between consecutive positions of agents and system dynamics. Finally, for R &gt; R c a third regime is observed. Here, all the units are connected into a single giant component and motion becomes unnecessary for synchronization, but still contributes to the evolution towards the final state of the system. When R is large enough that the connectivity becomes that of a complete graph, the fast switching assumption again predicts the system behavior.</p>
        <p>In the model discussed so far, agents follow Brownian motion. The case of superdiffusive motion has been, instead, considered in Ref. [265], using the following equations to describe the motion dynamics:In the model discussed so far, agents follow Brownian motion. The case of superdiffusive motion has been, instead, considered in Ref. [265], using the following equations to describe the motion dynamics:</p>
        <p>and phase dynamics:and phase dynamics:</p>
        <p>where ξ i (t) indicates Lévy noise, N i (t) represents the number of neighbors of agent i at time t and the termwhere ξ i (t) indicates Lévy noise, N i (t) represents the number of neighbors of agent i at time t and the term</p>
        <p>an additive, Gaussian white noise with intensity D φ . The Authors of Ref. [265] define a parameter, α in their study, that characterizes the distribution of the random displacements and tune the characteristics of the motion, from Brownian (obtained for α = 2) to Lévy flights (for α ∈ (0, 2)). A remarkable feature of the interplay between motion and dynamics that is unveiled by this model is that, while for Brownian motion, the order parameteran additive, Gaussian white noise with intensity D φ . The Authors of Ref. [265] define a parameter, α in their study, that characterizes the distribution of the random displacements and tune the characteristics of the motion, from Brownian (obtained for α = 2) to Lévy flights (for α ∈ (0, 2)). A remarkable feature of the interplay between motion and dynamics that is unveiled by this model is that, while for Brownian motion, the order parameter</p>
        <p>| 2 decreases according to a power law with the number of units, for Lévy flights the order parameter tends towards a nonzero constant value for large system sizes, indicating a robust synchronized state.| 2 decreases according to a power law with the number of units, for Lévy flights the order parameter tends towards a nonzero constant value for large system sizes, indicating a robust synchronized state.</p>
        <p>An important generalization of the model of mobile phase oscillators is dealt with in Ref. [80], where the hypothesis of point-like particles is removed to consider active disks with a non-zero volume. In this study, the dynamics of the oscillators is again described by phase oscillators as in Eq. ( 78), whereas the kinetic Monte Carlo model of Ref. [266] is used to represent particle motion. According to this model, agents are self-propelled hard disks with diameter σ . Their positions are updated in time as follows:An important generalization of the model of mobile phase oscillators is dealt with in Ref. [80], where the hypothesis of point-like particles is removed to consider active disks with a non-zero volume. In this study, the dynamics of the oscillators is again described by phase oscillators as in Eq. ( 78), whereas the kinetic Monte Carlo model of Ref. [266] is used to represent particle motion. According to this model, agents are self-propelled hard disks with diameter σ . Their positions are updated in time as follows:</p>
        <p>where P acc represents the acceptance probability of the update, allowing to model how interactions among particles take place. In particular, P acc = 1 if the update does not yield any overlap among particles, whereas P acc = 0 in the opposite case. δ i (t) represents the displacement with dynamics given by:where P acc represents the acceptance probability of the update, allowing to model how interactions among particles take place. In particular, P acc = 1 if the update does not yield any overlap among particles, whereas P acc = 0 in the opposite case. δ i (t) represents the displacement with dynamics given by:</p>
        <p>with δ i (0) = v 0 η i (0). Here, η i (t) is a vector of independent random components drawn at each time step from a uniform distribution in [-1, 1]. In addition, δ i (t k ) is constrained to lie in a square box of size v 0 . In the extreme case that v 1 ≪ v 0 , the random shift v 1 η i (t) becomes negligible and particle motion is ballistic with velocity v 0 . On the other extreme, when v 1 ≫ v 0 , one finds that the displacements are no more correlated in time and the particles move as Brownian disks. Finally, between these two extremes, the model describes an overdamped persistent random walk with persistence time and gel-like ones [266]. Considering excluded volume interactions yields P acc = 1 and φ = 0 in the model such that it is possible to recover the case of point-like particles. For other values of the parameters, the model offers a general framework to explore the interplay between self-propulsion, steric repulsion, and phase coupling.with δ i (0) = v 0 η i (0). Here, η i (t) is a vector of independent random components drawn at each time step from a uniform distribution in [-1, 1]. In addition, δ i (t k ) is constrained to lie in a square box of size v 0 . In the extreme case that v 1 ≪ v 0 , the random shift v 1 η i (t) becomes negligible and particle motion is ballistic with velocity v 0 . On the other extreme, when v 1 ≫ v 0 , one finds that the displacements are no more correlated in time and the particles move as Brownian disks. Finally, between these two extremes, the model describes an overdamped persistent random walk with persistence time and gel-like ones [266]. Considering excluded volume interactions yields P acc = 1 and φ = 0 in the model such that it is possible to recover the case of point-like particles. For other values of the parameters, the model offers a general framework to explore the interplay between self-propulsion, steric repulsion, and phase coupling.</p>
        <p>From the analysis of the model carried out in Ref. [80], several interesting conclusions may be drawn. It seems particularly relevant the fact that while, as we have seen, in the absence of particle-particle interactions self-propulsion promotes synchronization of locally coupled oscillators, the same is not true for disks of a finite volume. In fact, in the presence of repulsive interactions, one finds that the behavior of the synchronization time with the persistence time is not monotonic, and synchronization can be optimized by choosing a precise value of this parameter as a function of the density. This optimum represents a tradeoff between the enhancement of particle motion and the tendency to form clusters as the persistence of the particles is increased.From the analysis of the model carried out in Ref. [80], several interesting conclusions may be drawn. It seems particularly relevant the fact that while, as we have seen, in the absence of particle-particle interactions self-propulsion promotes synchronization of locally coupled oscillators, the same is not true for disks of a finite volume. In fact, in the presence of repulsive interactions, one finds that the behavior of the synchronization time with the persistence time is not monotonic, and synchronization can be optimized by choosing a precise value of this parameter as a function of the density. This optimum represents a tradeoff between the enhancement of particle motion and the tendency to form clusters as the persistence of the particles is increased.</p>
        <p>Another relevant scenario to consider is represented by the presence of non-identical natural frequencies in the oscillators [267,268]. Also in this case, agent mobility plays an important role in achieving synchronization, which in general is favored by an increasing velocity [267]. Quite interestingly, in this scenario explosive synchronization may also appear, as we now briefly discuss referring to the model presented in Ref. [268].Another relevant scenario to consider is represented by the presence of non-identical natural frequencies in the oscillators [267,268]. Also in this case, agent mobility plays an important role in achieving synchronization, which in general is favored by an increasing velocity [267]. Quite interestingly, in this scenario explosive synchronization may also appear, as we now briefly discuss referring to the model presented in Ref. [268].</p>
        <p>In classical complex networks of phase oscillators, although in most situations the transition to synchronization is second-order, there also exist settings where the transition is first-order. This phenomenon is known as explosive synchronization (ES) and has been observed in structures with a correlation between the node degree and its natural frequency [269], between the strength of the coupling and the node natural frequency [270], or between the strength of the coupling and the frequency mismatch with the neighboring oscillators [271], as well as in structures with an adaptive coupling modulated by a local order parameter [228,272] and several other settings (for a recent review see Ref. [273]). The Authors of Ref. [268] show that explosive synchronization may also be observed in systems of mobile phase oscillators. In particular, they model mobility as in Eq. ( 3) with agents interacting according to a temporal proximity graph, and assume the following dynamics for the phase of the oscillators:In classical complex networks of phase oscillators, although in most situations the transition to synchronization is second-order, there also exist settings where the transition is first-order. This phenomenon is known as explosive synchronization (ES) and has been observed in structures with a correlation between the node degree and its natural frequency [269], between the strength of the coupling and the node natural frequency [270], or between the strength of the coupling and the frequency mismatch with the neighboring oscillators [271], as well as in structures with an adaptive coupling modulated by a local order parameter [228,272] and several other settings (for a recent review see Ref. [273]). The Authors of Ref. [268] show that explosive synchronization may also be observed in systems of mobile phase oscillators. In particular, they model mobility as in Eq. ( 3) with agents interacting according to a temporal proximity graph, and assume the following dynamics for the phase of the oscillators:</p>
        <p>where α i is a parameter tuning the coupling strength and k i (t) the degree of node i at time t. The natural frequencies ω i of the oscillators are assumed to be uniformly distributed in the interval [-π, π]. For a fraction f of randomly chosen units, α i = r i wherewhere α i is a parameter tuning the coupling strength and k i (t) the degree of node i at time t. The natural frequencies ω i of the oscillators are assumed to be uniformly distributed in the interval [-π, π]. For a fraction f of randomly chosen units, α i = r i where</p>
        <p>⏐ ⏐ ⏐ is the local order parameter, whereas for the remaining fraction of units, i.e., 1 -f , α i = 1. In this way, similarly to Ref. [228], the effective strength of the coupling is correlated with the local level of synchrony. However, at variance with the model investigated in Ref. [228], here the neighborhood of each unit changes in time as the result of agent motion.⏐ ⏐ ⏐ is the local order parameter, whereas for the remaining fraction of units, i.e., 1 -f , α i = 1. In this way, similarly to Ref. [228], the effective strength of the coupling is correlated with the local level of synchrony. However, at variance with the model investigated in Ref. [228], here the neighborhood of each unit changes in time as the result of agent motion.</p>
        <p>The peculiar feature of this model is that for f = 0 the transition to synchronization is second-order, whereas for larger f it becomes first-order with a hysteresis window of increasing width. Fig. 26 illustrates this result, showing forward and backward continuations with the coupling strength ϵ for the order parameter r = lim T →∞The peculiar feature of this model is that for f = 0 the transition to synchronization is second-order, whereas for larger f it becomes first-order with a hysteresis window of increasing width. Fig. 26 illustrates this result, showing forward and backward continuations with the coupling strength ϵ for the order parameter r = lim T →∞</p>
        <p>An important application of the model of phase oscillators coupled according to a proximity graph is in multi-agent systems, where it is used to design a protocol for addressing the flocking problem [109]. The flocking problem consists of determining a distributed control law ensuring that the velocities of all agents asymptotically converge to the same value, while avoiding collisions among the agents. In recent years, diverse solutions to the problem have been proposed, including leaderless or leader-follower strategies, methods based on artificial potential fields, or techniques relying on the knowledge of the relative distances among the agents [106,108,274].An important application of the model of phase oscillators coupled according to a proximity graph is in multi-agent systems, where it is used to design a protocol for addressing the flocking problem [109]. The flocking problem consists of determining a distributed control law ensuring that the velocities of all agents asymptotically converge to the same value, while avoiding collisions among the agents. In recent years, diverse solutions to the problem have been proposed, including leaderless or leader-follower strategies, methods based on artificial potential fields, or techniques relying on the knowledge of the relative distances among the agents [106,108,274].</p>
        <p>The model of coupled phase oscillators provides a solution to the flocking problem when the neighbors' heading is the only information available to agents. The main idea is to reinterpret the phase of the oscillator as the heading of the agent, such that φ i = θ i . The dynamics of the system can be hence rewritten as:The model of coupled phase oscillators provides a solution to the flocking problem when the neighbors' heading is the only information available to agents. The main idea is to reinterpret the phase of the oscillator as the heading of the agent, such that φ i = θ i . The dynamics of the system can be hence rewritten as:</p>
        <p>or, in the case that a linear protocol is adopted, as: © 2020 with permission from Elsevier.or, in the case that a linear protocol is adopted, as: © 2020 with permission from Elsevier.</p>
        <p>There is a profound difference between system (86) and the model discussed in Section 4.1.3. Here, in fact, the dynamics 48 of synchronization influences the motion of the agents, such that the system becomes a closed-loop one, where motion affects synchronization and synchronization, in turn, impacts motion. Consequently, the structure of interactions is no more exclusively determined by the type of motion, but also by how the synchronization dynamics develops. In particular, it may happen that, because of how their headings evolve, some agents become disconnected from the network and are no more able to re-join the team. For this reason, flocking control requires algorithms that explicitly consider the problem of preserving network connectedness. The Authors of Ref. [109] demonstrate that both models ( 86) and ( 87) may solve flocking while preserving network connectedness and avoiding collisions among agents. To discuss in more detail the conditions guaranteeing this, let us start from the linear case.There is a profound difference between system (86) and the model discussed in Section 4.1.3. Here, in fact, the dynamics 48 of synchronization influences the motion of the agents, such that the system becomes a closed-loop one, where motion affects synchronization and synchronization, in turn, impacts motion. Consequently, the structure of interactions is no more exclusively determined by the type of motion, but also by how the synchronization dynamics develops. In particular, it may happen that, because of how their headings evolve, some agents become disconnected from the network and are no more able to re-join the team. For this reason, flocking control requires algorithms that explicitly consider the problem of preserving network connectedness. The Authors of Ref. [109] demonstrate that both models ( 86) and ( 87) may solve flocking while preserving network connectedness and avoiding collisions among agents. To discuss in more detail the conditions guaranteeing this, let us start from the linear case.</p>
        <p>Consider system (87). If the initial proximity graph is connected and the initial positions of the agents are distinct, then there exists a critical value ϵ c of the control gain, such that if ϵ &gt; ϵ c the proximity graph preserves connectedness for any time t &gt; 0, collisions are avoided and the flocking problem is solved. This result shows that, provided that the control gain is sufficiently large, flocking can be achieved for any velocity v. However, the value of ϵ c increases with v, meaning that faster agents require a higher control gain. The dependence of k c on the algebraic connectivity of the initial proximity graph, here indicated as µ, is also interesting: ϵ c scales as µ -1 , such that a larger connectivity yields a smaller threshold for flocking control.Consider system (87). If the initial proximity graph is connected and the initial positions of the agents are distinct, then there exists a critical value ϵ c of the control gain, such that if ϵ &gt; ϵ c the proximity graph preserves connectedness for any time t &gt; 0, collisions are avoided and the flocking problem is solved. This result shows that, provided that the control gain is sufficiently large, flocking can be achieved for any velocity v. However, the value of ϵ c increases with v, meaning that faster agents require a higher control gain. The dependence of k c on the algebraic connectivity of the initial proximity graph, here indicated as µ, is also interesting: ϵ c scales as µ -1 , such that a larger connectivity yields a smaller threshold for flocking control.</p>
        <p>A similar result holds for the nonlinear case (86). However, in this case a further condition is required: for any pair of agents, i and j, the difference of their initial headings need to be bounded, namely |θ j (0) -θ i (0)| &lt; π. The expression for k c in this case is different, but the functional dependence on v and µ is the same. Notice that the two results constitute sufficient, but not necessary conditions, and yet they provide an analytic proof of the existence of a solution to the problem based on the information on the headings on neighbors in the temporal proximity graph.A similar result holds for the nonlinear case (86). However, in this case a further condition is required: for any pair of agents, i and j, the difference of their initial headings need to be bounded, namely |θ j (0) -θ i (0)| &lt; π. The expression for k c in this case is different, but the functional dependence on v and µ is the same. Notice that the two results constitute sufficient, but not necessary conditions, and yet they provide an analytic proof of the existence of a solution to the problem based on the information on the headings on neighbors in the temporal proximity graph.</p>
        <p>An interesting generalization of this approach to flocking control is the inclusion of both metric and topological interactions [110]. The idea is that each agent interacts with all the other units within a circle of a fixed radius, and, in addition, with a number of other agents determined according to the topological criterion of the smallest distance. In more detail, the Authors of Ref. [110] set a minimum number of interacting agents, M, and, whenever an agent finds m &lt; M neighbors within its sensing radius, it also interacts with Mm further units, selected as those at the smallest distance from its position. In this way, in any condition each agent will interact at least with M agents, a configuration which proves to be particularly useful when the agent density is low and protocols based only on metric interactions may show poor performance. Such a hybrid protocol also solves the problem of preserving network connectedness, but requires a communication system that is able to reach any other agent, independently from its effective distance. Here, a large communication radius R and number M both favor synchronization, as they generate a configuration with a larger 31 number of neighbors. We will see that, while this principle holds true in different models of coupled phase oscillators, the same does not apply when the unit dynamics is more complex, as, for instance, it happens in the case of chaotic oscillators discussed in Section 4.3. After deriving the conditions guaranteeing convergence of the protocol, the Authors of Ref. [110] discuss a series of numerical results showing that the metric-topological model conjugates the advantages of the metric and topological interaction models and has the fastest convergent rate and smallest heading difference for different interaction ranges. We conclude this section remarking that there is a huge literature on flocking control, which goes beyond the purpose of this section whose aim was instead to hallmark the relationship between it and the model of coupled phase oscillators in temporal proximity graphs.An interesting generalization of this approach to flocking control is the inclusion of both metric and topological interactions [110]. The idea is that each agent interacts with all the other units within a circle of a fixed radius, and, in addition, with a number of other agents determined according to the topological criterion of the smallest distance. In more detail, the Authors of Ref. [110] set a minimum number of interacting agents, M, and, whenever an agent finds m &lt; M neighbors within its sensing radius, it also interacts with Mm further units, selected as those at the smallest distance from its position. In this way, in any condition each agent will interact at least with M agents, a configuration which proves to be particularly useful when the agent density is low and protocols based only on metric interactions may show poor performance. Such a hybrid protocol also solves the problem of preserving network connectedness, but requires a communication system that is able to reach any other agent, independently from its effective distance. Here, a large communication radius R and number M both favor synchronization, as they generate a configuration with a larger 31 number of neighbors. We will see that, while this principle holds true in different models of coupled phase oscillators, the same does not apply when the unit dynamics is more complex, as, for instance, it happens in the case of chaotic oscillators discussed in Section 4.3. After deriving the conditions guaranteeing convergence of the protocol, the Authors of Ref. [110] discuss a series of numerical results showing that the metric-topological model conjugates the advantages of the metric and topological interaction models and has the fastest convergent rate and smallest heading difference for different interaction ranges. We conclude this section remarking that there is a huge literature on flocking control, which goes beyond the purpose of this section whose aim was instead to hallmark the relationship between it and the model of coupled phase oscillators in temporal proximity graphs.</p>
        <p>Several complex phenomena in nature triggered the definition of models in which mobile phase oscillators have a two-way interaction. Most of the examples in the literature, in fact, deal with the influence of motion on the oscillator's phases, whereas a limited effort has been devoted to the converse phenomenon, that is, how the oscillator phase can influence motion. For example, it is well known that various animal species, such as frogs, crickets, and katydids, tend to synchronize their sound emissions [275][276][277][278]. However, whether the characteristics of such sound collective phenomena may drive collective or individual motion phenomena is still an open question. Motivating examples toward a mathematical modeling of such phenomena come from the physics of magnetic colloids [279][280][281] and the microfluidic mixtures of active spinners [282,283], where particles or spinners tend to attract or repel each other, based on their orientation. In the biological realm, a population of myxobacteria that exhibit a bidirectional coupling between spatial and phase dynamics has been observed and modeled by Igoshin and colleagues in Ref. [284]. Also Tanaka et al. in Ref. [9] laid the foundations of such investigations, considering chemotactic oscillators whose movements in space are mediated by the diffusion of a background chemical. These observations call for a new modeling paradigm, introduced by O'Keeffe and colleagues, called swarmalators, i.e., oscillators that synchronize and swarm [285]. Despite a simple mathematical formulation, the model exhibits an ample range of complex phenomena that can parallel relevant natural and physical phenomena.Several complex phenomena in nature triggered the definition of models in which mobile phase oscillators have a two-way interaction. Most of the examples in the literature, in fact, deal with the influence of motion on the oscillator's phases, whereas a limited effort has been devoted to the converse phenomenon, that is, how the oscillator phase can influence motion. For example, it is well known that various animal species, such as frogs, crickets, and katydids, tend to synchronize their sound emissions [275][276][277][278]. However, whether the characteristics of such sound collective phenomena may drive collective or individual motion phenomena is still an open question. Motivating examples toward a mathematical modeling of such phenomena come from the physics of magnetic colloids [279][280][281] and the microfluidic mixtures of active spinners [282,283], where particles or spinners tend to attract or repel each other, based on their orientation. In the biological realm, a population of myxobacteria that exhibit a bidirectional coupling between spatial and phase dynamics has been observed and modeled by Igoshin and colleagues in Ref. [284]. Also Tanaka et al. in Ref. [9] laid the foundations of such investigations, considering chemotactic oscillators whose movements in space are mediated by the diffusion of a background chemical. These observations call for a new modeling paradigm, introduced by O'Keeffe and colleagues, called swarmalators, i.e., oscillators that synchronize and swarm [285]. Despite a simple mathematical formulation, the model exhibits an ample range of complex phenomena that can parallel relevant natural and physical phenomena.</p>
        <p>The model comprises N oscillators, indexed with i = 1, 2, . . . , N, endowed with phase φ i and natural frequency ω i . Each oscillator occupies position x i = (x i , y i ) and moves with velocity v i .The model comprises N oscillators, indexed with i = 1, 2, . . . , N, endowed with phase φ i and natural frequency ω i . Each oscillator occupies position x i = (x i , y i ) and moves with velocity v i .</p>
        <p>The system dynamics is modeled asThe system dynamics is modeled as</p>
        <p>Functions I att and I rep model spatial attraction and repulsion between swarmalators, respectively. Function H att , on the other hand, models the phase interaction. Finally, function F in Eq. ( 88) quantifies the influence of phase similarity on spatial attraction and function G in Eq. ( 89) quantifies the influence of spatial proximity on phase attraction. Even a simplified version of the model can lead to a reach portfolio of complex phenomena. Let us consider for exampleFunctions I att and I rep model spatial attraction and repulsion between swarmalators, respectively. Function H att , on the other hand, models the phase interaction. Finally, function F in Eq. ( 88) quantifies the influence of phase similarity on spatial attraction and function G in Eq. ( 89) quantifies the influence of spatial proximity on phase attraction. Even a simplified version of the model can lead to a reach portfolio of complex phenomena. Let us consider for example</p>
        <p>Formulating the additional assumptions of identical swarmalators (ω i = ω and v i = v), a common propulsion velocity with constant magnitude and direction v = v 0 n, where n is a constant vector, choosing a reference frame such that ω = v 0 = 0, and rescaling time and space such that A = B = 1, the system behavior can be parameterized according to the values of J and K . These parameters regulate the interplay between motion and synchronization. In particular, parameter K regulates the strength of the phase difference, and parameter J regulates the relationship between phase similarity and spatial attraction. Positive values of J induce behaviors in which swarmalators tend to come closer in space to those with similar phases, whereas the opposite phenomenon occurs for negative values of J.Formulating the additional assumptions of identical swarmalators (ω i = ω and v i = v), a common propulsion velocity with constant magnitude and direction v = v 0 n, where n is a constant vector, choosing a reference frame such that ω = v 0 = 0, and rescaling time and space such that A = B = 1, the system behavior can be parameterized according to the values of J and K . These parameters regulate the interplay between motion and synchronization. In particular, parameter K regulates the strength of the phase difference, and parameter J regulates the relationship between phase similarity and spatial attraction. Positive values of J induce behaviors in which swarmalators tend to come closer in space to those with similar phases, whereas the opposite phenomenon occurs for negative values of J.</p>
        <p>Even in this simplified setting, the swarmalator system described by Eqs. ( 90)-( 91) exhibit five macroscopic behaviors, or states, corresponding to different areas of the (K , J) parameter space, illustrated in Fig. 27. Among these five states, three are ultimately static in space and phase, whereas in the remaining two, the swarmalators move.Even in this simplified setting, the swarmalator system described by Eqs. ( 90)-( 91) exhibit five macroscopic behaviors, or states, corresponding to different areas of the (K , J) parameter space, illustrated in Fig. 27. Among these five states, three are ultimately static in space and phase, whereas in the remaining two, the swarmalators move.</p>
        <p>The five states (where the first three states are static, and the last two comprise moving swarmalators) can be summarized as follows: 90)-( 91), in the (K , J) parameter space.The five states (where the first three states are static, and the last two comprise moving swarmalators) can be summarized as follows: 90)-( 91), in the (K , J) parameter space.</p>
        <p>Source: Reprinted with permission from Ref. [285].Source: Reprinted with permission from Ref. [285].</p>
        <p>• Static synchrony: The swarmalators are distributed in space according to a circularly symmetric, crystal-like 50 formation, and are fully synchronized in phase. This state occurs for K &gt; 0 and all J.• Static synchrony: The swarmalators are distributed in space according to a circularly symmetric, crystal-like 50 formation, and are fully synchronized in phase. This state occurs for K &gt; 0 and all J.</p>
        <p>• Static asynchrony: The swarmalators are uniformly distributed in space, and can assume any phase everywhere in the spaces (indeed, the phase distribution results uniform over the space). This state occurs for J &lt; 0 and K &lt; 0, and also for J &gt; 0, provided that it lies in the wedge J &lt; |K c |, where K c is derived in Ref. [285] through a semianalytical approximation.• Static asynchrony: The swarmalators are uniformly distributed in space, and can assume any phase everywhere in the spaces (indeed, the phase distribution results uniform over the space). This state occurs for J &lt; 0 and K &lt; 0, and also for J &gt; 0, provided that it lies in the wedge J &lt; |K c |, where K c is derived in Ref. [285] through a semianalytical approximation.</p>
        <p>• Static phase wave: In the last stationary state, the swarmalators' phases are frozen at their initial values. This state occurs for K = 0 and J &gt; 0, implying that swarmalators tend to settle closer to others with similar phases. This, in turn, results in an annular structure where the spatial angle of each swarmalator is perfectly correlated with its phase φ.• Static phase wave: In the last stationary state, the swarmalators' phases are frozen at their initial values. This state occurs for K = 0 and J &gt; 0, implying that swarmalators tend to settle closer to others with similar phases. This, in turn, results in an annular structure where the spatial angle of each swarmalator is perfectly correlated with its phase φ.</p>
        <p>• Splintered phase wave: The transition to K &lt; 0 leads to the regimes where swarmalators move in space. A static phase wave is observed along the space, which splinters into disconnected clusters, each characterized by a distinct phase. Within each cluster, swarmalators execute rapid motions oscillating both in position and in phase about their mean values. The role of the parameters in shaping this regime is still unclear and deserves further investigations, see Ref. [285].• Splintered phase wave: The transition to K &lt; 0 leads to the regimes where swarmalators move in space. A static phase wave is observed along the space, which splinters into disconnected clusters, each characterized by a distinct phase. Within each cluster, swarmalators execute rapid motions oscillating both in position and in phase about their mean values. The role of the parameters in shaping this regime is still unclear and deserves further investigations, see Ref. [285].</p>
        <p>• Active phase wave: With further decreases in K &lt; 0, swarmalators tend to execute regular cycles both in spatial angles and in phase. Further oscillations are revealed along the radial position, where each swarmalator travels back and forth from the inside to the outside of the global annular configuration, while orbiting around the annulus. Natural phenomena exhibiting states similar to the active phase state are double milling states in biological swarms [286] and vortex arrays observed in groups of sperm [287].• Active phase wave: With further decreases in K &lt; 0, swarmalators tend to execute regular cycles both in spatial angles and in phase. Further oscillations are revealed along the radial position, where each swarmalator travels back and forth from the inside to the outside of the global annular configuration, while orbiting around the annulus. Natural phenomena exhibiting states similar to the active phase state are double milling states in biological swarms [286] and vortex arrays observed in groups of sperm [287].</p>
        <p>The swarmalator model keeps its characteristics also when different, and more generic functions I att , I rep , and G are selected, thus highlighting the genericity of the model in describing models of dynamical systems that may synchronize and swarm at the same time, and where these two phenomena are intertwined. Qualitatively similar results are also found when some degree of heterogeneity among the swarmalator population is introduced in Ref. [285].The swarmalator model keeps its characteristics also when different, and more generic functions I att , I rep , and G are selected, thus highlighting the genericity of the model in describing models of dynamical systems that may synchronize and swarm at the same time, and where these two phenomena are intertwined. Qualitatively similar results are also found when some degree of heterogeneity among the swarmalator population is introduced in Ref. [285].</p>
        <p>In many natural and artificial systems, interactions do not occur continuously, but only at discrete time instants. Consider, for instance, an ensemble of neurons in the human brain or a swarm of flashing fireflies; they are both examples of biological systems where the units communicate through short pulses to synchronize their activity [288,289]. This type of coupling may also be found in engineered systems. An example is synchronization of the clocks embedded in a sensor network that may be efficiently achieved operating exclusively at the physical layer by transmitting pulses rather than packet messages [290][291][292]. All these systems may be conveniently modeled by pulse-coupled oscillators [293].In many natural and artificial systems, interactions do not occur continuously, but only at discrete time instants. Consider, for instance, an ensemble of neurons in the human brain or a swarm of flashing fireflies; they are both examples of biological systems where the units communicate through short pulses to synchronize their activity [288,289]. This type of coupling may also be found in engineered systems. An example is synchronization of the clocks embedded in a sensor network that may be efficiently achieved operating exclusively at the physical layer by transmitting pulses rather than packet messages [290][291][292]. All these systems may be conveniently modeled by pulse-coupled oscillators [293].</p>
        <p>Pulse-coupled oscillators are limit cycle oscillators coupled through the links of a network through which they exchange pulses at discrete time instants. In this section, we review the relevant literature concerning synchronization in these systems in the case that the network is time-varying, and is obtained as the result of agent motion.Pulse-coupled oscillators are limit cycle oscillators coupled through the links of a network through which they exchange pulses at discrete time instants. In this section, we review the relevant literature concerning synchronization in these systems in the case that the network is time-varying, and is obtained as the result of agent motion.</p>
        <p>Let us begin with the model discussed in Ref. [290], where the agents are described by a phase variable φ i (t) ∈ [0, 1], i = 1, 2, . . . , N, whose dynamics is given by φi = where T is the pulse period. In addition, a state variable x i (t) ∈ [0, 1], function of the oscillator phase, is associated with 37 each oscillator:Let us begin with the model discussed in Ref. [290], where the agents are described by a phase variable φ i (t) ∈ [0, 1], i = 1, 2, . . . , N, whose dynamics is given by φi = where T is the pulse period. In addition, a state variable x i (t) ∈ [0, 1], function of the oscillator phase, is associated with 37 each oscillator:</p>
        <p>where f : [0, 1] → [0, 1] is a smooth, monotonically increasing and concave down function, such that x i = 0 when φ i = 0 and x i = 1 when φ i = 1. Agents move as random walkers (see Eqs. ( 3)) and interact according to the temporal proximity graph. In particular, suppose that at time t the phase and state variables of oscillator i become equal to 1 and indicate with N i (t) the set of agents within distance R from i at this time. Then, the agent i will send to all its neighbors (j ∈ N i (t)) a signal pulse that will immediately increase by a quantity ϵ (equal to the coupling strength) the variable x j . At the same time the pulse firing will reset the phase and state variables of oscillator i. Hence, we have:where f : [0, 1] → [0, 1] is a smooth, monotonically increasing and concave down function, such that x i = 0 when φ i = 0 and x i = 1 when φ i = 1. Agents move as random walkers (see Eqs. ( 3)) and interact according to the temporal proximity graph. In particular, suppose that at time t the phase and state variables of oscillator i become equal to 1 and indicate with N i (t) the set of agents within distance R from i at this time. Then, the agent i will send to all its neighbors (j ∈ N i (t)) a signal pulse that will immediately increase by a quantity ϵ (equal to the coupling strength) the variable x j . At the same time the pulse firing will reset the phase and state variables of oscillator i. Hence, we have:</p>
        <p>For this model, there is a very interesting result that parallels a fundamental theorem derived for pulse-coupled oscillators in the case of static, full (i.e., all-to-all) connectivity [293]. Mirollo and Strogatz have, in fact, found that if a system of statically pulse-coupled oscillators has identical, smooth, monotonically increasing and concave down state functions, identical and instantaneous coupling, and full connectivity, then for any coupling strength ϵ &gt; 0, the set of initial states that do not lead to synchronization has zero measure [293]. Noticeably, the conditions on the state functions are very mild, and make the result applicable to many scenarios.For this model, there is a very interesting result that parallels a fundamental theorem derived for pulse-coupled oscillators in the case of static, full (i.e., all-to-all) connectivity [293]. Mirollo and Strogatz have, in fact, found that if a system of statically pulse-coupled oscillators has identical, smooth, monotonically increasing and concave down state functions, identical and instantaneous coupling, and full connectivity, then for any coupling strength ϵ &gt; 0, the set of initial states that do not lead to synchronization has zero measure [293]. Noticeably, the conditions on the state functions are very mild, and make the result applicable to many scenarios.</p>
        <p>The Authors of Ref. [290] have discovered that a very similar result holds for moving pulse-coupled oscillators. In fact, if the pulse-coupled oscillators move according to the random walker model (3), are characterized by identical, smooth, monotonically increasing and concave down state functions, identical and instantaneous coupling, and are linked each other according to a temporal proximity graph that is connected at any time, then the set of initial states that do not lead to synchronization has zero measure. Again the hypotheses on the state function are very mild and, hence, the result is quite general. It is also interesting to observe that for fully connected temporal proximity graphs one recovers the result valid for static networks. Finally, we notice that the conditions are sufficient, but not necessary for synchronization, as shown by the numerical examples given in Ref. [290], which prove that synchronization in a system of moving pulse-coupled oscillators can be achieved even if the network is not connected at any time.The Authors of Ref. [290] have discovered that a very similar result holds for moving pulse-coupled oscillators. In fact, if the pulse-coupled oscillators move according to the random walker model (3), are characterized by identical, smooth, monotonically increasing and concave down state functions, identical and instantaneous coupling, and are linked each other according to a temporal proximity graph that is connected at any time, then the set of initial states that do not lead to synchronization has zero measure. Again the hypotheses on the state function are very mild and, hence, the result is quite general. It is also interesting to observe that for fully connected temporal proximity graphs one recovers the result valid for static networks. Finally, we notice that the conditions are sufficient, but not necessary for synchronization, as shown by the numerical examples given in Ref. [290], which prove that synchronization in a system of moving pulse-coupled oscillators can be achieved even if the network is not connected at any time.</p>
        <p>The numerical simulations discussed in Ref. [290] reveal another important feature of the system: the synchronization time is not monotonic with respect to the coupling strength. In fact, while it first decreases with ϵ, then it increases, before finally decreasing with further increasing coupling strengths. A similar non-monotonic, and somewhat unexpected, dependence of the synchronization time on some of the system parameters, in particular agent speed, is observed in other setups of moving pulse-coupled oscillators [294][295][296][297] and worth to be discussed here in more detail.The numerical simulations discussed in Ref. [290] reveal another important feature of the system: the synchronization time is not monotonic with respect to the coupling strength. In fact, while it first decreases with ϵ, then it increases, before finally decreasing with further increasing coupling strengths. A similar non-monotonic, and somewhat unexpected, dependence of the synchronization time on some of the system parameters, in particular agent speed, is observed in other setups of moving pulse-coupled oscillators [294][295][296][297] and worth to be discussed here in more detail.</p>
        <p>To elucidate this aspect, let us consider the model with a minimal interaction rule discussed in Ref. [295]. The model consists of N integrate-and-fire oscillators characterized by a phase φ i (t) ∈ [0, 1] with dynamics as in Eq. (92). A firing event occurs when the phase reaches its maximum value, after which the phase of the agent is reset and a pulse of strength ϵ is sent to the nearest neighbor oscillator, labeled as nn, that is:To elucidate this aspect, let us consider the model with a minimal interaction rule discussed in Ref. [295]. The model consists of N integrate-and-fire oscillators characterized by a phase φ i (t) ∈ [0, 1] with dynamics as in Eq. (92). A firing event occurs when the phase reaches its maximum value, after which the phase of the agent is reset and a pulse of strength ϵ is sent to the nearest neighbor oscillator, labeled as nn, that is:</p>
        <p>In addition, upon the firing event, the heading of the nearest neighbor agent, θ nn (t), is also randomly updated according to the random walker model (3).In addition, upon the firing event, the heading of the nearest neighbor agent, θ nn (t), is also randomly updated according to the random walker model (3).</p>
        <p>As in this system synchronization occurs through a series of firing events at discrete times, the Authors of Ref. [295] define the synchronization time T sync as the number of cycles that a reference oscillator requires to enter the synchronized state, a definition in practice analogous to the parameter n T for continuously-coupled phase oscillators (Section 4.1.3). By monitoring this parameter as a function of the agent speed v, three regions are found (see Fig. 28 for an example): two regions of synchronization separated by an interval of values of v for which synchronization is impossible. The behavior of T sync with v is, therefore, highly non-monotonic with a region where increasing the agent speed, rather than favoring synchronization, on the contrary hinders it.As in this system synchronization occurs through a series of firing events at discrete times, the Authors of Ref. [295] define the synchronization time T sync as the number of cycles that a reference oscillator requires to enter the synchronized state, a definition in practice analogous to the parameter n T for continuously-coupled phase oscillators (Section 4.1.3). By monitoring this parameter as a function of the agent speed v, three regions are found (see Fig. 28 for an example): two regions of synchronization separated by an interval of values of v for which synchronization is impossible. The behavior of T sync with v is, therefore, highly non-monotonic with a region where increasing the agent speed, rather than favoring synchronization, on the contrary hinders it.</p>
        <p>In the regions of synchronization, however, two different mechanisms are at work. In the one on the left, for v &lt; v s , oscillators move slowly and, hence, tend to have the same neighbors for a long time. Under these conditions local synchronization is favored, and global synchronization requires a large number of firings and changes of topology. In the region on the right, for v &gt; v f , the fast switching approximation holds, and T sync becomes independent of v. Here, the oscillators fire at quickly-changing neighbors in such a way that they promote synchronization simultaneously with all the units of the system. Close to the boundaries with the other region, these mechanisms become less effective, until they become totally ineffective and synchronization is prohibited.In the regions of synchronization, however, two different mechanisms are at work. In the one on the left, for v &lt; v s , oscillators move slowly and, hence, tend to have the same neighbors for a long time. Under these conditions local synchronization is favored, and global synchronization requires a large number of firings and changes of topology. In the region on the right, for v &gt; v f , the fast switching approximation holds, and T sync becomes independent of v. Here, the oscillators fire at quickly-changing neighbors in such a way that they promote synchronization simultaneously with all the units of the system. Close to the boundaries with the other region, these mechanisms become less effective, until they become totally ineffective and synchronization is prohibited.</p>
        <p>The presence of a region in which the rate of change of the topology prevents synchronization is also found when a more general interaction scheme is adopted, as in Ref. [296]. Similarly to Ref. [295], the model investigated in Ref. [296] also considers mobile pulse-coupled integrate-and-fire oscillators. However, at variance of Ref. [295], interactions are not limited to a single neighbor, but account for a number K of nearest neighbors. In addition, also cone-vision connectivity 55 is studied. In this case, agent j is considered to be in contact with agent i if ∥y i -y j ∥ ≤ R andThe presence of a region in which the rate of change of the topology prevents synchronization is also found when a more general interaction scheme is adopted, as in Ref. [296]. Similarly to Ref. [295], the model investigated in Ref. [296] also considers mobile pulse-coupled integrate-and-fire oscillators. However, at variance of Ref. [295], interactions are not limited to a single neighbor, but account for a number K of nearest neighbors. In addition, also cone-vision connectivity 55 is studied. In this case, agent j is considered to be in contact with agent i if ∥y i -y j ∥ ≤ R and</p>
        <p>, where, where</p>
        <p>Θ is the cone vision angle. Finally, in view of a practical implementation of the system, the model does not assume cyclic boundary conditions, but a bounded environment, where each unit moves in a straight line until it reaches the arena boundary where it changes its heading to a new random value. With this general setup, several important features of moving pulse-coupled oscillators are revealed. First, the type of phase-response curve used for coupling has an important effect on the behavior of the synchronization time as a function of agent speed. While a non-monotonic behavior appears for a multiplicative phase-response curve, for other types of coupling functions the relations seem to be monotonic.Θ is the cone vision angle. Finally, in view of a practical implementation of the system, the model does not assume cyclic boundary conditions, but a bounded environment, where each unit moves in a straight line until it reaches the arena boundary where it changes its heading to a new random value. With this general setup, several important features of moving pulse-coupled oscillators are revealed. First, the type of phase-response curve used for coupling has an important effect on the behavior of the synchronization time as a function of agent speed. While a non-monotonic behavior appears for a multiplicative phase-response curve, for other types of coupling functions the relations seem to be monotonic.</p>
        <p>The type of the neighborhood model, as well as the exact number of neighbors considered, i.e., K in the K-nearest neighborhood interaction scheme, are also able to modulate the dependence of T sync on v. An example is shown in Fig. 29 which shows the synchronization time as a function of the agent speed for a system of N = 20 agents at different values of K . Notice that the region where synchronization is totally inhibited by the mobility, found for K = 1, disappears for the other values of K . For K = 2 the behavior of T sync with v is still non-monotonic, but for larger values it becomes monotonic. Similarly, for the cone-vision interaction scheme one finds either regions with a monotonic or a non-monotonic behavior, the latter typically arising for small sizes of the cone of vision, with a non-trivial interplay between the radius and the angle of the cone. An explanation of the non-monotonic behavior in terms of the typical timescales of the system is given in Ref. [297]. The system is, in fact, characterized by two time scales, that of the motion of the units and that of the synchronization of local clusters. When the pattern of interactions is sparse and nonreciprocal and the coupling is highly nonlinear, these two processes can interfere in a destructive manner. In fact, when the typical time between two consecutive changes in the connectivity becomes comparable with the time scale of local synchronization, then, on the one hand, groups of neighboring units are broken before they may synchronize, and, on the other hand, the interactions are not rewired fast enough to promote the mechanism needed for global synchronization.The type of the neighborhood model, as well as the exact number of neighbors considered, i.e., K in the K-nearest neighborhood interaction scheme, are also able to modulate the dependence of T sync on v. An example is shown in Fig. 29 which shows the synchronization time as a function of the agent speed for a system of N = 20 agents at different values of K . Notice that the region where synchronization is totally inhibited by the mobility, found for K = 1, disappears for the other values of K . For K = 2 the behavior of T sync with v is still non-monotonic, but for larger values it becomes monotonic. Similarly, for the cone-vision interaction scheme one finds either regions with a monotonic or a non-monotonic behavior, the latter typically arising for small sizes of the cone of vision, with a non-trivial interplay between the radius and the angle of the cone. An explanation of the non-monotonic behavior in terms of the typical timescales of the system is given in Ref. [297]. The system is, in fact, characterized by two time scales, that of the motion of the units and that of the synchronization of local clusters. When the pattern of interactions is sparse and nonreciprocal and the coupling is highly nonlinear, these two processes can interfere in a destructive manner. In fact, when the typical time between two consecutive changes in the connectivity becomes comparable with the time scale of local synchronization, then, on the one hand, groups of neighboring units are broken before they may synchronize, and, on the other hand, the interactions are not rewired fast enough to promote the mechanism needed for global synchronization.</p>
        <p>The models discussed in this section have been also validated experimentally. In particular, a practical implementation 24 of the cone-vision interaction scheme based on mobile robots, communicating each other by emitting light pulses, has been proposed in Ref. [296]. The Authors have found experimental evidence of the non-monotonic behavior of the synchronization time, demonstrating that by controlling the agent speed the system may be switched from a synchronous state to a desynchronized state and viceversa. This may be of practical interest in multi-robot teams that are required to switch between different tasks, one associated to synchronization in which the activities of teams need to be performed at the same time, and one associated to desynchronization, in which the individual activities need to be distributed over time.The models discussed in this section have been also validated experimentally. In particular, a practical implementation 24 of the cone-vision interaction scheme based on mobile robots, communicating each other by emitting light pulses, has been proposed in Ref. [296]. The Authors have found experimental evidence of the non-monotonic behavior of the synchronization time, demonstrating that by controlling the agent speed the system may be switched from a synchronous state to a desynchronized state and viceversa. This may be of practical interest in multi-robot teams that are required to switch between different tasks, one associated to synchronization in which the activities of teams need to be performed at the same time, and one associated to desynchronization, in which the individual activities need to be distributed over time.</p>
        <p>We now move to consider the case of multi-agent systems of limit cycle and chaotic oscillators that can be dealt with the same formalism. We will mostly refer to chaotic systems, as they represent the more general case, but keeping in mind that the results apply for both limit cycle and chaotic oscillators.We now move to consider the case of multi-agent systems of limit cycle and chaotic oscillators that can be dealt with the same formalism. We will mostly refer to chaotic systems, as they represent the more general case, but keeping in mind that the results apply for both limit cycle and chaotic oscillators.</p>
        <p>The model we start with (see Ref. [78]) consists of N agents that are free to move in a two-dimensional space and interact with each other on the basis of a proximity graph, that is, two agents interact only if their reciprocal distance is smaller than the interaction radius R. Each agent i = 1, . . . , N is characterized by a dynamical state x i (t) ∈ R n , which represents the state vector of the oscillator associated to the agent. This model may find application in the study of clock synchronization in mobile robots [298] and in sensor networks with limited communication [299], task coordination in swarming animals, or synchronized bulk oscillations in populations of yeast cells [300][301][302].The model we start with (see Ref. [78]) consists of N agents that are free to move in a two-dimensional space and interact with each other on the basis of a proximity graph, that is, two agents interact only if their reciprocal distance is smaller than the interaction radius R. Each agent i = 1, . . . , N is characterized by a dynamical state x i (t) ∈ R n , which represents the state vector of the oscillator associated to the agent. This model may find application in the study of clock synchronization in mobile robots [298] and in sensor networks with limited communication [299], task coordination in swarming animals, or synchronized bulk oscillations in populations of yeast cells [300][301][302].</p>
        <p>The agent dynamics is described byThe agent dynamics is described by</p>
        <p>for i = 1, . . . , N. Here, f is the uncoupled dynamics, B ∈ R n×n the inner coupling matrix, and ϵ the coupling strength. In Eq. ( 96), the time-varying pattern of interactions among agents is encoded in the coefficients A ij (t) of the adjacency matrix A(t) that is function of time t. We suppose that the interactions are mutual, so that at each time instant the adjacency matrix is symmetric. The coefficients are, therefore, defined as A ij (t) = A ji (t) = 1 if i and j are connected by a link at time t, while A ij (t) = A ji (t) = 0 otherwise, and A ii (t) = 0. Equivalently, model ( 96) may be reformulated by using the Laplacian L(t) as follows:for i = 1, . . . , N. Here, f is the uncoupled dynamics, B ∈ R n×n the inner coupling matrix, and ϵ the coupling strength. In Eq. ( 96), the time-varying pattern of interactions among agents is encoded in the coefficients A ij (t) of the adjacency matrix A(t) that is function of time t. We suppose that the interactions are mutual, so that at each time instant the adjacency matrix is symmetric. The coefficients are, therefore, defined as A ij (t) = A ji (t) = 1 if i and j are connected by a link at time t, while A ij (t) = A ji (t) = 0 otherwise, and A ii (t) = 0. Equivalently, model ( 96) may be reformulated by using the Laplacian L(t) as follows:</p>
        <p>The time evolution of A(t) or L(t) is determined by the motion of the agents. They are considered to lie in a planar space of size L and periodic boundary conditions. Each agent moves with velocity v i (t) and direction of motion θ i (t), i.e., v i (t) = ve ιθ i (t) . We assume that the modulus of the agent velocity v is constant in time and equal for all the agents. Let us now discuss the model for agent motion. Following the notation of Section 2.2.3 we indicate with y i (t) the position of the ith agent in the plane at time t and assume that agents perform a random walk on the plane, as in Eqs. (3) and incorporate in the model the possibility of long-distance jumps by including the parameter p j , representing the probability for an individual to jump into a completely random new position. Thus, the position of each agent is updated according integrating Eqs. (3) with probability 1 -p j , or to random coordinates in the plane with probability p j . We will show below that the parameter p j plays a fundamental role for the system behavior.The time evolution of A(t) or L(t) is determined by the motion of the agents. They are considered to lie in a planar space of size L and periodic boundary conditions. Each agent moves with velocity v i (t) and direction of motion θ i (t), i.e., v i (t) = ve ιθ i (t) . We assume that the modulus of the agent velocity v is constant in time and equal for all the agents. Let us now discuss the model for agent motion. Following the notation of Section 2.2.3 we indicate with y i (t) the position of the ith agent in the plane at time t and assume that agents perform a random walk on the plane, as in Eqs. (3) and incorporate in the model the possibility of long-distance jumps by including the parameter p j , representing the probability for an individual to jump into a completely random new position. Thus, the position of each agent is updated according integrating Eqs. (3) with probability 1 -p j , or to random coordinates in the plane with probability p j . We will show below that the parameter p j plays a fundamental role for the system behavior.</p>
        <p>From the positions of the agents at each time instant the coefficients of the time-varying adjacency matrix of the proximity graph are calculated. In particular, we have thatFrom the positions of the agents at each time instant the coefficients of the time-varying adjacency matrix of the proximity graph are calculated. In particular, we have that</p>
        <p>It is here useful to begin the analysis of the model by considering the assumption that the switching among the possible topologies occurs at a fast time scale. Under the hypothesis of fast switching, the stability of the synchronization manifold of the time-varying network can be studied by applying Lemma 1 of Section 3.1 and, hence, by inspecting the properties of its time average.It is here useful to begin the analysis of the model by considering the assumption that the switching among the possible topologies occurs at a fast time scale. Under the hypothesis of fast switching, the stability of the synchronization manifold of the time-varying network can be studied by applying Lemma 1 of Section 3.1 and, hence, by inspecting the properties of its time average.</p>
        <p>According to this result (Ref. [55]), when the switching is fast, the synchronous manifold in model ( 97) is stable if the system of coupled oscillators described byAccording to this result (Ref. [55]), when the switching is fast, the synchronous manifold in model ( 97) is stable if the system of coupled oscillators described by</p>
        <p>supports a stable synchronization manifold and there exists a constant T such that 1 T ∫ t+ T t L(τ )dτ = L. The network in Eqs. ( 98) is static and, therefore, its synchronization properties can be studied with the classical MSF approach. Thus, to perform the study of the stability of synchronization of the time-varying network inherited by mobile agents, it suffices to calculate L.supports a stable synchronization manifold and there exists a constant T such that 1 T ∫ t+ T t L(τ )dτ = L. The network in Eqs. ( 98) is static and, therefore, its synchronization properties can be studied with the classical MSF approach. Thus, to perform the study of the stability of synchronization of the time-varying network inherited by mobile agents, it suffices to calculate L.</p>
        <p>⟩ T vs. density ρ for identical (continuous line) and non-identical (dotted line) Rössler oscillators with N = 2, N = 10, and N = 100. The coupling is fixed to ϵ = 10, and the other parameters to τ M = 10 -3 , v = 1, R = 1. Results are averaged over 50 realizations. Source: Reprinted figure with permission from Ref. [78].⟩ T vs. density ρ for identical (continuous line) and non-identical (dotted line) Rössler oscillators with N = 2, N = 10, and N = 100. The coupling is fixed to ϵ = 10, and the other parameters to τ M = 10 -3 , v = 1, R = 1. Results are averaged over 50 realizations. Source: Reprinted figure with permission from Ref. [78].</p>
        <p>© 2008 by the American Physical Society. This calculation (detailed in Ref. [78]) gives L = π r 2 ρ N L K , where ρ = N L 2 is the agent density and L K the Laplacian 50 matrix of a complete graph K. The term πr 2 ρ N represents the probability, in the limit of fast switching, that two agents are neighbors. In fact, in this limit the agents at each time instant occupy random positions in the space and the probability that a link exists is simply the fraction of the total area that is overlapped by the sensing region, that is, π r 2© 2008 by the American Physical Society. This calculation (detailed in Ref. [78]) gives L = π r 2 ρ N L K , where ρ = N L 2 is the agent density and L K the Laplacian 50 matrix of a complete graph K. The term πr 2 ρ N represents the probability, in the limit of fast switching, that two agents are neighbors. In fact, in this limit the agents at each time instant occupy random positions in the space and the probability that a link exists is simply the fraction of the total area that is overlapped by the sensing region, that is, π r 2</p>
        <p>Suppose now to consider a system having type III MSF (see Section 3.1 and Ref. [3]) with thresholds α 1 and α 2 . Since the eigenvalues of the Laplacian matrix of the complete graph are γ i = N for i = 2, . . . , N, one can derive that α 1 &lt; σ πr 2 ρ &lt; α 2 , (99) and thusSuppose now to consider a system having type III MSF (see Section 3.1 and Ref. [3]) with thresholds α 1 and α 2 . Since the eigenvalues of the Laplacian matrix of the complete graph are γ i = N for i = 2, . . . , N, one can derive that α 1 &lt; σ πr 2 ρ &lt; α 2 , (99) and thus</p>
        <p>which expresses the fact that for type III MSF systems synchronization is attained if the density is in the intervalwhich expresses the fact that for type III MSF systems synchronization is attained if the density is in the interval</p>
        <p>]. With similar arguments, the condition for synchronization in type II systems can be obtained. In this case, one has that ρ &gt; α 1 π r 2 σ . This prediction by the model is particularly interesting as it is in agreement with what observed in some real systems, for instance in yeast cell populations where sustained oscillations emerge provided that the cell density is sufficiently high [300,301]. Fig. 30 illustrates an example of synchronization in a system of moving Rössler type III oscillators in the regime of fast switching, showing that, as predicted by Eq. ( 100), the thresholds for synchronization do not depend on N. Fig. 30 also shows that, while the theoretical predictions rigorously apply only to the case of identical oscillators, they still capture the behavior for non-identical agents.]. With similar arguments, the condition for synchronization in type II systems can be obtained. In this case, one has that ρ &gt; α 1 π r 2 σ . This prediction by the model is particularly interesting as it is in agreement with what observed in some real systems, for instance in yeast cell populations where sustained oscillations emerge provided that the cell density is sufficiently high [300,301]. Fig. 30 illustrates an example of synchronization in a system of moving Rössler type III oscillators in the regime of fast switching, showing that, as predicted by Eq. ( 100), the thresholds for synchronization do not depend on N. Fig. 30 also shows that, while the theoretical predictions rigorously apply only to the case of identical oscillators, they still capture the behavior for non-identical agents.</p>
        <p>According to the previous analysis, the density is a fundamental parameter to determine the conditions for synchronization. However, the other parameters are also important as they determine how fast is the switching among the possible topologies of interactions. Both the agent velocity ν and the jumping probability p j , for instance, increase the likelihood of the fast switching assumption, as they both increase the average distance covered at each motion step, resulting in a faster switching among the possible interaction configurations.According to the previous analysis, the density is a fundamental parameter to determine the conditions for synchronization. However, the other parameters are also important as they determine how fast is the switching among the possible topologies of interactions. Both the agent velocity ν and the jumping probability p j , for instance, increase the likelihood of the fast switching assumption, as they both increase the average distance covered at each motion step, resulting in a faster switching among the possible interaction configurations.</p>
        <p>The model discussed in this section has been extended in a three-dimensional space in Ref. [79]. Interestingly, under the hypothesis of fast switching, arguments similar to the two-dimensional case can be applied to derive the expression for the time-average Laplacian matrix L that generalizes the one holding for motion in a plane. In fact, the Authors of Ref. [79] show that it is given by:The model discussed in this section has been extended in a three-dimensional space in Ref. [79]. Interestingly, under the hypothesis of fast switching, arguments similar to the two-dimensional case can be applied to derive the expression for the time-average Laplacian matrix L that generalizes the one holding for motion in a plane. In fact, the Authors of Ref. [79] show that it is given by:</p>
        <p>where V R is the volume associated to the sensing region and V S the volume of the whole space. Once again, the ratio V R V S represents the probability, in the fast switching limit, that two agents are neighbors and, thus, interacting.where V R is the volume associated to the sensing region and V S the volume of the whole space. Once again, the ratio V R V S represents the probability, in the fast switching limit, that two agents are neighbors and, thus, interacting.</p>
        <p>Another important remark is that the analysis based on the MSF, being based on linearization, provides a local condition for stability. The study of the behavior for initial conditions far from the synchronization manifold requires the use of other techniques. A possibility, relying on numerical simulations, is the basin stability analysis [146], which is applied to the 33 case of moving chaotic oscillators in Ref. [79]. The main idea is to simulate a large number of initial conditions, say N sims , and count how many of them converge to the synchronous manifold, say N sync . The basin stability is then calculated as BS = N sync /N sims . For large N sims , the basin stability provides a measure of the volume of the basin of attraction of the synchronization manifold. For a time-varying network of moving Rössler oscillators with type II MSF, it is found, for instance, that BS is typically low for coupling strength close to the transition to synchronization, and then approaches one for larger values.Another important remark is that the analysis based on the MSF, being based on linearization, provides a local condition for stability. The study of the behavior for initial conditions far from the synchronization manifold requires the use of other techniques. A possibility, relying on numerical simulations, is the basin stability analysis [146], which is applied to the 33 case of moving chaotic oscillators in Ref. [79]. The main idea is to simulate a large number of initial conditions, say N sims , and count how many of them converge to the synchronous manifold, say N sync . The basin stability is then calculated as BS = N sync /N sims . For large N sims , the basin stability provides a measure of the volume of the basin of attraction of the synchronization manifold. For a time-varying network of moving Rössler oscillators with type II MSF, it is found, for instance, that BS is typically low for coupling strength close to the transition to synchronization, and then approaches one for larger values.</p>
        <p>When the unit dynamics are chaotic, but discrete-time [303], the qualitative scenario observed does not significantly differ from continuous-time oscillators. In particular, the analysis based on the interplay between the diverse time scales in the system (see Ref. [264] and Section 4.1.3) can still be applied, with the different synchronization mechanisms at work when the ratio between the time scales changes. In addition, in systems of moving chaotic maps (in particular, in Ref. [303] the case of the tent map is dealt with), the inclusion of noise induces a transition between synchronization and desynchronized states for any motion rate. However, when the agent speed is low, close to the transition a switching between the quasi-synchronized and desynchronized states is found. This switching is due to the large fluctuations of the transverse Lyapunov exponent arising because of the slow changes in the interaction topology. On the contrary, when the agent speed is large, these fluctuations are small and the Lyapunov exponent converges to that predicted by the fast switching approximation.When the unit dynamics are chaotic, but discrete-time [303], the qualitative scenario observed does not significantly differ from continuous-time oscillators. In particular, the analysis based on the interplay between the diverse time scales in the system (see Ref. [264] and Section 4.1.3) can still be applied, with the different synchronization mechanisms at work when the ratio between the time scales changes. In addition, in systems of moving chaotic maps (in particular, in Ref. [303] the case of the tent map is dealt with), the inclusion of noise induces a transition between synchronization and desynchronized states for any motion rate. However, when the agent speed is low, close to the transition a switching between the quasi-synchronized and desynchronized states is found. This switching is due to the large fluctuations of the transverse Lyapunov exponent arising because of the slow changes in the interaction topology. On the contrary, when the agent speed is large, these fluctuations are small and the Lyapunov exponent converges to that predicted by the fast switching approximation.</p>
        <p>As mentioned in Section 3.2 a key result in network control is the idea of pinning control [304], a technique that makes possible steering the dynamics of the entire network towards a target behavior without requiring the application of the control action to each single node of the structure. For networks with time-varying links, such as those deriving from the interaction of mobile agents, implementing a selective pinning would require, on the one hand, a non-trivial ranking on the agent topological features, and, on the other hand, controllers moving with the pinned agents.As mentioned in Section 3.2 a key result in network control is the idea of pinning control [304], a technique that makes possible steering the dynamics of the entire network towards a target behavior without requiring the application of the control action to each single node of the structure. For networks with time-varying links, such as those deriving from the interaction of mobile agents, implementing a selective pinning would require, on the one hand, a non-trivial ranking on the agent topological features, and, on the other hand, controllers moving with the pinned agents.</p>
        <p>These considerations motivated the introduction of the concept of spatial pinning control of multi-agent systems [88]. While in static networks feedback control is only applied to a fraction of nodes, in networks of multi-agent systems the control is only applied to a limited fraction of the available space.These considerations motivated the introduction of the concept of spatial pinning control of multi-agent systems [88]. While in static networks feedback control is only applied to a fraction of nodes, in networks of multi-agent systems the control is only applied to a limited fraction of the available space.</p>
        <p>In more detail, let us consider again the multi-agent system (97) and define the control region as the square ΓIn more detail, let us consider again the multi-agent system (97) and define the control region as the square Γ</p>
        <p>We assume that control acts only on the agents that at time t enter the control region, such that the dynamics of the controlled system is given by: ẋi = f(x i ) -ϵ N ∑ j=1 L ij (t)Bx j -ϵξ i (t)κB(x i -s), (102) where κ is the strength of the control action (measured in units of the coupling coefficient), and s(t) is the reference trajectory, which obeys to ṡ = f(s). The binary variable ξ i (t) encodes the information on which agents are pinned at time t, that is, ξ i (t) = 1 if y i (t) ∈ Γ c , and ξ i (t) = 0 otherwise. The control goal is to steer the agent trajectories towards the reference one, that is, the manifold defined by x 1 (t) = • • • = x N (t) = s(t) has to be exponentially stable. To study this problem, we can define an augmented network with N + 1 units, where agent N + 1 is defined as x N+1 (t) = s(t), see Ref. [192]. This extra, virtual agent is added to the original system to represent the dynamics of the pinned controller. The equations for the augmented network read:We assume that control acts only on the agents that at time t enter the control region, such that the dynamics of the controlled system is given by: ẋi = f(x i ) -ϵ N ∑ j=1 L ij (t)Bx j -ϵξ i (t)κB(x i -s), (102) where κ is the strength of the control action (measured in units of the coupling coefficient), and s(t) is the reference trajectory, which obeys to ṡ = f(s). The binary variable ξ i (t) encodes the information on which agents are pinned at time t, that is, ξ i (t) = 1 if y i (t) ∈ Γ c , and ξ i (t) = 0 otherwise. The control goal is to steer the agent trajectories towards the reference one, that is, the manifold defined by x 1 (t) = • • • = x N (t) = s(t) has to be exponentially stable. To study this problem, we can define an augmented network with N + 1 units, where agent N + 1 is defined as x N+1 (t) = s(t), see Ref. [192]. This extra, virtual agent is added to the original system to represent the dynamics of the pinned controller. The equations for the augmented network read:</p>
        <p>where i = 1, . . . , N + 1 and M(t) = {m ij (t)} ∈ R (N+1)×(N+1) is defined as:where i = 1, . . . , N + 1 and M(t) = {m ij (t)} ∈ R (N+1)×(N+1) is defined as:</p>
        <p>-ξ 2 (t)κ . . . . . . . . .-ξ 2 (t)κ . . . . . . . . .</p>
        <p>Solving the control problem in the original multi-agent system is equivalent to find a stable synchronous manifold in the augmented network, such that the method illustrated in Section 4.3.1 can be used. Here, we skip the calculations (which are detailed in Ref. [88]) and illustrate the condition for pinning control for a system having type III MSF. In this case the reference evolution s(t) will be stable, that is, the control goal will be reached, ifSolving the control problem in the original multi-agent system is equivalent to find a stable synchronous manifold in the augmented network, such that the method illustrated in Section 4.3.1 can be used. Here, we skip the calculations (which are detailed in Ref. [88]) and illustrate the condition for pinning control for a system having type III MSF. In this case the reference evolution s(t) will be stable, that is, the control goal will be reached, if</p>
        <p>Suppose that the interaction radius R is constant, as it is limited by the communication system equipped in the agents, then one can act on the two parameters L 2 c /L 2 and ρ to achieve pinning control. Equation (105) shows that this objective Source: Reprinted figure with permission from Ref. [88].Suppose that the interaction radius R is constant, as it is limited by the communication system equipped in the agents, then one can act on the two parameters L 2 c /L 2 and ρ to achieve pinning control. Equation (105) shows that this objective Source: Reprinted figure with permission from Ref. [88].</p>
        <p>© 2012 by the American Physical Society.© 2012 by the American Physical Society.</p>
        <p>can be reached if the control region is large enough and the agent density not too high. To illustrate this result, let us 47 consider again a system of moving Rössler oscillators and assume that the goal of the control is to steer the system dynamics towards the equilibrium point s = [ 0.0057 -0.0286 0.0286 ] T . As shown in Fig. 31, there exist suitable values of the parameters (size of the control region, density, and implicitly agent speed) such that the network can be fully controlled towards a desired trajectory (in this case an equilibrium point). Furthermore, pinning control is achieved in a region of the parameter space which well corresponds to the theoretical predictions by Eq. ( 105). This region shrinks for increasing values of the density. Finally, notice that, due to the fact that the system has a type III MSF, there are portions of the diagram where increasing the area of the control region is detrimental for stability of the target equilibrium point.can be reached if the control region is large enough and the agent density not too high. To illustrate this result, let us 47 consider again a system of moving Rössler oscillators and assume that the goal of the control is to steer the system dynamics towards the equilibrium point s = [ 0.0057 -0.0286 0.0286 ] T . As shown in Fig. 31, there exist suitable values of the parameters (size of the control region, density, and implicitly agent speed) such that the network can be fully controlled towards a desired trajectory (in this case an equilibrium point). Furthermore, pinning control is achieved in a region of the parameter space which well corresponds to the theoretical predictions by Eq. ( 105). This region shrinks for increasing values of the density. Finally, notice that, due to the fact that the system has a type III MSF, there are portions of the diagram where increasing the area of the control region is detrimental for stability of the target equilibrium point.</p>
        <p>In the previous subsection, we have illustrated a technique to steer a system of moving chaotic oscillators towards the trajectory generated by a reference extra node. In particular, when this extra node is in chaotic regime, then synchronization on a reference chaotic motion is achieved. The same control goal can be reached by using another strategy [305] that originates from a different assumption on the multi-agent system. In fact, in the previous section the communication network among agents is given, whereas, as we will see below, the communication network is here supposed to be adaptive, and, therefore, tuned by the control strategy. The strategy belongs to the more general class of adaptive control methods that have been successfully applied to the control of both synchronization in networks with time-independent links [185,189] and consensus in systems of moving oscillators [306,307].In the previous subsection, we have illustrated a technique to steer a system of moving chaotic oscillators towards the trajectory generated by a reference extra node. In particular, when this extra node is in chaotic regime, then synchronization on a reference chaotic motion is achieved. The same control goal can be reached by using another strategy [305] that originates from a different assumption on the multi-agent system. In fact, in the previous section the communication network among agents is given, whereas, as we will see below, the communication network is here supposed to be adaptive, and, therefore, tuned by the control strategy. The strategy belongs to the more general class of adaptive control methods that have been successfully applied to the control of both synchronization in networks with time-independent links [185,189] and consensus in systems of moving oscillators [306,307].</p>
        <p>The main idea of the connection adaptive control is to reconfigure at each time step the interaction matrix with the following algorithm:The main idea of the connection adaptive control is to reconfigure at each time step the interaction matrix with the following algorithm:</p>
        <p>• Select an arbitrary, fixed in time, position for the extra node/agent.• Select an arbitrary, fixed in time, position for the extra node/agent.</p>
        <p>• For each agent i, at each time t, fix a radius R and calculate the agents that are at a distance smaller than R.• For each agent i, at each time t, fix a radius R and calculate the agents that are at a distance smaller than R.</p>
        <p>• Select, among the agents found at the previous step, the nearest one (labeled as j) to the extra node/agent and set L ij (t) = -1, only if the distance between agent j and the extra node is smaller than that between agent i and the extra node.• Select, among the agents found at the previous step, the nearest one (labeled as j) to the extra node/agent and set L ij (t) = -1, only if the distance between agent j and the extra node is smaller than that between agent i and the extra node.</p>
        <p>• If some of the previous steps cannot be accomplished, repeat them by selecting a larger radius. This algorithm produces a network, which is time-varying as v ̸ = 0 and has a structure characterized by a set of directed trees. In addition, the network Laplacian matrix L(t) ∈ R (N+1)×(N+1) has some special features. In fact, it can be demonstrated that at any time its spectrum is given by: λ 1 = 0 and λ i = 1 for i = 2, . . . , N + 1, see Ref. [305]. This property is particularly important to determine the condition for obtaining a stable synchronization manifold. In fact, the Authors demonstrate that the condition of stable synchronization for this time-varying structure is similar to that for a static network with the same spectrum. More specifically, for a type III MSF system it is given by:• If some of the previous steps cannot be accomplished, repeat them by selecting a larger radius. This algorithm produces a network, which is time-varying as v ̸ = 0 and has a structure characterized by a set of directed trees. In addition, the network Laplacian matrix L(t) ∈ R (N+1)×(N+1) has some special features. In fact, it can be demonstrated that at any time its spectrum is given by: λ 1 = 0 and λ i = 1 for i = 2, . . . , N + 1, see Ref. [305]. This property is particularly important to determine the condition for obtaining a stable synchronization manifold. In fact, the Authors demonstrate that the condition of stable synchronization for this time-varying structure is similar to that for a static network with the same spectrum. More specifically, for a type III MSF system it is given by:</p>
        <p>From these considerations, it follows that there exists a range of the coupling strength where the multi-agent system can be controlled for any agent speed. This interval of values is clearly visible in Fig. 32 where the control error for a type III system is illustrated for different values of the agent speed. It is here interesting to note the effect of the velocity v. If this parameter is increased from zero, then, the range of the coupling strength yielding synchronization widens, up Source: Reprinted with permission from Ref. [305].From these considerations, it follows that there exists a range of the coupling strength where the multi-agent system can be controlled for any agent speed. This interval of values is clearly visible in Fig. 32 where the control error for a type III system is illustrated for different values of the agent speed. It is here interesting to note the effect of the velocity v. If this parameter is increased from zero, then, the range of the coupling strength yielding synchronization widens, up Source: Reprinted with permission from Ref. [305].</p>
        <p>to the point where the prediction by Eq. ( 106) becomes exact and a further increase of v does not change anymore the 36 thresholds for synchronization. We can conclude that increasing the agent speed is, in general, beneficial for control. As the analysis carried out in Ref. [305] clarifies, this is due to the fact that the chains appearing in the time-varying interaction network are continuously broken and reformed by agent motion. Since shorter chains are easier to control and increasing the velocity is equivalent to reduce their maximum length, then agent mobility generally favors synchronization control.to the point where the prediction by Eq. ( 106) becomes exact and a further increase of v does not change anymore the 36 thresholds for synchronization. We can conclude that increasing the agent speed is, in general, beneficial for control. As the analysis carried out in Ref. [305] clarifies, this is due to the fact that the chains appearing in the time-varying interaction network are continuously broken and reformed by agent motion. Since shorter chains are easier to control and increasing the velocity is equivalent to reduce their maximum length, then agent mobility generally favors synchronization control.</p>
        <p>Notice that in the connection adaptive control strategy, under particular circumstances (specifically, low density) it may occur that within the sensing radius there are no agents meeting the required criterion to be linked with. The solution discussed above requires to increase the radius, which, depending on the context, may not be always practical or convenient. An alternative solution is investigated in Ref. [308], where the Authors propose to tackle this case by allowing agent i to perform a jump into a randomly chosen position where it can start again the search for a node to connect with.Notice that in the connection adaptive control strategy, under particular circumstances (specifically, low density) it may occur that within the sensing radius there are no agents meeting the required criterion to be linked with. The solution discussed above requires to increase the radius, which, depending on the context, may not be always practical or convenient. An alternative solution is investigated in Ref. [308], where the Authors propose to tackle this case by allowing agent i to perform a jump into a randomly chosen position where it can start again the search for a node to connect with.</p>
        <p>Another way in which the basic control strategy may be enhanced is to partition the space where the agents move and instantiate an extra agent, i.e., a controller, in each of the cells of the partition [309]. In this way, the length of the chains appearing in the interaction matrix will be reduced, favoring synchronization as lengthy chains are more difficult to control.Another way in which the basic control strategy may be enhanced is to partition the space where the agents move and instantiate an extra agent, i.e., a controller, in each of the cells of the partition [309]. In this way, the length of the chains appearing in the interaction matrix will be reduced, favoring synchronization as lengthy chains are more difficult to control.</p>
        <p>To move forward the idea of agents moving in a space that is not homogeneous, but, on the contrary, is characterized by regions where different dynamics take place (as it occurs, for instance, in the case of spatial pinning control), we now examine two interesting models proposed in Refs. [310,311].To move forward the idea of agents moving in a space that is not homogeneous, but, on the contrary, is characterized by regions where different dynamics take place (as it occurs, for instance, in the case of spatial pinning control), we now examine two interesting models proposed in Refs. [310,311].</p>
        <p>We begin with the model in Ref. [310], where the agents are allowed to interact with each other only in a set of fixed zones in the space. Outside these zones, their dynamics is isolated. In other words, indicating with Γ h with h = 1, . . . , m z the m z zones of interactions, then L ij (t) = -1 whenever agents i and j lie in some of these zones (which do not necessarily need to be the same), i.e., whenever there exist h 1 and h 2 such that y i (t) ∈ Γ h 1 and y j (t) ∈ Γ h 2 , where eventually (but not necessarily) h 1 may be equal to h 2 . This model mimics a scenario where interactions among agents are restricted, as, for instance, it may occur in a wireless communication system where there are 'blind' areas that communication signals cannot reach [310].We begin with the model in Ref. [310], where the agents are allowed to interact with each other only in a set of fixed zones in the space. Outside these zones, their dynamics is isolated. In other words, indicating with Γ h with h = 1, . . . , m z the m z zones of interactions, then L ij (t) = -1 whenever agents i and j lie in some of these zones (which do not necessarily need to be the same), i.e., whenever there exist h 1 and h 2 such that y i (t) ∈ Γ h 1 and y j (t) ∈ Γ h 2 , where eventually (but not necessarily) h 1 may be equal to h 2 . This model mimics a scenario where interactions among agents are restricted, as, for instance, it may occur in a wireless communication system where there are 'blind' areas that communication signals cannot reach [310].</p>
        <p>Remarkably, the approach delineated in Section 4.3.1 can still be used to investigate the question whether synchronization can be attained even under the limitations imposed by these restrictive interactions. Here, the key parameter is the probability of interaction p I which is defined considering the ratio between the total area of the interaction zones, indicated as A z , and the area of the space where the agents move, given by L 2 , i.e., p I = Az L 2 . Let us, then, consider that switching among the possible interaction topologies occurs at high rate, then it can be demonstrated (see Ref. [310]) that, for this model, the time-average Laplacian matrix L is given by: L = p 2 I L K .Remarkably, the approach delineated in Section 4.3.1 can still be used to investigate the question whether synchronization can be attained even under the limitations imposed by these restrictive interactions. Here, the key parameter is the probability of interaction p I which is defined considering the ratio between the total area of the interaction zones, indicated as A z , and the area of the space where the agents move, given by L 2 , i.e., p I = Az L 2 . Let us, then, consider that switching among the possible interaction topologies occurs at high rate, then it can be demonstrated (see Ref. [310]) that, for this model, the time-average Laplacian matrix L is given by: L = p 2 I L K .</p>
        <p>((</p>
        <p>The expression holds for any space of dimension d, after replacing in the definition of p i the areas with the volumes in dimension d. From Eq. ( 107), following the same arguments of Section 4.3.1 one derives that for a type III MSF system synchronization requires that synchronization can be reached does not depend on the system size N, in contrast, in this model the ability to achieve synchronization strongly depends on the system size as p l I and p u I scales with N -1/2 . Hence, synchronization is more difficult for larger size systems. Equation ( 108) points out another interesting feature of this model: increasing the agent capability to interact with the other units is necessarily beneficial for synchronization. Instead, the synchronization region is independent on the number of interacting regions, provided that the total volume is kept the same and the agent speed is high enough to guarantee the validity of the fast switching assumption.The expression holds for any space of dimension d, after replacing in the definition of p i the areas with the volumes in dimension d. From Eq. ( 107), following the same arguments of Section 4.3.1 one derives that for a type III MSF system synchronization requires that synchronization can be reached does not depend on the system size N, in contrast, in this model the ability to achieve synchronization strongly depends on the system size as p l I and p u I scales with N -1/2 . Hence, synchronization is more difficult for larger size systems. Equation ( 108) points out another interesting feature of this model: increasing the agent capability to interact with the other units is necessarily beneficial for synchronization. Instead, the synchronization region is independent on the number of interacting regions, provided that the total volume is kept the same and the agent speed is high enough to guarantee the validity of the fast switching assumption.</p>
        <p>At variance with the models discussed so far, in Refs. [311,312] the scenario studied requires that to interact the agents need to be in the same zone. In more detail, the model discussed in Ref. [312] encompasses a single interaction region, whereas in Ref. [311] a more general scenario, with m z zones, is dealt with. In this case, thus, L ij (t) = -1 whenever there exists an integer h such that y i (t), y j (t) ∈ Γ¯h. Once again, under the hypothesis of fast switching, the method of Section 4.3.1 may be applied to find the expression for the time-average Laplacian matrix L, that in this case is given by:At variance with the models discussed so far, in Refs. [311,312] the scenario studied requires that to interact the agents need to be in the same zone. In more detail, the model discussed in Ref. [312] encompasses a single interaction region, whereas in Ref. [311] a more general scenario, with m z zones, is dealt with. In this case, thus, L ij (t) = -1 whenever there exists an integer h such that y i (t), y j (t) ∈ Γ¯h. Once again, under the hypothesis of fast switching, the method of Section 4.3.1 may be applied to find the expression for the time-average Laplacian matrix L, that in this case is given by:</p>
        <p>where A I is the area of the interacting zone.where A I is the area of the interacting zone.</p>
        <p>A very rich repertoire of dynamical features arise in this model when one further differentiates among zones with attractive interactions, i.e., with positive ϵ, indicated as ϵ a , and repulsive ones, i.e., with negative ϵ, indicated as ϵ r . A minimal scenario, represented by the presence of a single attractive and a single repulsive region, is investigated in Ref. [311] for a network of moving Lorenz chaotic systems.A very rich repertoire of dynamical features arise in this model when one further differentiates among zones with attractive interactions, i.e., with positive ϵ, indicated as ϵ a , and repulsive ones, i.e., with negative ϵ, indicated as ϵ r . A minimal scenario, represented by the presence of a single attractive and a single repulsive region, is investigated in Ref. [311] for a network of moving Lorenz chaotic systems.</p>
        <p>For this system four distinct dynamical regimes are observed when the two coupling parameters are varied (Fig. 33).For this system four distinct dynamical regimes are observed when the two coupling parameters are varied (Fig. 33).</p>
        <p>The region of synchronization appears for small ϵ r and large ϵ a , whereas for very negative ϵ r the system is desynchronized. Two other, very interesting, regions are found in between the synchronization and the desynchronization region. In more detail, starting from a high enough coupling strength ϵ a and a value of ϵ r close to zero, one may observe that decreasing ϵ r synchronization becomes intermittent due to the effect of the repulsion zone. If ϵ r is further decreased, then the dynamics may generate extreme events [313]. These are short-lasting events that appear at unpredictable times and are characterized by a very large synchronization error. Their appearance occurs at very low probability, but higher than that it would be observed if the event distribution were Gaussian. These extreme events are associated with the sudden changes of states in the underlying complex systems, and the occurrence of extreme events often results in large social impact. They crucially appear in a variety of diverse fields like share market crashes [314], electric power transmission system [315], earthquakes [316], and even epileptic seizures in the human brain [317]. The model discussed in this section demonstrates that they can also occur in a system of oscillators dynamically coupled via a temporal network induced by the agent motion.The region of synchronization appears for small ϵ r and large ϵ a , whereas for very negative ϵ r the system is desynchronized. Two other, very interesting, regions are found in between the synchronization and the desynchronization region. In more detail, starting from a high enough coupling strength ϵ a and a value of ϵ r close to zero, one may observe that decreasing ϵ r synchronization becomes intermittent due to the effect of the repulsion zone. If ϵ r is further decreased, then the dynamics may generate extreme events [313]. These are short-lasting events that appear at unpredictable times and are characterized by a very large synchronization error. Their appearance occurs at very low probability, but higher than that it would be observed if the event distribution were Gaussian. These extreme events are associated with the sudden changes of states in the underlying complex systems, and the occurrence of extreme events often results in large social impact. They crucially appear in a variety of diverse fields like share market crashes [314], electric power transmission system [315], earthquakes [316], and even epileptic seizures in the human brain [317]. The model discussed in this section demonstrates that they can also occur in a system of oscillators dynamically coupled via a temporal network induced by the agent motion.</p>
        <p>So far, synchronization of chaotic oscillators in temporal networks induced by agents moving as random walkers has been dealt with. An interesting aspect to study is the effect of the type of motion on synchronization [318]. This issue can be conveniently investigated considering the Vicsek's model [319], which as a function of a single parameter can tune detail, the Authors of Ref. [324] have introduced a model where attractive interactions (i.e., associated with a positive 32 coupling strength) are set for agents at a reciprocal distance greater than a threshold d R , whereas repulsive interactions (i.e., associated with a negative coupling strength) for agents at a reciprocal distance smaller than the threshold d R . In particular, they have studied a system of Stuart-Landau oscillators and have found that the coexistence of competing interactions can prevent the synchronous behavior that is observed when only attractive coupling is used. Instead, as a function of its parameters, the system typically displays inhomogeneous small oscillations, intermittent dynamics, and even states that can be classified as extreme events.So far, synchronization of chaotic oscillators in temporal networks induced by agents moving as random walkers has been dealt with. An interesting aspect to study is the effect of the type of motion on synchronization [318]. This issue can be conveniently investigated considering the Vicsek's model [319], which as a function of a single parameter can tune detail, the Authors of Ref. [324] have introduced a model where attractive interactions (i.e., associated with a positive 32 coupling strength) are set for agents at a reciprocal distance greater than a threshold d R , whereas repulsive interactions (i.e., associated with a negative coupling strength) for agents at a reciprocal distance smaller than the threshold d R . In particular, they have studied a system of Stuart-Landau oscillators and have found that the coexistence of competing interactions can prevent the synchronous behavior that is observed when only attractive coupling is used. Instead, as a function of its parameters, the system typically displays inhomogeneous small oscillations, intermittent dynamics, and even states that can be classified as extreme events.</p>
        <p>Another relevant configuration of moving oscillators considers the presence of interdependencies among different networked structures [241]. The model consists of two layers of oscillators, where in each layer agents perform a random walk on a two-dimensional lattice, and interact with their neighbors and their replicas in the other layer. For a small intra-layer coupling strength, inter-layer synchronization without intra-layer synchronization may be observed, such that the replicas are synchronized even if each individual layer is not synchronous. Instead, for larger intra-layer coupling strength, complete synchronization is found. Quite interestingly, the phenomenon of inter-layer synchronization is found to be robust with respect to the presence of static nodes, i.e., nodes for which the mobility is inhibited in both layers.Another relevant configuration of moving oscillators considers the presence of interdependencies among different networked structures [241]. The model consists of two layers of oscillators, where in each layer agents perform a random walk on a two-dimensional lattice, and interact with their neighbors and their replicas in the other layer. For a small intra-layer coupling strength, inter-layer synchronization without intra-layer synchronization may be observed, such that the replicas are synchronized even if each individual layer is not synchronous. Instead, for larger intra-layer coupling strength, complete synchronization is found. Quite interestingly, the phenomenon of inter-layer synchronization is found to be robust with respect to the presence of static nodes, i.e., nodes for which the mobility is inhibited in both layers.</p>
        <p>After this long journey inside synchronization of temporal networks, it is clear to us that this area of investigation will become crucial and attract more and more attention in nonlinear science within the years to come. We therefore end our report with a brief discussion on what are, in our opinion, the challenges that still need to be addressed and the noteworthy routes for future research.After this long journey inside synchronization of temporal networks, it is clear to us that this area of investigation will become crucial and attract more and more attention in nonlinear science within the years to come. We therefore end our report with a brief discussion on what are, in our opinion, the challenges that still need to be addressed and the noteworthy routes for future research.</p>
        <p>First of all, despite the many advances summarized in our Section 3, it is clear that the necessary and sufficient conditions (in terms of the critical coupling strength) for the stability of synchronization (even the least complicated complete synchronization states) in temporal networks have remained elusive so far in the general case in which one considers an arbitrary switching frequency. This, together with the more complicated case of a multilayer network with several tiers of connections where intra-and inter-layer synchronization states may occur, needs therefore an extra effort from the side of the community of network scientists.First of all, despite the many advances summarized in our Section 3, it is clear that the necessary and sufficient conditions (in terms of the critical coupling strength) for the stability of synchronization (even the least complicated complete synchronization states) in temporal networks have remained elusive so far in the general case in which one considers an arbitrary switching frequency. This, together with the more complicated case of a multilayer network with several tiers of connections where intra-and inter-layer synchronization states may occur, needs therefore an extra effort from the side of the community of network scientists.</p>
        <p>In the case of mobile oscillators, we have seen in Section 4 that tuning the agents' mobility is effective in controlling synchronization (and desynchronization) in the network. This property is particularly relevant, and its potentialities in practical applications have not yet been fully exploited. For instance, one may think of making explicit use of such a feature in man made systems, as a driving paradigm at the initial moment of engineering the system itself.In the case of mobile oscillators, we have seen in Section 4 that tuning the agents' mobility is effective in controlling synchronization (and desynchronization) in the network. This property is particularly relevant, and its potentialities in practical applications have not yet been fully exploited. For instance, one may think of making explicit use of such a feature in man made systems, as a driving paradigm at the initial moment of engineering the system itself.</p>
        <p>As for applications, biological and technological networks seem to be the realms where the predictions of the theory may be implemented, especially in view of the fact that adaptivity seems to be the key mechanism through which biological networks operate. Yet, many questions remain open about the role and the specific features of the different adaptive mechanisms in the onset of mesoscopic and macroscopic functional behavior. At the same time, it is certainly to be pointed out the current lack of experimental realizations of the different synchronization patterns emerging in time-varying networks. This constitutes undoubtedly a big limitation, and we are convinced that future efforts should be spent to realize controlled laboratory setups (either with electronic circuits, or with moving robots carrying electronically implemented dynamical systems) by means of which a confirmation of the distinct collective and emerging dynamical organizations predicted by the theory can be obtained, and their robustness conveniently tested.As for applications, biological and technological networks seem to be the realms where the predictions of the theory may be implemented, especially in view of the fact that adaptivity seems to be the key mechanism through which biological networks operate. Yet, many questions remain open about the role and the specific features of the different adaptive mechanisms in the onset of mesoscopic and macroscopic functional behavior. At the same time, it is certainly to be pointed out the current lack of experimental realizations of the different synchronization patterns emerging in time-varying networks. This constitutes undoubtedly a big limitation, and we are convinced that future efforts should be spent to realize controlled laboratory setups (either with electronic circuits, or with moving robots carrying electronically implemented dynamical systems) by means of which a confirmation of the distinct collective and emerging dynamical organizations predicted by the theory can be obtained, and their robustness conveniently tested.</p>
        <p>Finally, another very hot topic in network science is to consider the effects of higher-order interactions, i.e., to discuss the dynamics of hypergraphs and/or simplicial complexes [325,326]. This is because, from biology to social science, the functioning of a wide range of systems is the result of interactions which involve more than two constituents. While a lot of studies have recently focused on processes and dynamics of static networks with higher-order interplay, such group interactions have not yet been considered in the context of temporal networks. We are convinced that this will be one of the major challenges that nonlinear scientists have to deal with within the years to come, starting from giving answers to fundamental questions such as: how higher-order interactions manifest themselves in time-varying structures, how they change the functioning of such systems, and how can they be of use for engineering and/or controlling the behavior of real world networks.Finally, another very hot topic in network science is to consider the effects of higher-order interactions, i.e., to discuss the dynamics of hypergraphs and/or simplicial complexes [325,326]. This is because, from biology to social science, the functioning of a wide range of systems is the result of interactions which involve more than two constituents. While a lot of studies have recently focused on processes and dynamics of static networks with higher-order interplay, such group interactions have not yet been considered in the context of temporal networks. We are convinced that this will be one of the major challenges that nonlinear scientists have to deal with within the years to come, starting from giving answers to fundamental questions such as: how higher-order interactions manifest themselves in time-varying structures, how they change the functioning of such systems, and how can they be of use for engineering and/or controlling the behavior of real world networks.</p>
        <p>D.Ghosh, M. Frasca, A. Rizzo et al. Physics Reports xxx (xxxx) xxxD.Ghosh, M. Frasca, A. Rizzo et al. Physics Reports xxx (xxxx) xxx</p>
        <p>The Authors would like to gratefully acknowledge R. E. Amritkar, B. Barzel, I. Belykh, V. Belykh, E. M. Bollt, J. Bragard, J. M. Buldú, J. Burguete, A. Buscarino, A. Cardillo, T. Carroll, M. G. Clerc, R. Criado, S. K. Dana, P. De Lellis, M. Di Bernardo, A. Diaz Guilera, R. D'Souza, E. Estrada, S. Focardi, U. Feudel, L. Fortuna, N. Frolov, L. V. Gambuzza, J. Garcia-Ojalvo, A. Garcimartin, G. Giacomelli, J. Gómez-Gardeñes, S. Havlin, J. Hizanidis, P. Hövel, A. E. Hramov, S. Jafari, S. Jalan, M. Jusup,The Authors would like to gratefully acknowledge R. E. Amritkar, B. Barzel, I. Belykh, V. Belykh, E. M. Bollt, J. Bragard, J. M. Buldú, J. Burguete, A. Buscarino, A. Cardillo, T. Carroll, M. G. Clerc, R. Criado, S. K. Dana, P. De Lellis, M. Di Bernardo, A. Diaz Guilera, R. D'Souza, E. Estrada, S. Focardi, U. Feudel, L. Fortuna, N. Frolov, L. V. Gambuzza, J. Garcia-Ojalvo, A. Garcimartin, G. Giacomelli, J. Gómez-Gardeñes, S. Havlin, J. Hizanidis, P. Hövel, A. E. Hramov, S. Jafari, S. Jalan, M. Jusup,</p>
        <p>Source: Reprinted figure from Ref. [189], with the permission of AIP Publishing.Source: Reprinted figure from Ref. [189], with the permission of AIP Publishing.</p>
        <p>Interestingly, the conditions above mentioned for synchronization stability have been extended to a wider class of 49 edge-based adaptive laws in Ref. [190], where the Authors demonstrate that they also hold when a general formulation of the update law for the weights is adopted: ẇij = g(e ij ), (50) with e ij = x j -x i . In more detail, the Authors of Ref. [190]Interestingly, the conditions above mentioned for synchronization stability have been extended to a wider class of 49 edge-based adaptive laws in Ref. [190], where the Authors demonstrate that they also hold when a general formulation of the update law for the weights is adopted: ẇij = g(e ij ), (50) with e ij = x j -x i . In more detail, the Authors of Ref. [190]</p>
        <p>) is a monotonously increasing function of the error norm and is such that g(0) = 0 and for some finite constant m, 0 ≤ g(e ij ) ≤ m, or, alternatively, g(e ij ) = µ∥e ij ∥ p with 0 &lt; p ≤ 2, then the edge-based strategy guarantees synchronization for any initial condition, i.e., lim t→+∞ e ij = 0 ∀e ij (0), and lim t→+∞ w ij (t) = wij ∀(i, j) ∈ E.) is a monotonously increasing function of the error norm and is such that g(0) = 0 and for some finite constant m, 0 ≤ g(e ij ) ≤ m, or, alternatively, g(e ij ) = µ∥e ij ∥ p with 0 &lt; p ≤ 2, then the edge-based strategy guarantees synchronization for any initial condition, i.e., lim t→+∞ e ij = 0 ∀e ij (0), and lim t→+∞ w ij (t) = wij ∀(i, j) ∈ E.</p>
        <p>The conditions on the node dynamics can also be relaxed. The hypothesis we have discussed so far practically requires that the node vector field is contracting, but a larger class of systems, including unstable linear systems, systems in Lur'e form, Lipschitz vector fields, and systems with bounded Jacobians, can be considered as discussed in Ref. [191].The conditions on the node dynamics can also be relaxed. The hypothesis we have discussed so far practically requires that the node vector field is contracting, but a larger class of systems, including unstable linear systems, systems in Lur'e form, Lipschitz vector fields, and systems with bounded Jacobians, can be considered as discussed in Ref. [191].</p>
        <p>Two other results of Ref. [191] are worth mentioning here. First, by strengthening the condition of the vector field f , the stability analysis can be also extended to the case where the oscillators are coupled through a subset of the state variables, i.e., h((x j )) = Bx j with B positive semi-definite. Second, synchronization can be adaptively achieved also by only acting on a reduced set of the network links. According to this strategy, named edge pinning synchronization, the majority of the links are kept constant, whereas a fraction of them is updated according to:Two other results of Ref. [191] are worth mentioning here. First, by strengthening the condition of the vector field f , the stability analysis can be also extended to the case where the oscillators are coupled through a subset of the state variables, i.e., h((x j )) = Bx j with B positive semi-definite. Second, synchronization can be adaptively achieved also by only acting on a reduced set of the network links. According to this strategy, named edge pinning synchronization, the majority of the links are kept constant, whereas a fraction of them is updated according to:</p>
        <p>for (i, j) ∈ Ē ⊂ E, with µ ij positive constants.for (i, j) ∈ Ē ⊂ E, with µ ij positive constants.</p>
        <p>Here, the term 'pinning' is borrowed from a more vast literature that deals with techniques aimed at controlling synchronization and, more in general, the collective behavior of a complex network, only acting on a subset of the nodes, and that also includes decentralized adaptive strategies. The problem tackled in pinning control [192,193] is to steer the network towards a specific reference trajectory s(t) by controlling a fraction of its nodes, at variance with self-organized synchronization where the trajectory on which all the nodes converge is spontaneously chosen by the system itself. In the classical scheme several parameters, such as the control gain and the coupling coefficients, need to be tuned, such that the inclusion of adaptive mechanisms may be beneficial. This idea has been pursued in several works [194][195][196][197][198][199]. To illustrate it, let us consider the following system of coupled oscillators:Here, the term 'pinning' is borrowed from a more vast literature that deals with techniques aimed at controlling synchronization and, more in general, the collective behavior of a complex network, only acting on a subset of the nodes, and that also includes decentralized adaptive strategies. The problem tackled in pinning control [192,193] is to steer the network towards a specific reference trajectory s(t) by controlling a fraction of its nodes, at variance with self-organized synchronization where the trajectory on which all the nodes converge is spontaneously chosen by the system itself. In the classical scheme several parameters, such as the control gain and the coupling coefficients, need to be tuned, such that the inclusion of adaptive mechanisms may be beneficial. This idea has been pursued in several works [194][195][196][197][198][199]. To illustrate it, let us consider the following system of coupled oscillators:</p>
        <p>where δ i = 1 for the pinned nodes and δ i = 0 otherwise, q i (t) is the adaptive control gain, and w ij (t) (with (i, j) ∈ E) the adaptive weights of the network links. Although also centralized strategies may be used to adjust the control gain [195,196] or both the control gain and the common coupling coefficient [194], here we focus on the local, decentralized adaptive strategy studied in Ref. [199]. To update the weights, the following law is adoptedwhere δ i = 1 for the pinned nodes and δ i = 0 otherwise, q i (t) is the adaptive control gain, and w ij (t) (with (i, j) ∈ E) the adaptive weights of the network links. Although also centralized strategies may be used to adjust the control gain [195,196] or both the control gain and the common coupling coefficient [194], here we focus on the local, decentralized adaptive strategy studied in Ref. [199]. To update the weights, the following law is adopted</p>
        <p>for (i, j) ∈ E, while the control gain is updated as follows:for (i, j) ∈ E, while the control gain is updated as follows:</p>
        <p>⟩ T as a function of the coupling strength ϵ and noise level η in a system of Rössler oscillators moving according to the Vicsek's model (110). The system is formed by N = 10 agents. Other parameters are: ρ = 0.04, R = 1, and v = 1. The predictions of the synchronization thresholds, represented with a red continuous line, are derived from Eq. ( 112), under the hypothesis of fast switching. Source: Reprinted figure from Ref. [318], with the permission of AIP Publishing.⟩ T as a function of the coupling strength ϵ and noise level η in a system of Rössler oscillators moving according to the Vicsek's model (110). The system is formed by N = 10 agents. Other parameters are: ρ = 0.04, R = 1, and v = 1. The predictions of the synchronization thresholds, represented with a red continuous line, are derived from Eq. ( 112), under the hypothesis of fast switching. Source: Reprinted figure from Ref. [318], with the permission of AIP Publishing.</p>
        <p>the agent motion from disordered (e.g., random walkers) to ordered (e.g., platoons). In the Vicsek's model the positions 36 of the agents are updated as follows:the agent motion from disordered (e.g., random walkers) to ordered (e.g., platoons). In the Vicsek's model the positions 36 of the agents are updated as follows:</p>
        <p>where, as usual, v i = e ιθ i indicates the agent velocity. The equation for the update of the agent heading includes the noise term ∆θ i , which is generated at each time step with uniform probability in the interval [-η 2 , η 2 ] (being η the noise level). The term ⟨θ i (t k )⟩ R represents the average direction of the units in the disc of radius R of agent i and is calculated as follows:where, as usual, v i = e ιθ i indicates the agent velocity. The equation for the update of the agent heading includes the noise term ∆θ i , which is generated at each time step with uniform probability in the interval [-η 2 , η 2 ] (being η the noise level). The term ⟨θ i (t k )⟩ R represents the average direction of the units in the disc of radius R of agent i and is calculated as follows:</p>
        <p>In this model the noise level η modulates the type of motion, which is ordered for low values of η, and disordered for high values. To elucidate the effect of motion on synchronization of moving chaotic agents, Eq. ( 97) can be used with agent motion ruled by the Vicsek's model (110). In fact, the motion model determines how the Laplacian L(t) appearing in Eq. ( 97) evolves in time, and so whether and how synchronization is attained.In this model the noise level η modulates the type of motion, which is ordered for low values of η, and disordered for high values. To elucidate the effect of motion on synchronization of moving chaotic agents, Eq. ( 97) can be used with agent motion ruled by the Vicsek's model (110). In fact, the motion model determines how the Laplacian L(t) appearing in Eq. ( 97) evolves in time, and so whether and how synchronization is attained.</p>
        <p>Once again, setting a high speed v entitles the application of the MSF formalism under the fast switching scenario, yielding the following condition for synchronization in the more general case of type III systems:Once again, setting a high speed v entitles the application of the MSF formalism under the fast switching scenario, yielding the following condition for synchronization in the more general case of type III systems:</p>
        <p>where ⟨k⟩ denotes the average degree in the networks representing interactions in the Vicsek's model. Note that this parameter is a function of the noise level, agent density and speed, such that the two synchronization thresholds are influenced by the motion characteristics via this parameter.where ⟨k⟩ denotes the average degree in the networks representing interactions in the Vicsek's model. Note that this parameter is a function of the noise level, agent density and speed, such that the two synchronization thresholds are influenced by the motion characteristics via this parameter.</p>
        <p>The analysis of this model reveals interesting and unexpected features of the interplay between motion and synchronization dynamics. In fact, depending on the system parameters, synchronization of all chaotic oscillators may be induced either by a coordinated motion or, at the opposite, by a disordered motion. An example is shown in Fig. 34 where three regions are visible: one, occurring for small coupling strength, ϵ &lt; 0.4, where noise deteriorates synchronization, which is therefore lost in the transition from ordered to disordered motion; a second region for 0.4 &lt; ϵ &lt; 40 where noise favors synchronization, which is then associated to a disordered motion; and, finally, a third region, for ϵ &gt; 40 where synchronization is not possible for any value of η.The analysis of this model reveals interesting and unexpected features of the interplay between motion and synchronization dynamics. In fact, depending on the system parameters, synchronization of all chaotic oscillators may be induced either by a coordinated motion or, at the opposite, by a disordered motion. An example is shown in Fig. 34 where three regions are visible: one, occurring for small coupling strength, ϵ &lt; 0.4, where noise deteriorates synchronization, which is therefore lost in the transition from ordered to disordered motion; a second region for 0.4 &lt; ϵ &lt; 40 where noise favors synchronization, which is then associated to a disordered motion; and, finally, a third region, for ϵ &gt; 40 where synchronization is not possible for any value of η.</p>
        <p>For brevity we have not discussed the case of chaotic oscillators moving as random walkers in a finite lattice dealt with in Ref. [64], but we mention here that this important study was one of the first papers posing the synchronization problem in a stochastic dynamic framework.For brevity we have not discussed the case of chaotic oscillators moving as random walkers in a finite lattice dealt with in Ref. [64], but we mention here that this important study was one of the first papers posing the synchronization problem in a stochastic dynamic framework.</p>
        <p>Several works have investigated the effect of the simultaneous presence of competitive interactions in complex networks with links fixed in time, revealing diverse stationary regimes, chimera states, and solitary waves [320][321][322][323].Several works have investigated the effect of the simultaneous presence of competitive interactions in complex networks with links fixed in time, revealing diverse stationary regimes, chimera states, and solitary waves [320][321][322][323].</p>
        <p>In the context of temporal networks induced by agent mobility, this issue has been investigated in Ref. [324]. In moreIn the context of temporal networks induced by agent mobility, this issue has been investigated in Ref. [324]. In more</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </text>
</tei>

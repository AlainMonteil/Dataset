<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-25T06:29+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>The analysis of nuclear magnetic resonance (NMR) spectra to detect peaks and characterize their parameters, often referred to as deconvolution, is a crucial step in the quantification, elucidation, and verification of the structure of molecular systems. However, deconvolution of 1D NMR spectra is a challenge for both experts and machines. We propose a robust, expert-level quality deep learning-based deconvolution algorithm for 1D experimental NMR spectra. The algorithm is based on a neural network trained on synthetic spectra. Our customized pre-processing and labeling of the synthetic spectra enable the estimation of critical peak parameters. Furthermore, the neural network model transfers well to the experimental spectra and demonstrates low fitting errors and sparse peak lists in challenging scenarios such as crowded, high dynamic range, shoulder peak regions as well as broad peaks. We demonstrate in challenging spectra that the proposed algorithm is superior to expert results.</p>
        <p>Nuclear magnetic resonance (NMR) spectroscopy is a powerful and diverse tool for the quantitative analysis of molecules. Spectra can vary from a few peaks to highly overlapping regions with hundreds of peaks. The process of retrieving these peaks, i.e. signals, information from a 1D NMR spectrum is called deconvolution. Deconvolution is an essential step for the quantification, elucidation of the structure and verification of the structure of small molecules, and it facilitates workflows, for example, in drug discovery and the study of molecules [1]. In detail, the deconvolution process obtains a list of peaks and their parameters from a spectrum without additional information on the molecular structure.</p>
        <p>Creating a spectrum from a peak list, known as convolution, is a straightforward and well-defined task if distortions are neglected. On the other hand, inverting this operation, that is, determining a peak list from a given spectrum, is an ill-posed inverse problem [2]. Hence, there is an infinite number of solutions to a deconvolution problem. A solution with a small residual error (the difference between the original and reconstructed spectrum) is not necessarily practical since too many peaks could be picked to fit the noise, distortions, or other influences. This notion is also known as over-fitting. To eliminate overfitting and decide which of all possible solutions to a deconvolution problem to choose, one needs to add information in the form of regularization or constraints. But even the commonly used approaches of sparsity promoting regularization meet their limits, depending on the regularization used [3,4]. Forcing the solution to be sparse can still lead to ambiguous results, see Fig. 1: In this example, experts usually prefer the sparser three-peak solution, although the spectrum was created using the five-peak annotation. Peaks are particularly challenging to be resolved and an expert could not distinguish between one or two peaks, especially if the chemical shift of two peaks is nearly identical, cf. Fig. 1 (bottom, center), or if the amplitudes are factors of magnitude apart, cf. Fig. 1 (bottom, right). Increasing the number of detected peaks, i.e. model order, and thus, the number of free parameters for the fitting, will generally reduce the mean absolute error (MAE), reflected by the lower residual error of the five-peak fit. Furthermore, the intrinsic ambiguity of the measurement process and inaccurate spectral processing steps contribute to further unknown physical and experimental factors. Together, these result in line shape distortions, spectral artifacts, and low signal-to-noise ratio (SNR). All of these factors make deconvolution of NMR spectra challenging for both experts and machines.</p>
        <p>The deconvolution processes typically require significant manual intervention, operator supervision, and expert knowledge to achieve a good result. However, increasing the automatization of the deconvolution process can enhance the accessibility of NMR spectroscopy for non-experts, improve reproducibility, and reduce labor costs. Thus, automatic extraction of various peak parameters in NMR spectroscopy is essential to facilitate the NMR workflow.</p>
        <p>Classical approaches focus on information based on simple signal properties such as integral, amplitude, SNR, or geometric features [5,6]. One of the first steps in deconvolution methods usually consists of determining the number of peaks and their corresponding positions. Various approaches such as wavelet [7], matrix factorization [8], singular value decomposition [9], and Bayesian methods [10] have been proposed for the peak picking problem. Classical approaches that cover the complete deconvolution workflow often rely on the fitting of the free induction decay (FID) or the spectrum in the frequency domain, combined with an explicit criterion for determining the number of peaks, i.e., model order selection. However, trading off fitting error against model order can be pretty cumbersome [3,4]. The so-called CRAFT (Complete Reduction to Amplitude-Frequency Table) algorithm is an example of a classical FID-based fitting approach [11]. In contrast, another whole family of approaches is based on the autoregression properties of the FID, for example, Hankel matrix [12], filter diagonalization [13], and fast pad√© transform-based methods [14]. They are based on the fact that the subsequent points in the FID signal can be modeled as a linear combination of the preceding ones. On the other hand, an approach for deconvolution in the frequency domain is the Global Spectral Deconvolution (GSD) algorithm from Cobas et al. [15], which relies on the spectrum and its first and second derivatives for peak picking and fitting. Unfortunately, many classical algorithms for deconvolution of 1D NMR spectra still need some expert knowledge to set the hyperparameters of the method, rely on simple and fixed patterns, such as the spectrum and its derivatives, and exhibit difficulties in finding the correct number of peaks, that is, in the selection of the model order [3]. Instead of using simple and fixed patterns such as the signal and its derivatives or simple geometrical features, machine learning techniques can be utilized to learn more expressive, domain-specific features for deconvolution, such as multiplet patterns. For example, triplets are a typical pattern in NMR spectra that can be recognized; see Fig. 1.</p>
        <p>Machine learning methods, particularly deep learning methods, have proven their powerful capabilities to find and exploit patterns in all types of data. They have become indispensable in many domains, such as natural language processing or computer vision, to extract expressive features from nonlinear, high-dimensional signals. Deep neural networks (DNNs) are suitable for a wide range of learning tasks, ranging from classification and regression to unsupervised learning.</p>
        <p>Neural networks have a long history in NMR spectroscopy, with roots dating back to the 1980s [16]. Despite the early beginnings, most practical deep learning algorithms in NMR have only emerged in recent years due to improved algorithmic performance and computational capabilities [17]. Deep learning methods have been successfully applied in other areas of spectroscopy, for example, in IR spectroscopy to identify functional groups [18] or to process two-electron dipolar spectroscopy (DEER) data [19]. However, NMR spectroscopy also seems well suited for applying deep learning techniques, primarily because of its richness of information, quantitative nature, and high reproducibility. Chemical shift prediction is one of the most prominent applications [20][21][22][23][24]. Especially in the field of 2D NMR spectroscopy, deep learning has found various applications in recent years. The reconstruction of 2D spectra from nonuniformly sampled (i.e., undersampled) spectra (NUS) [25][26][27], or the denoising of low-SNR spectra [28] has been tackled. Deep learning-based peak picking in 2D-4D spectra (ARTINA) was carried out by Klukowski et al. [29]. In terms of learning-based deconvolution approaches, there are time and frequency domain techniques. Huang et al. [30] propose a deep Hankel matrix algorithm for the deconvolution of FID data. Recently, Li et al. [31] introduced a deconvolution method for 2D experimental NMR spectra that handles strongly overlapping, relatively narrow peaks called DEEP Picker. Generally, frequently recurring multiplet patterns and the complexity of inverse problems in NMR spectroscopy are factors in favor of the strengths of deep learning. This has led to the aforementioned promising applications of deep learning in spectroscopy, particularly in 2D NMR spectroscopy.</p>
        <p>However, applications in the more common high dynamic range (HDR) 1D NMR spectroscopy and the demonstration of robust transferability to experimental 1D NMR spectra for DNNs trained on synthetic data are not fully solved [17]. Especially in challenging regions with strongly overlapping, very broad peaks, shoulder peaks, and HDR issues, many methods struggle to demonstrate robust, expert-level performance [32].</p>
        <p>Our method addresses these problems: We propose a deep learning-based deconvolution algorithm for 1D NMR spectra with expert-level performance in various experimental spectra. More precisely, our contributions, which will be explained and discussed later in detail, are the following: Creation of realistic synthetic 1D NMR data set that allows a robust model transfer to experimental spectra An implicit regularization approach through efficient automatic labeling of these synthetic spectra Custom data pre-processing for high dynamic range regions and broad lines A neural network for the training with the synthetic spectra and the application to the experimental spectra that handles broad lines and their overlap with narrow lines, highly crowded regions, and HDR spectra Altogether, a fully automated method that yields expert-level quality sparse deconvolution results on experimental spectra</p>
        <p>The paper is organized as follows: Section 2 describes the data creation, pre-processing, and labeling, as well as the deep learning method. Section 3 presents the results of the deconvolution in synthetic and experimental spectra and compares them with expert annotations. Furthermore, different aspects of deep learning as a tool for deconvolution are discussed. Finally, Section 4 concludes. Various technical details of our setup are explained in the SI Section.</p>
        <p>We present a combined deconvolution algorithm that uses deep learning and classical methods. The DNN is trained using a synthetic data set that has been pre-processed and labeled with an NMR data-specific approach; cf. Fig. 2 (top row). In a second step, the trained model is applied to the pre-processed experimental spectra, cf. Fig. 2 (bottom row), and yields, together with a subsequent automatic classical peak parameter fine-tuning, a peak list.</p>
        <p>Consistently labeled and high-quality experimental NMR data are expensive and generally unavailable in the quantities needed to train modern neural networks [1]. Thus, our method is based on synthetic training data sets [17] that are freely available and can be generated in large amounts. Distortion-and artifact-free synthetic data can be generated from first principles, since NMR is a well-understood physical phenomenon. However, creating perfectly realistic synthetic data, including distortions and artifacts, is challenging, often leading to a slight discrepancy between synthetic training data and experimental test data. The notion of having different types of data sets for training and testing in supervised learning is known as distribution shift [33] and dealing with distinct distribution shifts is a common problem in machine learning called transfer learning [34]. We approach this problem from two sides.</p>
        <p>Firstly, we make our training spectra as realistic as possible. For data creation, we use synthetic multiplet structures consisting of pseudo-Voigt lines [35]. Pseudo-Voigt lines correspond to an experimental Gaussian line shape distortion to the theoretical Lorentzian resonance profiles. Distortions, such as inaccurate phase and baseline corrections, are applied to the pure signal. We then add a small linear frequency-dependent phase error to the spectrum for phase distortions. Incorrect baseline correction is emulated with randomly sampled points connected through a spline smoothing model. These procedures serve as NMR spectroscopyspecific data augmentation, which helps to prevent overfitting and promote transferability to experimental spectra [36]. Further-more, Gaussian noise is added to the spectrum to produce a random maximum SNR level in the range of ¬Ω10 2 ; 10 4 , cf. SI 5.1 for details and an illustration of the data creation process. An advantage of creating a custom synthetic data set is that the data distribution is malleable to cover the space of all possible experimental spectra.</p>
        <p>Secondly, we process the experimental spectra to match the training spectra. Therefore, we correct the experimental spectra with the automatic phase and baseline correction command "apbk" from 
            <rs type="software">TopSpin</rs> [37]. The corrected experimental spectra are then line-broadened with 0.3 Hz and resampled to match the number of points per Hz in the training spectra. This steps ensure that the peaks in the spectrum have approximately Voigt lineshapes with the same width range with respect to the number of points as in the training set. Altogether, the realistic data set creation and the experimental spectra adaption reduce transfer learning issues as much as possible.
        </p>
        <p>Peaks in 1D NMR spectra can have parameters that span orders of magnitude. Some peaks in a spectrum have a very low SNR, e.g., below 10; others have a very high SNR, e.g., above 10 4 . Such a broad range of signal intensities is referred to as a high dynamic range (HDR). Signals being orders of magnitude apart makes it numerically difficult for deep learning approaches to deconvolve complex spectra. Therefore, scaling methods for HDR data in other deep learning domains are common practice, for example, Mel frequency cepstral coefficients (MFCC) in audio [38]. However, in 1D NMR, analogous embeddings of HDR signals are missing.</p>
        <p>We introduce two pre-processing methods to make large parameter ranges better amenable to deep learning. The first preprocessing algorithm is called dynamic scaling. The method reduces the dynamic scale of the input spectrum and enhances the local contrast. The approach is inspired by techniques from computer vision for edge detection [39] and histogram equalization techniques [40]. This dynamic scaling is accomplished by applying a signal-dependent local contrast enhancement filter, cf. Algorithm 1. Therefore, the local maximum filter K max b and minimum filter K min b of different kernel widths b, where b is in the set of kernel widths B, are applied to the real part of the spectrum S. To avoid large jumps due to discontinuous minimum and maximum filter mapping, the filtered spectra S min and S max are smoothed with a normalized Gaussian kernel G√∞l ¬º 0; r ¬º b√û. Then the local minimum filtered spectrum S min g is subtracted from the original spectrum S and scaled by the local dynamic range, that is, the local maximum filtered minus the local minimum filtered spectrum Another variable with values in a wide range is the width of the peaks. On the one hand, there are peaks in 1D NMR spectra that have a width in the sub-Hz regime. On the other hand, exchangeables can have a width on the order of dozens of Hz, leading to spatial dependencies over dozens of Hz. Since we rely on the accuracy of deep learning in terms of position, we cannot use frequency resolution downsampling in the DNN. Having filters in the neural network that cover such a wide range adds more trainable parameters, that is, network weights, and hence entails a higher computational burden for training the network. Therefore, our method relies on shifting the spectrum. Instead of one spectrum being used as input to the network, several shifted versions of the same spectrum are fed into the network. This pre-processing-based solution increases the receptive field of the convolutional neural network (CNN) [41,42] part of the network. The receptive field is the part of the input spectrum to which a single output node is connected. Increasing the receptive field enables detecting broad signals.</p>
        <p>Employing the peak parameters used to create the synthetic spectra is not a sensible option. Due to the inconclusive ways in which a spectrum can be reconstructed using different peak lists (cf. Fig. 1), the labeling would be ambiguous and inconsistent, making it difficult to train the neural network. Since synthetic spectra are utilized for training, an automatic labeling procedure is used for the data. Labeling the spectra in the case of machine learning methods corresponds to explicit regularization in classical fitting-based approaches. We use an automatic annotation for the position and width of the peaks. However, the linear parameters of the amplitude and combination between the Gaussian and Lorentzian lineshape are not learned but fitted afterwards. Therefore, we rely on a custom labeling approach.</p>
        <p>Our labeling of the position of the peaks distinguishes between what should be detected and what not, or, in terms of performance, what is feasible to detect given the uncertainty in the experimental data. For this labeling technique, the width of lines of the original spectrum is shrunk by a factor that depends on the SNR in the original spectrum, cf. Fig. 4 (top and middle), and SI 5.4. The shrinking factor has a trade-off, cf. SI 5.4 for an illustration of the effect of different shrinking factors. On the one hand, not shrinking the linewidths leads to picking the local maxima as peak position labels, which is oversimplified. On the other hand, maximal shrinking results in restoring the original peak list, making the network potentially oversensitive toward minor distortions in the lineshapes. However, note that the recreation of the spectrum with the shrunk lines for the labeling is done without noise for accuracy reasons. The local maxima of the shrunk spectrum are then harnessed as labels for the original spectrum; see Fig. 4 (middle). The peak position labeling can be viewed as a local maximum picking in an altered, i.e., shrunk with respect to linewidths, spectrum. Similarly to the positions, we cannot use the widths from the synthetic spectrum generation since overlapping peaks will result in the same inconsistencies of the trained network. We also do not want to use an expensive fitting algorithm on the full spectrum to obtain reasonable estimates for the line widths. The present approach reduces the problem to a system of equations with 2n unknowns of amplitude and width and 2n equations at the n peak positions. Using an iteration approach to solve the problem reduces the equations even further to n block 2 √Ç 2 systems that can, in special cases, be solved analytically, cf. SI 5.5 for technical details.</p>
        <p>We use deep learning to find peak positions and widths. More precisely, we use one network for different but related tasks, also known as multitask learning. The network output determines for every point in the input spectrum whether the point represents a peak position or not. There is one no-peak class called baseline class and two peak classes. The peaks are divided into the two categories narrow peaks and broad peaks, i.e. exchangeables. At each peak position, the network further regresses the width of the corresponding peak.</p>
        <p>We use a CNN for the extraction of local features and a type of recurrent neural network, a bidirectional long-short-term memory (LSTM), [43,44] to relate local characteristics to each other, cf. Fig. 5. 10 channels are fed into the network, that is, 2 dynamic scaling channels (kernel width in points b 2 B ¬º f128; 2048g) and 8 shifting channels with a relative offset of 128 points, that is, approximately 25 Hz. The CNN starts with an inception layer [45] which is capable of extracting features of different bandwidths simultaneously, cf. the detailed architecture and hyperparameter settings in SI 5.6. The inception layer connects over timedistributed dense layers to the bidirectional LSTM. The bidirectional LSTM is responsible for relating the local features extracted by the previous convolutional and dense layers and setting them into a more global perspective. Thus, the network should be able to relate and use information from peaks dozens of Hz apart, e.g. from multiplets with high coupling constants. The output of the LSTM connects over another sequence of time-distributed dense layers to the 5 output channels. These 5 channels produce for each point on the frequency axis three classes of baseline, narrow peaks, and broad peaks, and their respective peak widths.</p>
        <p>The whole network is designed in such a way that it is translational-invariant [46]. This means that we are not restricted to a particular spectral width, and the network is equivariant towards the position of a peak in the input spectrum. Therefore, the network applies the same mapping and thus gives the same output for a specific region independent of the chemical shift of the former region, e.g. whether the region is at 1 ppm or at 5 ppm. A data set of 250'000 synthetic spectra with 8192 points each was created and used for training. This allows the fitting of broad features, such as broad peaks, into a training sample. Network training is performed using the pre-processed spectrum as input and our labeling as target. The width of the binary position labels in discrete points is 3 for the narrow peak class and is proportional to the width for the broad peak class. We use a binary cross-entropy loss for the classification tasks (output channels 1-3) and a mean squared error (MSE) loss for the regression (output channels 4-5). Classification and regression losses are equally weighted. Logarithmic scaling for the broad peak class is applied to compactly represent the wide range of width values. All regression parameters are normalized to the same order of magnitude and thus stabilize the training process numerically. Note that for nonpeak positions, the regression loss is masked to avoid contamination of the error. The purpose of deep learning is to obtain the correct number of peaks, that is, the correct order of the model, and a promising valley, i.e. proximity of a local minimum, for further convex peak parameter fitting.</p>
        <p>To apply a trained network to the experimental spectra, the experimental spectra are phase and baseline corrected, line broad-Fig. 4. Data Labeling Process: Synthetic Spectrum Creation (top panel): In the first step, peaks (top, green) are sampled. Their profiles are given in dashed green lines, while the position of the peaks is displayed in light red. All the lines together build the original spectrum (black). Position labeling process on synthetic spectra (center panel): The sampled peaks are shrunk relatively in width, by a SNR dependent factor (here 0.4) for the displayed peaks (dark blue dashed), and summed to a line shrunk spectrum (middle, light blue). In that line shrunk spectrum, the local maxima (dark blue dots) yield the positions of the frequency labels (red bars). Width labeling process (bottom panel): The original spectrum (black) is convolved with a Lorentzian peak to an exponential line broadened spectrum (orange). The two amplitudes (olive) of the original and the respective line broadened peak with their maxima (red dots) and the difference between the latter maxima (red dotted line) allow us to derive the width of the labeled peak through a system of equations. Fig. 5. Data flow diagram for the neural network: This data flow diagram shows the neural network architecture and its corresponding inputs and outputs. First, the input spectrum (top panel) is pre-processed, leading to the multi-channel feature encoding (green panel) of the input spectrum. The spectral features are then fed into the translational-invariant neural network which yields three class outputs, i.e., top three rows for baseline, narrow peak and broad peak categories (black = true, white = false), and two regression outputs, i.e., bottom two rows for narrow peak width and broad peak width (greyscale for width within class bounds). The network output is post-processed and the parameters are fitted using the postprocessed neural network output as an initial guess. This results in a peak list through which we can reconstruct the input spectrum (bottom panel). ened and resampled, see Section 2.1. Afterward, the experimental spectra are pre-processed with the dynamic scaling filter and the shifting method in the same way as the training data, cf. Section 2.2. These pre-processed versions of the spectrum are then fed into the neural network. The output of the neural network is post-processed with the help of the baseline class and nonmaximum suppression (NMS) [47] in order to obtain sparse peak positions. The neural network predictions are then used for a subsequent iterative classical fitting procedure, where the number of peaks is fixed. The individual peak parameters are sequentially fitted for each peak with a gradient-free optimization algorithm. The quality of the predictions of the neural network is crucial as it serves as an initial guess for the fitting.</p>
        <p>In this section, we demonstrate the deconvolution performance of our method on synthetic and experimental spectra. Furthermore, we discuss the methodological advantages of our approach compared to other techniques. The deconvolution result of the same spectrum may vary from expert to expert, especially if they cannot rely on additional structural information. Thus, we demonstrate our results in awareness of a remaining ambiguity regarding an optimal solution. The main application of our method is 1D high-field 1 H spectra. However, the algorithm is generally not restricted to this use case, hence applications in other situations may benefit from retraining or fine-tuning the network.</p>
        <p>The deconvolution performance statistics on synthetic data, cf. details on synthetic testset supplementary information (SI) 5.2, are presented in Table 1 and Fig. 6A. The statistics are divided into three subcategories: Picking Accuracy Score M 1 2 ¬Ω0; 1, Overpicking/Sparsity Score M 2 2 ¬Ω0; 1 and Reconstruction Score M 3 2 ¬Ω0; 1. These three criteria quantify what we consider to be a good deconvolution result. Together, these three scores yield a Total Score M tot ¬º M 1 √Å M 2 √Å M 3 2 ¬Ω0; 1 for each test region; cf. SI 5.8 for details on the metrics. The results on synthetic data are excellent with high values in all three individual scores M 1 ; M 2 , and M 3 , and also in the total score M tot . Since the synthetic test spectra possess minor phase and baseline distortions, line shape distortions, that is, Voigt lines, and solvent peaks (cf. SI 5.2), we conclude that we have achieved a certain robustness toward small distortions. Furthermore, the over-picking score M 2 is worth highlighting, with a perfect score of M 2 ¬º 1 for all synthetic test regions. This means that the network never predicts more peaks than peaks that were annotated, and hence yields a sparse peak list. Distortions and crowded regions, such as in the synthetic testset, generally make deconvolution more arduous and can lead to performance degradation; cf. SI 5.10, 5.11, and 5.12. However, overall, we observe high scores in all categories despite distortions and overlapping signals.</p>
        <p>The deconvolution performance statistics on experimental data, cf. for details of the experimental test set in SI 5.3, are presented in Table 2 and Fig. 6B. The median of the overall score M tot on the experimental data is only 8% lower than on the synthetic data and therefore high at 82%. Due to the distribution shift between the synthetic and experimental data, a minor performance degradation on the experimental data is expected. However, in the peak picking M 1 and reconstruction score M 3 , the median value on experimental regions is even better than on synthetic data. This can be explained by the higher crowdedness and more substantial distortions, i.e., deconvolution difficulty, of the synthetic spectra.</p>
        <p>Sparser and less distorted synthetic data sets lead to superior results for M 1 and M 3 compared to the deconvolution of the experimental test set, cf. SI 5.12. On the other hand, the median of the sparsity score M 2 on experimental data is lower than on synthetic data, cf. Tables 1 and2. A factor for this discrepancy is the smaller peaks, e.g. satellites, that were ignored by the experts but not by the algorithm (cf. SI 5.9 for how to discard low SNR peaks). We conclude that the network trained on synthetic data can be well transferred to experimental data, since the differences in the scores are minor and well-explainable.</p>
        <p>Crowded regions are challenging to deconvolve, since peaks are easier to distinguish if they do not overlap. Therefore, the stronger a region is crowded, the more diverse are the expert deconvolution results. For three high-field 1 H regions with different amounts of overlapping peaks, we compare the results of our method, cf. Fig. 7 and cf. Table 3, with the manual results of 10 experts, cf. Table 3. The experts have tried to detect the lines and adjusted the peak parameters (frequency, amplitude, width, and line shape) to minimize the residual upon visual inspection. Note that these experts did not have additional structural information. In terms of the number of picked peaks, we gave the minimum, maximum, and median numbers of the expert group. For the simplest region A, the experts agree on the number of peaks, whereas for the most challenging region C the expert annotations vary between 14 and 24 peaks. The result of our method in terms of the number of peaks matches the median of the experts for two of three regions. In terms of residual, we calculated the average and standard deviation (Std) of the expert group's MAE. The largest MAE and Std of the MAE are observed for region C. The results of the expert deconvolutions exhibit significant discrepancies for crowded regions regarding peaks picked and reconstruction error.</p>
        <p>The results of our method for these three regions lie within the expert group's range in picked peaks and yield a lower residual than most experts, cf. Table 3. The mean absolute error MAE of our method lies at least one Std lower than the average of the experts for all three regions. Thus, we observe good transferability to sparse and crowded regions in experimental data by the example of three different regions and thus, can cover a wide range of deconvolution problems. Note that the result of our method in the most crowded region, cf. Fig. 7 C, is especially interesting, since there might be significantly fewer local maxima than lines that generate the region. The peak list of this crowded region is sparse, but the solution is not oversimplified, e.g., two doublets between 0:33 √Ä 0:35 ppm are found, despite observing only two local maxima. Altogether, these results suggest that our method can keep up with and, in terms of MAE, even substantially outperform the manual deconvolution results of experts.</p>
        <p>We compared the deconvolution results of our method with the expert results of three regions, A, B, and C, in Fig. 7.</p>
        <p>In this section, applications of our method are illustrated by four different regions, cf. Fig. 8. In the example of the first region with broad and narrow peaks that overlap, we observe excellent results of our method in terms of low residual and sparsity of the peak list, cf. high-field 1 H region in Fig. 8A. By separating the prediction of broad peaks from the prediction of narrow peaks in the neural network, the algorithm can disentangle overlapping lines of broad and narrow peaks that are orders of magnitude apart in terms of their width.</p>
        <p>Features orders of magnitude apart pose challenges to neural networks since different scales of inputs cause different parameter updates and optimizer steps towards the minimum of the loss function. Our method enables good deconvolution results for spectra of a very high dynamic range. Hence, the algorithm does not miss low SNR peaks. The second showcase region illustrates this ability, see high-field 1 H region Fig. 8B. Although the magnitudes of the peaks are orders apart, the reconstruction is nearly perfect and the residual small, also compared to the peak intensities.</p>
        <p>The third example shows a 13 C spectrum region. 13 C nuclei have a lower natural abundance than 1 H nuclei, usually resulting in lower sensitivity. Although our method focuses on 1 H high-field spectra, we can detect and accurately fit low SNR peaks of 13 C spectra, cf. 13 C region in Fig. 8C.</p>
        <p>The last showcase displays a spectrum acquired on a low-field (80 MHz) Benchtop NMR system, cf. 1 H Benchtop region in Fig. 8D. Benchtop spectra are challenging for deconvolution due to their intrinsic lower resolution, i.e., the large ratio of line widths to coupling constants, which often leads to overlapping peaks. Furthermore, the ratio of the coupling constant to the difference in frequency is much smaller in Benchtop spectra resulting in highly asymmetric peaks through strong coupling.</p>
        <p>3.5. Discussion on training with synthetic vs. experimental data 1D NMR data can have strongly divergent properties: Single peaks can have various shapes, widths, and SNRs. Furthermore, spectral regions can be sparse or crowded, and they can have a low or a high dynamic range. This often leads to an underrepresentation of specific spectral characteristics in experimental NMR data sets. Hence, we think that relying on synthetic data, and therefore being able to shape the data distribution, is a significant advantage. Using experimental data sets, one must necessarily deal with imbalanced data, which makes it difficult to solve rare but important region types [17,48].</p>
        <p>Spectra annotated by experts suffer from label noise. Label noise refers to variations in quality and the lack of consistency in the expert annotation of experimental spectra, cf. Section 3.3. Hence, label noise can hamper the training convergence, since the algorithm does not have a unique reference what it should learn, e.g. two identical regions are labeled differently. This problem becomes especially obvious in regions with strongly overlapping peaks, resulting in a respective performance drop.</p>
        <p>Compared to other synthetic data-based machine learning methods for peak picking and deconvolution in NMR, which randomly sampled single peaks [31], we model signals through multiplet structures. This bears the benefit that peak parameters are shared within the same multiplet, e.g., width, coupling constants, and intensity ratios, and can be learned and exploited by the deep learning model.</p>
        <p>Our pre-processing, namely the dynamic scaling and shifting approach, essentially supports the neural network to get the data needed to detect peaks in these challenging situations, see. SI 5.13 for a quantitative analysis on the benefits of the dynamic scaling. Due to the augmented wide field of view and the bidirectional LSTM, the network can capture more far-reaching dependencies, which are needed e.g., to determine the width of broad peaks or to extract shared information in multiplets with high coupling constants. However, other deep learning algorithms for peak picking or deconvolution in NMR rely solely on local CNN-based feature extraction and a single peak class [31,49]. A narrow field of view combined with a single peak class cannot handle broad features and the overlap of broad and narrow peaks, for example, see Fig. 8A.</p>
        <p>Classical fitting-based 1D deconvolution approaches already exist and are frequently used; cf. Chapter 1. It is shown that it is unnecessary to solve linear parts of the optimization problem, for instance, for the amplitude and Lorentzian-to-Gaussian fraction parameter of the pseudo-Voigt lines, with deep learning. These cases can be solved with less computational cost and more accurately with convex solvers [50]. However, particularly difficult parts of the deconvolution problem greatly benefit from deep learning. Therefore, our approach relies on deep learning for critical components of the inverse problem of deconvolution, namely the integer optimization problem of finding the number of peaks and the non-convex program for determining their corresponding position and width. The reasons for this superiority of deep learning approaches for these tasks are manifold. On the one hand, to get the proper model order with classical optimization algorithms, one has to trade off the fitting error and the number of peaks explicitly. This often does not work robustly in complex scenarios [3]. On the other hand, circumventing an explicit regularization criterion with labeling, and thus reducing it to a pattern recognition problem, has several advantages. Through labeling, one can essentially teach the network what a peak is and what not. In particular, one can make use of regular patterns in NMR. The deconvolution solution becomes tunable by adapting the line shrinking parameter in the labeling; cf. SI 5.4. In contrast, in classical fitting-based deconvolution methods, this would often mean optimizing explicit regularization parameters whose implications on the deconvolution result are difficult to assess. For example, we compare a classical with a deep learning-based deconvolution algorithm, cf. Fig. 9. The classical algorithm is oriented on Mestre-GSD [15], but not an exact implementation of it. The deep learning algorithm is our approach. We manually tuned the parameters of the classical algorithm to yield good results in two specific regions. However, this also demonstrates a downside of many classical algorithms, i.e., the need for region-or spectrum-specific tuning of parameters. Furthermore, we observed superior results of our approach, especially on shoulder and broad peaks, which is demonstrated in Fig. 9. In the upper panel of Fig. 9, a rather crowded region is shown. Our method finds shoulder peaks more reliably, e.g., zoom at 5.12 ppm and 4.89 ppm, and does not over-pick in contrast to the classical picker, e.g., zoom at around 5.05 ppm for a false picked peak. In the second panel of Fig. 9, a broad peak is exhibited. We observe that the classical algorithm has problems with the broad peak and picks several peaks close to the maximum of the broad peak. In contrast our method yields a sparse single peak solution for this region. Additionally, having a reasonable estimate of the nonlinear parameters and model order through the prediction of the neural network reduces the rest of the optimization problem often to a locally convex fine-tuning, which can be solved efficiently. The deconvolution of a reasonably crowded spectrum, i.e. more than 500 peaks, is completed in a few seconds.</p>
        <p>For efficient training and good deconvolution performance, consistent labeling is essential. In contrast to labeling methods based on line refitting of peaks, cf. Li et al. [31], our labeling approach is fast and, therefore, can be computed on the fly. Compared to labeling based on local maxima, the present methods can detect multiple peaks even if there is just a single local maximum within the region.</p>
        <p>We introduce a robust, expert-level quality deep learning-based deconvolution algorithm for 1D experimental NMR spectra. Our custom network, labeling, and pre-processing enable excellent performance in challenging scenarios such as crowded, high dynamic range and shoulder peak regions, as well as in cases of overlap of narrow with broad peaks. On experimental high-field 1 H spectra, our algorithm yields low residual errors between the reconstructed spectrum and the original spectrum and sparse peak lists. The number of peaks proposed by our algorithm and the residual error lie within the uncertainty range of expert annotations. Furthermore, the algorithm copes well with Gaussian line-shape distortions, i.e. Voigt lines, and small baseline and phase distortions, reflected by good test results with respect to the number of peaks picked and residual on distorted synthetic spectra. Thus, the presented algorithm is at least comparable, if not superior, to human experts and robust toward small distortions. Although developed for high-field proton spectroscopy, we find that our method can process different nuclei, base frequencies, resolutions, and sensitivities. Altogether, by proving its usefulness in challenging situa- tions, our approach is an attractive 1D NMR deconvolution algorithm.</p>
        <p>Further testing robustness against substantial distortions, quantifying the uncertainty of predictions, and interpreting the neural network model could be interesting future research directions. Moreover, we are planning to extend our research towards the extraction of multiplet information, which assists structure elucidation and verification.</p>
        <p>Part of this work was supported by Innosuisse -Swiss Innovation Agency, Grant No. 2155007318. Furthermore, we thank the experts for annotating the utilized experimental 1D NMR spectra.</p>
        <p>Experimental data will be available upon request.</p>
        <p>The method is available within the 
            <rs type="software">TopSpin TS</rs>
            <rs type="version">4.2</rs> release of 
            <rs type="creator">Bruker Biospin</rs> with the command "mldcon".
        </p>
        <p>The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: All authors except Jan Dirk Wegner, Roland Sigel and Helmut Grabner report financial support was provided by Innosuisse Swiss Innovation Agency. All authors from Bruker Switzerland AG report a relationship with Bruker BioSpin AG that includes: employment.</p>
        <p>Supplementary data associated with this article can be found, in the online version, at https://doi.org/10.1016/j.jmr.2022.107357.</p>
    </text>
</tei>

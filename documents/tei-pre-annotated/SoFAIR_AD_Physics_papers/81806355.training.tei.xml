<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T14:37+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Many searches for physics beyond the Standard Model at the Large Hadron Collider (LHC) rely on top tagging algorithms, which discriminate between boosted hadronic top quarks and the much more common jets initiated by light quarks and gluons. We note that the hadronic calorimeter (HCAL) effectively takes a "digital image" of each jet, with pixel intensities given by energy deposits in individual HCAL cells. Viewed in this way, top tagging becomes a canonical pattern recognition problem. With this motivation, we present a novel top tagging algorithm based on an Artificial Neural Network (ANN), one of the most popular approaches to pattern recognition. The ANN is trained on a large sample of boosted tops and light quark/gluon jets, and is then applied to independent test samples. The ANN tagger demonstrated excellent performance in a Monte Carlo study: for example, for jets with p T in the 1100-1200 GeV range, 60% top-tag efficiency can be achieved with a 4% mis-tag rate. We discuss the physical features of the jets identified by the ANN tagger as the most important for classification, as well as correlations between the ANN tagger and some of the familiar top-tagging observables and algorithms.Many searches for physics beyond the Standard Model at the Large Hadron Collider (LHC) rely on top tagging algorithms, which discriminate between boosted hadronic top quarks and the much more common jets initiated by light quarks and gluons. We note that the hadronic calorimeter (HCAL) effectively takes a "digital image" of each jet, with pixel intensities given by energy deposits in individual HCAL cells. Viewed in this way, top tagging becomes a canonical pattern recognition problem. With this motivation, we present a novel top tagging algorithm based on an Artificial Neural Network (ANN), one of the most popular approaches to pattern recognition. The ANN is trained on a large sample of boosted tops and light quark/gluon jets, and is then applied to independent test samples. The ANN tagger demonstrated excellent performance in a Monte Carlo study: for example, for jets with p T in the 1100-1200 GeV range, 60% top-tag efficiency can be achieved with a 4% mis-tag rate. We discuss the physical features of the jets identified by the ANN tagger as the most important for classification, as well as correlations between the ANN tagger and some of the familiar top-tagging observables and algorithms.</p>
        <p>Many extensions of the Standard Model (SM) predict new particles with masses around the TeV scale. Searches for such new particles form a major component of the experimental program at the Large Hadron Collider (LHC). In most models, the new particles are unstable, and their decays often contain weak-scale SM states, namely the W and Z bosons, the Higgs boson, and the top quark. Searches for final states containing top quarks are particularly important, due to the special role played by the top sector in many models of electroweak symmetry breaking. Decays of heavy new particles with mass above the electroweak scale typically result in highly energetic, relativistic top quarks in the lab frame. Identifying and characterizing such "boosted" top quarks in the data is crucial for new physics searches and tests of naturalness [1] at the LHC, especially as the bounds on the new physics mass scales in many candidate models are pushed higher. Examples of new physics leading to boosted top signatures include Kaluza-Klein gluons [2,3] and string Regge states [4] of the Randall-Sundrum model, stops [5] and gluinos [6] of supersymmetry, top and light quark partner decays in Composite Higgs models [7][8][9][10][11][12], and many others.Many extensions of the Standard Model (SM) predict new particles with masses around the TeV scale. Searches for such new particles form a major component of the experimental program at the Large Hadron Collider (LHC). In most models, the new particles are unstable, and their decays often contain weak-scale SM states, namely the W and Z bosons, the Higgs boson, and the top quark. Searches for final states containing top quarks are particularly important, due to the special role played by the top sector in many models of electroweak symmetry breaking. Decays of heavy new particles with mass above the electroweak scale typically result in highly energetic, relativistic top quarks in the lab frame. Identifying and characterizing such "boosted" top quarks in the data is crucial for new physics searches and tests of naturalness [1] at the LHC, especially as the bounds on the new physics mass scales in many candidate models are pushed higher. Examples of new physics leading to boosted top signatures include Kaluza-Klein gluons [2,3] and string Regge states [4] of the Randall-Sundrum model, stops [5] and gluinos [6] of supersymmetry, top and light quark partner decays in Composite Higgs models [7][8][9][10][11][12], and many others.</p>
        <p>Due to relativistic kinematics, the decay products of a boosted top quark are highly collimated. For instance, hadronic decay of a top quark of p T ∼ 1 TeV would produce three quarks collimated into a cone of rough size R ∼ 0.4 and result in a specific pattern of hadronic activity in the detector. Classical event reconstruction techniques are inadequate to tag and measure such topologies, as most of the showered radiation falls into a small angular region. One solution is to cluster the event with a large jet cone (R ∼ 1), and consider the features of energy distribution inside such "fat" jets (so-called jet substructure), instead of correlations between individual small radius jets. Over the past decade, a variety of methods for boosted top tagging via jet substructure have been developed (see refs. [13,14] for a review), most of which can be cast into several (non exclusive) groups. Jet shapes are observables based on various moments of the jet energy distribution. Notable examples are angular correlations studied extensively in ref. [15], sphericity JHEP07(2015)086 tensors [16,17] and other perturbatively calculable jet shapes [18]. Considerations of jet clustering history led to development of several jet grooming methods [19][20][21], where the differences in the late steps of jet clustering between heavy SM states and QCD jets from light partons have been successfully applied in tagging of heavy SM states. HEPTopTagger [22] applied mass-drop filtering to top tagging. Furthermore, 
            <rs type="software">Prong Taggers</rs> such as Johns Hopkins tagger [23] and N -subjettiness [24,25] exploit the differences in the number of hard energy depositions within the boosted jet (e.g. three-body top decays compared to the typical two-body splitting of a light jet). Parton level models of boosted decays and kinematic constraints built into them can also be used to study jet substructure, with the Template Overlap Method (TOM) [26][27][28][29] being the most notable example. More recently, Matrix Element Method [30,31] inspired techniques such as Shower Deconstruction have emerged [32,33], where a boosted jet is tagged using approximations to hard matrix elements and the parton shower. Soft drop declustering (a generalization of modified mass drop tagging) is another method which has been recently developed for removing nonglobal contributions (soft radiation) to the jet [34]. Several of these methods have been implemented in the analyses of the LHC data by the CMS and ATLAS collaborations; see, for example ref. [35][36][37][38].
        </p>
        <p>In this paper, we pursue an alternative approach to jet substructure. Experimentally, information about hadronic activity in an event comes mainly from the hadronic calorimeter (HCAL), with the basic observable being the energy deposited in each of the HCAL cells. One can think of the information provided by the HCAL as a digital image, with each cell (or topo-cluster) being identified as a pixel, and with energy deposit in the cell corresponding to the intensity (or grayscale color) of that pixel. From this point of view, boosted top identification is simply a classic image-recognition problem: distinguishing the energydeposit patterns characteristic of boosted tops from patterns due to other sources, such as the usual QCD jets. This suggests that computational algorithms developed in the field of image recognition could be of use in boosted top tagging. In a recent application of this idea, ref. [39] studied jet substructure as an image recognition problem in the context of boosted W tagging as well gluon/quark discrimination. The authors utilized a linear Fisher discriminant trained on a sample of signal and background events, in order to distinguish the desired events from the backgrounds. The method out-performs the existing methods of W tagging, illustrating the benefits of the image recognition approach to jet substructure.In this paper, we pursue an alternative approach to jet substructure. Experimentally, information about hadronic activity in an event comes mainly from the hadronic calorimeter (HCAL), with the basic observable being the energy deposited in each of the HCAL cells. One can think of the information provided by the HCAL as a digital image, with each cell (or topo-cluster) being identified as a pixel, and with energy deposit in the cell corresponding to the intensity (or grayscale color) of that pixel. From this point of view, boosted top identification is simply a classic image-recognition problem: distinguishing the energydeposit patterns characteristic of boosted tops from patterns due to other sources, such as the usual QCD jets. This suggests that computational algorithms developed in the field of image recognition could be of use in boosted top tagging. In a recent application of this idea, ref. [39] studied jet substructure as an image recognition problem in the context of boosted W tagging as well gluon/quark discrimination. The authors utilized a linear Fisher discriminant trained on a sample of signal and background events, in order to distinguish the desired events from the backgrounds. The method out-performs the existing methods of W tagging, illustrating the benefits of the image recognition approach to jet substructure.</p>
        <p>For earlier examples of image-recognition techniques applied to jets, see refs. [40][41][42][43].For earlier examples of image-recognition techniques applied to jets, see refs. [40][41][42][43].</p>
        <p>With this motivation, we constructed a new top tagger algorithm based on one of the most popular approaches to image recognition, Artificial Neural Networks (ANNs). In this approach, each jet is classified as top or non-top according to a highly non-linear scoring function. The function contains multiple adjustable parameters, called weights. These are chosen using a training procedure, in which the ANN is presented with a large sample of jets that are known to be top or non-top, and the weights are chosen to maximize the number of correctly identified jets in this sample. (In our study, all samples are generated by Monte Carlo simulations. In experimental applications, ANN may be trained on either MC samples or carefully selected "calibration" data sets.) Having fixed the weights, the ANN is then applied to independent samples containing both top and non-top jets, and JHEP07(2015)086 asked to discriminate between them. We find that the performance of the ANN tagger significantly exceeds that of several popular tagging algorithms currently in use over a wide range of p T , demonstrating the practical utility of this approach.With this motivation, we constructed a new top tagger algorithm based on one of the most popular approaches to image recognition, Artificial Neural Networks (ANNs). In this approach, each jet is classified as top or non-top according to a highly non-linear scoring function. The function contains multiple adjustable parameters, called weights. These are chosen using a training procedure, in which the ANN is presented with a large sample of jets that are known to be top or non-top, and the weights are chosen to maximize the number of correctly identified jets in this sample. (In our study, all samples are generated by Monte Carlo simulations. In experimental applications, ANN may be trained on either MC samples or carefully selected "calibration" data sets.) Having fixed the weights, the ANN is then applied to independent samples containing both top and non-top jets, and JHEP07(2015)086 asked to discriminate between them. We find that the performance of the ANN tagger significantly exceeds that of several popular tagging algorithms currently in use over a wide range of p T , demonstrating the practical utility of this approach.</p>
        <p>The paper is organized as follows. Section 2 describes the MC event samples used for training and testing the ANN tagger, as well as the pre-processing steps applied to these samples before the ANN is applied. Section 3 contains a detailed description of the ANN tagger, including the network architecture and the training algorithms we employed. In section 4, we present the results of our study of ANN tagger performance and comparisons with other popular taggers. We also discuss the physical features of jets that are dominant in the ANN classification, and the extent to which ANN output is correlated with that of other taggers. We conclude with a recap and a brief discussion of directions for future research in section 5. An appendix contains a brief description of the top taggers we use for the purpose of comparison with the ANN tagger.The paper is organized as follows. Section 2 describes the MC event samples used for training and testing the ANN tagger, as well as the pre-processing steps applied to these samples before the ANN is applied. Section 3 contains a detailed description of the ANN tagger, including the network architecture and the training algorithms we employed. In section 4, we present the results of our study of ANN tagger performance and comparisons with other popular taggers. We also discuss the physical features of jets that are dominant in the ANN classification, and the extent to which ANN output is correlated with that of other taggers. We conclude with a recap and a brief discussion of directions for future research in section 5. An appendix contains a brief description of the top taggers we use for the purpose of comparison with the ANN tagger.</p>
        <p>We generate benchmark event samples with 
            <rs type="software">MadGraph 5</rs> [44] at leading order, and shower them with Pythia 6 [45]. In order to study the effects of different showering algorithms on the results, we also generate separate data samples showered with 
            <rs type="software">Pythia</rs>
            <rs type="version">8</rs> [46]. For simplicity, we extract a pure sample of top jets from a Standard Model top pair-production simulation, at leading order with no matching. The tops are decayed in 
            <rs type="software">MadGraph</rs>
            <rs type="version">5</rs>, so that the angular distribution of the decay products is modeled correctly. Only hadronic top decays, t → bjj, are included. Similarly, we generate the light jet sample from a simulation of the QCD di-jet process, including both quarks and gluons in the final state, but no matching to extra jets. Fiducial cut |η| ≤ 5.0 is imposed at the hadron level. We cluster the events using the fastjet [47] implementation of the anti-k T algorithm [48] with a large jet cone of R = 1.0. For our analysis, we only use the highest p T jet in each event, and impose the cut |η jet | ≤ 2.5. We consider samples of jets within three jet p T ranges: 500-600 GeV, 800-900 GeV and 1100-1200 GeV. These three bins span a range of jet p T values relevant for top tagging at the LHC, while analyzing them separately provides information about p T sensitivity of the tagging efficiency and other parameters. Unless otherwise noted, we impose a cut on the jet mass (i.e. the invariant mass of all particles assigned to the jet), selecting jets within a window 130 GeV &lt; m R=1.0 J &lt; 210 GeV.
        </p>
        <p>(2.1)(2.1)</p>
        <p>A vast majority of top jets fall within this mass range, while most QCD jets are rejected by this cut. Discriminating the remaining QCD jets from top jets is the task for the top tagger.A vast majority of top jets fall within this mass range, while most QCD jets are rejected by this cut. Discriminating the remaining QCD jets from top jets is the task for the top tagger.</p>
        <p>In order to form an input to the ANN tagger, we preprocess each jet as follows. First, we find the center of the jet, defined by the sum of the coordinates of all particles weighted by their energies,In order to form an input to the ANN tagger, we preprocess each jet as follows. First, we find the center of the jet, defined by the sum of the coordinates of all particles weighted by their energies,</p>
        <p>where E = j E j is the total energy of the jet. We then shift the coordinates of each particle so that the jet is centered at the origin in the new coordinates:where E = j E j is the total energy of the jet. We then shift the coordinates of each particle so that the jet is centered at the origin in the new coordinates:</p>
        <p>Further, we find the jet "principal axis" in the (η, φ) plane, defined byFurther, we find the jet "principal axis" in the (η, φ) plane, defined by</p>
        <p>and rotate the coordinate system so that this principal axis is the same direction (+η) for all jets:and rotate the coordinate system so that this principal axis is the same direction (+η) for all jets:</p>
        <p>))</p>
        <p>These coordinate transformations remove information about the jet position in the calorimeter and its orientation in the (η, φ) plane. Both pieces of information are irrelevant for top tagging, and removing them from consideration allows the ANN tagger to focus on the irreducible physical differences between top and QCD jets. 1In the new coordinates, nearly all (98%) of the particles assigned to a given jet fall within a window of η ∈ [-π/2, π/2] and φ ∈ [π/2, π/2]. We model the HCAL response to the jet by dividing this window into 30 × 30 square cells. (The cell size is approximately 0.1×0.1, close to the realistic values in ATLAS and CMS.) The normalized energy deposited in each cell, ε ab (a, b = 1 . . . 30), is computed by adding up the energies of all particles falling within that cell, and dividing by the total energy of the jet. The last step is necessary to render the algorithm insensitive to the total jet energy: once the jet p T is confined to a narrow range, the jet energy is very well correlated with its direction, which is irrelevant for top tagging. By construction, ε ab is dimensionless and lies between 0 and 1. In the language of image processing, each jet has been converted into an image with 30 ×30 pixels, with a grayscale color of each pixel given by the corresponding ε ab . These images can now be classified by an Artificial Neural Network (ANN), described in the following section.These coordinate transformations remove information about the jet position in the calorimeter and its orientation in the (η, φ) plane. Both pieces of information are irrelevant for top tagging, and removing them from consideration allows the ANN tagger to focus on the irreducible physical differences between top and QCD jets. 1In the new coordinates, nearly all (98%) of the particles assigned to a given jet fall within a window of η ∈ [-π/2, π/2] and φ ∈ [π/2, π/2]. We model the HCAL response to the jet by dividing this window into 30 × 30 square cells. (The cell size is approximately 0.1×0.1, close to the realistic values in ATLAS and CMS.) The normalized energy deposited in each cell, ε ab (a, b = 1 . . . 30), is computed by adding up the energies of all particles falling within that cell, and dividing by the total energy of the jet. The last step is necessary to render the algorithm insensitive to the total jet energy: once the jet p T is confined to a narrow range, the jet energy is very well correlated with its direction, which is irrelevant for top tagging. By construction, ε ab is dimensionless and lies between 0 and 1. In the language of image processing, each jet has been converted into an image with 30 ×30 pixels, with a grayscale color of each pixel given by the corresponding ε ab . These images can now be classified by an Artificial Neural Network (ANN), described in the following section.</p>
        <p>ANN tagger is based on a feed-forward neural network with an input layer consisting of 30 × 30 = 900 nodes, one for each calorimeter cell; two hidden layers, of 100 nodes each, to process the signal; and an output layer consisting of a single node, whose value Y is interpreted as the probability that a given jet comes from a boosted top decay. The architecture of the network is shown in figure 1. (For pedagogical introduction to Artificial Neural Networks in the context of image recognition, see for example [49].) Mathematically, the ANN can be thought of as a succession of non-linear transformations:ANN tagger is based on a feed-forward neural network with an input layer consisting of 30 × 30 = 900 nodes, one for each calorimeter cell; two hidden layers, of 100 nodes each, to process the signal; and an output layer consisting of a single node, whose value Y is interpreted as the probability that a given jet comes from a boosted top decay. The architecture of the network is shown in figure 1. (For pedagogical introduction to Artificial Neural Networks in the context of image recognition, see for example [49].) Mathematically, the ANN can be thought of as a succession of non-linear transformations:</p>
        <p>) where f is the so-called activation function, chosen to be) where f is the so-called activation function, chosen to be</p>
        <p>(3.(3.</p>
        <p>2)2)</p>
        <p>The inputs i are simply the normalized energy deposits ε ab defined above, rearranged in a single 900-dimensional vector: ε ab ≡ 30a+b . The weightsThe inputs i are simply the normalized energy deposits ε ab defined above, rearranged in a single 900-dimensional vector: ε ab ≡ 30a+b . The weights</p>
        <p>and the biases b (L) i are numbers determined by the training procedure, which we will now describe.and the biases b (L) i are numbers determined by the training procedure, which we will now describe.</p>
        <p>To train the network, we use a set of N/2 top and N/2 QCD jets, where N is a large number. For the i-th jet, we assign the "target output" variable: y i = 1 if it is a top jet, and y i = 0 if it is a QCD jet. Training consists of adjusting the weights so that the actual outputs of the ANN Y i correspond as close as possible to the target outputs y i , across the training set. To quantify the error, we use the logarithmic loss variableTo train the network, we use a set of N/2 top and N/2 QCD jets, where N is a large number. For the i-th jet, we assign the "target output" variable: y i = 1 if it is a top jet, and y i = 0 if it is a QCD jet. Training consists of adjusting the weights so that the actual outputs of the ANN Y i correspond as close as possible to the target outputs y i , across the training set. To quantify the error, we use the logarithmic loss variable</p>
        <p>The goal of training is to choose weights that minimize this function. We use the backpropagation algorithm [50], combined with gradient-descent minimization. In its simplest version, the algorithm can be summarized as follows [51]:The goal of training is to choose weights that minimize this function. We use the backpropagation algorithm [50], combined with gradient-descent minimization. In its simplest version, the algorithm can be summarized as follows [51]:</p>
        <p>1. Initialize the weights of each link to small random values.1. Initialize the weights of each link to small random values.</p>
        <p>2. Repeat until convergence of log-loss, for each input vector i :2. Repeat until convergence of log-loss, for each input vector i :</p>
        <p>JHEP07(2015)086JHEP07(2015)086</p>
        <p>• Forward: compute the output of each neuron until the output layer is reached, according to eq. (3.1).• Forward: compute the output of each neuron until the output layer is reached, according to eq. (3.1).</p>
        <p>• Backward: adjust the weights of each neuron by propagating backward the error at the output using• Backward: adjust the weights of each neuron by propagating backward the error at the output using</p>
        <p>where η is a small parameter called the learning rate.where η is a small parameter called the learning rate.</p>
        <p>We used several well-known tricks to make this algorithm more efficient. First, instead of updating the weights after each jet i , we used what is known as batch gradient descent so that the update on the weights is only done after all the jets in the training set have been processed. In that scenario, the updates on the weights are an average of the individual updates caused by each jet. Moreover, to reduce the odds of getting stuck at local minima we add what is known as a "momentum" to the updates. This means that the weights at iteration t, W t ij , are still being pushed by the update from the previous iteration ∆W t-1 ij , for exampleWe used several well-known tricks to make this algorithm more efficient. First, instead of updating the weights after each jet i , we used what is known as batch gradient descent so that the update on the weights is only done after all the jets in the training set have been processed. In that scenario, the updates on the weights are an average of the individual updates caused by each jet. Moreover, to reduce the odds of getting stuck at local minima we add what is known as a "momentum" to the updates. This means that the weights at iteration t, W t ij , are still being pushed by the update from the previous iteration ∆W t-1 ij , for example</p>
        <p>where α ∈ (0, 1) is a fixed parameter. A major concern in using ANN classifiers is over-fitting the network to the training data. Over-fitting is a common problem in machine learning, in which the training procedure produces a classifier that emphasizes random fluctuations in the training data set, as opposed to the underlying trend. An over-fitted classifier would achieve excellent performance on the training set, but this will not generalize well to data sets which were not part of the training set, rendering it useless. Many techniques for avoiding over-fitting have been proposed in the literature. However, over several experiments we found that it was easier to avoid over-fitting simply by using more training data and ensembling several neural networks together. To determine the size of the training set N tr needed to saturate the learning of our neural network, we studied the performance of the trained network on a cross-validation set of 50000 top and QCD jets, as a function of N tr . For this analysis, the performance is characterized by the ROC AUC (area under the receiver operating characteristic curve) performance metric, which assigns a value of 0.5 to a random classifier and a value of 1.0 to a perfect classifier. 3 As can be seen on figure 2 To further improve the performance of our tagger, we ensembled multiple neural networks together. The idea is to train B neural networks together, with the output given by the average of their outputs,where α ∈ (0, 1) is a fixed parameter. A major concern in using ANN classifiers is over-fitting the network to the training data. Over-fitting is a common problem in machine learning, in which the training procedure produces a classifier that emphasizes random fluctuations in the training data set, as opposed to the underlying trend. An over-fitted classifier would achieve excellent performance on the training set, but this will not generalize well to data sets which were not part of the training set, rendering it useless. Many techniques for avoiding over-fitting have been proposed in the literature. However, over several experiments we found that it was easier to avoid over-fitting simply by using more training data and ensembling several neural networks together. To determine the size of the training set N tr needed to saturate the learning of our neural network, we studied the performance of the trained network on a cross-validation set of 50000 top and QCD jets, as a function of N tr . For this analysis, the performance is characterized by the ROC AUC (area under the receiver operating characteristic curve) performance metric, which assigns a value of 0.5 to a random classifier and a value of 1.0 to a perfect classifier. 3 As can be seen on figure 2 To further improve the performance of our tagger, we ensembled multiple neural networks together. The idea is to train B neural networks together, with the output given by the average of their outputs,</p>
        <p>In our application, B = 10. All networks are trained using the same training set, but the jets are weighted. For the first network, all weights are set to one. Jets which are heavily misclassified by the first network are then assigned a larger weight, while jets which are correctly classified are assigned a smaller weight. This re-weighted training set is then used to train the second network, and so on. This procedure allows the training algorithm to focus on specific events that are particularly arduous to classify, improving overall performance. For some parameter choices, this method can be mapped to boosted methods such as ADAboost [52], where the weak classifiers are feed-forward ANNs.In our application, B = 10. All networks are trained using the same training set, but the jets are weighted. For the first network, all weights are set to one. Jets which are heavily misclassified by the first network are then assigned a larger weight, while jets which are correctly classified are assigned a smaller weight. This re-weighted training set is then used to train the second network, and so on. This procedure allows the training algorithm to focus on specific events that are particularly arduous to classify, improving overall performance. For some parameter choices, this method can be mapped to boosted methods such as ADAboost [52], where the weak classifiers are feed-forward ANNs.</p>
        <p>The ensemble of ANNs described above has been trained on sets of about 50,000 top and QCD jets each, in each of the three p T bins, 500-600 GeV, 800-900 GeV, and 1100-1200 GeV. These sets are large enough to avoid over-fitting, see figure 2. The ANN ensemble has then been applied to test sets consisting of about 15,000 top and QCD jets each, in the same p T bins. 4 The distribution of the neural network output O on the test sets is shown in figure 3. The classification power of this observable is clear from the figure:The ensemble of ANNs described above has been trained on sets of about 50,000 top and QCD jets each, in each of the three p T bins, 500-600 GeV, 800-900 GeV, and 1100-1200 GeV. These sets are large enough to avoid over-fitting, see figure 2. The ANN ensemble has then been applied to test sets consisting of about 15,000 top and QCD jets each, in the same p T bins. 4 The distribution of the neural network output O on the test sets is shown in figure 3. The classification power of this observable is clear from the figure:</p>
        <p>top jets are predominantly assigned O ≈ 1.0, while QCD jets are predominantly assigned O ≈ 0.0. To use the ANN ensemble as a top-tagger, we simply choose a threshold value O th , and assign the "top tag" to any jet with O ≥ O th and the "QCD tag" to any jet with O &lt; O th .top jets are predominantly assigned O ≈ 1.0, while QCD jets are predominantly assigned O ≈ 0.0. To use the ANN ensemble as a top-tagger, we simply choose a threshold value O th , and assign the "top tag" to any jet with O ≥ O th and the "QCD tag" to any jet with O &lt; O th .</p>
        <p>To discuss the performance of the ANN tagger, it is convenient to define efficiency and mis-tag rates as follows:To discuss the performance of the ANN tagger, it is convenient to define efficiency and mis-tag rates as follows:</p>
        <p>where N top and N QCD are the total number of jets in the top and QCD jet samples, respectively, and N b a is the number of jets in sample a tagged as jets of type b (a, b =top, QCD). Efficiency and mis-tag rates can be varied by varying the threshold O th . The performance of the ANN tagger is shown in figure 4, where for comparison we also show the performance of three representative existing taggers, described in the appendix. In all cases, the ANN tagger outperforms the existing taggers, achieving lower mis-tag rates for the same tagging efficiency. The improvement is especially dramatic for high jet p T : for example, for jets with p T ∈ [1.1, 1.2] TeV range, the ANN tagger achieves 60% tagging efficiency with about 4% mis-tag rate, about a factor of 2 lower than the best of the existing taggers in our comparison pool. This clearly demonstrates the promise of the ANN-based approach.where N top and N QCD are the total number of jets in the top and QCD jet samples, respectively, and N b a is the number of jets in sample a tagged as jets of type b (a, b =top, QCD). Efficiency and mis-tag rates can be varied by varying the threshold O th . The performance of the ANN tagger is shown in figure 4, where for comparison we also show the performance of three representative existing taggers, described in the appendix. In all cases, the ANN tagger outperforms the existing taggers, achieving lower mis-tag rates for the same tagging efficiency. The improvement is especially dramatic for high jet p T : for example, for jets with p T ∈ [1.1, 1.2] TeV range, the ANN tagger achieves 60% tagging efficiency with about 4% mis-tag rate, about a factor of 2 lower than the best of the existing taggers in our comparison pool. This clearly demonstrates the promise of the ANN-based approach.</p>
        <p>What physical features of the jet are identified by the ANN as the primary characteristics of a top jet? Some insight is provided by the energy deposit patterns of the highestscoring and lowest-scoring jets, according to the ANN output O, in the top sample. These are shown in figure 5. It is clear that the jets receiving high scores are characterized by well-defined three-prong structure, with each of the three quarks from top decay forming a well-defined, relatively isolated subjet. The lowest-scoring jets are those where either the quarks are nearly collinear, or one of them is much softer than the other two (in the detector frame). Likewise, the QCD jets receiving the highest scores, and thus most likely to be mis-identified as tops, have well-defined, isolated subjets, while the QCD jets correctly tagged as such do not: see figure 6.What physical features of the jet are identified by the ANN as the primary characteristics of a top jet? Some insight is provided by the energy deposit patterns of the highestscoring and lowest-scoring jets, according to the ANN output O, in the top sample. These are shown in figure 5. It is clear that the jets receiving high scores are characterized by well-defined three-prong structure, with each of the three quarks from top decay forming a well-defined, relatively isolated subjet. The lowest-scoring jets are those where either the quarks are nearly collinear, or one of them is much softer than the other two (in the detector frame). Likewise, the QCD jets receiving the highest scores, and thus most likely to be mis-identified as tops, have well-defined, isolated subjets, while the QCD jets correctly tagged as such do not: see figure 6.</p>
        <p>To gain further insight, we studied correlations of rankings based on the ANN scores with other observables used to tag tops. table 1 contains the ranking correlation coefficients between the ANN score and the output of the other taggers in our comparison pool, on a variety of samples used in our analysis. (The correlation coefficients are normalized so that 1.0 indicates perfect correlation and -1.0 perfect anti-correlation, while 0 indicates absence of correlation.) In all cases, we observe significant, though far from perfect, positive correlations, with coefficients ranging from about 0.3 to 0.7. A visual illustration is provided by figure 7, which shows that the ranking of jets according to the ANN score and the Nsubjettiness are indeed correlated, in both top and light-jet samples; correlation plots for all other taggers and p T ranges look very similar. This should not be surprising since all top taggers to some extent exploit the same physical characteristics of the boosted top jets. Nevertheless, as noted above, ANN systematically outperforms the other taggers in terms of tagging efficiency vs. mistag rates, indicating that the complicated non-linear JHEP07(2015)086 observable created by the ANN learning process captures the information present in the jet substructure in a more optimal way. In other words, it seems that all taggers find roughly the same subset of jets to be "easily classifiable", and all have a very good success rate on this subset. However, the ANN tagger seems to be able to correctly classify a higher fraction of the jets outside of this subset, leading to higher overall success rate.To gain further insight, we studied correlations of rankings based on the ANN scores with other observables used to tag tops. table 1 contains the ranking correlation coefficients between the ANN score and the output of the other taggers in our comparison pool, on a variety of samples used in our analysis. (The correlation coefficients are normalized so that 1.0 indicates perfect correlation and -1.0 perfect anti-correlation, while 0 indicates absence of correlation.) In all cases, we observe significant, though far from perfect, positive correlations, with coefficients ranging from about 0.3 to 0.7. A visual illustration is provided by figure 7, which shows that the ranking of jets according to the ANN score and the Nsubjettiness are indeed correlated, in both top and light-jet samples; correlation plots for all other taggers and p T ranges look very similar. This should not be surprising since all top taggers to some extent exploit the same physical characteristics of the boosted top jets. Nevertheless, as noted above, ANN systematically outperforms the other taggers in terms of tagging efficiency vs. mistag rates, indicating that the complicated non-linear JHEP07(2015)086 observable created by the ANN learning process captures the information present in the jet substructure in a more optimal way. In other words, it seems that all taggers find roughly the same subset of jets to be "easily classifiable", and all have a very good success rate on this subset. However, the ANN tagger seems to be able to correctly classify a higher fraction of the jets outside of this subset, leading to higher overall success rate.</p>
        <p>Another interesting question is how the ANN performance varies with the jet mass m J . The training samples and test samples in all plots shown so far only contain jets in a 130 . . . 210 GeV mass window, where most top jets are expected to lie. We also applied the ANN tagger to the full sample of jets in the [800, 900] GeV p T range, without the mass cut. The jet mass distributions in this sample, before and after the ANN tagger is applied, as well as the tagging probability as a function of the jet mass, are shown in figure 8. (The cut on the ANN output used in the figure corresponds to the overall tag efficiency in the m J = 130 . . . 210 GeV window of 70%.) For jet mass below 130 GeV, the probability of a positive top tag drops rapidly, for both top and QCD jets. This is presumably due to the fact that jets with a clear three-prong structure are unlikely to have a low mass. On the other hand, for jet mass above 210 GeV, the probability of a top jet being correctly identified is roughly independent of m J , while the probability of a QCD jet being misidentified as a top grows linearly with m J , presumably because large-mass QCD jets are more likely to have a top-like multi-prong structure. It should also be noted that the tag probability is smooth on the boundaries of the mass window selected for training, indicating that there is no strong dependence on the choice of the training sample. The ability of the ANN tagger to reject jets with low invariant mass may be useful in reducing effects of the pile-up.Another interesting question is how the ANN performance varies with the jet mass m J . The training samples and test samples in all plots shown so far only contain jets in a 130 . . . 210 GeV mass window, where most top jets are expected to lie. We also applied the ANN tagger to the full sample of jets in the [800, 900] GeV p T range, without the mass cut. The jet mass distributions in this sample, before and after the ANN tagger is applied, as well as the tagging probability as a function of the jet mass, are shown in figure 8. (The cut on the ANN output used in the figure corresponds to the overall tag efficiency in the m J = 130 . . . 210 GeV window of 70%.) For jet mass below 130 GeV, the probability of a positive top tag drops rapidly, for both top and QCD jets. This is presumably due to the fact that jets with a clear three-prong structure are unlikely to have a low mass. On the other hand, for jet mass above 210 GeV, the probability of a top jet being correctly identified is roughly independent of m J , while the probability of a QCD jet being misidentified as a top grows linearly with m J , presumably because large-mass QCD jets are more likely to have a top-like multi-prong structure. It should also be noted that the tag probability is smooth on the boundaries of the mass window selected for training, indicating that there is no strong dependence on the choice of the training sample. The ability of the ANN tagger to reject jets with low invariant mass may be useful in reducing effects of the pile-up.</p>
        <p>The final issue we address is the IR-safety of the ANN output. As any observable in jet physics, the ANN score must be IR-safe (or at least Sudakov-safe [53]) to be useful. Canonically, IR-safety simply requires that the observable be unchanged by exactly collinear 1 → 2 parton splitting, or an emission of an infinitely soft gluon. Since neither process affects the energy deposits in calorimeter cells ε ab , and since those energy deposits are the only information used by the ANN, its output is manifestly IR-safe by this definition. As a practical matter, however, one might still worry about the sensitivity of the output to non-perturbative physics involved in splittings at small, but finite, angles, and emission of gluons with small, but finite, energy. The modeling of this physics in MC generators such as 
            <rs type="software">Pythia</rs> involves approximations with poorly understood systematic errors, and if the ANN output were determined predominantly by features that depend strongly on the showering model, MC studies would clearly be of very limited utility in assessing the ANN performance on real data. To address this concern, we applied the ANN tagger, trained as described above on jet samples showered with 
            <rs type="software">Pythia</rs>
            <rs type="version">6</rs>, to alternative jet samples generated with the same physics inputs but showered with 
            <rs type="software">Pythia</rs>
            <rs type="version">8</rs>. 
            <rs type="software">Pythia</rs>
            <rs type="version">8</rs> implements p T -ordered showering, while the version of 
            <rs type="software">Pythia 6</rs> used throughout this paper uses the invariant mass as the evolution variable. There are also significant differences in the modeling of multiple interactions and initial state radiation between the two versions. We applied the ANN and the three taggers in our comparison pool to the same samples. The result is shown in figure 9. The ANN tagger continues to perform well on test samples generated with a showering model different from the one used in the training set. This indicates that the features ANN uses to classify jets are physical, rather than artifacts of a particular showering model. Moreover, while there is a non-trivial dependence of the efficiency/mis-tag rate curves on the generator, the effect is of the same size for all taggers considered here. In other words, ANN does not appear to be unusually sensitive in this regard.
        </p>
        <p>In this paper, we proposed and explored a new approach to the analysis of jet substructure, specifically top-jet tagging, based on Artificial Neural Network (ANN). The main result of the analysis is captured in figure 4: the ANN tagger significantly outperforms traditional taggers on the MC "datasets" used in our study. In a sense, this should not come as a surprise: while the ANN uses the same input information as any other tagger, the training procedure constructs a non-linear function of these inputs which is specifically chosen to maximize its power to classify jets. This maximization takes place on a restricted but extremely broad set of functions, encoded in figure 1 or eq. (3.1), and the resulting observable is probably not far away from the theoretical upper limit on classification performance, given the angular resolution fixed by the calorimeter cell size (in our study, 0.1×0.1). If this is indeed the case, the ANN can be useful in theoretical studies, serving as a benchmark for other observables used for boosted top tagging.In this paper, we proposed and explored a new approach to the analysis of jet substructure, specifically top-jet tagging, based on Artificial Neural Network (ANN). The main result of the analysis is captured in figure 4: the ANN tagger significantly outperforms traditional taggers on the MC "datasets" used in our study. In a sense, this should not come as a surprise: while the ANN uses the same input information as any other tagger, the training procedure constructs a non-linear function of these inputs which is specifically chosen to maximize its power to classify jets. This maximization takes place on a restricted but extremely broad set of functions, encoded in figure 1 or eq. (3.1), and the resulting observable is probably not far away from the theoretical upper limit on classification performance, given the angular resolution fixed by the calorimeter cell size (in our study, 0.1×0.1). If this is indeed the case, the ANN can be useful in theoretical studies, serving as a benchmark for other observables used for boosted top tagging.</p>
        <p>Being the first study of this novel approach to top tagging, the analysis presented here does not yet fully capture the complexity of the problem in a realistic experimental environment. The very promising results of this analysis strongly motivate further explorations. Some of the important outstanding issues include:Being the first study of this novel approach to top tagging, the analysis presented here does not yet fully capture the complexity of the problem in a realistic experimental environment. The very promising results of this analysis strongly motivate further explorations. Some of the important outstanding issues include:</p>
        <p>• The jets were extracted from event samples including only leading-order SM processes, t t and dijet. Subleading processes need to be included. In spite of their smaller rate, they may have outsize effect on the tagger performance: for example,• The jets were extracted from event samples including only leading-order SM processes, t t and dijet. Subleading processes need to be included. In spite of their smaller rate, they may have outsize effect on the tagger performance: for example,</p>
        <p>pure QCD processes with high multiplicity of partons in the final state can create "accidental substructure" [54,55], and the ANN would need to learn to distinguish it from real top jets.pure QCD processes with high multiplicity of partons in the final state can create "accidental substructure" [54,55], and the ANN would need to learn to distinguish it from real top jets.</p>
        <p>• Pile-up has not been included in our simulations. While many methods to reduce the effects of pile-up have been suggested [20,21], their interaction with the ANN tagger needs to be explored.• Pile-up has not been included in our simulations. While many methods to reduce the effects of pile-up have been suggested [20,21], their interaction with the ANN tagger needs to be explored.</p>
        <p>• Our study did not include detector simulation, which is needed, for example, to assess the impact of magnetic field.• Our study did not include detector simulation, which is needed, for example, to assess the impact of magnetic field.</p>
        <p>• Before the method can be applied to real data, concerns about possible MC biases in training the ANN need to be addressed. A preliminary study of this issue suggests that the features that determine the ANN output are not strongly MC-dependent, see figure 9. However, a more extensive study of this issue is needed, ideally using control/validation samples from real LHC data. In principle, it may even be possible to train the ANN directly on real data, assuming that sufficiently robust training samples can be extracted. This approach would entirely remove concerns about MC biases, and warrants further investigation.• Before the method can be applied to real data, concerns about possible MC biases in training the ANN need to be addressed. A preliminary study of this issue suggests that the features that determine the ANN output are not strongly MC-dependent, see figure 9. However, a more extensive study of this issue is needed, ideally using control/validation samples from real LHC data. In principle, it may even be possible to train the ANN directly on real data, assuming that sufficiently robust training samples can be extracted. This approach would entirely remove concerns about MC biases, and warrants further investigation.</p>
        <p>We plan to address some of these issues in future work. Another important direction is to further improve the tagger performance. A clear limitation of our tagger is that it only uses HCAL information. Other pieces of information are highly relevant for top tagging, the most obvious one being a sub-jet b-tag. This information can certainly be combined with the algorithm presented here to construct an even more powerful tagger. Also, the tagger presented here is based on a rather simple NN architecture and training procedure; more advanced techniques, such as using a convolutional neural network or pre-training the neural network with unsupervised techniques, may result in improved performance. Likewise, it may be possible to further optimize jet preprocessing (section 2) to improve performance and/or reduce the required training set size.We plan to address some of these issues in future work. Another important direction is to further improve the tagger performance. A clear limitation of our tagger is that it only uses HCAL information. Other pieces of information are highly relevant for top tagging, the most obvious one being a sub-jet b-tag. This information can certainly be combined with the algorithm presented here to construct an even more powerful tagger. Also, the tagger presented here is based on a rather simple NN architecture and training procedure; more advanced techniques, such as using a convolutional neural network or pre-training the neural network with unsupervised techniques, may result in improved performance. Likewise, it may be possible to further optimize jet preprocessing (section 2) to improve performance and/or reduce the required training set size.</p>
        <p>Finally, while in this paper we focused exclusively on tops, this approach can equally well be applied to other boosted-object jets, such as W and h. It would be interesting to see if performance improvements with respect to traditional taggers can also be achieved in those cases.Finally, while in this paper we focused exclusively on tops, this approach can equally well be applied to other boosted-object jets, such as W and h. It would be interesting to see if performance improvements with respect to traditional taggers can also be achieved in those cases.</p>
        <p>In summary, the novel approach to jet tagging based on pattern-recognition techniques, specifically Artificial Neural Networks, shows promise of significant improvements in tagger performance. While the analysis presented in this paper is only the first step, we hope that this approach will eventually become a useful tool in experimental searches for new physics.In summary, the novel approach to jet tagging based on pattern-recognition techniques, specifically Artificial Neural Networks, shows promise of significant improvements in tagger performance. While the analysis presented in this paper is only the first step, we hope that this approach will eventually become a useful tool in experimental searches for new physics.</p>
        <p>in part by the Belgian Federal Science Policy Office through the Interuniversity Attraction Pole P7/37. LGA's research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n • 604102 (HBP).in part by the Belgian Federal Science Policy Office through the Interuniversity Attraction Pole P7/37. LGA's research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement n • 604102 (HBP).</p>
        <p>For the purpose of comparison of the ANN tagger to the existing algorithms, we have chosen three existing methods, each one exploiting a different approach to boosted top tagging. In the following list, we give a brief description of the algorithms and the parameters we use for the analysis, while we refer the reader to the references within for detailed discussions.For the purpose of comparison of the ANN tagger to the existing algorithms, we have chosen three existing methods, each one exploiting a different approach to boosted top tagging. In the following list, we give a brief description of the algorithms and the parameters we use for the analysis, while we refer the reader to the references within for detailed discussions.</p>
        <p>• Template Overlap Method (TOM). TOM [26][27][28][29] is a jet substructure algorithm which aims to match the energy distribution of a fat jet to a partonic structure which models the decay of a heavy boosted particle. TOM algorithm proceeds by comparing libraries of kinematically allowed parton level decays of massive particles ("templates") to the energy distribution of a fat jet. The quality of a match is quantified by the overlap function Ov, which minimises the difference between the parton transverse momenta and the amount of p T deposited in small angular regions around the template patrons ("template sub cones"). An Ov ∼ 1 score signals a top like jet, while a Ov ∼ 0 is characteristic of light QCD jets. Here we use the TemplateTagger v.1.
            <rs type="version">0</rs> [56] implementation of the TOM algorithm.
        </p>
        <p>There are many ways generation of template libraries can be implemented. For simplicity and processing speed, here we consider templates at fixed total transverse momentum matched to the mid-point in each fat jet p T bin of the event samples (e.g. 550 GeV for fat jet p T = 500-600 GeV). We generate the template states using a sequential scan of 40 steps in η, φ over the angular region of R = 1.0 around the fat jet axis. We match the template libraries to the energy distribution of the fat jet using fixed template sub cones of size r 3 = 0.1, 0.15, 0.2 for template p T = 1150, 850, 550 GeV respectively, while we allow for the template resolution parameter σ a = p T, a /3, where p T, a is the transverse momentum of an individual template parton.There are many ways generation of template libraries can be implemented. For simplicity and processing speed, here we consider templates at fixed total transverse momentum matched to the mid-point in each fat jet p T bin of the event samples (e.g. 550 GeV for fat jet p T = 500-600 GeV). We generate the template states using a sequential scan of 40 steps in η, φ over the angular region of R = 1.0 around the fat jet axis. We match the template libraries to the energy distribution of the fat jet using fixed template sub cones of size r 3 = 0.1, 0.15, 0.2 for template p T = 1150, 850, 550 GeV respectively, while we allow for the template resolution parameter σ a = p T, a /3, where p T, a is the transverse momentum of an individual template parton.</p>
        <p>• N-subjettiness. Perhaps the most notable example of a "prong" tagger is Nsubjettiness [24,25]. The algorithm is based on calculating moments τ N , which serve as estimates of how well the jet energy distribution can be divided into N regions. The τ N are calculated by minimizing the p T weighted distances between calorimeter energy depositions and trial axes which divide the distribution into N regions, over the space of possible axis configurations. The N -subjettiness tagger used in our comparisons is the version publicly available on 
            <rs type="software">HepForge</rs>. 5For the purpose of top tagging the most useful observable is typically the ratio τ 3 /τ 2 , where a low score means that a jet distribution is described better by a three prong
        </p>
        <p>As an exercise, we also attempted to train the neural network on a set of jets with randomly oriented principal axes, i.e. without the rotation (2.6). We found that this procedure still yields an effective tagger; presumably, the neural net learns to ignore the axis orientation information during the training process. However, to achieve the same tagging performance, the randomly-oriented training set needs to be significantly larger.As an exercise, we also attempted to train the neural network on a set of jets with randomly oriented principal axes, i.e. without the rotation (2.6). We found that this procedure still yields an effective tagger; presumably, the neural net learns to ignore the axis orientation information during the training process. However, to achieve the same tagging performance, the randomly-oriented training set needs to be significantly larger.</p>
        <p>In eq. (3.1) and below, repeated indices are always summed over.In eq. (3.1) and below, repeated indices are always summed over.</p>
        <p>ROC AUC is a metric for quantifying performance of binary classifiers, widely used in machine learning literature. The ROC curve is identical to the "Efficiency vs. Mis-tag" curve familiar to particle physicists. The probability of a false positive ("mis-tag") is plotted on the horizontal axis, while the probability of a true positive ("efficiency") is plotted on the vertical axis. Changing the threshold of the classifier observable, in our case O, maps out a curve in the [0, 1] ranges on both axes; this is the ROC curve. The area under this curve is ROC AUC.ROC AUC is a metric for quantifying performance of binary classifiers, widely used in machine learning literature. The ROC curve is identical to the "Efficiency vs. Mis-tag" curve familiar to particle physicists. The probability of a false positive ("mis-tag") is plotted on the horizontal axis, while the probability of a true positive ("efficiency") is plotted on the vertical axis. Changing the threshold of the classifier observable, in our case O, maps out a curve in the [0, 1] ranges on both axes; this is the ROC curve. The area under this curve is ROC AUC.</p>
        <p>The CPU time required to train the network ensemble for each pT bin is about 12 hrs on a 2.4 GHz CPU, and processing time is about 0.01 sec per jet.The CPU time required to train the network ensemble for each pT bin is about 12 hrs on a 2.4 GHz CPU, and processing time is about 0.01 sec per jet.</p>
        <p>See http://fastjet.hepforge.org/contrib/contents/latest.html.See http://fastjet.hepforge.org/contrib/contents/latest.html.</p>
        <p>The authors are grateful for conversations with Fabio Maltoni and Jesse Thaler. We acknowledge the support of the U.S. National Science Foundation through grants PHY-1316222 (MC and MP) and PHY-0844667 (MP). SL is supported in part by the National Research Foundation of Korea grant MEST No. 2012R1A2A2A01045722. MB is supportedThe authors are grateful for conversations with Fabio Maltoni and Jesse Thaler. We acknowledge the support of the U.S. National Science Foundation through grants PHY-1316222 (MC and MP) and PHY-0844667 (MP). SL is supported in part by the National Research Foundation of Korea grant MEST No. 2012R1A2A2A01045722. MB is supported</p>
        <p>Arbitrary Units configuration. Conversely, a high τ 3 /τ 2 score is characteristic of two prong jets. Note that in the analysis of this paper we used the angular weight exponent β = 1 in calculations of τ N moments, as suggested in ref. [25].Arbitrary Units configuration. Conversely, a high τ 3 /τ 2 score is characteristic of two prong jets. Note that in the analysis of this paper we used the angular weight exponent β = 1 in calculations of τ N moments, as suggested in ref. [25].</p>
        <p>• ATLAS top tagger. Jet clustering history can provide useful insight into jet substructure. A notable example is the ATLAS top tagger [57] which utilises the differences between the top and light jets in the last step of jet clustering. The observable ATLAS uses is the "k T splitting scale" d 12 , defined as the value of the k T norm at the clustering step which goes from two subjects to one final jet. The d 12 observable is sensitive to the dynamics of hard splittings within the fat jet. The highly asymmetric splittings of typical light jets tend to be characterised by low values of d 12 with a distribution which falls off sharply with the increase in d 12 , while we expect typical top jets to be characterised by d 12 ∼ m 2 t /4. In addition to d 12 , ATLAS also imposes a lower cut on the trimmed jet mass of m j &gt; 130 GeV. Unless otherwise noted, here we omit the lower mass cut as the data samples we use for comparison are already restricted to a jet mass window in eq. (2.1).• ATLAS top tagger. Jet clustering history can provide useful insight into jet substructure. A notable example is the ATLAS top tagger [57] which utilises the differences between the top and light jets in the last step of jet clustering. The observable ATLAS uses is the "k T splitting scale" d 12 , defined as the value of the k T norm at the clustering step which goes from two subjects to one final jet. The d 12 observable is sensitive to the dynamics of hard splittings within the fat jet. The highly asymmetric splittings of typical light jets tend to be characterised by low values of d 12 with a distribution which falls off sharply with the increase in d 12 , while we expect typical top jets to be characterised by d 12 ∼ m 2 t /4. In addition to d 12 , ATLAS also imposes a lower cut on the trimmed jet mass of m j &gt; 130 GeV. Unless otherwise noted, here we omit the lower mass cut as the data samples we use for comparison are already restricted to a jet mass window in eq. (2.1).</p>
        <p>Open Access. This article is distributed under the terms of the Creative Commons Attribution License (CC-BY 4.0), which permits any use, distribution and reproduction in any medium, provided the original author(s) and source are credited.Open Access. This article is distributed under the terms of the Creative Commons Attribution License (CC-BY 4.0), which permits any use, distribution and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:17+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Background and purpose: Access to healthcare data is indispensable for scientific progress and innovation. Sharing healthcare data is time-consuming and notoriously difficult due to privacy and regulatory concerns. The Personal Health Train (PHT) provides a privacy-by-design infrastructure connecting FAIR (Findable, Accessible, Interoperable, Reusable) data sources and allows distributed data analysis and machine learning. Patient data never leaves a healthcare institute. Materials and methods: Lung cancer patient-specific databases (tumor staging and post-treatment survival information) of oncology departments were translated according to a FAIR data model and stored locally in a graph database. Software was installed locally to enable deployment of distributed machine learning algorithms via a central server. Algorithms (
            <rs type="software">MATLAB</rs>, 
            <rs type="software">code</rs> and documentation publicly available) are patient privacy-preserving as only summary statistics and regression coefficients are exchanged with the central server. A logistic regression model to predict post-treatment two-year survival was trained and evaluated by receiver operating characteristic curves (ROC), root mean square prediction error (RMSE) and calibration plots. Results: In 4 months, we connected databases with 23 203 patient cases across 8 healthcare institutes in 5 countries (Amsterdam, Cardiff, Maastricht, Manchester, Nijmegen, Rome, Rotterdam, Shanghai) using the PHT. Summary statistics were computed across databases. A distributed logistic regression model predicting post-treatment two-year survival was trained on 14 810 patients treated between 1978 and 2011 and validated on 8 393 patients treated between 2012 and 2015.
        </p>
        <p>The PHT infrastructure demonstrably overcomes patient privacy barriers to healthcare data sharing and enables fast data analyses across multiple institutes from different countries with different regulatory regimens. This infrastructure promotes global evidence-based medicine while prioritizing patient privacy.The PHT infrastructure demonstrably overcomes patient privacy barriers to healthcare data sharing and enables fast data analyses across multiple institutes from different countries with different regulatory regimens. This infrastructure promotes global evidence-based medicine while prioritizing patient privacy.</p>
        <p>Distributed (machine) learning reformulates conventional data analysis algorithms so that data centralization becomes unnecessary. Consequently, data transfer agreements are not needed. Distributed algorithms iteratively analyze separate databases and return the same solution as if data were centralized: essentially sharing research questions and answers between databases instead of data.Distributed (machine) learning reformulates conventional data analysis algorithms so that data centralization becomes unnecessary. Consequently, data transfer agreements are not needed. Distributed algorithms iteratively analyze separate databases and return the same solution as if data were centralized: essentially sharing research questions and answers between databases instead of data.</p>
        <p>We are convinced that only sharing research questions (and answers) between healthcare providers is a better, sustainable approach to medical data analysis, and can unlock orders of magnitude more data without violating privacy. To this end, we have developed an infrastructure (see Fig. 1) called the Personal Health Train [2] (PHT) consisting of -healthcare sites (''stations") containing FAIR [3] (Findable, Accessible, Interoperable, Reusable) data, -technical network connections and legal frameworks (''tracks"), -statistical learning applications (''trains").We are convinced that only sharing research questions (and answers) between healthcare providers is a better, sustainable approach to medical data analysis, and can unlock orders of magnitude more data without violating privacy. To this end, we have developed an infrastructure (see Fig. 1) called the Personal Health Train [2] (PHT) consisting of -healthcare sites (''stations") containing FAIR [3] (Findable, Accessible, Interoperable, Reusable) data, -technical network connections and legal frameworks (''tracks"), -statistical learning applications (''trains").</p>
        <p>A global community of likeminded healthcare providers and academic partners called CORAL (Community in Oncology for RApid Learning) was initiated at the 2016 European Society for Radiotherapy and Oncology (ESTRO) conference. In various research projects across the globe, CORAL members have worked on the realization of the PHT.A global community of likeminded healthcare providers and academic partners called CORAL (Community in Oncology for RApid Learning) was initiated at the 2016 European Society for Radiotherapy and Oncology (ESTRO) conference. In various research projects across the globe, CORAL members have worked on the realization of the PHT.</p>
        <p>An infrastructure to bring research questions to the data has been demonstrated to work recently in projects such as euroCAT [4,5], DataSHIELD [6] and OHDSI [7]. However, challenges remain in terms of the number of data subjects, number of data providers, and global coverage.An infrastructure to bring research questions to the data has been demonstrated to work recently in projects such as euroCAT [4,5], DataSHIELD [6] and OHDSI [7]. However, challenges remain in terms of the number of data subjects, number of data providers, and global coverage.</p>
        <p>The aim of this study is to show that the PHT distributed learning infrastructure can be scaled to many thousands of patients, approaching the size of national healthcare registries. Specifically, we set the goal (as registered on clinicaltrials.gov [8]) to machine learn a predictive model for post-treatment two-year survival on more than 20 000 non-small cell lung cancer (NSCLC) patients, in at least five healthcare providers from more than five countrieswithout any patient data leaving a healthcare provider.The aim of this study is to show that the PHT distributed learning infrastructure can be scaled to many thousands of patients, approaching the size of national healthcare registries. Specifically, we set the goal (as registered on clinicaltrials.gov [8]) to machine learn a predictive model for post-treatment two-year survival on more than 20 000 non-small cell lung cancer (NSCLC) patients, in at least five healthcare providers from more than five countrieswithout any patient data leaving a healthcare provider.</p>
        <p>This study was registered on clinicaltrials.gov [8] (https:// www.clinicaltrials.gov/ct2/show/NCT03564457) on 11-06-2018 (first posted date: 20-06-2018, actual study start date: 01-07-2018). Official project invitations were sent to eight sites on 18-06-2018 and two additional sites were contacted later but before the deadline of September 1. Fig. 2 shows the project timeline.This study was registered on clinicaltrials.gov [8] (https:// www.clinicaltrials.gov/ct2/show/NCT03564457) on 11-06-2018 (first posted date: 20-06-2018, actual study start date: 01-07-2018). Official project invitations were sent to eight sites on 18-06-2018 and two additional sites were contacted later but before the deadline of September 1. Fig. 2 shows the project timeline.</p>
        <p>In all participating sites, the project was approved by their institutional review boards (IRBs) or was conform to national information and research governance regulations. Given that the PHT is a privacy-by-design infrastructure where no individual patient data leaves the individual healthcare provider, no researcher has access to the data, data is anonymized or pseudonymized, and given the number of patients involved, internal privacy officers often felt informed consent was neither feasible nor necessary.In all participating sites, the project was approved by their institutional review boards (IRBs) or was conform to national information and research governance regulations. Given that the PHT is a privacy-by-design infrastructure where no individual patient data leaves the individual healthcare provider, no researcher has access to the data, data is anonymized or pseudonymized, and given the number of patients involved, internal privacy officers often felt informed consent was neither feasible nor necessary.</p>
        <p>Patient cohorts from routine clinical care databases (sites A-B and D-H) or clinical studies (site C) identified as non-small cell lung cancer patients were included in this study (Table 1). Data elements retrieved were -diagnosis, -diagnosis date, -vital status at last follow-up (alive or dead), -date of last follow-up after the diagnosis date, and cancer staging as defined by the American Joint Committee on Cancer [23]:Patient cohorts from routine clinical care databases (sites A-B and D-H) or clinical studies (site C) identified as non-small cell lung cancer patients were included in this study (Table 1). Data elements retrieved were -diagnosis, -diagnosis date, -vital status at last follow-up (alive or dead), -date of last follow-up after the diagnosis date, and cancer staging as defined by the American Joint Committee on Cancer [23]:</p>
        <p>Fig. 1. Personal Health Train infrastructure consisting of a cloud server and network (''tracks"), hospitals with FAIR data (''stations"), and learning applications (''trains").Fig. 1. Personal Health Train infrastructure consisting of a cloud server and network (''tracks"), hospitals with FAIR data (''stations"), and learning applications (''trains").</p>
        <p>-tumor (T) stage, -lymph node (N) stage, -metastasis (M) stage, -overall disease stage.-tumor (T) stage, -lymph node (N) stage, -metastasis (M) stage, -overall disease stage.</p>
        <p>If the diagnosis date was not available, date of first treatment, date of histology or date of intake were allowed as a surrogate for the date of diagnosis. Various staging editions (AJCC TNM cancer staging editions 1-8) were published and implemented during the period of treatment. Two-year survival was defined as a reported time interval between date of diagnosis and date of last follow-up of more than 2 * 365.24 days with a vital status 'alive' at last follow-up or a reported time interval between date of diagnosis and date of death of more than 2 * 365.24 days. Two-year death was defined as date of death less than 730.48 days after the date of diagnosis. Two-year survival was labelled missing if date of diagnosis, date of last follow-up, or vital status at last follow-up were missing. Two-year survival was also defined as missing if the date of last follow-up was earlier than two years after the date of diagnosis and the vital status at last follow-up was 'alive' (right-censored).If the diagnosis date was not available, date of first treatment, date of histology or date of intake were allowed as a surrogate for the date of diagnosis. Various staging editions (AJCC TNM cancer staging editions 1-8) were published and implemented during the period of treatment. Two-year survival was defined as a reported time interval between date of diagnosis and date of last follow-up of more than 2 * 365.24 days with a vital status 'alive' at last follow-up or a reported time interval between date of diagnosis and date of death of more than 2 * 365.24 days. Two-year death was defined as date of death less than 730.48 days after the date of diagnosis. Two-year survival was labelled missing if date of diagnosis, date of last follow-up, or vital status at last follow-up were missing. Two-year survival was also defined as missing if the date of last follow-up was earlier than two years after the date of diagnosis and the vital status at last follow-up was 'alive' (right-censored).</p>
        <p>To make data FAIR, a data model has to be agreed upon between parties. As per prior work [9] we have implemented this model using Semantic Web technology. In Fig. S2, a graphical representation of the model is shown and on github [10] (https://github.com/ RadiationOncologyOntology/20kChallenge/wiki) the mapping file containing the full data model including used classes and properties can be found. The 'FAIRness' of our implementation is described in the Supplementary Information (Section I).To make data FAIR, a data model has to be agreed upon between parties. As per prior work [9] we have implemented this model using Semantic Web technology. In Fig. S2, a graphical representation of the model is shown and on github [10] (https://github.com/ RadiationOncologyOntology/20kChallenge/wiki) the mapping file containing the full data model including used classes and properties can be found. The 'FAIRness' of our implementation is described in the Supplementary Information (Section I).</p>
        <p>Creating FAIR data out of clinical information systems generally involved the following tools at each institution: Source systems: these are the clinical systems in which the data elements required for this study were stored Extract, transform, load (ETL): software to extract data from source systems, transform data, and load it into a local data warehouse Data warehouse: a local database where data from multiple source systems (within a single institution) are combined Mapping: transformation from the local data warehouse schema to medical ontologies, e.g., the Radiation Oncology Ontology [9] (ROO) or the National Cancer Institute thesaurus [11] (NCIt) Graph database: Resource description framework (RDF) database where data elements are FAIR Table 2 shows an overview of the tools used at the various care providers. To support the setup of mapping and graph database software, installation manuals were distributed and remote support was provided. A tutorial describing how to set up software, map data to the required format, and upload it to a local Blazegraph endpoint is available on github [10].Creating FAIR data out of clinical information systems generally involved the following tools at each institution: Source systems: these are the clinical systems in which the data elements required for this study were stored Extract, transform, load (ETL): software to extract data from source systems, transform data, and load it into a local data warehouse Data warehouse: a local database where data from multiple source systems (within a single institution) are combined Mapping: transformation from the local data warehouse schema to medical ontologies, e.g., the Radiation Oncology Ontology [9] (ROO) or the National Cancer Institute thesaurus [11] (NCIt) Graph database: Resource description framework (RDF) database where data elements are FAIR Table 2 shows an overview of the tools used at the various care providers. To support the setup of mapping and graph database software, installation manuals were distributed and remote support was provided. A tutorial describing how to set up software, map data to the required format, and upload it to a local Blazegraph endpoint is available on github [10].</p>
        <p>Network for secure application distribution, execution, and communication (''tracks")Network for secure application distribution, execution, and communication (''tracks")</p>
        <p>For the secure distribution of and messaging between applications, a solution called the Varian Learning Portal (VLP, Varian Medical Systems, Palo Alto, CA) was used. The VLP is a cloudbased system which has implemented user, site, and project management so that a research project consisting of multiple data providers and researchers can securely share applications and communication between applications. To connect the VLP to a local data station, a learning connector is installed at each data provider. The learning connector is a gateway through which applications and communication are handled. The iterative execution of applications and communication between them is called a learning run and each data provider can accept or deny each learning run. All communication and other actions are logged and auditable by members of a given project.For the secure distribution of and messaging between applications, a solution called the Varian Learning Portal (VLP, Varian Medical Systems, Palo Alto, CA) was used. The VLP is a cloudbased system which has implemented user, site, and project management so that a research project consisting of multiple data providers and researchers can securely share applications and communication between applications. To connect the VLP to a local data station, a learning connector is installed at each data provider. The learning connector is a gateway through which applications and communication are handled. The iterative execution of applications and communication between them is called a learning run and each data provider can accept or deny each learning run. All communication and other actions are logged and auditable by members of a given project.</p>
        <p>The VLP allows a certificate-based upload of applications. Each application group has two parts. One that runs at the VLP in the cloud (master application) and one at each of the sites (site application). Multiple application groups were developed in this project.The VLP allows a certificate-based upload of applications. Each application group has two parts. One that runs at the VLP in the cloud (master application) and one at each of the sites (site application). Multiple application groups were developed in this project.</p>
        <p>The first application group's aim is cohort discovery. An application is sent to each site to determine and communicate generic statistics (counts) of the available data in the FAIR data station. This cohort discovery application includes a SPARQL Protocol and RDF Query Language (SPARQL) query that can be executed against the graph database. Each site application reports its site statistics to a master application running at the VLP which are then reported back to the researcher who initiated the application. Multiple variations of this application group were employed to generate summary statistics for patient subgroups. The second application group aims to train a logistic regression (LR) model. Each LR site application can, given a SPARQL query, train a LR model from the local dataset. The regression coefficients of each site LR model and patient counts are then sent to the master application that reaches consensus in an iterative manner. Fig. 3 illustrates the process followed in the LR application group. The third application group validates a given LR model on the sites. An application is sent to each site to compute model performance metrics (RMSE, ROC curve, AUC, calibration plots) and transfers these back to the master application which combines and passes them on to the researcher. Calibration plots reporting calibration-in-the-large and calibration slope are generated following Steyerberg [13] and include Wilson confidence intervals implemented by Winkler and Nichols [14].The first application group's aim is cohort discovery. An application is sent to each site to determine and communicate generic statistics (counts) of the available data in the FAIR data station. This cohort discovery application includes a SPARQL Protocol and RDF Query Language (SPARQL) query that can be executed against the graph database. Each site application reports its site statistics to a master application running at the VLP which are then reported back to the researcher who initiated the application. Multiple variations of this application group were employed to generate summary statistics for patient subgroups. The second application group aims to train a logistic regression (LR) model. Each LR site application can, given a SPARQL query, train a LR model from the local dataset. The regression coefficients of each site LR model and patient counts are then sent to the master application that reaches consensus in an iterative manner. Fig. 3 illustrates the process followed in the LR application group. The third application group validates a given LR model on the sites. An application is sent to each site to compute model performance metrics (RMSE, ROC curve, AUC, calibration plots) and transfers these back to the master application which combines and passes them on to the researcher. Calibration plots reporting calibration-in-the-large and calibration slope are generated following Steyerberg [13] and include Wilson confidence intervals implemented by Winkler and Nichols [14].</p>
        <p>The LR model is trained on patients treated between 1978 and 2012 and validated on all patients treated between 2012 and 2015. Only patients with complete diagnosis date, follow-up date, follow-up status, and complete T, N, M, and overall stage after imputation are included. This approach simulates the development of an LR model and sequential validation on new data becoming available over time. This is a TRIPOD type 2b validation [15].The LR model is trained on patients treated between 1978 and 2012 and validated on all patients treated between 2012 and 2015. Only patients with complete diagnosis date, follow-up date, follow-up status, and complete T, N, M, and overall stage after imputation are included. This approach simulates the development of an LR model and sequential validation on new data becoming available over time. This is a TRIPOD type 2b validation [15].</p>
        <p>The application used to train the LR coefficients in a distributed manner is based on the Alternating Direction Method of Multipliers (ADMM) and exemplary implementations by Boyd et al. [16,17]. A short description of ADMM is provided in the Supple-The application used to train the LR coefficients in a distributed manner is based on the Alternating Direction Method of Multipliers (ADMM) and exemplary implementations by Boyd et al. [16,17]. A short description of ADMM is provided in the Supple-</p>
        <p>The levels for each variable (T, N, M, and overall stage) are grouped in supercategories (Table 3) to allow regression on data of different AJCC TNM cancer staging editions and to bundle similar categories. T, N, M, and overall stages were dummy-coded to estimate the individual effect of each stage on two-year survival. A reference category was used to avoid multicollinearity issues in the regression model. The combination T1, N0, M0 and overall stage I was chosen as the reference because it is arguably the initial lung cancer stage. For example, the ordinal variable T stage, which takes six values (0 to 4, X), is converted to five binary variables representing T0, T2, T3, T4, TX.The levels for each variable (T, N, M, and overall stage) are grouped in supercategories (Table 3) to allow regression on data of different AJCC TNM cancer staging editions and to bundle similar categories. T, N, M, and overall stages were dummy-coded to estimate the individual effect of each stage on two-year survival. A reference category was used to avoid multicollinearity issues in the regression model. The combination T1, N0, M0 and overall stage I was chosen as the reference because it is arguably the initial lung cancer stage. For example, the ordinal variable T stage, which takes six values (0 to 4, X), is converted to five binary variables representing T0, T2, T3, T4, TX.</p>
        <p>If a patient misses entries for one or more of the variables T, N, M, and/or overall staging (but not all of them), imputation of the missing values is attempted. First, the missing values are logically induced from the permitted combinations of T, N, M, and overall stages. If the logical imputation is ambiguous because multiple imputation results are possible, the missing values are imputed probabilistically based on a subset of patients from the training cohort treated at the same site. A detailed imputation process description is presented in Fig. S3 (Supplementary Information) and an outline is given in the Supplementary Information (Section III).If a patient misses entries for one or more of the variables T, N, M, and/or overall staging (but not all of them), imputation of the missing values is attempted. First, the missing values are logically induced from the permitted combinations of T, N, M, and overall stages. If the logical imputation is ambiguous because multiple imputation results are possible, the missing values are imputed probabilistically based on a subset of patients from the training cohort treated at the same site. A detailed imputation process description is presented in Fig. S3 (Supplementary Information) and an outline is given in the Supplementary Information (Section III).</p>
        <p>In total, eight healthcare providers (''stations") were contacted on 18-06-2018 and two additional sites were contacted later. At the deadline of 01-09-2018 (71 days after the first formal project invitation), eight sites (in Amsterdam, Cardiff, Maastricht, Manchester, Nijmegen, Rome, Rotterdam, Shanghai) made NSCLC patient data available in their local database endpoints and two sites did not participate for logistical reasons: delayed response to first formal invitation in one case and too little time to participate after a second round of invitations in another case.In total, eight healthcare providers (''stations") were contacted on 18-06-2018 and two additional sites were contacted later. At the deadline of 01-09-2018 (71 days after the first formal project invitation), eight sites (in Amsterdam, Cardiff, Maastricht, Manchester, Nijmegen, Rome, Rotterdam, Shanghai) made NSCLC patient data available in their local database endpoints and two sites did not participate for logistical reasons: delayed response to first formal invitation in one case and too little time to participate after a second round of invitations in another case.</p>
        <p>A summary statistics application was sent via the Varian Learning Portal. It computed patient counts for each variable category, displayed in Table 4. Each site confirmed the validity of the summary statistics, a quality control step to ensure that correct data was used for modelling. A total number of 37 090 patients became available in the system. When restricting the search to patients:A summary statistics application was sent via the Varian Learning Portal. It computed patient counts for each variable category, displayed in Table 4. Each site confirmed the validity of the summary statistics, a quality control step to ensure that correct data was used for modelling. A total number of 37 090 patients became available in the system. When restricting the search to patients:</p>
        <p>-diagnosed or treated from 01-01-1978 (effective date of the AJCC TNM cancer staging edition 1) and before 01-01-2016 (allowing at least two years survival follow-up), -with complete diagnosis date, follow-up date, and follow-up status (to calculate two-year survival), the number of available patients decreased to 28 178, which forms the modelling cohort. Data of patients diagnosed before 2005 were mainly collected by two sites (with minor contributions from two other sites). Data of patients diagnosed after 2005 were made available by all sites. Overall, recent data was more abundant. More than half of the modelling data was provided by two sites: site G (43.0%) and site E (17.0%). Less than 6% of the mod-elling data was sourced from three sites: site D (2.4%), site C (2.3%), and site B (1.0%).-diagnosed or treated from 01-01-1978 (effective date of the AJCC TNM cancer staging edition 1) and before 01-01-2016 (allowing at least two years survival follow-up), -with complete diagnosis date, follow-up date, and follow-up status (to calculate two-year survival), the number of available patients decreased to 28 178, which forms the modelling cohort. Data of patients diagnosed before 2005 were mainly collected by two sites (with minor contributions from two other sites). Data of patients diagnosed after 2005 were made available by all sites. Overall, recent data was more abundant. More than half of the modelling data was provided by two sites: site G (43.0%) and site E (17.0%). Less than 6% of the mod-elling data was sourced from three sites: site D (2.4%), site C (2.3%), and site B (1.0%).</p>
        <p>Histograms for T, N, M, and overall stage categories after binning into supercategories (Table 3) but before imputation are shown in Fig. 4. Patients with missing or right-censored two-year survival are excluded. The percentage of patients alive at two years differed greatly in the provided data across sites (Fig. 4): from 89.1% in site A to 18.8% in site H. The distribution of T, N, M, and overall stage categories also varied across sites. Notably, T1 clearly dominated in sites A and C but other sites display a more balanced distribution of T categories (Fig. 4a). In sites A-E, N0 is the modal lymph node category but N2 is most frequent in sites F-H (Fig. 4b). All sites report most patients in the M0 category but the decrease in M0 patients correlates loosely with the percentage of patients alive at two years per site, e.g., site H reports 41.4% M1 compared to 8.8% in site A (Fig. 4c). As a direct consequence of the differences in T, N, and M category distributions, the overall stage distribution varies across sites (Fig. 4d).Histograms for T, N, M, and overall stage categories after binning into supercategories (Table 3) but before imputation are shown in Fig. 4. Patients with missing or right-censored two-year survival are excluded. The percentage of patients alive at two years differed greatly in the provided data across sites (Fig. 4): from 89.1% in site A to 18.8% in site H. The distribution of T, N, M, and overall stage categories also varied across sites. Notably, T1 clearly dominated in sites A and C but other sites display a more balanced distribution of T categories (Fig. 4a). In sites A-E, N0 is the modal lymph node category but N2 is most frequent in sites F-H (Fig. 4b). All sites report most patients in the M0 category but the decrease in M0 patients correlates loosely with the percentage of patients alive at two years per site, e.g., site H reports 41.4% M1 compared to 8.8% in site A (Fig. 4c). As a direct consequence of the differences in T, N, and M category distributions, the overall stage distribution varies across sites (Fig. 4d).</p>
        <p>In general, data completeness is not consistent in the network (Table 4). Sufficient follow-up information to compute two-year survival ranges from 92.1% (site D) to 44.1% (site B). Note that patients with incomplete follow-up (right-censored) have not been included in the modelling cohort displayed in Fig. 4 and Fig. 5. T, N, M, or overall stage information is frequently missing in half of the sites (sites E-H). Overall stage categories are not always reported: sites E and H do not provide overall stage information. Sites G, F, and A miss it for 39.8%, 31.8%, and 2.2% of their patients, respectively.In general, data completeness is not consistent in the network (Table 4). Sufficient follow-up information to compute two-year survival ranges from 92.1% (site D) to 44.1% (site B). Note that patients with incomplete follow-up (right-censored) have not been included in the modelling cohort displayed in Fig. 4 and Fig. 5. T, N, M, or overall stage information is frequently missing in half of the sites (sites E-H). Overall stage categories are not always reported: sites E and H do not provide overall stage information. Sites G, F, and A miss it for 39.8%, 31.8%, and 2.2% of their patients, respectively.</p>
        <p>Based on the temporal distribution of patients in the modelling cohort, we selected patients from 01-01-1978 until and including 31-12-2011 for training and patients from 01-01-2012 until and including 31-12-2015 for validation so that we achieved a split of approximately 2/3 to 1/3 (Fig. 5a).Based on the temporal distribution of patients in the modelling cohort, we selected patients from 01-01-1978 until and including 31-12-2011 for training and patients from 01-01-2012 until and including 31-12-2015 for validation so that we achieved a split of approximately 2/3 to 1/3 (Fig. 5a).</p>
        <p>Only 14 660 patients of 28 178 patients were complete cases (T, N, M, overall stage, and two-year survival) in the modelling cohort (Table 6). Imputation did not result in complete cases for some patients (see methods section for details) yielding a total of 23 203 patients, 14 810 (63.8%) patients for training and 8 393 (36.2%) patients for validation.Only 14 660 patients of 28 178 patients were complete cases (T, N, M, overall stage, and two-year survival) in the modelling cohort (Table 6). Imputation did not result in complete cases for some patients (see methods section for details) yielding a total of 23 203 patients, 14 810 (63.8%) patients for training and 8 393 (36.2%) patients for validation.</p>
        <p>The logistic regression application trained a model from the training data (years 1978-2011) with coefficients as displayed in Table 5. The convergence criteria of the algorithm are met after 81 iterations (25 minutes). The convergence of the algorithm is displayed in Fig. 5b: the root mean square error (RMSE, equivalent to the Brier score for binary outcomes) for predicting the probability of two-year survival (left y-axis) in the training cohort decreases per iteration and approaches 0.42. Although the RMSE has stabilized, not all regression coefficients (right y-axis) have converged.The logistic regression application trained a model from the training data (years 1978-2011) with coefficients as displayed in Table 5. The convergence criteria of the algorithm are met after 81 iterations (25 minutes). The convergence of the algorithm is displayed in Fig. 5b: the root mean square error (RMSE, equivalent to the Brier score for binary outcomes) for predicting the probability of two-year survival (left y-axis) in the training cohort decreases per iteration and approaches 0.42. Although the RMSE has stabilized, not all regression coefficients (right y-axis) have converged.</p>
        <p>The validation application assessed the model's performance on the validation cohort (years 2012-2015). The validation performance is described by the combined RMSE for patients from all sites (Fig. 5b), the receiver operating characteristic (ROC) curve per site and their corresponding areas under the curve (AUCs) (Fig. 5c), and by an exemplary calibration plot of the site with most patient data provided for training and validation (site G, Fig. 5d). Calibration plots for all other sites are displayed in Fig. S1 (Supplementary Information). Table 6 summarizes patient counts (available in the system and in the modelling cohort before and after imputation) and model performance per site. The validation RMSE almost-monotonically decreases during optimization on the training cohort. Discriminative performance of the model (as measured by the AUC), varies across sites from 0.85 (site A) to 0.58 (site D). Model calibration in site G is good with a calibration-in-the-large of 0.02 and calibration-slope of 0.75 but calibration varies strongly across sites. For example, site A (Supplementary Information, Fig. S1) displays a calibration-in-the-large of 2.39 and a calibration slope of 1.09.The validation application assessed the model's performance on the validation cohort (years 2012-2015). The validation performance is described by the combined RMSE for patients from all sites (Fig. 5b), the receiver operating characteristic (ROC) curve per site and their corresponding areas under the curve (AUCs) (Fig. 5c), and by an exemplary calibration plot of the site with most patient data provided for training and validation (site G, Fig. 5d). Calibration plots for all other sites are displayed in Fig. S1 (Supplementary Information). Table 6 summarizes patient counts (available in the system and in the modelling cohort before and after imputation) and model performance per site. The validation RMSE almost-monotonically decreases during optimization on the training cohort. Discriminative performance of the model (as measured by the AUC), varies across sites from 0.85 (site A) to 0.58 (site D). Model calibration in site G is good with a calibration-in-the-large of 0.02 and calibration-slope of 0.75 but calibration varies strongly across sites. For example, site A (Supplementary Information, Fig. S1) displays a calibration-in-the-large of 2.39 and a calibration slope of 1.09.</p>
        <p>We trained a distributed logistic regression model on 14 810 NSCLC patients and validated it on 8 393 patients from eight sites worldwide, yielding a total of 23 203 patients. While we thus easily exceeded the goal of 20 000 by 16.0%, the eight participating sites originate from only five countries which is one country short of the intended goal.We trained a distributed logistic regression model on 14 810 NSCLC patients and validated it on 8 393 patients from eight sites worldwide, yielding a total of 23 203 patients. While we thus easily exceeded the goal of 20 000 by 16.0%, the eight participating sites originate from only five countries which is one country short of the intended goal.</p>
        <p>Applying FAIR principles in this project highlighted the challenges in introducing modern data storage and processing approaches in a clinical research context. Semantic web technology allows concepts and relationships between concepts to be coded which makes data more interpretable -an important FAIR principle. The use of semantic web technology requires expertise that is often not present at healthcare institutes. In this project, we worked closely with all partners to support installations. Future projects would benefit from user-friendly software assisting healthcare institutes in transforming their data according to FAIR principles. Creating such software is the goal of an ongoing research project in CORAL.Applying FAIR principles in this project highlighted the challenges in introducing modern data storage and processing approaches in a clinical research context. Semantic web technology allows concepts and relationships between concepts to be coded which makes data more interpretable -an important FAIR principle. The use of semantic web technology requires expertise that is often not present at healthcare institutes. In this project, we worked closely with all partners to support installations. Future projects would benefit from user-friendly software assisting healthcare institutes in transforming their data according to FAIR principles. Creating such software is the goal of an ongoing research project in CORAL.</p>
        <p>We observed heterogeneity in modelled variables (T, N, M, and overall stage) and outcome (two-year survival) between sites. Sites provided different cohort types, either (complete) clinical records of heterogeneous NSCLC cases or study cohorts with narrower inclusion criteria which can explain much of this heterogeneity (Table 1). Specifically, site A had a biased inclusion towards surviving patients (89.1% two-year survival, Fig. 4) and site C provided two study cohorts. For both sites, these biases skewed T, N, M, and overall stage distributions towards lower stages. Even for sites providing data based on their full clinical records, different model variable distributions are not surprising since healthcare providers treat different patient subgroups. For example, data in site F originates from a radiotherapy clinic while the data in site G is provided by a comprehensive cancer care center offering different treatments (surgery, (chemo-)radiotherapy, etc.).We observed heterogeneity in modelled variables (T, N, M, and overall stage) and outcome (two-year survival) between sites. Sites provided different cohort types, either (complete) clinical records of heterogeneous NSCLC cases or study cohorts with narrower inclusion criteria which can explain much of this heterogeneity (Table 1). Specifically, site A had a biased inclusion towards surviving patients (89.1% two-year survival, Fig. 4) and site C provided two study cohorts. For both sites, these biases skewed T, N, M, and overall stage distributions towards lower stages. Even for sites providing data based on their full clinical records, different model variable distributions are not surprising since healthcare providers treat different patient subgroups. For example, data in site F originates from a radiotherapy clinic while the data in site G is provided by a comprehensive cancer care center offering different treatments (surgery, (chemo-)radiotherapy, etc.).</p>
        <p>For differences in model outcome (two-year survival), there are multiple (possible) causes. For example, site A experienced a biased collection of survival information due to its unavailability in the healthcare provider's Electronic Medical Records (EMR) and the difficulty of retrospectively gathering this missing information when there is no access to survival registries. Furthermore, some sites contributed historical data dating back to 1978 where treatment outcomes were generally worse. Additionally, treatment choices for patient subgroups differ due to national and local treatment guidelines, e.g., patients with metastasized NSCLC. Heterogeneity throughout the network is generally advantageous for prediction modelling as it allows models to be trained that are generalizable to a wider range of patients. On the other hand, if the difference in cohorts is caused by characteristics not considered by the model, e.g., difference in treatments or data collection biases, then these differences can have a negative effect on model performance. In our study, site A suffered from a biased inclusion of surviving patients. The effect on the trained model should be low as site A only contributed 7.3% of the training cohort (Fig. 5a). However, the usefulness of this dataset for model validation is limited because the performance of this model has not been evaluated for the entire patient population of the site but only for the subgroup following the biased collection (long survivors or recent patients, Table 1). A further inclusion bias is present in site C which provided two study cohorts (predominantly overall stage I and III) for training and validation. Care has to be taken when interpreting validation results: one can only draw conclusions for the patient subpopulation from which the validation dataset has been sampled.For differences in model outcome (two-year survival), there are multiple (possible) causes. For example, site A experienced a biased collection of survival information due to its unavailability in the healthcare provider's Electronic Medical Records (EMR) and the difficulty of retrospectively gathering this missing information when there is no access to survival registries. Furthermore, some sites contributed historical data dating back to 1978 where treatment outcomes were generally worse. Additionally, treatment choices for patient subgroups differ due to national and local treatment guidelines, e.g., patients with metastasized NSCLC. Heterogeneity throughout the network is generally advantageous for prediction modelling as it allows models to be trained that are generalizable to a wider range of patients. On the other hand, if the difference in cohorts is caused by characteristics not considered by the model, e.g., difference in treatments or data collection biases, then these differences can have a negative effect on model performance. In our study, site A suffered from a biased inclusion of surviving patients. The effect on the trained model should be low as site A only contributed 7.3% of the training cohort (Fig. 5a). However, the usefulness of this dataset for model validation is limited because the performance of this model has not been evaluated for the entire patient population of the site but only for the subgroup following the biased collection (long survivors or recent patients, Table 1). A further inclusion bias is present in site C which provided two study cohorts (predominantly overall stage I and III) for training and validation. Care has to be taken when interpreting validation results: one can only draw conclusions for the patient subpopulation from which the validation dataset has been sampled.</p>
        <p>Inter-comparison of summary statistics between sites highlights significant differences in variable distributions that can then be investigated to assure data quality. For example, earlier in this study, the N stage statistics showed one site to have an excess of N3 incidence as compared to other sites. This was subsequently investigated and uncovered a processing error at that site. This role will become increasingly important as outcome modelling studies move away from curated clinical trial datasets and towards routinely collected data and structured information retrospectively extracted from clinical notes.Inter-comparison of summary statistics between sites highlights significant differences in variable distributions that can then be investigated to assure data quality. For example, earlier in this study, the N stage statistics showed one site to have an excess of N3 incidence as compared to other sites. This was subsequently investigated and uncovered a processing error at that site. This role will become increasingly important as outcome modelling studies move away from curated clinical trial datasets and towards routinely collected data and structured information retrospectively extracted from clinical notes.</p>
        <p>We also observed varying model performance between sites: the validation cohort AUCs ranged from 0.58 (site D) to 0.85 (site A) and calibration plots (Supplementary Information, Fig. S1) display obvious differences. Multiple factors might influence stable performance across sites: e.g., the aforementioned heterogeneity due to unobserved but important variables, or different staging practices across sites.We also observed varying model performance between sites: the validation cohort AUCs ranged from 0.58 (site D) to 0.85 (site A) and calibration plots (Supplementary Information, Fig. S1) display obvious differences. Multiple factors might influence stable performance across sites: e.g., the aforementioned heterogeneity due to unobserved but important variables, or different staging practices across sites.</p>
        <p>We observe that our results are qualitatively in accordance with the AJCC TNM cancer staging system: the regression coefficients of the presented model (Table 5) indicate decreased survival probabilities for increases in T, N, M, and overall stage supercategories (with exception of T4). For example, the regression coefficients for overall stage supercategories decrease from 1.05 for overall stage category 0 to À0.82 for overall stage category IV. Additionally, we quantitatively compared the presented model to the AJCC TNM cancer staging system: we retrieved two-year survival probabilities for the overall stages IA, IB, IIA, IIB, IIIA, IIIB, IV of the AJCC TNM cancer staging edition 7 [18] (which is the effective edition of the validation cohort) and predicted two-year survival in the validation cohort. Patients with overall stages other than IA, IB, IIA, IIB, IIIA, IIIB, IV were excluded because these stages are either not defined or survival probabilities are not reported in TNM edition 7. AUCs of the presented model and the AJCC TNM cancer staging edition 7 coincided (Supplementary Information, Table S2). A discussion of other survival prediction models is available in the Supplementary Information (Section IV).We observe that our results are qualitatively in accordance with the AJCC TNM cancer staging system: the regression coefficients of the presented model (Table 5) indicate decreased survival probabilities for increases in T, N, M, and overall stage supercategories (with exception of T4). For example, the regression coefficients for overall stage supercategories decrease from 1.05 for overall stage category 0 to À0.82 for overall stage category IV. Additionally, we quantitatively compared the presented model to the AJCC TNM cancer staging system: we retrieved two-year survival probabilities for the overall stages IA, IB, IIA, IIB, IIIA, IIIB, IV of the AJCC TNM cancer staging edition 7 [18] (which is the effective edition of the validation cohort) and predicted two-year survival in the validation cohort. Patients with overall stages other than IA, IB, IIA, IIB, IIIA, IIIB, IV were excluded because these stages are either not defined or survival probabilities are not reported in TNM edition 7. AUCs of the presented model and the AJCC TNM cancer staging edition 7 coincided (Supplementary Information, Table S2). A discussion of other survival prediction models is available in the Supplementary Information (Section IV).</p>
        <p>The results demonstrate the capabilities of distributed learning infrastructures to proffer patient cohorts for statistical analysis. However, it shall be clearly stated that the presented model (Table 5) should not be applied in the clinic as this was not the goal of this study. The modelling methodology could be improved by explicitly encoding different AJCC staging editions and years of treatment which would consider improvements in treatments and outcomes over four decades. Additionally, employing Cox regression instead of logistic regression would allow including right-censored patient data in the analysis.The results demonstrate the capabilities of distributed learning infrastructures to proffer patient cohorts for statistical analysis. However, it shall be clearly stated that the presented model (Table 5) should not be applied in the clinic as this was not the goal of this study. The modelling methodology could be improved by explicitly encoding different AJCC staging editions and years of treatment which would consider improvements in treatments and outcomes over four decades. Additionally, employing Cox regression instead of logistic regression would allow including right-censored patient data in the analysis.</p>
        <p>For this study, we have implemented logistic regression, a tool popular in statistical analysis and machine learning for its simplicity and interpretability. The presented logistic regression algorithm is unpenalized. Penalization might help the individual regression coefficients to converge as it alleviates the multicollinearity problem (Fig. 5b) and will be explored in future studies. We extend the list of distributed methods that are already implemented in the PHT: Bayesian networks [4] and linear support vector machines [5]. Cox regression, a survival analysis methodology to model more than one time point, has been implemented previously in a dis- tributed setting [19]. Distributed learning approaches for other popular machine learning methods are available for future implementation, e.g., (convolutional) neural networks [20]. An alternative to the PHT is 
            <rs type="software">DataSHIELD</rs> [21], a mature opensource distributed data analysis and machine learning platform with multiple applications. It is based on the open-source software 
            <rs type="software">R</rs> and 
            <rs type="software">Opal</rs> data warehouses. The PHT infrastructure differentiates itself from 
            <rs type="software">DataSHIELD</rs> in multiple aspects:
        </p>
        <p>-it is not limited to R but is compatible with multiple languages (e.g., Java, 
            <rs type="software">MATLAB</rs>, C#, Python, R), -it offers analytical flexibility by not limiting the researcher to a fixed function library (
            <rs type="software">DataSHIELD</rs> v
            <rs type="version">4.0 140 R</rs> functions [21]), -it uses Semantic Web technology to store and query data at sites but also allows relational databases and SQL queries.
        </p>
        <p>The presented PHT study only considers a very limited number of clinical data elements (T, N, M, overall stage, diagnosis year, survival follow-up). Arguably, individual predictions need many more data elements. Additional clinical (e.g., age, comorbidities), biological (e.g., genomics, proteomics), imaging (e.g., screening, radiomics [22]) and treatment sources (e.g., radiotherapy treatment planning) are likely to contain relevant data elements for the prediction of a survival outcome. Furthermore, the two-year survival outcome is not sufficient for clinical decision support: quality-oflife, toxicity and cost are also relevant for a balanced decision to be taken. However, due to the limited number of data elements required for inclusion, we could reach very high inclusion numbers and could show that the methodology of distributed learning scales to these numbers. Although the data quality is improving in routine care, the more data elements a study requires, the less complete datasets will be available. As quality improves, future studies are possible where additional data elements (not only prognostic but also predictive for treatment outcomes) can be included and thus better and more clinically relevant models can be developed using the proposed infrastructure.The presented PHT study only considers a very limited number of clinical data elements (T, N, M, overall stage, diagnosis year, survival follow-up). Arguably, individual predictions need many more data elements. Additional clinical (e.g., age, comorbidities), biological (e.g., genomics, proteomics), imaging (e.g., screening, radiomics [22]) and treatment sources (e.g., radiotherapy treatment planning) are likely to contain relevant data elements for the prediction of a survival outcome. Furthermore, the two-year survival outcome is not sufficient for clinical decision support: quality-oflife, toxicity and cost are also relevant for a balanced decision to be taken. However, due to the limited number of data elements required for inclusion, we could reach very high inclusion numbers and could show that the methodology of distributed learning scales to these numbers. Although the data quality is improving in routine care, the more data elements a study requires, the less complete datasets will be available. As quality improves, future studies are possible where additional data elements (not only prognostic but also predictive for treatment outcomes) can be included and thus better and more clinically relevant models can be developed using the proposed infrastructure.</p>
        <p>The PHT enables machine learning studies on more data: more data is generally preferable over too little data. Combining data from multiple institutes, however, comes with challenges faced by any multi-institutional machine learning study (regardless whether it was conducted via a distributed infrastructure or in data centralization projects). Model performance can vary across cohorts (Table 6) or models trained on individual cohorts may perform better. These and other, unexpected results could have different causes, e.g., unobserved confounding factors or different outcome collection standards. Multi-institutional machine learning studies will require a clear methodology to a priori identify and afterwards report on such causes. Experience from and tech-niques used for clinical trial designs should form the basis for such methodology.The PHT enables machine learning studies on more data: more data is generally preferable over too little data. Combining data from multiple institutes, however, comes with challenges faced by any multi-institutional machine learning study (regardless whether it was conducted via a distributed infrastructure or in data centralization projects). Model performance can vary across cohorts (Table 6) or models trained on individual cohorts may perform better. These and other, unexpected results could have different causes, e.g., unobserved confounding factors or different outcome collection standards. Multi-institutional machine learning studies will require a clear methodology to a priori identify and afterwards report on such causes. Experience from and tech-niques used for clinical trial designs should form the basis for such methodology.</p>
        <p>This project shows distributed learning infrastructures are capable of delivering cohort sizes to rival those available to researchers from national registries. However, distributed approaches such as the PHT, where each institute must only satisfy its local information and research governance requirements, ease the bureaucratic burden of learning from internationally separated pools of patients, particularly between countries with differing information governance regimes. Furthermore, the system is much more flexible and makes including additional data elements into analyses a simple process. If an item is not present in a registry dataset, retrospectively adding this information to previous years is very difficult if not logistically impossible. Lastly, the infrastructure provides a mechanism to expedite the external validation of prognostic and predictive models in cohorts from different countries with different patient demographics, organizational cultures, and treatment regimens.This project shows distributed learning infrastructures are capable of delivering cohort sizes to rival those available to researchers from national registries. However, distributed approaches such as the PHT, where each institute must only satisfy its local information and research governance requirements, ease the bureaucratic burden of learning from internationally separated pools of patients, particularly between countries with differing information governance regimes. Furthermore, the system is much more flexible and makes including additional data elements into analyses a simple process. If an item is not present in a registry dataset, retrospectively adding this information to previous years is very difficult if not logistically impossible. Lastly, the infrastructure provides a mechanism to expedite the external validation of prognostic and predictive models in cohorts from different countries with different patient demographics, organizational cultures, and treatment regimens.</p>
        <p>This study has shown that distributed machine learning using Semantic Web technology can be implemented in a short time frame to answer specific research questions. In future work, we will extend CORAL with more cancer centers and include more data elements noted in routine care (we invite all interested parties to contact the corresponding author). As new patients and data elements become available, we expect that the PHT will enable researchers to rapidly train new prediction models: accelerating the speed at which clinical observations are turned into actionable knowledge.This study has shown that distributed machine learning using Semantic Web technology can be implemented in a short time frame to answer specific research questions. In future work, we will extend CORAL with more cancer centers and include more data elements noted in routine care (we invite all interested parties to contact the corresponding author). As new patients and data elements become available, we expect that the PHT will enable researchers to rapidly train new prediction models: accelerating the speed at which clinical observations are turned into actionable knowledge.</p>
        <p>The Personal Health Train infrastructure was deployed across eight healthcare institutes in five countries in four months. A two-year survival prediction model was trained and validated in more than 20 000 non-small cell lung cancer patients. This infrastructure demonstrably overcomes patient privacy barriers to healthcare data sharing and implements distributed data analysis and machine learning across healthcare providers worldwide.The Personal Health Train infrastructure was deployed across eight healthcare institutes in five countries in four months. A two-year survival prediction model was trained and validated in more than 20 000 non-small cell lung cancer patients. This infrastructure demonstrably overcomes patient privacy barriers to healthcare data sharing and implements distributed data analysis and machine learning across healthcare providers worldwide.</p>
        <p>BlazegraphBlazegraph</p>
        <p>mentary Information (Section II). For an excellent technical explanation of ADMM, we suggest Boyd et al.mentary Information (Section II). For an excellent technical explanation of ADMM, we suggest Boyd et al.</p>
        <p>[16][16]</p>
        <p>. All application groups are implemented in 
            <rs type="software">MATLAB</rs>
            <rs type="version">R2018a</rs> (
            <rs type="creator">Mathworks</rs>, Natick, MA). 
            <rs type="software">Code</rs> and accompanying documentation are available opensource
        </p>
        <p>[10][10]</p>
        <p>(https://github.com/RadiationOncologyOntology/ 20kChallenge).(https://github.com/RadiationOncologyOntology/ 20kChallenge).</p>
        <p>T.M. Deist et al. / Radiotherapy and Oncology 144 (2020) 189-200T.M. Deist et al. / Radiotherapy and Oncology 144 (2020) 189-200</p>
        <p>Distributed learning on 20 000+ lung cancer patients Dr. Jochems has (minority) shares in the company Oncoradiomics.Distributed learning on 20 000+ lung cancer patients Dr. Jochems has (minority) shares in the company Oncoradiomics.</p>
        <p>Mr. van Soest has been a paid consultant and received speaker honoraria for Varian Medical Systems. Mr. van Soest is a founder and employee of Medical Data Works B.V.Mr. van Soest has been a paid consultant and received speaker honoraria for Varian Medical Systems. Mr. van Soest is a founder and employee of Medical Data Works B.V.</p>
        <p>This study has been completed using 
            <rs type="software">Varian Medical Systems</rs> software and with technical support by 
            <rs type="software">Varian</rs>
            <rs type="creator">Medical Systems</rs>.
        </p>
        <p>We would like to thank Wolfgang Wiessler (Varian Medical Systems) for his advice and technical support. Sophie Stovold is acknowledged for her work in developing the Velindre (Cardiff) database. Mieke Basten and Thierry Felkers are acknowledged for their work in developing the Nijmegen database. Els Berenschot-Huijbregts and Andras Zolnay are acknowledged for their work in developing the Rotterdam database. Robbert Hardenberg and Tony van de Velde are acknowledged for their work in developing the Amsterdam database.We would like to thank Wolfgang Wiessler (Varian Medical Systems) for his advice and technical support. Sophie Stovold is acknowledged for her work in developing the Velindre (Cardiff) database. Mieke Basten and Thierry Felkers are acknowledged for their work in developing the Nijmegen database. Els Berenschot-Huijbregts and Andras Zolnay are acknowledged for their work in developing the Rotterdam database. Robbert Hardenberg and Tony van de Velde are acknowledged for their work in developing the Amsterdam database.</p>
        <p>We would like to thank the following colleagues of the MDTB: -Giovanna Mantini, [6,7] Department Radiation Oncology. -A. Martino, [7] Department Radiation Oncology. -L. Boldrini, [6,7] Department Radiation Oncology. -A. Damiani, [7] Department Radiation Oncology. -S. Margaritora, [6,7] Department of Surgery. -M.T. Cogedo, [7] Department of Surgery. -F. Lococo, [7] Department of Surgery. -A. Farchione [7] Department Radiology. -G. Rindi, [6,7] Department of Pathology. We wish to acknowledge technical and financial support from the following organizations: Varian Medical Systems (VLP, SAGE); Netherlands Organisation for Scientific Research (grant n°10696 DuCAT, BIONIC, VWData, grant n°P14-19 Radiomics STRaTegy); Province of Limburg (LIME); Dutch Cancer Society (TraIT2HealthRI, PROTRAIT); Health-RI; Netherlands Federation of University Medical Centres (Data4LifeSciences). This research is also supported by ERC advanced grant (ERC-ADG-2015, n°694812), EUROSTARS (DART, DECIDE), the European Program H2020-2015-17 Immuno-SABR -n°733008, PREDICT -ITN -n°766276, TRANSCAN Joint Transnational Call 2016 (JTC2016 ''CLEARLY"-n°UM 2017-8295), Interreg V-A Euregio Meuse-Rhine (''Euradiomics") and Kankeronderzoekfonds Limburg from the Health Foundation Limburg; Cardiff University Data Innovation Research Institute Seedcorn Fund grant n°23020-AC23024072/16; Velindre NHS Trust Charitable Funds grant n°2017/12. Gareth Price and Corinne Faivre-Finn acknowledge the support of Cancer Research UK via funding to the Cancer Research Manchester Centre [C147/A18083] and [C147/A25254]. Corinne Faivre-Finn is supported by the NiHR Manchester Biomedical Research Centre.We would like to thank the following colleagues of the MDTB: -Giovanna Mantini, [6,7] Department Radiation Oncology. -A. Martino, [7] Department Radiation Oncology. -L. Boldrini, [6,7] Department Radiation Oncology. -A. Damiani, [7] Department Radiation Oncology. -S. Margaritora, [6,7] Department of Surgery. -M.T. Cogedo, [7] Department of Surgery. -F. Lococo, [7] Department of Surgery. -A. Farchione [7] Department Radiology. -G. Rindi, [6,7] Department of Pathology. We wish to acknowledge technical and financial support from the following organizations: Varian Medical Systems (VLP, SAGE); Netherlands Organisation for Scientific Research (grant n°10696 DuCAT, BIONIC, VWData, grant n°P14-19 Radiomics STRaTegy); Province of Limburg (LIME); Dutch Cancer Society (TraIT2HealthRI, PROTRAIT); Health-RI; Netherlands Federation of University Medical Centres (Data4LifeSciences). This research is also supported by ERC advanced grant (ERC-ADG-2015, n°694812), EUROSTARS (DART, DECIDE), the European Program H2020-2015-17 Immuno-SABR -n°733008, PREDICT -ITN -n°766276, TRANSCAN Joint Transnational Call 2016 (JTC2016 ''CLEARLY"-n°UM 2017-8295), Interreg V-A Euregio Meuse-Rhine (''Euradiomics") and Kankeronderzoekfonds Limburg from the Health Foundation Limburg; Cardiff University Data Innovation Research Institute Seedcorn Fund grant n°23020-AC23024072/16; Velindre NHS Trust Charitable Funds grant n°2017/12. Gareth Price and Corinne Faivre-Finn acknowledge the support of Cancer Research UK via funding to the Cancer Research Manchester Centre [C147/A18083] and [C147/A25254]. Corinne Faivre-Finn is supported by the NiHR Manchester Biomedical Research Centre.</p>
        <p>Dr. Lambin reports grants/sponsored research from Oncoradiomics, ptTheragnostic/DNAmito. Dr. Lambin reports Advisor (SAB)/presenter fee from Varian, Oncoradiomics, PTT/DNAmito. Dr. Lambin is inventor of two patents on radiomics, one on mtDNA and two non patentable inventions (softwares), licensed to Oncoradiomics &amp; PTT/DNAmito and has (minority) shares in the company Oncoradiomics.Dr. Lambin reports grants/sponsored research from Oncoradiomics, ptTheragnostic/DNAmito. Dr. Lambin reports Advisor (SAB)/presenter fee from Varian, Oncoradiomics, PTT/DNAmito. Dr. Lambin is inventor of two patents on radiomics, one on mtDNA and two non patentable inventions (softwares), licensed to Oncoradiomics &amp; PTT/DNAmito and has (minority) shares in the company Oncoradiomics.</p>
        <p>Dr. Dekker has been a paid consultant and received speaker honoraria for Varian Medical Systems. Dr. Dekker is a founder and former employee of Medical Data Works B.V.Dr. Dekker has been a paid consultant and received speaker honoraria for Varian Medical Systems. Dr. Dekker is a founder and former employee of Medical Data Works B.V.</p>
        <p>Patient counts and model performance per site. Sites E and H are listed as incomplete as neither site published overall staging data (which may be imputed from T, N and M stages). AUC: area under the receiver operating characteristic curve. CI: confidence interval using 1000 bootstraps. Author contributions -TD, FD conducted the analysis.Patient counts and model performance per site. Sites E and H are listed as incomplete as neither site published overall staging data (which may be imputed from T, N and M stages). AUC: area under the receiver operating characteristic curve. CI: confidence interval using 1000 bootstraps. Author contributions -TD, FD conducted the analysis.</p>
        <p>-TD, FD, JvS developed software.-TD, FD, JvS developed software.</p>
        <p>-TD, FD, JvS assisted in the technical implementation across sites.-TD, FD, JvS assisted in the technical implementation across sites.</p>
        <p>-TD, FD, JvS, AD managed the data collection &amp; technical implementation in Maastricht.-TD, FD, JvS, AD managed the data collection &amp; technical implementation in Maastricht.</p>
        <p>-FD, RM, managed the data collection &amp; technical implementation in Nijmegen.-FD, RM, managed the data collection &amp; technical implementation in Nijmegen.</p>
        <p>-PO, SM, TJ managed the data collection &amp; technical implementation in Amsterdam.-PO, SM, TJ managed the data collection &amp; technical implementation in Amsterdam.</p>
        <p>-CFF, GP managed the data collection &amp; technical implementation in Manchester.-CFF, GP managed the data collection &amp; technical implementation in Manchester.</p>
        <p>-CM, VV managed the data collection &amp; technical implementation in Rome.-CM, VV managed the data collection &amp; technical implementation in Rome.</p>
        <p>-JW, JC, ZZ managed the data collection &amp; technical implementation in Shanghai.-JW, JC, ZZ managed the data collection &amp; technical implementation in Shanghai.</p>
        <p>-ES, MB managed the data collection &amp; technical implementation in Cardiff.-ES, MB managed the data collection &amp; technical implementation in Cardiff.</p>
        <p>-JJN, RV managed the data collection &amp; technical implementation in Rotterdam.-JJN, RV managed the data collection &amp; technical implementation in Rotterdam.</p>
        <p>-TD, FD, TJ, GP, AD developed the study design.-TD, FD, TJ, GP, AD developed the study design.</p>
        <p>-AJ, RM, JB, GP, PL, AD supervised the research project.-AJ, RM, JB, GP, PL, AD supervised the research project.</p>
        <p>-All authors contributed in the writing of the manuscript.-All authors contributed in the writing of the manuscript.</p>
        <p>Supplementary data to this article can be found online at https://doi.org/10.1016/j.radonc.2019.11.019.Supplementary data to this article can be found online at https://doi.org/10.1016/j.radonc.2019.11.019.</p>
    </text>
</tei>

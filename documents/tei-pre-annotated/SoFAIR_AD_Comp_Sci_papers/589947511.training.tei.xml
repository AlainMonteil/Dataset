<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:33+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Diabetic Retinopathy is a retina disease caused by diabetes mellitus and it is the leading cause of blindness globally. Early detection and treatment are necessary in order to delay or avoid vision deterioration and vision loss. To that end, many artificial-intelligence-powered methods have been proposed by the research community for the detection and classification of diabetic retinopathy on fundus retina images. This review article provides a thorough analysis of the use of deep learning methods at the various steps of the diabetic retinopathy detection pipeline based on fundus images. We discuss several aspects of that pipeline, ranging from the datasets that are widely used by the research community, the preprocessing techniques employed and how these accelerate and improve the models' performance, to the development of such deep learning models for the diagnosis and grading of the disease as well as the localization of the disease's lesions. We also discuss certain models that have been applied in real clinical settings. Finally, we conclude with some important insights and provide future research directions.</p>
        <p>Diabetes Mellitus is a serious public health problem, affecting 463 million people worldwide and this number is projected to rise to 700 million by 2045 [1]. At least one third of diabetics also suffer from an eye disease which is related to diabetes, of which diabetic retinopathy (DR) is the most common one [2]. DR is characterized by progressive vascular disruptions in the retina caused by chronic hyperglycemia and can be developed by any diabetes patient, regardless of its severity [3]. It is the leading cause of blindness among working age adults around the world and it is estimated that there are approximately 93 million people with DR worldwide [4]. These numbers are expected to rise even more, mainly because of the rising prevalence of diabetes in emerging Asian countries such as India and China [5,6].</p>
        <p>Although diabetic retinopathy is largely asymptomatic in the early stages, neural retinal damage and clinically invisible microvascular changes progress during these early stages [7]. Thus, there is a need for regular eye screening for patients with diabetes, as timely diagnosis and subsequent management of the condition is essential [8]. Since the only preventive strategy is the control of hyperglycemia, hyperlipidemia and hypertension early detection of DR becomes even more essential [7]. In addition, regarding its treatment, currently available interventions, such as laser photocoagulation, significantly decrease the likelihood of blindness in proliferative retinopathy and diabetic maculopathy in up to 98%, if the eyes are treated at an early stage of the disease [9]. It becomes evident that the key to the delay or even prevention of blindness from diabetic retinopathy is due to early detection and appropriate treatment [10].</p>
        <p>Although initial diagnosis of DR may be based on functional changes in electroretinography (ERG), retinal blood flow and retinal blood vessel calibre [11], in clinical practice early diagnosis is based on fundus examination [12]. Fundus photography is a rapid, non-invasive, well-tolerated and widely available imaging technique [13] that constitutes one of the most used methods to assess the extent of DR. Utilizing fundus images, ophthalmologists observe retina lesions at high resolution in order to diagnose diabetic retinopathy and assess its severity. However, manually diagnosing DR from fundus images demands a high level of expertise and effort by a professional ophthalmologist, especially in densely populated or remote areas like in India and Africa, where the number of people with diabetes and DR is projected to increase dramatically in the next years, while the number of ophthalmologists is disproportionally low [14][15][16][17]. This has motivated the research community to develop computer-aided diagnosis systems, which will reduce the cost, time and effort needed by a medical expert to diagnose DR.</p>
        <p>Recent advancements in Artificial Intelligence (AI) and the increase of computational resources and capabilities have created the opportunity to develop Deep Learning (DL) applications for accurate DR detection and classification. In this review article, recent DL-based methods, i.e. published after 2016, the detection and classification of DR presented and critically discussed. Although some review articles regarding the application of deep learning methods on DR have been published during the past few years [18][19][20][21][22], most of them focus only on specific aspects of the data analysis and modeling pipeline, as is presented in Fig. 1, which in some cases is limited to the reporting of the model's performance [21,22], or in the commonly used preprocessing methods [19,20], while in Ref. [22], a detailed account of the publicly available datasets is not included. These fragmented efforts call for a more detailed and integrated effort to review the technical implementations and progress in this really active research area. To this end, we present a novel holistic overview of the analysis pipeline (Fig. 1), in which apart from presenting comparative technical information regarding the development of published DL models for the classification and segmentation of fundus images, we also include a thorough analysis of the publicly available datasets, the commonly used preprocessing pipelines, as well as a presentation of models that have been applied in real clinical settings.</p>
        <p>In particular, this article provides a thorough analysis regarding the whole analysis pipeline, starting from the data preparation and preprocessing methods presented in Section 5, followed by the data analysis stage employing deep learning models. Regarding the latter, we include two distinct sections of artificial intelligence in the context of diabetic retinopathy. The first focuses on the evaluation of strengths and weaknesses of published efforts to develop deep learning models for the classification of DR's grading in Section 6, while the second provides a similar analysis, regarding efforts to develop deep learning models for the automatic segmentation of lesions that are related to DR, such as exudates (EX), microaneurysms (MA) and haemorrhages (HE), in Section 7. As for the remaining sections of this article, Section 2 and Section 3 include introductory information regarding Diabetic Retinopathy and Deep Learning respectively. In Section 4 we provide a detailed description and assessment of the various public datasets that can be used for DL development, discussing several critical characteristics of such datasets (e.g. class balancing, grading protocol used, etc.). It is worth mentioning that the reviewed datasets target both the classification and segmentation tasks. We also provide information regarding several DL models that have been applied in a real clinical setting or have been approved by regulatory agencies for use in clinical decision processes in Section 8. Finally, we conclude this article with an elaborate discussion of our main findings and proposals for future research, in Section 9 and Section 10 respectively.</p>
        <p>During the early stages of diabetic retinopathy microaneurysms can be observed on the retina, and are caused by degeneration and loss of pericytes, leading to capillary wall dilatation [8,23]. When the wall of a capillary or microaneurysm is ruptured, intraretinal haemorrhages occur. Other lesions of non-proliferative diabetic retinopathy include soft and hard exudates, intraretinal microvascular abnormalities (IRMA), venous beading and venous loops or reduplication [8,23]. According to Stitt et al. [24], IRMAs appear as large calibre tortuous vessels in areas of ischemia and may represent attempted vascular remodelling. Finally, the distinction between non-proliferative and proliferative diabetic retinopathy is based on the presence of neovascularization, which essentially refers to the growth of new retina vessels due to ischemia to preexisting ones. Fig. 2 presents some lesions on an indicative fundus image of a retina.</p>
        <p>At any stage of diabetic retinopathy, diabetic macular edema (DME) can occur, an endpoint which constitutes the most common cause of blindness [25]. The presence of edema is accompanied by abnormalities such as exudates within one disc diameter of the centre of the fovea, exudates within the macula, retinal thickening within one disc diameter of the centre of the fovea and microaneurysms or haemorrhages within one disc diameter of the centre of the fovea [26].</p>
        <p>Regarding the clinical grading protocols of DR, although the gold standard is the Early Treatment Diabetic Retinopathy Study (ETDRS) grading scheme [27], its use in everyday clinical practice has not proven to be easy or practical. Several alternative scales have been proposed in an effort to improve the screening of patients and communication among caregivers [28]. The development of such simplified diabetic retinopathy severity scales in several countries [29][30][31], had not led to a single international severity scale so far. To that end, the Global Diabetic Retinopathy Project Group has proposed the International Clinical Diabetic Retinopathy Disease Severity Scale [28], which classifies DR in 5 severity scales, as shown in Table 1.</p>
        <p>Deep Learning (DL) is a class of Artificial Intelligence (AI) methods inspired by the structure of human brain and is based on artificial neural networks. Essentially, DL refers to methods learning the mathematical representation of the latent and intrinsic relations of the data in an automatic manner. Unlike traditional machine learning methods, deep learning ones require much less human guidance, since they are not based on the generation of hand-crafted features, a task that can be very laborious and time consuming, but instead learn appropriate features directly from the data. In addition, DL methods scale much better than traditional ML methods as the amount of data increases. In this section, a short overview of some key DL concepts is provided.</p>
        <p>The simplest form of a neural network refers to an Artificial Neural Network (ANN), which consists of 3 layers of neurons, one input layer, one hidden layer and a final output layer. Such networks are known as Shallow (Feed-Forward) Neural Networks, because they only have one hidden layer. In contrast, a Deep (Feed-Forward) Neural Network (DNN) consists of more than two hidden layers. Each hidden and output layer consists of several artificial neurons and every input node and hidden neuron node is connected to each neuron in the next layer through a connection link. In addition, these networks accept a one-dimensional array as their input and thus cannot be directly used with imaging data.</p>
        <p>Convolutional Neural Networks (CNN), which unlike shallow neural networks accept 2D arrays as their input, were inspired by human vision and their concept is based on a fundamental mathematical operation, namely "convolution". The main difference of a CNN from a DNN is that for the latter all the neurons at a given layer contribute to the computation of the output of every neuron at the next layer, which is not the case for a CNN. Instead, a CNN utilizes filters or kernels to compute convolutions by sliding over a part of the original image to produce a feature map. Thus, if the size of the filter is x × x, then only a window of x 2 pixels will play role in computing the value of each unit of the next layer's feature map, which directly impacts the receptive field that is defined as the region in the input space that a particular CNN's feature is affected by. Finally, the convolutional part is often referred to as "the feature extraction part" of the network, while the rest is referred to "the classification part". The former learns the imaging features that are then reduced to a one-dimensional array and fed through the latter, which essentially is a Deep Neural Network, in order to classify the input image based on the generated features.</p>
        <p>UNet [32] architectures are more suitable for semantic segmentation than traditional CNNs, because of their ability to preserve the structural integrity of the image. In particular, they consist of a contracting path to capture the relevant context and a symmetric expanding path, enabling precise and accurate segmentation. In addition, a UNet architecture has less parameters and is faster than traditional CNNs due to the fact that it processes the image in one pass, rather than processing multiple patches in a sliding window approach, as a CNN would, and that is why such architectures are called "Fully Convolutional Networks" (FCN). Finally, it requires much less data than traditional CNNs to perform a segmentation task, which is crucial for medical image analysis, where the number of available data is much smaller than in other fields of computer vision.</p>
        <p>It is well known that human vision and perception relies on attention mechanisms to focus on specific parts of a scene or an object instead of processing the whole scene at once [33][34][35]. On the other hand, traditional CNNs have yet to fully and successfully incorporate such a mechanism. To that end, many studies have recently proposed such mechanisms, called attention modules, in order to improve the performance and robustness of the models [36][37][38][39].</p>
        <p>Finally, another important class of convolutional neural networks regards the Generative Adversarial Network (GAN) [40]. A traditional GAN consists of two separate models, the generative network which generates candidate samples based on the original data distribution and the discriminator which tries to distinguish the generated candidate samples from the true data distribution. Following such a training strategy, the generator is able to produce candidate samples that are closely related to the true data distribution. Application domains of GANs include image super-resolution (i.e. generate high resolution versions of the input image), creating art and image-to-image translation (e.g. transform a day image to its night equivalent) [41].</p>
        <p>Training a deep neural network is very demanding in terms of computational resources and data required. The world's largest object detection database, 
            <rs type="software">ImageNET</rs> [42], consists of over 14 million real life images, such as animals, devices, food, people, vehicles, etc. On the other hand, the largest dataset reviewed in this article consists of a little more than 80.000 fundus images. That difference is based on the fact that unlike images of everyday objects, medical images are very hard to obtain due to the necessary curation, annotation and legal issues involved.
        </p>
        <p>Thus, training robust and accurate models can be quite difficult when it comes to medical problems. It is possible, however, to leverage models that are trained on large datasets, such as ImageNet, by transferring the obtained knowledge to another model, even if the application field differs. Transfer Learning does exactly that, i.e. improve the learning in one task by transferring knowledge from another task that is already learned [43]. Transferring knowledge from ImageNet to a medical imaging domain ultimately makes the network able to easier detect low level features of the image (i.e. edges, contours, etc.). In order to actually detect DR, one has to fine-tune (i.e. retrain) the model on the new task (i. e. new dataset), a process that, however, will be much faster and more accurate than training it from scratch.</p>
        <p>Another very important topic of research in AI regards Ensemble Learning, which refers to the exploitation of multiple models (i.e. base models) to produce stronger predictive results than those produced by the individual models. This learning strategy aims at reducing the generalization error of the model and is a promising technique to fuse data from multiple modalities. In ensemble learning modeling infrastructure, a diverse set of base models is operated on the same dataset, or on a subset of the given available data, towards providing single predictions from a combination of their individual outputs. There are several ensemble techniques that can be used with all the possible models, given that the task at which each individual model has been trained on is the same across all models. Such methods comprise majority voting, averaging, bagging, stacking and boosting.</p>
        <p>An intuitive example of majority voting regards a classification problem, where each individual classifier produces an outcome, and the final prediction is made based on which outcome has concentrated the most votes. Averaging is used for regression problems, based on which a weighted or not average of the individual predictions are combined for the final outcome. In bagging, many models are trained each on only a subset of the original dataset, and then their outputs are combined either by majority voting, averaging or another strategy to produce the final prediction outcome. Furthermore, in stacking a separate meta-model is trained on the output predictions of the individual models, in order to produce the final prediction. Ensemble methods based on boosting aim at incorporating models that are trained multiple times based on the performance errors of previously trained and poorly performing models. Then a weighted average of the predictions is computed based on the predictive performance of the individual models.</p>
        <p>Other important concepts and learning strategies include Multitask Learning, Multimodal Learning and Active Learning. A simple illustration of the discussed strategies is presented in Fig. 3. By Multitask Learning one can predict the outcome for two different tasks utilizing a single data encoding network. Multimodal Learning is mostly seen in biology, pathology and radiology domains, where multiple imaging and non-imaging source (e.g. MRI, CT, molecular and clinical data) are combined for data analysis [44]. Each data modality is firstly processed by its dedicated model and then the fused features are used for training the common model. Finally, Active Learning regards the process of training the model on a small labelled subset of the data, produce the predictions for the rest of the unlabelled subset of the data, prioritize them based on a given strategy and query a user for the ground truth labels of a proportion of the unlabelled data based on the prioritization score [45]. Then the model is trained on the new labelled subset of the data [45]. Active learning is mostly used in application domains where the amount of data is too large to be labelled and a priority should be given to label the data.</p>
        <p>In this section we present various retina fundus datasets for developing and benchmarking Deep Learning Systems (DLS) in the context of diagnosing Diabetic Retinopathy. Table 2 presents details of all datasets regarding their size, resolution, Field of View (FoV) and the annotation protocol used. Table 3 presents details regarding the class-wise distribution of the severity gradings and the grading protocol used for each classification dataset. Table 4 presents similar information regarding the datasets used for segmentation purposes.</p>
        <p>Kaggle EyePACS is the most used and largest public dataset for Diabetic Retinopathy classification, containing more than 80.000 fundus images and was provided by the EyePACS platform for the Diabetic Retinopathy Detection competition which was sponsored by the California Healthcare Foundation [46]. It consists of a large number of high-resolution fundus images of the retina of both eyes, which were obtained under a variety of imaging conditions by various devices at multiple primary care sites throughout California and elsewhere. However, due to such variability, both the data (e.g. artifacts, blurring, focusing and exposure problems) and the ground truth labels exhibit noise, which was an intended goal in order to better simulate a real world scenario. The images were graded by a trained professional according to the ICDRDSS scale [28].</p>
        <p>Kaggle APTOS 2019 Challenge [47] dataset was collected by Aravind Eye Hospital in India's rural areas, in an effort to build powerful tools to automatically diagnose Diabetic Retinopathy and improve the hospital's ability to identify potential patients. It is the third largest dataset, consisting of 5590 images. However, one of its limitations is the large class imbalance, especially for Severe NPDR class, which has only 193 images. Just like Kaggle EyePACS dataset, due to them being collected in a real world multicentre environment, APTOS dataset also exhibits variations due to different camera settings across centres and noise both in the data (i.e. artifacts, focus problems, be under/overexposed) and the labels.</p>
        <p>The Messidor dataset [48] consists of 1200 retina fundus images which were collected by 3 ophthalmology departments, in France, between 2005 and 2006. Pupil dilation was used prior to capturing 800 images, while the rest 400 were captured without pupil dilation. Messidor 2 dataset [48,49] contains 1058 of the images of the original Messidor dataset, as well as 690 additional images that were collected between 2009 and 2010 in the Ophthalmology department of Brest University Hospital, France.</p>
        <p>Unlike Kaggle EyePACS dataset, the images of both datasets have very good quality, without any form of noticeable noise in them. The datasets contain an image-level medical diagnosis for each of the images, regarding the severity of Diabetic Retinopathy, but not any pixelwise lesion segmentation information. However, their custom grading system was not consistent with the widely used ICDRS protocol, which limits its validity and applicability.</p>
        <p>The IDRiD [50] dataset consists of 516 high-quality images collected at an ophthalmology clinic in Nanded, India, using a Kowa VX -10α fundus camera. Both eyes of all subjects were dilated prior to the image capture procedure. It provides image-level grading about the severity of Diabetic Retinopathy according to the ICDRS scale and grading regarding the risk of Diabetic Macula Edema (DME) for all 516 images. It also provides pixel-wise annotations of the relevant lesions (i.e. Hard &amp; Soft Exudates, Microaneurysms and Haemorrhages) and the optical-disc structure for 81 images of the dataset.</p>
        <p>The DDR [51] dataset is the second largest dataset when considering the classification task, consisting of 12522 images, but it is a fairly new dataset and hasn't been used widely yet. The data were collected between 2016 and 2018 across 147 hospitals in China's 23 provinces and annotated by multiple professionals according to the ICDRDSS scale using a majority voting schema. In addition, a sixth grade was provided in order to distinguish poor quality images into a single category. However, there is a great imbalance between the healthy/moderate DR classes and the rest, i.e. mild, severe and proliferative DR, which can lead to overfitting. Regarding the relevant DR lesions, 757 images of the dataset were annotated at a pixel-level for lesion segmentation purposes, as well as bounding boxes around them were also provided for lesion detection purposes.</p>
        <p>The E-Ophtha [52] dataset consists of 463 images, of which 268 regard healthy subjects, 148 patients with microaneurysms or other small red lesions and 47 with exudates. It has been used for automatic prediction of DR in a binary task (healthy vs diseased) [53]. However, due to the low number of images contained in the dataset compared to the larger datasets (Kaggle and Messidor), it is mostly used in the literature for developing segmentation algorithms, and not for classification ones.</p>
        <p>DiaRetDB1 [54] consists of 89 fundus images which were collected at Kuopio university under a controlled environment and were graded by 4 experts. However, their distribution does not reflect a typical population, since not only the data sample is small and from a single clinical site, but also all the images were captured under a controlled environment without significant variations in the capturing procedure [54].</p>
        <p>DRiDB [55] consists of 50 fundus images and included annotations regarding the structure of the retina's optic disc and vessels, any present pathologies, neovascularizations and disease grading, all of which were determined by multiple experts. Although it is a fairly small dataset, it is also the most informative.</p>
        <p>Additional retina fundus datasets, such as STARE [57], DRIVE [58], ORIGA light [59], CHASE_DB1 [60], HRF [61] and others do exist. However, they are not discussed in this paper, since their main objective regards other purposes, such as retina vessel segmentation, which are out of scope of the present review.</p>
        <p>As discussed in Section 4, capturing fundus images using a variety of hardware devices, under a variety of environmental conditions induces noise to the final image. In order to reduce such heterogeneity, which ultimately affects the performance of the classification model, as well as to highlight some fine details of the images, pre-processing of the images is usually a necessary step in most of the studies reviewed. In this section, we discuss several such techniques used in the reviewed literature.</p>
        <p>To begin with, contrast enhancement is a common preprocessing technique used for highlighting the foreground from the background, in any image processing or analysis pipeline. A simple method for contrast enhancement in fundus images is the histogram equalization [62][63][64][65], which increases the global contrast of the image, but neglects the local variations across the image. A more advanced algorithm for contrast adjustment, which takes into consideration the local variations around a specific area of each pixel, is Adaptive Histogram Equalization. However, regarding fundus imaging, Contrast Limited Adaptive Histogram Equalization (CLAHE) is more commonly used by the research community [66][67][68][69]. CLAHE solves the issue of over-amplifying the contrast in near-constant areas of the image, in contrast to the original Adaptive Histogram Equalization algorithm. By adjusting the contrast of the image, which all of the previously mentioned methods achieve, the subtle structures of the retina become more visible and easier to detect. An example of each method is illustrated in Fig. 4. Other researchers [ [70][71][72][73][74][75][76][77][78] subtract the local average color from each pixel and map it to 50% grayscale, to reduce differences in the lightning conditions across the images and highlight the subtle lesions, as in Fig. 4.</p>
        <p>Non-Local Means Denoising (NLMD) is applied by Refs. [68,79] in order to remove potential noise in the image. However, it should be noted that although the stronger the denoising algorithm is, the more noise it will eliminate, it will also degrade the fine details of the image (i. e. the image becomes blurry).</p>
        <p>Also, image intensity normalization is applied in order to avoid introducing bias and high training times to the network as well as to standardize the data to a particular scale (e.g. each image having a mean value of 0 and standard deviation of 1, regarding its pixels' intensity) [53,63,68,72,77,[79][80][81][82][83][84][85][86].</p>
        <p>Apart from contrast enhancement, normalization and noise reduction, transforming the color image into another color model or even simply utilizing only one of the RGB channels, have increased the model's performance. Lin et al. [81] transformed the data to entropy images, which led to the DLS outperforming the models which were trained on standard datasets. In addition, the extraction of the green channel out of the fundus color image is commonly applied, due to its rich information and high contrast in comparison with the other two color channels [64,67,77,78,[87][88][89][90][91][92][93][94][95]. Similarly, Pao et al. [96] combined the entropy images of the grayscale image and the green channel of the original fundus image into a dual-path CNN for DR classification, effectively outperforming a CNN trained on a standard dataset.</p>
        <p>Furthermore, the datasets may contain images that vary in terms of resolution and aspect ratio. The images could also contain uninformative black space areas. In order to standardize the image size and to remove such black space areas, the images may be cropped, rescaled and resized to a specific resolution [51, 53, 62-64, 66, 70, 71, 73-75, 79-86, 97-101]. Bravo et al. [74] conducted their experiments using two different cropping techniques. In particular, in one experiment they crop the images so that the retina incircles the cropped image, whereas in the second experiment they cropped the largest square image inscribed in the retina.</p>
        <p>Orlando et al. [102], Chudzik et al. [88] and Appan et al. [103] observed that, regarding lesion detection in a fundus image, many false-positive samples are caused by vascular branching and vessel segments misclassification. Yu et al. [92] segmented and removed the vessel structures out of the green channel of the raw image in order to enhance exudate detection, by utilizing an opening morphological operation. Chudzik et al. also utilized Otsu's thresholding [104] on the green channel of the image to segment the vessel structures, as well as morphological operations to remove noisy regions of the image. Imani et al. [89] also segmented the vessel structures, with the MCA algorithm [105], Shift Invariant Shearlet Transform (SIST) [106] and Non Subsampled Contourlet Transform (NSCT) [107] based on the structural differences in the morphology of the vessels (curved-like structures) and the exudates (spot-like structures). Morphological operations, such as opening and closing, were also used in order to refine either the vessel [102] or the lesion segmentation [89,92]. Finally, Adem et al. [65] used the Canny Edge detector and a Circular Hough Transform method to segment the Optic Disc from retina fundus images. By removing such a complex anatomical structure, which is also similar to exudates, the performance of lesion detection improved.</p>
        <p>Although DL has been proven to work well in an end-to-end manner, where the raw data is fed into a single-model pipeline, it has also been reported, that applying certain preprocessing techniques, such as those reported in this section, leads to performance improvement [66,81,96], especially for fundus images. In addition, due to the lack of rich and balanced datasets and in order to enhance the model's robustness and accuracy, data augmentation techniques are also used. In case of imaging datasets such techniques can refer to rotating, shifting (translation), rescaling, shearing and flipping the images, color and brightness augmentation [73,80,83,84,108], as well as the use of Generative Adversarial Networks for image synthesis [78,109]. Regarding the reviewed articles, most utilize some augmentation method in order to increase the number of the available images and thus accelerate the training of the model.</p>
        <p>The main objective of DR classification is on the detection of diabetic retinopathy and its grading using fundus images of the retina. As described in Section 1, DR is graded by physicians with respect to a 5class protocol. In this section we present information regarding published deep learning models that detect and grade the disease's severity on the image level.</p>
        <p>In Section 2 we presented the common 5-class grading scale that ophthalmologists utilize in order to grade a fundus image regarding the DR disease. However, there are cases in which researchers decided to classify them differently, by ultimately merging several classes together. In particular, authors in Ref. [66] conducted experiments regarding DR grading with respect to a 2-class (i.e. detecting the presence of the disease), 3-class (i.e. no DR, mild DR and severe DR) and a 4-class (i.e. no DR, mild DR, moderate DR and severe DR) classification. They defined referable (i.e. the patient should be referred to an ophthalmologist) DR when at least moderate NPDR lesions are observed and vision threatening DR when at least severe NPDR or PDR lesions are observed. In Ref. [62], the authors created a 4-class scale in order to encode similar clinical manifestation between the different stages of the traditional scaling protocol (ICDRS), among other reasons. Islam et al. [71] developed two binary classification models, one for detecting the presence of the disease (healthy vs diseased) and one for grading its severity (grade 0,1 vs 2,3,4). Li et al. [51] used an additional class (6 in total) to classify ungradable images as well. Table 5 summarizes the different classification strategies utilized in the reviewed literature grouped with the corresponding references. Binary classification (i.e. detection of referable DR) is commonly formulated as classifying grades '0,1 vs 2,3,4 ′ or grades '0 vs 1,2,3,4'. Most reviewed articles utilize the '0,1 vs 2,3,4 ′ grading schema regarding the binary classification task, due to the 'mild DR' misclassification problems, which are discussed further below in Section 6.3.5.</p>
        <p>The performance of a binary classification model can be represented by the confusion matrix [118]. Each measure in the confusion matrix is calculated based on the predictions and the ground truth. Based on these measures, some more specialized metrics are defined, such as Sensitivity, Specificity, Accuracy, Precision, F1_score and Cohen's Kappa. In addition, the Receiver Operating Characteristic (ROC) curve presents the performance of a binary classifier by plotting its Sensitivity against its Specificity at various thresholds settings regrading the classification outcome (i.e. at which probability a given sample is considered as a positive or negative outcome). Finally, Area Under the Curve (AUC) measures the area underneath the entire ROC curve, providing an aggregate performance measure across all classification thresholds.</p>
        <p>In this section, the various DL methods for DR classification are presented. Table 6 presents the well-established architectures (either using their pre-trained versions or not) that were used as the backbone of each proposed classification model. The most utilized architectures are VGG16, the Inception family and ResNet. We urge the reader to read an in-depth analysis of the popular architectures for deep learning based image analysis, which was recently published by Alzubaidi et al. [119].</p>
        <p>Pratt et al. [82] published one of the first studies employing a CNN based model for the quinary classification of DR (5-classsimilarly to the clinical grading protocol). The authors used a class-weighted strategy to update the parameters during backpropagation, for every batch, in order to compensate for the class imbalance in the dataset and reduce over-fitting.</p>
        <p>Islam et al. [71] converted the quinary (5-class) classification problem to a regression problem, in order to better predict NPDR and PDR cases. In addition, the authors developed a blending network, by combining the feature vectors of the CNN for each eye, in an attempt to improve the performance of the model. Similarly, Torre et al. [101] developed a CNN model, which analyzed the images of both eyes and effectively combined those representations, in order to perform the classification. They also proposed using small convolutions, and adaptations to the network's architecture in order to have a final receptive field as similar as possible to the original image's size. Raju et al. [84] also reported that when using smaller (4x4) filters in the Conv2D layers, the DR classification performance was better, due to the smaller sized lesions, such as microaneurysms. Inception modules have also been utilized in Refs. [66,70,110] in order to extract features at different resolutions, in an attempt to capture relevant lesion marks, which in turn vary in size.</p>
        <p>Gulshan et al. [110] utilized a pre-trained InceptionV3 model for DR and DME classification on a dataset consisting of 128.175 images, which were retrospectively obtained from EyePACS in the US and 3 eye hospitals in India, and of which 33.246 were referable, while they tested their model on two external datasets. The images were graded between three and seven times by a pool of 54 US licensed ophthalmologists and ophthalmology senior residents for the quality of image and the presence and severity of DR and DME. Firstly, the effects of training the model on subsets of dataset with varying size were examined, with the performance reaching a plateau with the training size at around 60.000 images (with 17.000 referable). Secondly, a second subsampling experiment was performed, regarding the existence of multiple ground truth grades per image, which indicated that the performance benefited from a majority voting on those multiple grades per image. On the other hand, Krause et al. [115] determined how the use of an adjudication grading system for the development of the ground-truth labelling affects the algorithm's training performance. They used the pre-trained model by Gulshan et al. [110], fine-tuning it on a small dataset, on which an adjudication grading protocol was applied. The authors reported that even a small set of adjudicated images, allowed a slight performance improvement when using adjudication as the ground truth development standard compared to majority voting.</p>
        <p>Attention modules have also been utilized for improving the detection performance of CNNs. Zhao et al. [63] utilized an attention mechanism and a bilinear strategy, in order to train a CNN and improve the classification performance on subtle regions. Wang et al. [116] also utilized an attention mechanism to generate attention maps, which were then used by a Crop-Network, which zoomed in the highest attention regions to further improve the classification accuracy. Li et al. [85] proposed a novel architecture, which focused on jointly detecting DR and DMR, by utilizing attention modules to explore inter-disease correlations. Lin et al. [120] developed a deep learning pipeline for lesion detection, which is then used in par with the original fundus image for DR severity classification. During the detection phase, a lesion clustering method was used in order to decrease the impact of missing lesion annotations. The lesions maps from the detection model are fused with the feature maps of the classification model by the Attention Fusion Network, which also weights the importance of each lesion area. Finally, Zhou et al. [121] proposed a collaborative weakly-supervised learning model to improve the performance of DR's grading and lesion segmentation with an attention mechanism from image-level annotated data.</p>
        <p>Furthermore, one would want to train the model with very high resolution fundus images, in order for small lesions to be easier detected. However, the computational complexity as well as the vanishing/exploding gradient problem of deep CNNs forbid this. On the other hand, directly downsampling the images leads to a huge information loss. Zhou et al. [113] developed a novel architecture, Multi-Cell Multi-Task CNN (M 2 CNN), consisting of an Inception-ResNet-V2 stage connected to a Multi-Cell stage, which gradually increases the depth and the kernel size of the network along with the input image's resolution, in order to capture high-resolution details. Finally, a Multi-Task stage is applied, during which both a classification and a regression score are computed. Doing so, the authors formulated a more appropriate training loss function, based on the consideration that DR is a gradually progressing disease and thus discrete labelling can be misleading.</p>
        <p>Li et al. [114] experimented with several image resolutions, concluding that the performance of the model increases logarithmically with respect to higher input image resolutions. However, as the input image resolution increases, the complexity of the network also increases exponentially. Thus, given the complexity constraints, the optimal image's resolution was 896 × 896, which boosted the performance of the algorithm especially for the correct classification of the mild DR case, which depends on extracting subtle features.</p>
        <p>A major issue regarding deep learning and especially DL applied on medical imaging regards the availability of sufficient data to train the models. As discussed in Section 3, it is possible to overcome this issue by transferring knowledge from one field where there is plenty of data (i.e. computer vision), to another with limited data (i.e. medical imaging). Many of the reviewed articles have utilized a transfer learning approach to develop their classification models [51,62,66,72,74,75,80,83,86,99,100,110,117]. Wan et al. [83] compared several pre-trained models, which were fine tuned on the Kaggle EyePACS dataset. They reported the best results for VGGNet-s architecture, achieving an accuracy of 95.68%, specificity of 97.43%, sensitivity of 86.47% and an AUC of 0.979, outperforming other more complex architectures. Hagos et al. [70] used transfer learning by utilizing an ImageNet pre-trained InceptionV3 model. The authors fine-tuned the classifier on a small balanced subset of Kaggle EyePACS dataset. Wang et al. [99] also reported that the InceptionV3 model achieved the best results (63% Accuracy), when transfer learning was applied on a dataset of 166 images of Kaggle EyePACS dataset. Others [51,66], have also reported the best results when using a pre-trained Inception backbone network (
            <rs type="software">GoogLeNet</rs>, InceptionV3, InceptionV4).
        </p>
        <p>Ensemble learning has also played an important role in developing robust and powerful AI frameworks for DR classification, by combining the advantages of several classifiers [53,63,70,72,73,80,100]. Ensemble learning has been reported to perform better than the respective standalone models due to the information gain caused by their complementarity. That indicates that the different base models can implicitly learn different levels of semantic representations, either because of the differences in their architecture as in Refs. [62,100], or the training procedure as in Ref. [73].</p>
        <p>Zhang et al. [62] developed two ensemble models, one for the identification of the disease (binary classification) and one for the grading of the disease (quinary classification). The individual models were based on several pre-trained networks, which acted as the feature extraction part, and a custom standard dense neural network, which acted as the classifier. The ensemble models outperformed the individual ones in both tasks, achieving a sensitivity of 98.10% and specificity of 98.56%. The authors also note that, the 'stronger' the base learner was (pre-trained network), the higher the performance was, in general. In addition, a dual ensemble (ensemble of the ensembles) performed better than a single ensemble in some cases.</p>
        <p>Jiang et al. [100] developed an ensemble model, using the Adaboost classifier on 3 models, which were based on the InceptionV3, ResNet152 and Inception-Resnet-V2 architectures. they trained the model on a private dataset, which was developed in collaboration with Beijing Tongren Eye Centre. The ensemble model outperformed the individual models, achieving a Sensitivity = 85.57%, Specificity = 90.85%, Accuracy = 88.21% and AUC = 0.946. However, InceptionV3 performed better in terms of Specificity, which was 91.46%.</p>
        <p>Quellec et al. [73] trained a CNN model, which was exported at multiple iterations during the training procedure, because as the authors claim, each unique lesion type is optimally detected at different training iterations. Subsequently, they combined the saved models using ensemble learning (Random Forest Classifier) to predict DR's severity score.</p>
        <p>Although deep learning has been proven to be very effective and accurate in analyzing medical images, even surpassing human performance in some tasks [110], its clinical use has yet to be widely accepted. The main reason for that regards the fact that deep learning models produce a prediction without explaining the reasoning behind it, which is a crucial step in gaining the clinician's trust. Regarding the scope of this review article, there have been several attempts to develop an interpretable model for predicting DR.</p>
        <p>Quellec et al. [73] utilized a modification of the sensitivity criterion [122] in order to produce a heatmap, visualizing the contribution that each pixel had in the prediction of the output. Moreover, by analyzing the classification results and the heatmaps produced, the authors report that the performance trajectory experiences some leaps roughly at the time that it learns to identify the different lesion types. It should be noted that the most obvious lesions are detected earlier in the training loop, while the more subtle lesions later. Jiang et al. [100] build an interpretable ensemble classifier, by utilizing the Class Activate Maps (CAMs) [123] technique for each individual model as well as for the ensemble model. Similarly, Torre et al. [101] introduced a receptive field score distribution model, which scores the importance of each pixel of the input image in the final classification prediction.</p>
        <p>Sayres et al. [117] evaluated the performance of ten ophthalmologists, under 3 conditions: (a) the physicians were provided with the raw fundus images, (b) the grading results of the DLS were also made available to them and (c) the grading results and an interpretable heatmap were also provided. The heatmaps regarded the pixel-wise contribution to the final prediction, which in turn indicates possible lesions. They measured three primary outcomes: diagnosis accuracy, subjective confidence in DR grading and time spent grading. They found a trend toward higher accuracy and confidence, but also higher grading times, with model assistance. As readers gained more familiarity with model assistance, there was a trend toward increased accuracy and decreased grading time. An increased sensitivity was also observed without a significant impact on specificity. Across all images, their results indicated that the grades-only condition provided a stronger benefit than grades plus heatmap.</p>
        <p>Lam et al. [66] developed several classification models regarding binary, ternary and quinary labelling tasks. Although they achieved high performance, with respect to sensitivity and specificity, regarding the 'no DR ′ or 'severe DR ′ cases, they achieved only 7% sensitivity for the 'mild DR ′ case. They managed to increase that percentage to almost 30% by preprocessing the images, with the cost of dropping the performance for the other 2 classes by an amount of 10%. Others also noted that the misclassification was more common for mild DR than the other classes [51,53,63,72,79,82], which confirms that mild DR detection is a very challenging problem and the intricate details of the disease are harder identified, because their size and number are very small (1% of image) [53].</p>
        <p>Table 7 summarizes the performance results at the highest sensitivity point of the ROC curve of the classification methods as they were reported in the corresponding papers.</p>
        <p>While classification models are essential in detecting and grading DR, which effectively accelerates DR screening, detecting and segmenting relevant lesions at the pixel level is also a crucial stage of a screening pipeline. Identifying such diseased areas on the retina plays a significant role in diagnosing and treating diabetic retinopathy, as those are the main findings an expert ophthalmologist observes, as discussed in the introductory section. Thus, in this section we present information regarding published deep learning methods focused on the automatic segmentation of lesions that are related to DR, such as exudates, microaneurysms and haemorrhages.</p>
        <p>When considering a segmentation problem, the ground truth information relates to every pixel of the image instead of the entire image, as in a classification task. Because in most cases the background of the image (i.e. healthy part of retina) prevails the foreground (i.e. actual lesions), utilizing the traditional metrics at the pixel-level, i.e. accuracy, sensitivity, specificity, etc., would be misleading. This is due to the fact that since most of the ground truth image refers to the healthy part of the retina and only a small proportion of the pixels refer to lesions. Hence, the pixel-wise accuracy of a segmentation algorithm would continuously be almost perfect, without necessarily correctly detecting the relevant lesions, just because the background is mostly matched with it self. Thus, the following metrics are most suitable to evaluate the performance of a segmentation model, instead of the traditional classification-oriented ones.</p>
        <p>A metric that is commonly used in segmentation problems is the Intersection-over-Union (IoU), which is computed by dividing the overlapping area by the area of union between the predicted (P) and ground truth (G) segmentation areas, IoU = P∩G P∪G . Its value ranges from 0 to 1, with 1 signifying perfect match, and 0 meaning completely disjoint. The evaluation metric is then calculated by averaging the IoUs of every class. Another metric is the DICE coefficient, which is computed by dividing the double overlapping area between P and G by the total number of pixels in both areas, DICE = 2 * |P∩G| |P|+|G| . Its value is also equal to the value of the F 1 score metric. The DICE coefficient is similar to the IoU, also ranging from 0 to 1. As a matter of fact, DICE and IoU are positively correlated, meaning that although their value may not be the same, they will indicate towards the same result. What this means is that when one metric indicates that a classifier A is better than another classifier B, the other metric does it too. However, their difference emerges when quantifying how much better is one classifier from another. Based on these metrics, one can calculate the lesion-level detection accuracy, sensitivity, specificity and other classificationrelated metrics of a segmentation model by setting a threshold for each of the IoU and DICE metrics, above or below which a true positive or a false negative detected lesion is counted.</p>
        <p>Free-response Receiver Operating Characteristic (FROC) curve is a graphical representation of the model's performance at all decision thresholds. It is similar to ROC curves, but in FROC's case a threshold definition is needed regarding a detected region to be considered as true or false positive/negative. For example, one could set a 50% overlap between the annotated and detected regions to indicate a true positive.</p>
        <p>In addition, there are also other metrics for evaluating a segmentation method, such as the Hausorff Distance (HD) [126] and its variants, i.e. Average Hausorff Distanec (AHD) and Hausdorff quantile method, the Euclidean Distance Difference between the centre of masses of the two segmentation masks (ΔCMD), the Surface Distance, etc. However these metrics will not be covered in depth in this review, because none of them are mentioned or used in the reviewed papers. We urge the reader to study the following articles for more in-depth information of these metrics [127][128][129].</p>
        <p>It should be noted that many of the reviewed studies report their evaluation results using metrics that are most suitable for a classification task, as reported in Section 6.2. Although we discussed the problems of such metrics in a segmentation task at the beginning of this section, we included those studies in our review, but we recommend caution when interpreting their results.</p>
        <p>Xue et al. [130] proposed a deep membrane system for multitask segmentation of microaneurysms and exudates. In particular, they utilized 
            <rs type="software">Mask</rs> R-CNN [131] for implementing each new hybrid membrane structure, which were a combination of tissue-like [132] and cell-like [133] membrane systems. Guo et al. [134] proposed LWENet, a lightweight segmentation network utilizing an encoder-decoder architecture and having 10 times fewer parameters than other popular architectures, i.e. 
            <rs type="software">DeepLab</rs> v
            <rs type="version">3+</rs> [135], FCRN [136] and HED [137]. At the same time, pre-training the encoder on the DDR classification dataset, helped increase the F1 score at the segmentation task by almost 10%.
        </p>
        <p>Guo et al. [138] also proposed L-Seg network in order to simultaneously segment all four DR related lesions in a fundus image, i.e. Soft/Hard Exudates, Haemorrhages and Microaneurysms. Their model's output consists of 4 individual segmentation maps, one for each lesion type. They also propose a multi-channel bin loss function that combines all four outputs, to avoid class-and loss-imbalance issues. They utilized several feature maps of the network to incorporate multi-scale analysis and handle lesions of different sizes, as well as a weighted fusion module to integrate all this information and effectively analyze complex lesions.</p>
        <p>In order to properly up-sample each feature map, a deconvolution layer is used, in par with a hidden loss function to avoid gradient vanishing, which is known as deep supervision. Although they report very good results in terms of AUC in contrast to other competing networks, they also state that there is a serious misclassification problem in small lesions, such as microaneurysms, etc. Otalora et al. [139] proposed a model that was based on Active Learning to train a CNN for exudate detection in loosely labelled fundus images. In particular, the images were cropped into several smaller patches, of which only a few have relevant ground-truth information. The rest unlabelled patches are ranked based on their "interestingness", as the authors mention, which essentially encodes how much information the patch has regarding the specific lesion. In order to initially train the network, they use the labelled part of the dataset and, subsequently, they iteratively use a portion of unlabelled ranked patches, starting from the most interesting one, in order to train a network, while asking a medical professional to annotate only those specific patches. The training is stopped when the network has converged, even if not all images were fed to the model, which ultimately ensures that only the most informative ones are actually used during the training.</p>
        <p>Khojasteh et al. [140] compared the performance of a CNN, a Discriminative Restricted Boltzmann Machines and the deep features extracted out of a Resnet-50 CNN in combination with several Machine Learning classifiers for detecting exudates in fundus images. The best results were achieved using the deeply learnable features with an SVM classifier. Orlando et al. [102] combined the deep features of a CNN with hand-crafted features into a Random Forest Classifier for detecting early lesions of DR.</p>
        <p>Although segmentation CNNs can be applied on the image at its original resolution, it is very resource-heavy and inefficient. Thus, instead of training the model on the entire image, many researchers crop it in smaller patches instead of just resizing the image at a lower resolution [78,116,[141][142][143]. This is necessary when the computational resources are limited or when larger spatial resolution is required, at the expense of a smaller field of view. In Zheng et al. [78], a standalone UNet was trained on fundus patches in order to segment the relevant DR lesions from the images. Two different partitioning approaches were examined for the creation of patches. The first was based on randomly selecting a pixel that was part of an exudate lesion, and taking a patch of 48x48 pixels around it. This approach ensured that the selected patches contained an exudate, but the exudates in several patches may overlap with each other. The second approach was based on iteratively cropping discrete patches out of the image. Although there were no overlapping areas among the various patches, the percentage of them containing an exudate was very small. The model achieved the best F1score of 92.8%, when trained with a mixture of 75% patches from the first approach and 25% from the second one.</p>
        <p>Several other authors have trained a traditional CNN to generate probabilistic maps, in order to indicate where the lesions are located. In order to avoid redundant boundaries and cluttered pixels around the segmented signs, Khojasteh et al. [76] applied three morphological operations, i.e. closing, opening and finally erosion. However, due to the fact that they generated three output probabilistic maps, one for each of the lesions, it is probable that some pixels will probably belong to more that one lesion. In order to overcome this obstacle, they assigned the pixel with the most probable class, i.e. the one with the highest probability among the three lesions. Both Lam et al. [141] and Benzamin et al. [143] computed the probability of each pixel belonging to each lesion type by scanning each image using a sliding window approach and a traditional CNN. Saha et al. [144] utilized an Encoder-Decoder CNN to segment DR lesions using probabilistic maps. They also included an additional class corresponding to Optic Disc (OD) in order for the network to be able to better differentiate it from exudates.</p>
        <p>As discussed in Section 3, UNets have played a major role in advancing semantic segmentation in many fields. Regarding the fundus imaging and especially Diabetic Retinopathy, several published architectures and AI models have been proposed that are based on a UNet architecture. Yan et al. [145] developed a mutual Global-Local U-Net for segmenting DR related lesions on fundus images. A Global Net and a Local Net are mutually trained using the entire image and its patches respectively, in order for the complete framework to incorporate both local and global information. Sambyal et al. [146] proposed a modified UNet architecture, which utilized a pre-trained ResNet34 as the encoder. For the decoder, 4 UNet blocks with sub-pixel convolutional upsampling with periodic ICNR shuffling [147], which is used to alleviate any checkerboard noise induced by the upsampling procedure.</p>
        <p>Gondal et al. [148] used Class Activation Maps in order to visualize possible lesion areas that played a significant role in predicting the severity grade of DR using a classification CNN model. Although attention maps can designate DR lesions, the generated attention maps cannot segment the lesions in detail due to the fact that the main purpose of those networks is to classify the image for its severity. In contrast, such rough estimation of the lesion areas can be interpreted or considered as an estimate of where the network is focusing in order to make its decision. Linking those highlighted areas with true lesions is based on how medical professionals diagnose DR, which is performed by examining the presence of those lesions. However, proving that the model is actually learning such intrinsic characteristics of the data is an open issue, referring to Interpretability and Explainability issues of deep learning models, and cannot be inferred directly. Although attention maps can not be considered accurate and reliable segmentation maps, a very interesting fact was mentioned by Quellec et al. [73], who reported that their classification network managed to optimally detect the various lesions at different training iterations of the model. In particular, the networks (regardless of their architecture) detected the more obvious lesions, i.e. hard and soft exudates, earlier during the training iterations than the more subtle ones, i.e. haemorrhages and then microaneurysms.</p>
        <p>In an effort to generate synthetic images and enlarge their fundus dataset, many have been utilizing Generative Adversarial Networks (GANs) [78,103,109,149]. A conditional Generative Adversarial Network (cGAN) based on a UNet, was developed in Zheng et al. [78] in order to tackle the problem of the limited and severly imbalanced fundus imaging datasets. By doing so, the authors managed to increase the F1 score up to approximately 4% for lesion segmentation. Zhou et al. [109] proposed a GAN model to generate high resolution fundus images that can be manipulated with arbitrary grading and lesion information, which can be used to train both a classification and a segmentation network. GANs have also been used for the segmentation task, apart from the data augmentation. Xiao et al. [68] incorporated a Discriminator network into the pipeline of a traditional segmentation CNN, namely 
            <rs type="software">HEDNet</rs>, to refine its segmentation, by also minimizing the Discriminator's loss, which was added to the original loss of the CNN.
        </p>
        <p>Although a similar number of models for segmenting each lesion have been developed, i.e. 19 for exudates [65, 68, 73, 76, 78, 92-94, 121, 130, 134, 138, 140, 143-146, 148, 150] 18 for microaneurysms [68, 73, 76, 88, 91, 95, 102, 121, 130, 138, 144-146, 148, 150-153], 12 for haemorrhages [68,73,76,94,102,121,138,144,145,148,150,151] and 1 for all lesions combined in a 2-class segmentation task [141], their performance is not the same across all lesion types. In particular, the mean accuracy and AUC of the segmentation models regarding the exudate lesions are 97.98 ± 2.35 and 0.684 ± 0.263 respectively. On the other hand, the mean accuracy and AUC regarding the microaneurysms are 92.15 ± 10.17 and 0.565 ± 0.337, while for the haemorrhages these metrics are 93 ± 4.24 and 0.56 ± 0.307 respectively. It becomes evident that exudates are the easiest of the three lesion types to be detected. This is something that can be partly explained by the fact that exudates have a much bigger size than microaneurysms and a distinct yellow color compared to both the haemorrhages and microaneurysms whom color is similar to the retina's vessels' color, as seen in Fig. 2.</p>
        <p>Tan et al. [154] noticed that the number of pixels in their dataset that were related to healthy parts of the retina (background) were significantly higher than those related to lesions (29 m background points vs 300k lesion points). Thus, their training procedure was split into 2 phases. During the first phase a smaller but balanced subset of the training set was used (6.4k-9k points each class) to initially train the network, while a much larger one (120k points each class) was used to train the network during phase 2. Eftekhari et al. [152] proposed a 2-stage pipeline for MA detection. In the first stage, the input image is passed through a CNN model to produce a probability map of possible MA regions. Combining that map with the original image, a second CNN detects specific MA and non-MA spots. The authors claim that by doing so, they overcome the serious challenges of the imbalanced datasets, which results in the decrease of the model's false positive rate.</p>
        <p>Table 8 summarizes the performance results of the segmentation methods as they were reported in the corresponding papers. It should be noted that although we questioned the use of classification-oriented metrics for evaluating the performance of a segmentation model in Section 7.1, we included the performance results of the works using them.</p>
        <p>Contrary to the findings of a recent review article [20], which clames that "there are not many methods based on deep learning, and advanced deep learning techniques must be developed in order to solve this problem", our analysis indicates that a significant number of DL models has been developed to date and that several of them have actually been employed in the context of clinical decision making in real life environments. What is even more important is that some of them have successfully gone through the regulatory process, having gained approval by relevant international regulatory agencies, such as FDA [108,155].</p>
        <p>Abramoff et al. [108] reported the performance of IDx-DR X2.1 device, which is equipped with a non-mydriatic retinal camera and an automated system for the detection of DR based on CNNs. Although it does not grade the severity of the disease, the system recommends a follow-up with an ophthalmologist regarding the referable DR cases and a 1-year follow-up screening for the non-referable DR cases. In a prospective clinical study, the system was tested on 900 patients on 10 sites [156], and reported a sensitivity of 87.2% and a specificity of 90.7% for more-than-mild DR detection. That led to IDx-DR X2.1 being the first commercial AI device which got the US Food &amp; Drug Administration (FDA) approval in April 2018 [157].</p>
        <p>Another FDA-approved commercial AI software for diabetic retinopathy screening is 
            <rs type="software">EyeArt</rs> [155], which as they claim achieves a sensitivity of 96% and a specificity of 88% for detecting more than mild diabetic retinopathy and a sensitivity of 92% and a specificity of 94% for detecting vision-threatening diabetic retinopathy. EyeArt was also validated on multi-centre dataset of 30405 images from the English Diabetic Eye Screening Programme, achieving high sensitivity for referable DR detection and an acceptable specificity score [125].
        </p>
        <p>Ting et al. [112] proposed a DL-based model to detect referable DR and vision-threatening DR as well as other eye-related diseases. The model was trained and validated on 500.000 images, which were obtained from Singapore National Integrated Diabetic Retinopathy Screening Programme (SiDRP). Their primary objective was to compare the results of their model to several trained and experienced professionals, achieving an AUC of 0.936, sensitivity of 90.5% and specificity of 91.6%, close to that of the professionals who achieved sensitivity of 91.2% and specificity of 99.3% for detecting referable DR. As far as the vision-threatening DR is concerned, the DL-based model achieved an AUC of 0.958, sensitivity of 100% and specificity of 91.1% against the professionals who achieved a sensitivity of 88.5% and a specificity of 99.6%. The model achieved comparable or greater performance in terms of sensitivity but struggled compared to the experts in terms of specificity. In another clinical validation study, Raumviboonsuk et al. [158] reported higher sensitivity but lower specificity scores compared to medical professionals as well. Thus, it is argued that a human-supervised deployment of such a model would be preferable, in order to utilize its high sensitivity but also compensate for its low specificity with the expert's second diagnosis.</p>
        <p>Bellemo et al. [159] trained an ensemble of CNNs for diabetic retinopathy grading on fundus images of more than 13.000 patients who had participated in the Singapore Integrated Diabetic Retinopathy Program. Their model was validated through a clinical study on 1574 patients in five urban centres in the Copperbelt province of Zambia. The model achieved clinically acceptable performance in detecting referable DR, vision-threatening DR and diabetic macular edema. These results show that the adoption of a deep learning system for detecting DR is possible and can help developing countries, such those in Africa, where there is lack of expertise and resources, even if the model is trained on a different population. However, their model is trained to detect only the severe non-proliferative and proliferative diabetic retinopathy, leaving out any milder stage of the disease. As they state, a future study which would incorporate all of the stages of diabetic retinopathy is needed, especially because it is equally as important to detect the milder stages of the disease in order to prevent it from deteriorating and ensure early treatment.</p>
        <p>Many datasets, such as Messidor, IDRiD, etc. consist of high-quality images which were captured under controlled, non-standard conditions (i.e. similar environmental and hardware conditions across captures). Thus, it can be argued that the algorithms trained on such datasets will perform poorly under typical practical situations, where the images may not be directly comparable and the environmental and hardware details may differ. On the other hand, although Kaggle Eye-PACS and APTOS datasets address these issues and closely resemble a real world scenario, since they consist of images which were captured from a variety of camera models, under various non-typical conditions, the noise which is present due to those variations makes it very difficult for the algorithms to accurately and effectively perform the analysis. However, taking into account those poor-quality images that reflect the actual data, one can develop robust algorithms which can be effective in the clinical practice.</p>
        <p>In addition, some datasets, including the large Kaggle EyePACS and APTOS 2019 [46,47], have been graded by only one expert, which can lead to an annotation bias. Several other datasets either focusing on the DR severity classification [48,50,51] or on lesions segmentation [52,54,55,160] have proposed a more complex grading method (majority voting or adjudication on multiple gradings), in order to remove such a bias and develop a robust and accurate ground truth information for those datasets. Gulshan et al. [110] also proposed that the collected data should be graded multiple times from different professionals, to increase the robustness of the ground truth and in turn the accuracy of the model. Towards that direction, a uniform reference standard should be established to mitigate graders' disagreements [115]. The latter study, showed that an adjudication grading standard was more rigorous, especially in detecting artifacts and missed microaneurysms, than a majority decision protocol.</p>
        <p>Poor image quality of the data can affect the training procedure as well as the performance of the model. Subtle signs of retinopathy at an early stage can be easily masked on a low contrast or blurred image. Rakhlin et al. [98] proposed a quality assessment module in their diagnostic pipeline, which discards ungradable images from the dataset. Subsequently, these images are referred to a professional ophthalmologist for examination. Jiang et al. [100] also rejected the low quality images from the final dataset. Li et al. [51] defined the classification as a (6-class) scenary grading problem; the first 5 classes referred to the ICDR grading scale protocol and the 6-th class referred to ungradable images, effectively incorporating the quality assessment module into the deep learning model.</p>
        <p>Tan et al. [154] utilized a dataset collected at 11 different clinical sites, using a variety of fundus cameras, and was used to train and test a single CNN model for DR lesion segmentation. During their normalization step, they calculated an A score to assess the quality of the image. Its value was calculated based on the amount of grey pixels, which emerged during normalization of the areas of the image that have little to no illumination. Such dark areas do not provide any meaningful information regarding any anatomical feature of the retina, when they are brightened up, and thus such images were discarded from both the training and the testing set.</p>
        <p>On the other hand, Quellec et al. [73] reports that their ensemble model's performance was not largely affected by the image's quality. Nevertheless, increasing the uniformity and consistency among the data, either by controlling the camera's settings and the environmental conditions during the capture or by excluding low-quality images, can improve the model's performance or at least facilitate the training procedure.</p>
        <p>Finally, the development of large training and evaluation datasets is one of the many necessary steps towards the development of robust and accurate AI models. However, most of the aforementioned datasets lack sufficient data or suffer from imbalance between their classes. With that in mind, there are several ways to overcome this issue by employing augmentation techniques or generate synthetic data using GANs [109], as well as utilize transfer learning to leverage the knowledge of trained models on large datasets, such as 
            <rs type="software">ImageNET</rs> [42]. It is also important to increase the diversity of the data regarding their demographics, in order to ensure the model's generazability [53,86,110]. Gargeya et al. [53] also proposed to incorporate additional patient metadata, such as genetic factors, duration of diabetes, hemoglobin A1C value, and other clinical data that may influence their risk for developing retinopathy. It also may be of interest to include specific information related to explicit lesion features within the classification models [110]. Doing so, the AI model may yield insightful correlations into underlying DR risk factors and potentially increasing the diagnostic performance.
        </p>
        <p>Artificial Intelligence (AI)-and especially Deep Learning (DL)-based methods hold promise for improving and accelerating healthcare. However, there are several key constraints that need to be addressed in order to facilitate AI's adoption in clinical settings [161]. Apart from the traditional methods that are used to assess the model's performance, i.e. accuracy metrics, several others are proposed as important elements towards the acceptance of AI models through regulatory processes [162]. The progress from traditional machine learning approaches to deep learning ones, although it has improved the performance of such analyses, has also been accompanied by a lack of explainability and transparency. The interpretability of such models is however a crucial element affecting their acceptance and integration in the clinical practice. The clinical operator needs to understand the model's decision process which should ideally provide explanations regarding its predictions (e.g. why these predictions were made and what alternatives were considered). Regarding DR, several researchers have generated evidence heatmaps, in an attempt to aggregate the importance of each pixel to the prediction across the several network's layers [73,100,117]. Such visualizations allow the clinicians to determine whether the model bases its prediction on relevant clinical features, which in the case of DR would include exudates, microaneurysms and haemorrhages as previously discussed. In addition, two other crucial elements regarding the robustness and reliability of the models, need to be properly addressed prior to clinical integration. These terms ultimately refer to the need of the models to consistently perform accurately across expected variations encountered in the clinical environment, including variations regarding data collected from multiple centres or machines from various vendors.</p>
        <p>Diabetic retinopathy is a serious complication of diabetes mellitus, leading to progressive damage and even blindness of the retina. Its early detection and treatment is important in order to prevent its deterioration and the retina's damage. The interest in applying deep learning in detecting diabetic retinopathy has increased during the past years and as several DL systems evolve and become integrated into the clinical practice, they will enable the clinicians to treat the patients in need more effectively and efficiently. This article presents the current state of research regarding the application of deep learning in diagnosing diabetic retinopathy. Although deep learning has paved the way for more accurate diagnosis and treatment, further improvements are still necessary regarding performance, interpretability and trustworthiness from ophthalmologists.</p>
        <p>(continued on next page)</p>
        <p>Computers in Biology and Medicine135 (2021) 104599</p>
        <p>Microaneurysm</p>
        <p>This work was partially supported by the H2020 specific targeted research project SeeFar: Smart glasses for multifacEted visual loss mitigation and chronic disEase prevention indicator for healthier, saFer, and more productive workplAce foR ageing population. (H2020-SC1-DTH-2018-1, GA No 826429) (www.see-far.eu). This paper reflects only the author's view and the Commission is not responsible for any use that may be made of the information it contains.</p>
        <p>All authors have read and approved the manuscript, and each author has participated sufficiently in developing the manuscript. N.T., D.T, and G.M. contributed to the literature review and analysis of the study and drafting the manuscript. E.K., and O.B. contributed to the clinical aspects and drafting the manuscript. A.B., F.S., A.S., D.I.F., and K.M reviewed the manuscript and contributed in the interpretation of the findings.</p>
        <p>Supplementary data to this article can be found online at https://doi. org/10.1016/j.compbiomed.2021.104599.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:21+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>In this paper, we develop four malware detection methods using Hamming distance to find similarity between samples which are first nearest neighbors (FNN), all nearest neighbors (ANN), weighted all nearest neighbors (WANN), and k-medoid based nearest neighbors (KMNN). In our proposed methods, we can trigger the alarm if we detect an 
            <rs type="software">Android</rs> app is malicious. Hence, our solutions help us to avoid the spread of detected malware on a broader scale. We provide a detailed description of the proposed detection methods and related algorithms. We include an extensive analysis to asses the suitability of our proposed similaritybased detection methods. In this way, we perform our experiments on three datasets, including benign and malware 
            <rs type="software">Android</rs> apps like 
            <rs type="software">Drebin</rs>, 
            <rs type="software">Contagio</rs>, and 
            <rs type="software">Genome</rs>. Thus, to corroborate the actual effectiveness of our classifier, we carry out performance comparisons with some state-of-the-art classification and malware detection algorithms, namely Mixed and Separated solutions, the program dissimilarity measure based on entropy (PDME) and the FalDroid algorithms. We test our experiments in a different type of features: API, intent, and permission features on these three datasets. The results confirm that accuracy rates of proposed algorithms are more than 90% and in some cases (i.e., considering API features) are more than 99%, and are comparable with existing state-of-the-art solutions.
        </p>
        <p>Nowadays, the widespread use of mobile devices in comparison with personal computers has begun a new era of information exchange. Besides, the increased power of mobile devices, coupled with the portability of user attention has attracted. Smartphones and tablets are prevalent in recent years. By the end of 2014, the number of active mobile devices around the world was about 7 billion, and in developed countries, the proportion of mobile devices and people are estimated to be 120.8%, respectively. Due to their widespread distribution and their abilities, mobile devices have become the main target of the attackers in recent years [1]. 
            <rs type="software">Android</rs> is currently the most widely used mobile smartphone platform in the world, which occupies 85% of the market share. Recent reports indicate an increase in the number of 
            <rs type="software">Android</rs> programs in recent years. As the number of 
            <rs type="software">Android</rs> applications on 
            <rs type="software">Google Play</rs> in December 2009 was 16,000, in July 2013 one million, in February 2016 it was about 2 million and in December 2017 it was five million. [2,3].
        </p>
        <p>
            <rs type="software">Android</rs> app is in two categories: Benign and Malware. Samples that are safe and do not show malicious behaviors are called benign samples. In contrast, examples of software that create a security threat are named malware samples. In recent years, the variety of malware in 
            <rs type="software">Android</rs> mobile networks is continuously increasing and thus causes a risk to users' privacy. Furthermore, the popularity of 
            <rs type="software">Android</rs> with cyber-criminals is also high and creates a lot of malicious programs to steal sensitive information and compromise mobile systems, and these conditions represent the need for security in the mobile app. Unlike other smartphone platforms like 
            <rs type="software">iOS</rs>, 
            <rs type="software">Android</rs> users can install their apps from unverified sources, such as file sharing websites. In 
            <rs type="software">Android</rs> apps, the issue of malware infection is very serious, and recent reports show that 97% of the attacks on mobile malware came from 
            <rs type="software">Android</rs> devices. In 2016 alone, more than 3.25 million 
            <rs type="software">Android</rs> malicious apps were detected. That means almost every 10 seconds a new malicious 
            <rs type="software">Android</rs> application is created [4, 5? ]. Malware term is created by combining the words "malicious" and "software". Malware is a serious threat to the computer world, and this threat is increasing and complicated. When malicious software finds its way into the system, it scans the OS's vulnerabilities, performs unwanted actions on the system, and ultimately reduces system performance [6]. Hence, an important problem with cybersecurity is malware analysis [7? ].
        </p>
        <p>In addition to accurate precision and the precision recognition rates, a malware detection system could generalize to new malicious families. For 
            <rs type="software">Android</rs> malware detection, two types of solutions, namely Static and Dynamic, have been proposed. Features like APIs, permissions, intent, URLs are analyzed in static solutions. In another category of malware, malicious components are downloaded at run-time, which requires dynamic analysis to detect these malwares [8]. For instance, the authors [9] have provided a method for detecting malware concerning the correlation between static and dynamic features. Also, the authors [10] have come up with a way to detect malware in 
            <rs type="software">Android</rs> applications, by combining static analysis and outlier detection.
        </p>
        <p>Another important point is that the system does not need to compute too much to deploy on mobile devices. Hence, the system should adopt models (e.g., machine learning models) to estimate the malicious behavior in a short time [11]. Machine learning (ML) methods are part of the artificial intelligencebased system in which solutions are provided to improve the decision-making process [12]. An ML method is widely used for specific decision-making tasks such as detecting malware, network penetration detection, and general pattern recognition issues. This method is very effective in identifying well-known and unknown malware families with high accuracy. In various studies, they design ML-based classification methods to categorize different types of samples (for example, static-based, logicbased, perception-based and sample-based types samples) and detect traffic networks on 
            <rs type="software">Android</rs> mobile devices [13].
        </p>
        <p>An advantage of using the ML method is its ability to identify different types of malware [14]. In ML methods, complex pattern recognition and optimization of parameters are well investigated [15]. The current study indicates that the damage caused by malware programs, hidden among millions of mobile applications, is increasing, and this has been a visible motivation for researchers to deal with more complex applications. Some 
            <rs type="software">Android</rs> software analyzes the malware behaviors at the API level. For example, the authors [16] give a precise analysis of an opcode-based 
            <rs type="software">Android</rs> software based on finding the similarity measurements inspired by simple substitution distance of the features. They indicate that their technique provides a useful means of classifying metamorphic malware. Some ML solutions adopt several distance calculation mechanisms to find similar samples to a specific sample. For example, the authors in [17] add new distance measure using entropy for two computer programs which are called program dissimilarity measure or PDME. PDME introduces a measure for the degree of metamorphism for samples. Also, the authors in [18] elicit several types of behavior static features from 
            <rs type="software">Android</rs> apps and apply Support Vector Machine (SVM), K Nearest Neighbor (KNN), Naive Bayes (NB), Classification and Regression Tree (CART) and Random Forest (RF) classifiers to detect malware from benign apps. KNN algorithm is classified as a supervised ML algorithm that could solve the classification and regression problems. KNN is easy to implement, no need to build a model, tune several parameters, or make additional assumptions. However, it is a slow method for large datasets. KNN algorithm can find the k nearest samples to a specific query which have distances between a query and all the samples in the dataset. Then, it votes for the most frequent label or averages the labels.
        </p>
        <p>Among different methods to calculate the distance, the Hamming distance applies between two vectors with the same length and indicates the number of entries where injected elements are different. In other words, the Hamming distance achieves the minimum number of errors while converting one vector to another one. Suppose x (x 1 , x 2 , . . . , x n ) is a sample vector and y (y 1 , y 2 , . . . , y n ) is a corresponding label of vector x on n dimensional space, the Manhattan distance is the sum of the peer to peer distances between same indexes (see equation (1)).</p>
        <p>And Minkowski distance presents by equation (2):</p>
        <p>where p ≥ 1. In this paper, using replacement method we prove that with the binary representation of the data, we calculate the Hamming distance, and the distance calculated by this method is the same as the distance used by other methods like Euclidean distance and Manhattan distance.</p>
        <p>As we described earlier, ML has been widely used in the classification of various types of 
            <rs type="software">Android</rs> OS like API, permission, intent and 
            <rs type="software">Android</rs> malware detection. For example, the paper [19] applies API system call and shapes the API graph, the reference [20] utilizes a score function to the extracted permission feature set, and finally, the paper [21] adopts weighted mutual information to select prominent features. All of these research papers used the KNN algorithm to detect malware; however, due to the lack of binary representation of data, they need several calculations to extract malware vectors from benign samples.
        </p>
        <p>Finding a threshold for k in the KNN algorithm has been considered in many studies which are important in the malware detection methods [22]. Another category of studies has suggested methods using ensemble learning that employ other algorithms such as decision tree, SVM and RF for malware detection. However, due to the simultaneous using of multiple algorithms, these methods have a high time complexity [11]. In some studies, a framework for detecting malware has been presented, which different classification methods such as SVM are applied in them [23]. In [23], the authors propose a structure that uses the KNN algorithm based on Hamming distance for malware detection system. It used a fixed k value for KNN which limits their structure.</p>
        <p>The purpose of this paper is to investigate the effect of the distance between samples to classify into malware and benign. Due to the sparse feature vectors, the Hamming distance is an appropriate measure for the discrimination of samples. We propose a modified supervised KNN Algorithm using the Hamming distance to classify the samples. Then, we combine it with an unsupervised K-Medoids algorithm to detect malware based on static features. In the proposed framework of this paper, we use the Hamming distance to apply proposed classification methods which are the modified form of the KNN method.</p>
        <p>Due to the widespread use of 
            <rs type="software">Android</rs> apps, finding a way of identifying malicious files is a critical problem that needs to be solved instantly. This paper use static analysis technology and propose four detection methods based on similarity for 
            <rs type="software">Android</rs> malware by calculating distance of samples using a Hamming distance measure. The proposed methods are flexible solutions for the problem. It means, the generated model by each scenario learns the patterns in the features and can be used to classify the samples into malware and benign. Our proposed methods well generalize the patterns even for new samples. To do so, first, we find the related set of features from the manifest part of apk file. Then, we use the RF regressor as a feature selection algorithm and rank the features. The main reason behind selecting the RF as a feature selection algorithm is that we could have better control over the results using RF when we consider different random subsamples of the original dataset [24]. Finally, we use the proposed methods based on the nearest neighbors of each sample and classify them.
        </p>
        <p>In this research, we deploy several methods that applied on APIs, Permissions, and Intents used by 
            <rs type="software">Android</rs> applications to identify malware samples or apps. We carry out extensive experiments to compare proposed solutions with existing solutions and examine the validity of the proposed detection model. To sum up, we make the following contributions:
        </p>
        <p>• We prove that the result of using the Hamming distance with other methods is the same for the binary vectors and apply the Hamming distance in the distance-based malware detection methods.</p>
        <p>• We propose four scenarios for malware detection based on the nearest neighbor approach in which we use Hamming distance to find neighbors.</p>
        <p>• We obtain the maximum achievable accuracy with the Hamming distance method as a threshold. We present the accuracy threshold calculation strategy in Section 5.2.</p>
        <p>• We evaluate the proposed malware detection methods using three standard datasets: Drebin, Contagio, and Genome. Besides, by analyzing the time and space complexity, we performed a theoretical analysis to realize the scalability of our approach.</p>
        <p>• We compare the proposed malware detection methods against the state-of-the-art methods applied for malware detection. At first, the proposed methods are compared to [22], which is 
            <rs type="software">Android</rs> malware detection based on a combination of clustering and classification. The next comparison solution in literature uses an entropy-based distance measure to detect malware [17]. In the third comparison method [19], malware samples classify into different families, making it possible for each family to share the features of the samples in a better way. The main reason behind selecting such schemes for comparison is that our proposed methods and these cuttingedge solutions using similarity-based metrics for detecting malware. Moreover, the papers [19] and [22] carry out their numerical validations in Drebin dataset in which we adapt our results on the same dataset.
        </p>
        <p>The remainder of the paper organizes as follows: We discuss related work in Section 2. In Section 3 we study the preliminary essential malware analysis. Section 4 describes the distance calculation measures in binary representation, explore the detection strategies, our defined scenarios, designs our proposed architecture for malware detection systems and provides a toy scenario and delineates the proposed algorithms, while Section 5 presents the experimental results of our proposed scenarios. Section Section 6 reports the achievement of the experiment and provide some discussions regarding our method. Finally, in Section 7 we summarize our research and provides future directions.</p>
        <p>Machine learning techniques use static, dynamic, and hybrid analysis methods to classify 
            <rs type="software">Android</rs> applications. In the following subsections, we introduce them. Also, we study some important researches in malware analysis and malware detection.
        </p>
        <p>Some techniques using static permission features, such as 
            <rs type="software">Drebin</rs> [25], 
            <rs type="software">StormDroid</rs> [26], and 
            <rs type="software">DroidSIFT</rs> [27] which are applied on 
            <rs type="software">Android</rs> apps [28].
        </p>
        <p>The authors in [29] propose a new detection system called 
            <rs type="software">ANASTASIA</rs> to identify malicious samples using intents, permissions, system commands, and API calls features. ANASTA-SIA uses several classifiers by applying deep learning method and can extract several feature types from Android applications using the conditions of the app. Additionally, The authors in [30] investigate 
            <rs type="software">Android</rs> apps to describe their resource usage and leverage the profiles to detect 
            <rs type="software">Android</rs> malware.
        </p>
        <p>The authors in [31] present an automatic signature generation approach called 
            <rs type="software">AndroSimilar</rs> in which to detect malware for the static syntactic features in 
            <rs type="software">Android</rs> apps. Also, 
            <rs type="software">An-droSimilar</rs> can detect unintelligible malware with techniques such as junk method insertion, renaming method, string encryption, and changing control flow that can be used to evade fixed signatures working against malware. Besides, 
            <rs type="software">AndroSimilar</rs> can detect unknown types of existing malware. Also, the authors build an 
            <rs type="software">AndroSimilar</rs> generation approach based on digital forensics Similarity Digest Hash (
            <rs type="software">SDHash</rs>) to distinguish similar documents. In 
            <rs type="software">SDHash</rs>, unrelated apps receive a lower probability of having standard features. Also, it helps to control false positive rates for two separate apps that share some features. Another method [32] applies the same strategy to extract fixed-size byte-sequence features using their entropy values and searches for popular features and selects some of them using KNN strategy.
        </p>
        <p>Dynamic solutions could run an 
            <rs type="software">Android</rs> app in a protected environment and provide all the required emulated resources to identify malicious activities. In literature, we find some implemented dynamic analysis methods -however, they suffer from resource constraints of a smartphone. In another group, some papers concentrate on the behavioral class of the malware detection solutions. For example, in [23], the authors define the malware types based on their behavioral class. They propose a new scheme which identifies the misbehavior classes modified by each malware type by correlating the features extracted at four different levels: kernel level, application level, user level, and package level. At the kernel level, their solution could monitor the system calls and hijacks them if any app triggers them. At the application level, it controls the critical APIs to detect the malicious behaviors posed by the apps such as the installation of new applications, requests for administrative privileges, generating too many processes, constant app monitoring on the active application. At the user level, they monitor user activities and detect malicious events when the user is idle or not interacting with the device. At the package level, they propose a new system to identify the risky applications under observation based on permissions requested by the app and market information.
        </p>
        <p>The fatal limitation of dynamic approaches is if they trigger with some non-trivial events, then they can miss some malicious execution path. For example, anti-emulation techniques such as Sandbox [33] and reference [34] detection mechanism are unable to timely analyze the environment and lead to delaying the identifying malware and raise the evasion of the dynamic analysis methods.</p>
        <p>We can generate hybrid solutions when we apply static and dynamic approaches in the same time. Hybrid solutions can borrow the characteristics of static and dynamic solutions to improve malware detection strategies like 
            <rs type="software">DroidDetector</rs> [35]. 
            <rs type="software">DroidDetector</rs> could apply static and dynamic analysis usign deep learning to distinguish malicious software from normal applications. It uses permissions and sensitive API for static analysis. These static behaviors extract the features using 
            <rs type="software">TinyXml</rs> [36], 
            <rs type="software">7-zip</rs> [37], and 
            <rs type="software">Baksmali</rs> tools [38]. After that 
            <rs type="software">DroidDetector</rs> dynamic features analysis using 
            <rs type="software">DroidBox</rs> tool. Furthermore, Shanmugam [16] propose an alternative distance for metamorphic malware. Their distance measurement solution is based on the opcode-based similarity method and simple encryption reported in [39]. They use this distance measurement to classify malicious programs. The application, which is sufficiently similar to the metamorphic malware is classified as malicious. Some malware detection methods use Euclidean histogram distance metrics to compare two program files -for example, Rad et al. [40] suggest that a histogram of opcodes can be used to detect metamorphic viruses. Some studies apply statistical methods to detect malware. For example, Toderici [41] use an analytical approach based on a chi-squared test to improve the hidden markov models Based malware detection. In another work, Ambra Demontis et al. [42] elaborate a solution to mitigate evasion attacks like malware data manipulation. In that paper, their method utilizes a secure SVM algorithm that can enforce its features to have evenly-distributed weight.
        </p>
        <p>In this section, we review some of the essentials for malware analysis and how to model malware. In applied mathematics and computer science, a sparse matrix is a matrix in which most of the elements are zero. In Fig. 1, we use sparse matrix representation which contains important information related to 
            <rs type="software">Android</rs> app features such as APIs, permissions and intents. In this study, we follow the general setting for designing a malware detection system that contains the benign B and malware samples M. To do so, we select the performance evaluation settings and store a dataset that includes the labeled examples (i.e., with n samples) and the m elements for each sample. Hence, in equation (3) we have
        </p>
        <p>where x i is the i-th malware sample vector of each component presents the selected feature; y i ∈ {0, 1} is the corresponding label of the features; x i j is the binary value of the j-th feature in i-th sample where {∀ j = 1, . . . , m}. Also, we can set x i j = 1 if x i has the j-th component and x i j = 0 otherwise; n is the total number of samples, and X ⊆ {0, 1} m is an m-dimensional feature space.</p>
        <p>In this section, we first apply replacement method and prove that in the binary representation, Manhattan distance, Minkowski distance, and Hamming distance are the same. Then, due to the simplicity of computation, we use the Hamming distance method in the proposed detection algorithms. After that, we present our proposed architecture. The main notations and symbols used in this paper are listed in Table 1. 4.1. Equivalence of distance calculation measures in binary representation In our paper, we introduce methods to identify malware samples from benign samples using the distance measure. Given the fact that the samples are binary vectors, the existence of a feature means a value of 1 and the absence of a feature means zero. The proposed method for computing the distance between the samples is to use the Hamming distance of the two vectors. On the other hand, it can easily be shown that in the binary mode of vectors, the result of using different criteria is to calculate the same distance. Suppose the binary vector Y = [y 1 , ..., y n ] is the most similar vector to X = [x 1 , ..., x n ]. It means, d(X, Y) ≤ d(X, Z), ∀Z. Several distance formulas apply to find the most similar vector to vector X. We list some of them as follows.</p>
        <p>• Taxicab distance which is also called the Manhattan distance presents in the equation (1):</p>
        <p>• Minkowski distance presents as equation (2). Since we need the most similar vector so we have equation (4):</p>
        <p>which is equivalent to:</p>
        <p>On the other hand, since our vectors are binary, so we have:</p>
        <p>Different values of p determine the application of this equation. For p ≥ 1, the Minkowski distance is a metric as a result of the Minkowski inequality. Minkowski distance is typically used with p being 1 or 2, which corresponds to the Manhattan distance and the Euclidean distance, respectively.</p>
        <p>We can rewrite equation (4) using equation (6) as</p>
        <p>Then, we have:</p>
        <p>and we can conclude</p>
        <p>The last equation imposes that the X and Y vectors result from each other. Formally speaking, we show that using the d 2 measure, vector Y is the most similar vector to the binary vector X. In Fig. 2, we introduce our proposed architecture. In this figure, we select the static features of data samples (out of N samples) in the dataset (see the rectangular feature selection component in Fig. 2). Then, using the Random Forest feature selection algorithm, we select the α percentage of featuresn/n = α. The α value will be in the following set.</p>
        <p>α ∈ {10, 20, 30, 40, 50, 60, 70, 80, 90, 100}</p>
        <p>For example, if α = 10, it means we select 10% of features from feature selection component. After that, we convert the selected features of the samples to a vector. Then, we generate a binary vector for each sample by placing the value of 1 for each feature that exists in the sample and the value of 0 for each non-existent feature (see the Binary Vectors component in Fig. 2). Then, we generate our ML model using each of our proposed detection algorithms as classification algorithms based on Hamming distance similarities between the samples and use the ML model to detect malware among benign samples.</p>
        <p>Measuring the similarity between samples is a significant operation in the classification algorithms. Classifiers which use similarity strategy can estimate the label of a sample in test set based on the similarities between that sample and label of samples in a training set, and the pairwise similarities between the training samples. In the following, there are several ways to detect malware, which, despite the simplicity, represents a good result. Suppose that we want to find the most similar members (i.e., find the most similar vectors) of the train set to the vector x which belongs to the test set. From the mathematical point of view, the element y is the most similar to x if we have equation (11):</p>
        <p>In which, d(x, y) represents the difference between x and y, which is also called distance. There are several methods to calculate the distance such as the Hamming distance, Minkowski distance and so on, which discussed earlier. Due to the binary nature of the elements (samples), we will show that the distance results of all these methods are similar, and therefore there is no ambiguity in selecting the specific method. For malware detection, we introduce several scenarios and present the results.</p>
        <p>To this end, we summarize each proposed malware detection method as follows:</p>
        <p>FNN : First Nearest Neighbors-In FNN, the first member of the training dataset is considered as the most similar member of the input data, and if a member is found to be more similar, the new member is considered as the most similar. The pseudocode of this method is shown Alg. 1.</p>
        <p>ANN: All Nearest Neighbors -ANN is similar to the FNN, with the difference that at each stage all similar neighbors are stored and all of these elements are involved in the conclusion process. The pseudocode of this method is as Alg. 1. Note that in this method, the voting process is based on the population of the labels, and the features of the malware will have no effect on the decision-making process. This issue is discussed further.</p>
        <p>WANN: Weighted All Nearest Neighbors -In WANN, the importance of features is examined. For this purpose, first, a variable W is defined, each element of which holds the percentage of the frequency of its corresponding feature. Similar to the previous method, in this method, all the neighbors are firstly calculated with the input element x and among them, we store elements whose features are close to x according to the frequency of the features. In this case, if we find several similar elements, we will take the voting process. In this method, the probability of the features is also effective in the voting process.</p>
        <p>KMNN: K-Medoid based Nearest Neighbors -K-Medoid clustering method is a type of K-means that can be more robust to noises and outliers. Medoid is the central point of the cluster, which is an actual point of the cluster and has the minimum distance to other points s [43]. This scenario is a combination of KNN and K-medoids. It is similar to previous scenarios, with the difference that the label recognition process is based on the closest nearest neighbor. First, the most similar neighbors are calculated using the second scenario (i.e., the scenario is based on finding all the same neighbors with the same similarity). Then, the neighboring set is divided into two clusters.</p>
        <p>In each cluster, one of the samples, which has the smallest distance with other samples, is selected as the cluster head. Then, the distance to each of the samples is calculated from the cluster heads and sort in terms of distance. In the last step, ten percent of the samples, which have more distance than the clusters, are ignored and voted between the rest of the remaining samples to obtain the label for the sample x. The reason for this clustering is that one of the clusters is likely to represent malware and another to represent benign software. After that, a cluster head is calculated for each of these clusters and used to determine the probable unrelated neighbors (outlier data). To do this, we consider k percent of the neighbors with the most distance to these clusters as the outlier data, and remove them from the list of neighbors. In this paper, we consider k = 10. Finally, similar to the previous scenarios, the voting process is performed, and the test data label is determined. The process is presented as Alg. 1.</p>
        <p>To better understand the proposed methods, we use a toy example presented in appendix A that clearly outlines the algorithms.</p>
        <p>In following section, we conduct time complexity analysis on the presented detection methods. Hence, we detail the time complexity of our proposed methods as follows:</p>
        <p>• FNN. In FNN, we first obtain the distance between each sample and other samples in the training dataset. Then, we aim to select the first nearest sample. Assuming that in the training dataset we have n samples and each sample has m features, the time complexity of finding the first similar sample in the worst case will be O(n × m).</p>
        <p>• ANN. In ANN, similar to FNN, we first obtain the distance between each sample and other samples in the training dataset.</p>
        <p>The time complexity of finding the most similar samples in the worst case is O(n × m). The next step in the ANN algorithm is voting on all similar samples. Suppose that we have k similar samples. The time complexity of this step also is</p>
        <p>• WANN. The first step in the WANN algorithm is finding the vector W, which indicates the weight of the features. To compute the weight of the features, we calculate the abundance of features in the training dataset and divide it to the number of samples. Assuming that there are n samples in the training dataset, and each sample contains m features. The duration takes to find the vector W in training dataset is in the order of O(n × m). The next step in this method is similar to the previous methods and includes finding all similar instances and voting between them. In this way, the time complexity of the</p>
        <p>• KMNN. In KMNN method, K-Medoid is used with the nearest neighbor method. Assuming that k is the number of clusters which anyone has c i elements, the time complexity for this algorithm is about</p>
        <p>, that i is the number of algorithmic repetitions to achieve the optimal answer. Given that only two clusters are chosen, k is equal to 2, and we set i = 10. Therefore, the time complexity of this part is O(n 2 × m). In the second part of this algorithm, the distance of the selected samples with the CHs are calculated and these distances are sorted. The time complexity of this part is O(n × log(n)). Therefore, the time complexity of the KMNN algorithm is O(n 2 × m).</p>
        <p>In calculating the time complexities of proposed methods, we estimate duration takes for finding the distance between two arbitrary vectors X and Y with m features. In worse case, we consider the samples X and Y as binary vectors with length m and compare the elements of them by computing Hamming distance between the similar entry of vectors. In implementation, we present examples in the form of sparse collections. In this case, we can apply the following mathematical equation to calculate the distance between two sets of X and Y vectors as follows:</p>
        <p>In this regard,the symbol (#) presents the number of members for each set. We confirm that this mathematical equation provide more accurate results. For example, In the tested dataset in this paper, in the worst case, we have m = 21, 492 features per sample, while using the above equation, the distance will be at most the order of 925 simple calculations.</p>
        <p>In this section, we report an experimental evaluation of the proposed clustering algorithms by testing them under different scenarios.</p>
        <p>In the following, we describe the datasets, mobile application static features, test metrics, and comparison solutions' tuning.</p>
        <p>We conduct our experiments on three datasets which are explained below:</p>
        <p>• Drebin dataset: The Drebin dataset is a Android example collection that we can apply directly. The Drebin dataset includes 118,505 applications/samples from various Android sources [25].</p>
        <p>• Genome dataset: The genome project is supported by the National Science Foundation (NSF) of the United States. From August 2010 to October 2011, the authors collected about 1,200 samples of 
            <rs type="software">Android</rs> malware from different categories as a genome dataset [44].
        </p>
        <p>• Contagio dataset: it consists of 11,960 mobile malware samples and 16,800 benign samples [45].</p>
        <p>In this paper, we consider various malicious sample features like permissions, APIs and intents. We summarize them as follows:</p>
        <p>• Permission: permission is a essential profile of an 
            <rs type="software">Android</rs> application (apk) file that includes information about the application. The 
            <rs type="software">Android</rs> operating system processes these permission files before installation.
        </p>
        <p>• API: API feature monitors various calls to APIs on an 
            <rs type="software">Android</rs> OS, such as sending SMS or accessing a user's location or device ID.
        </p>
        <p>• Intent: Intent feature applies to represent the communication between different components which is known as a medium.</p>
        <p>Due to a large number of features, we first ranked the features using the 
            <rs type="software">RandomForestRegressor</rs> algorithm. Then, we repeat our experiments for {10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100%} of the manifest features with higher ranks to determine the optimal number of features for modification in each method. The evaluation of a model skill on the training dataset would result in a biased score. Therefore the model is evaluated on the held-out sample to give an unbiased estimate of model skill. This is typically called a train-test split approach to algorithm evaluation. In this paper, at each test, we randomly consider 60% of the dataset as training samples, 20% as validation samples and 20% as testing samples. We repeated this operation ten times for each algorithm and averaged the results.Each of these 10 sets of train, validation and test were generated using non-duplicate seed. Table 2 shows the accuracy of the proposed methods in this paper on train, validation and test data. We run our experiments on an eight-core Intel Core i7 with speed 4 GHz, 16 GB RAM, OS 
            <rs type="software">Win</rs>10 64-bit.
        </p>
        <p>We compare our proposed algorithms against the corresponding ones of some state-of-the-art classification and malware detection algorithms, namely joint solution [22], the program dissimilarity measure based on entropy (PDME) [17] and the Fal-Droid [19] algorithms. In detail, two detection approaches are proposed in [22]. In the first method, we consider a random member called k from train set as a CH and divide the train set into k clusters. Then, we calculate the distance between all the elements of each cluster and consider the element that minimizes the total distance from all members as a new CH. All members are re-clustered using a new cluster. We repeat this process and change the elements of the clusters in each step. Once the end cluster has been identified in the last replication, these clusters are used to identify the label. The label of the element (test element) is considered to be the label of the cluster label that has the smallest distance with the desired element. In the event that several clusters are found with this feature, they will be voted on. The second method presented in [22] is very similar to the first method. The only difference between the first and second methods is that we divide the train set into two parts: malware and benign. We now run the first method for both malware and benign separately. Therefore, the entire train set in the second method is divided into 2k clusters. In the following, similar to the first method, the test element spacing is calculated with all the clusters of the malware and benign sets. Finally, the labels are select by voting between the nearest clusters of malware and benign sets.</p>
        <p>In [17], the proposed method is based on entropy. First, all neighbors with the least distance from the sample are calculated and voted between them. The only difference in this method is calculating distance. In this method, using the concept of entropy, which is one of the most famous concepts of information theory, the distance between the two samples S 1 and S 2 is calculated as equation ( 13):</p>
        <p>In which, the S im(S 1 , S 2 ) is a similarity measure for two samples and is computed by (14):</p>
        <p>In this definition, En(S 1 ) represents an entropy that indicates the probability of the random variable S 1 . In this paper, S 1 is considered as an j-th binary vector with a maximum of m features. The entropy is calculated as equation ( 15), in which p(S i ) is the probability of i-th feature occurrence of the vector S 1 :</p>
        <p>In our paper, we use FalDroid as a method for classifying the samples and compare their results against our proposed methods. Obviously, other innovations expressed in [19] are not recovered.</p>
        <p>In the ML-based malware detection methods the confusion matrix is compute and then performance metrics are calculated using the confusion matrix. The confusion matrix contains the following parts:</p>
        <p>• True Positive (τ): the number of correctly classified samples that belongs to the class.</p>
        <p>• True Negative (δ): the number of correctly classified samples that do not belong to the class.</p>
        <p>• False Positive (ρ): the number of samples which were incorrectly classified as belonging to the class.</p>
        <p>• False Negative (µ): the number of samples which were not classified as class samples.</p>
        <p>• Accuracy: This metric is described as equation ( 16):</p>
        <p>• Precision: This is the fraction of relevant samples between the retrieved samples. Equation (17) shows how to calculate this metric:</p>
        <p>• Recall: This Recall is defined as equation ( 18):</p>
        <p>• f1-Score: The harmonic mean of precision and recall defines as F1-score which we describe it in equation ( 19):</p>
        <p>• False Positive Rate (FPR): The FPR is computed as the ratio between the number of negative events incorrectly classified as positive (false positives) and the total number of actual negative events. This metric is described as equation ( 20):</p>
        <p>• Area Under Curve (AUC): This metric is a method for determining the best model for predicting the class of samples using all thresholds. That is, AUC measures the trade off between misclassification rate and FPR. This metrics can be calculated as (21):</p>
        <p>• Receiver Operating characteristic Curve (ROC): ROC a graphical plot that illustrates the detection ability of a binary classifier system as its discrimination threshold is varied.</p>
        <p>In the similarity-based methods which are proposed in this paper, for each sample in the test set, we opt the samples that are the most similar to the test sample. Afterward, we determine the label of each sample using the labels of samples in the most relevant neighbors. We determine it when the main label of the test sample is not the same as the neighboring set. As an example, let us assume that the test element has a label of 1, and all the elements in the nearest neighbors are labeled 0. If we select the first element that is located in the nearest neighbor or select it based on polling which is conducted between neighbors, then, in both cases the 0 labels will suggest for the sample incorrectly. We can use this strategy to calculate the upper-bound and the lower-bound for accuracy in a similarity-based method. Therefore, we consider this ranges in calculating the maximum accuracy of similarity methods and add them to the Tables 345as the last three columns. Formally speaking, suppose that we want to calculate the maximum value of F x x+y , hence, we have</p>
        <p>To maximize the expression x x+y , we just need to minimize 1 + y x . Therefore, by calculating the minimum y and the maximum x we can obtain the maximum x x+y expression. Regarding the method of calculating the accuracy presented in equation ( 16), the following substitution defines as below:</p>
        <p>Where maximizing the value of accuracy depends on minimizing the summation of µ and ρ and maximizing the summation of the value of τ and δ. Therefore, in the case of the maximum value for τ + δ and minimum value for ρ + µ, the accuracy maximizes. To obtain the maximum value of τ + δ and the minimum ρ + µ value, we can write</p>
        <p>where Real Label(x) ∈ ANN(X) ( 25)</p>
        <p>where Real Label(x) / ∈ ANN(X) (26) Similarly, the minimum FPR and the maximum AUC can be calculated. As recall it, we report these cases in Tables 3-5 as MAA.</p>
        <p>In this section, we apply the proposed methods for detecting malware on three datasets, Drebin, Contagio and Genome, and compare the results with three of the state-of-art researches.</p>
        <p>In the reference [22], the authors propose two different methods based on the KNN algorithm. Hence, they consider fixed values for k for two different methods called Mixed and Separated ones. In Fig. 3, we test them on different k methods (i.e., k = {30, 40, 120, 240}) for the Drebin, Contagio, and Genome datasets. From this figure, we can draw three conclusions. Firstly, we understand that our methods (i.e., FNN algorithm as the worst method and WANN as the best method) have the highest accuracy rate compared to the methods presented in [22]: Mixed and Separated algorithms. Secondly, as we present in the three sets of comparison plots (Figs. 3a-3c, Figs. 3d-3f, and Figs. 3g-3i) the Mixed algorithm provides higher accuracy rather than the Separated algorithm (i.e., for each of four selected samples). This means that the average rate of accuracy (i.e., detecting malware from API, intent, and permission features) for the Separated method applied to all type of datasets is less than 90% and for the Mixed algorithm less than 98%. Finally, if we increase the k value in KNN algorithm, the accuracy of both Separated and Mixed methods increase. The solutions presented in [22] depending on the optimal k value. Hence, they require optimization algorithms or trial and error methods to find the appropriate k value to approach current accuracy ratios which raise the complexity of their methods. While in the methods proposed in this paper, we try to find the nearest neighbors regardless of the value of k and compare the proposed methods with the modification in the number of features.</p>
        <p>Score In Fig. 4, we provide the precision and recall values for the different algorithms. Both recall (sensitivity) and precision (specificity) measures to use to determine generated errors. The recall is a measure that could show the rate of total detected malware. That is, the proportion of those correctly identified is the sum of all malware (i.e., those that are correctly identified by the malware plus those that are incorrectly detected by benign). Our goal in this section is to design a model with high recall that is more appropriate to identify malware. In this way, the set of Figs. 4a-4c Seprt, k=70 Seprt, k=120 FNN (worst) WANN (best) (i) Genome-intent Figure 3: Comparison accuracy value between Mixed algorithm against Separated (or Seprt) algorithm reported in [22] and our best (FNN) and worst (WANN) algorithms for API, intent and permission features for various datasets. aforementioned values for the permission, API and intent data for the Drebin, Contagio, and Genome datasets, respectively. The precision measure shows the same concept for benign samples. It means, how many benign samples can we detect from all benign samples. Precision model is the proportion of samples that are not malware to the total benign samples (i.e., those that are detected as benign and those that are incorrectly considered as malware). With these explanations, the recall and precision metrics use instead of the accuracy metric and have a wider application in machine learning. In most cases, these two metrics do not improve together. Sometimes we compute the precision of the model with more precise algorithms, that is, the ones we announce malware is most likely malware. Examples that are incorrectly classified as malware are very few, which means the precision of our algorithm is very high. But we may not consider the particular aspect of the data, and the total number of malware samples is much more than our declared examples, that is, we have a very low recall. On the other hand, it is possible to simplify our detection algorithm to increase the number of detected malware. In this case, the number of our incorrectly classified samples is increased, the precision value of the algorithm is low, and the recall value is high. On the other hand, if we can find a combination of both recall and precision values to measure the classification algorithms, the focus on that measure will be more appropriate than the simultaneous review of recall and precision. For example, we use the average of these two metrics as a new benchmark and try to raise the average of these two metrics. For this purpose, the mean harmonic is the two recall and precision criteria, which is called the f1-Score. In this criterion, if the two values of recall and precision are small or even zero, the result will be small or zero.</p>
        <p>Hence, in Fig. 4, as we can see, the FalDroid method has fewer values for precision and recall compared to other methods. The high precision measurements in the FNN algorithm on API features from the Drebin dataset shows that this algorithm is able to identify the more benign samples compared to other methods correctly. On the opposite side, the ANN, WANN, and KMNN algorithms have higher recall values. It indicates that these methods accurately detect malware samples more than other methods, and the higher accuracy of these algorithms are confirmed the result. Concerning the permission features of the WANN algorithm, it has a higher recall value and can detect more malware samples. While on these features, the KMNN algorithm has a higher precision value and therefore can detect more benign samples. The third group of features is intents. In these features, PDME [17] and ANN algorithms have the highest recall value and can better identify malware samples. While in this feature group, the FNN algorithm has a higher precision value and can detect more benign samples correctly.</p>
        <p>In this part, we aim to present the different f1-Score for different feature types in different datasets. To do so, we rely our results on the equation ( 19) which shows the f1-Score formula. In this equation, we use two criteria: recall and precision. These criteria can be between zero and one. f1-Score is calculated based on multiple of these two criteria values. Thus, the final result tends to be smaller than each of these criteria values. If both of them are large numbers (approach to one), the final result will be near to one. With this explanation, the higher value for the f1-Score means that the algorithm could detect more malware and benign. In Fig. 5, we present the f1-Score for the API, permission, and intent features of different algorithm for different datasets. To be precise, the f1-Score for API and intent features of the Drebin dataset has the highest value and especially this rate is higher for the ANN algorithm. After that, the f1-Score for the FNN and WANN algorithms are the second and third biggest f1-Score which can present more malware and benign detection. Interestingly, in the three types of API, permission, and intent features the three ANN, WANN and KMNN algorithms have a higher f1-Score and this value for the PDME method [17] is placed in the next rank. FalDroid algorithm always has the lowest f1-Score. In Fig. 5, we notice that the f1-Score value increases with increasing number of features. The best results of the three datasets, are from the Contagio dataset. By examining Fig. 5, it can be concluded that f1-Score value for the API features has the highest rate.</p>
        <p>In this section, we compare algorithms based on Accuracy, FPR and, AUC metrics with both PDME [17] and FalDroid [19] algorithms which we present them in Tables 345. To be precise, in Table 3 for Drebin dataset, for each algorithm, increasing the number of features increases the accuracy and AUC values, and decreases the FPR values. Specifically, focusing on FNN algorithm by considering all API features and 10% API features, the value of Acc and AUC increase about 15% and 21%, respectively while the FPR value decreases exponentially approximately 95% and approaches to 0.004. Focusing on ANN algorithm, by considering all API features, the AUC value is 98.96%, FPR value is 0.004, and Acc value is 99.33% which is about 0.02% above algorithm. The WANN and KMNN algorithms also achieve FPR value about 0.004 for both methods; their AUC values are 98.90% and 98.84%, and their accuracy values are 99.31% and 99.28%, respectively. Focusing on the state-of-the art method like PDME algorithm [17], the highest accuracy is 99.21%, which is obtained for 90% of the API features with the FPR value of 0.005 and the AUC 98.96%, while the FalDroid algorithm [19] has the best accuracy by considering 60% of the API features, which is about 90.89% with AUC value 96.17% and with FPR of 0.099, and this method has the worst results (in all metrics presented in Table 3) than the proposed methods. By considering the results of the API features, the accuracy of FNN, ANN, WANN, KMNN, and PDME [17] methods are more than 99% with 80% of the features, and the FalDroid method [19] has no accuracy of more than 90.89%. As a result, focusing on Tables 345, we can understand five conclusions. First, by examining the permissions and intent features of the Drebin dataset in Table 3 and the API, permission, and intent features from the two Contagio and Genome datasets reported in Tables 4 and5, we realize that the described ML metric results for all algorithms are roughly the same. Second, focusing on our proposed algorithms, interestingly, the highest accuracy is usually achieved with the use of 70% or 80% or 90% of the features, and we do not have to choose all the features. Third, if the highest accuracy of different algorithms is ranked with respect to intent, permission, API features and their average ratings, the WANN algorithm has the highest accuracy value compared to others and the ANN, KMNN algorithms are in the next rank, and the PDME algorithm proposed in [17] is in third place. The FalDroid algorithm [19] has the least accuracy in all cases. Fourth, in all of the proposed methods, the accuracy of the API features are highest. Finally, the presented results of the Genome dataset are better than the Drebin and Contagio datasets.</p>
        <p>Some other remarks for Table 3 are in order. We understand that increasing the number of features increases the accuracy and AUC, and decreases the FPR. Increasing the amount of AUC means that algorithms with higher precision can separate malware and benign samples. It is also observed that the four proposed methods and the PDME algorithm [17] with accuracy more than 99% have a high AUC. This process has also been repeated for other features, namely permission and intent, as well as Contagio and Genome datasets. Hence, in best case, accuracy and AUC rates are not 100% and the FPR value is not even zero.</p>
        <p>In this section, we compare algorithms based on false positive rate (FPR) and true positive rate (TPR) metrics with both PDME [17] and FalDroid [19] algorithms which we present them in Figs. 678. Given that FPR and TPR are both numbers between zero and one, the area under the ROC obtained on the basis of these two measures, in the ideal case, represents the number one and in the random mode, the number is 0.5, and in most cases, the number between them will be closer to each other, indicating the greater accuracy of the proposed model in detecting malware samples. This area, which is shown by the ROC measure, is another yardstick for measuring the performance of a model. The TPR with a value close to one (i.e., 100%) means that the model is more precise, and the fact that it is close to zero means the poor performance of the model in specifying the samples. Fig. 6 plots the TPR of different algorithms over their FPR values. We can see that for Drebin with API features, over around 90% of our designed learning model can correctly find malware samples even just below 20% of FPR and the WANN and KMNN show the best rate among other methods. Focusing on the Drebin permission features, with over 90% TPR rate and lower FPR around 20% we can have the best performance of our ML model for FNN approach and this rate follow the same for other datasets. In all methods, 
            <rs type="software">FalDroid</rs> [19] algorithm does not have acceptable performance for high Low FPR and high TPR.
        </p>
        <p>In this study, we also investigate the application of popu- lar ML algorithms like SVM, Decision Tree, RF, and neural network. Using a random forest feature selection algorithm, we selected 300 important features. Then we implemented the above algorithms for 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of the selected features. Examination of the results, according to Table 6 shows that the proposed methods in this paper have higher accuracy than these algorithms.</p>
        <p>In this study, we aim to compare our proposed methods considering various popular ML algorithms, e.g., SVM, Decision Tree, RF, and neural network (NN). Using a RandomForestRegressor feature selection algorithm, we selected 300 important features. Then we implemented the above algorithms for 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100% of the selected features. Examination of the results, according to Table 7 shows that the proposed methods in this paper have higher accuracy than these algorithms. To be precise, random forest (RF) is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. We apply Neural Network (NN) as a multi-layer Perceptron classifier. This model optimizes the log-loss function using stochastic gra-dient descent. Additionally, we run the algorithm 200 epochs, and the learning rate is 0.01. It controls the step-size in updating the weights. The solver is '
            <rs type="software">adam</rs>' and hidden layer sizes=(5, 2). Focusing on SVM, we use the C-Support vector classification. The implementation is based on 
            <rs type="software">libsvm</rs>. The fit time scales at least juridically with the number of samples and maybe impractical beyond tens of thousands of samples. Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
        </p>
        <p>In the following, we explain the constraints on our similarity based detection algorithms and give some of our ongoing re- search directions. One of the limitations of our methods is that the Hamming distance measure calculates the distance between two programs based on the static features. The presented methods are similar to most of the other malware detection methods which can find two similar programs if they have similar features. For example, if we have two malware programs that have the same functionality with quite different features most of our methods are unable to find that these two programs are similar.</p>
        <p>Despite the effectiveness of the proposed methods, several issues are remaining to be solved. First, time complexity related to three of the proposed algorithms is o(n × m), and its rate for the KMNN algorithm is o(n 2 × m). However, a large amount of data in the smartphones that come with 
            <rs type="software">Android</rs> software rarely happens, but with a o(n × m) time complexity it needs a lot of time to run when the volume of data is enormous. Besides, as stated, static analysis usually separates and analyzes the various sources of the binary file without executing it. This analysis finds available features of malware at a decent speed. Dynamic analysis, also called behavioral analysis, is performed by observing malware behavior while running on the host system. Compared to static analysis, dynamic analysis is more effective as there is no need to disassemble the infected file to analyze it. In addition, dynamic analysis can detect known and unknown malware. So, we can design the ML model and detect malicious apps faster using static features compared with dynamic features. However, we can not recognize some unknown behavior malware in many cases. Therefore, we need to enhance our method and use static and dynamic features together as hybrid features in which improve our accuracy in selecting the various type of malware based on similarity-based strategies. Hybrid analysis gathers information about malware from static analysis and dynamic analysis. By using hybrid analysis, security researchers gain the benefits of both analyses, static and dynamic. Therefore, increasing the ability to detect malicious programs correctly. Third, to increase the efficiency of the similarity-based detection mechanism we can corporate deep learning, but it raises the time complexity of our methods. Hence, we should manage the convergence speed (i.e., detection speed) and accuracy rate which is another aspect of our future direction. Fourth, to provide robust detection algorithm, it is important to train our ML model using some adversarial examples and design new methods such as generative adversarial networks (GAN) to understand the simulated samples and detect compromised samples (i.e., malware samples in which change their behavior to fool the classifier). Finally, in this paper, we test our ML model on the three types of features (intent, permission, and API), some other features are existing in such datasets like URL-based 
            <rs type="software">Android</rs> services which are important for future ML model generation. Note Unlike two other used papers as a comparison, [17] and [22], which are accurately implemented, the 
            <rs type="software">FalDroid</rs> [19] method originally is suggested for detecting malware families. In this paper, we re-code the ideas mentioned in the FalDroid paper to detect malware. It is the main reason for the inefficiency of FalDroid compared to other methods.
        </p>
        <p>In this paper, we consider the Hamming distance as the benchmark for the similarity of samples and present four methods for detecting malware based on the nearest neighborhoods. To identify and analyze 
            <rs type="software">Android</rs> malware, and by using the binary features of Android applications, we provide machine learning-based methods to detect new malicious software with high precision and recall rates. Our approaches complement existing KNN-based solutions and validate our algorithms using three public datasets, namely the Drebin, Genome, and Contagio datasets, and applying API, intent and permission file types. Specifically, we demonstrate that the permission and intentbased method can classify malware from goodware inappropriate percent of cases. By considering API features of Drebin dataset, the accuracy of ANN, FNN, WANN, and KMNN methods are 99.31%, 99.33%, 99.31%, 99.28%, respectively, which are improved than PDME(99.19%) and 
            <rs type="software">FalDroid</rs>(88.07%). Similarly, with respect to the Permission features of the Drebin dataset, the accuracy of ANN, FNN WANN, and KMNN methods are 96.64%, 97.92%, 98.04%, 97.76%, respectively, while they are comparable with PDME with accuracy rate 97.76% and have better accuracy than 
            <rs type="software">FalDroid</rs> with accuracy rate 87.57%. Finally, about the Intents features of this dataset, the accuracy of ANN, FNN, WANN, and KMNN methods are 90.05%, 91.77%, 91.72%, 91.70%, respectively. In this type of features, the accuracy of PDME is 91.67% and FalDroid is 66.55%. Similar enhancements have been made about the different type of features in the Contagio and Genome datasets. The results showed that there is a superiority of proposed methods to the newest researches. The results of the proposed methods showed that the WANN algorithm has the highest accuracy in terms of permissions and intents and the ANN algorithm is in the next rank. Also, with API features, KMNN and ANN algorithms can provide higher accuracy. By comparing the accuracy obtained in the proposed methods for the different features of the three used datasets, we always consider that the accuracy of these methods is higher than the PDME and 
            <rs type="software">FalDroid</rs> methods.
        </p>
        <p>For the future works, it is possible to define other (distance) similarity measures between programs that consider some other features of the program instead of just using features frequencies. One idea is to utilize the correlation between features. Thus, using several other features of two applications such as the correlation between them may be useful to detect the similarity between two malware programs.</p>
        <p>D ← Remove 10% of the last samples in D 15: y ← Votes on the samples which are in D Add y to the Y 16: end for 17: return Y</p>
        <p>Mohammad Shojafar is supported by Marie Curie Global Fellowship (MSCA-IF-GF) funded by European Commission agreement grant number MSCA-IF-GF-839255 and Mauro Conti is supported by a Marie Curie Fellowship funded by the European Commission (agreement PCIG11-GA-2012-321980).</p>
        <p>In this section, we aim to give a simple example how our proposed similarity-based algorithms adopt to detect 
            <rs type="software">Android</rs> malware in a binary dataset.
        </p>
        <p>Definition 1. Suppose X is the sample that we want to predict its label. As an example, vector X can be defined as following: X = 0 0 0 1 0 0 0 1 0 0 Also, this vector can be written as follows. The numbers of this vector are the sample locations that have a value of 1. So, we have: X = 4 8 Definition 2. Suppose the training set namely S is used as follows. Given the fact that this matrix is sparse, it is possible to write the matrix only by storing the features of the value of 1. Hence, we have: 0</p>
        <p>Considering the presented definitions, in the following we examine our methods for the defined samples.</p>
        <p>Applying FNN Algorithm: In Table 8, the first nearest sample to X, which is selected by the FNN algorithm, is S 2 . Since the label of sample S 2 is 0, the value of 0 is assigned to the sample X.</p>
        <p>Applying ANN Algorithm: Focusing on ANN algorithm, we select all similar samples. In this example, S 2 , S 4 , S 7 , and S 8 have been selected according to Table 8 (i.e., see lower values; we select four vectors with value 2). By voting between labels of these samples, the value of 1 is assigned to the sample X.</p>
        <p>Applying WANN Algorithm: Focusing on WANN algorithm, we first count the number of features in the training samples to find the vector w (see Table 9 which includes the weight of each feature). Now, we compute the weight of each sample. The weight of each sample is the total weight of the features of that sample, which is 1. Given that the weight of sample X is equal to 6 and as we can see from the Table 10, samples S 2 , S 3 , S 5 , and S 7 are similar to X and by voting between them the label of sample X is will be 1.</p>
        <p>Applying KMNN Algorithm: Focusing on KMNN method, we first select the same sample X as the ANN method and then select S 2 , S 4 , S 7 and S 8 samples. Now, we create two clusters by placing similar samples in the same cluster. The similarity measure will be the distance between samples in each cluster.</p>
        <p>For this purpose, we determine the matrix of the intervals between these samples namely I as follows:</p>
        <p>Each entry of a matrix I represents the distance between the two samples, which is obtained by comparing peer to peer elements of corresponding vectors. Focusing on matrix I, the distance between the S 2 and S 8 samples is the smallest distance, so we can place them in a cluster. Similarly, the samples of S 4 and S 7 are near each other and we can place them in another cluster. Now, in each cluster, we select one of the samples which has a minimum distance from other samples as a cluster head (CH). In this example, since we have only two samples per cluster, we can consider each cluster sample as a CH. Hence, we define S 2 as the CH in the first cluster and S 4 as the CH in the second cluster. Then, we compute the total distance (i.e., d) of all the samples from two CHs as (See Table 11). In the last step, we should leave a k percentage of the most distant samples and vote among the other samples. In the proposed method, we consider k = 10, but for more clarity in these examples, we define k = 25, and we do not consider just the last sample. After that, we vote among the rest of the samples. As a result, the result of the voting obtains the value of 1 for the label of the sample X.</p>
    </text>
</tei>

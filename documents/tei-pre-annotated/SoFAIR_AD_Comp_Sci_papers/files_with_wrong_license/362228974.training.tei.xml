<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:20+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.</p>
        <p>During the past decade, Artificial Intelligence (AI), and by extension Machine Learning (ML), have seen an unprecedented rise in both industry and research. The progressive improvement of computer hardware associated with the need to process larger and larger amounts of data made these underestimated techniques shine under a new light. Reinforcement Learning (RL) focuses on learning how to map situations to actions, in order to maximize a numerical reward signal [1]. The learner is not told which actions to take, but instead must discover which actions are the most rewarding by trying them. Reinforcement learning addresses the problem of how agents should learn a policy that take actions to maximize the cumulative reward through interaction with the environment [2].During the past decade, Artificial Intelligence (AI), and by extension Machine Learning (ML), have seen an unprecedented rise in both industry and research. The progressive improvement of computer hardware associated with the need to process larger and larger amounts of data made these underestimated techniques shine under a new light. Reinforcement Learning (RL) focuses on learning how to map situations to actions, in order to maximize a numerical reward signal [1]. The learner is not told which actions to take, but instead must discover which actions are the most rewarding by trying them. Reinforcement learning addresses the problem of how agents should learn a policy that take actions to maximize the cumulative reward through interaction with the environment [2].</p>
        <p>Recent progress in Deep Learning (DL) for learning feature representations has significantly impacted RL, and the combination of both methods (known as deep RL) has led to remarkable results in a lot of areas. Typically, RL is used to solve optimization problems when the system has a very large number of states and has a complex stochastic structure. Notable examples include training agents to play Atari games based on raw pixels [3,4], board games [5,6], complex real-world robotics problems such as manipulation [7] or grasping [8] and other real-world applications such as resource management in computer clusters [9], network traffic signal control [10], chemical reactions optimization [11] or recommendation systems [12].Recent progress in Deep Learning (DL) for learning feature representations has significantly impacted RL, and the combination of both methods (known as deep RL) has led to remarkable results in a lot of areas. Typically, RL is used to solve optimization problems when the system has a very large number of states and has a complex stochastic structure. Notable examples include training agents to play Atari games based on raw pixels [3,4], board games [5,6], complex real-world robotics problems such as manipulation [7] or grasping [8] and other real-world applications such as resource management in computer clusters [9], network traffic signal control [10], chemical reactions optimization [11] or recommendation systems [12].</p>
        <p>The success of Deep RL could augur an imminent arrival in the industrial world. However, like many Machine Learning algorithms, RL algorithms suffer from a lack of explainability. This defect can be highly crippling as many promising RL applications (defense, finance, medicine, etc.) need a model that can explain its decisions and actions to human users [13] as a condition to their full acceptation by society. Furthermore, deep RL models are complex to debug for developers, as they rely on many factors: environment (in particular the design of the reward function), observations encoding, large DL models and the algorithm used to train the policy. Thus, an explainable model could aid fixing problems quicker and drastically speed up new development in RL methods. Those last two points are the main arguments in favor of the necessity of explainable reinforcement learning (XRL).The success of Deep RL could augur an imminent arrival in the industrial world. However, like many Machine Learning algorithms, RL algorithms suffer from a lack of explainability. This defect can be highly crippling as many promising RL applications (defense, finance, medicine, etc.) need a model that can explain its decisions and actions to human users [13] as a condition to their full acceptation by society. Furthermore, deep RL models are complex to debug for developers, as they rely on many factors: environment (in particular the design of the reward function), observations encoding, large DL models and the algorithm used to train the policy. Thus, an explainable model could aid fixing problems quicker and drastically speed up new development in RL methods. Those last two points are the main arguments in favor of the necessity of explainable reinforcement learning (XRL).</p>
        <p>While explainability starts being well developed for standard ML models and neural networks [14,15,16], the particular domain of RL has yet many intricacies to be better understood: both in terms of its functioning, and in terms of conveying the decisions of an RL model to different audiences. The difficulty lies in the very recent human-level performance of deep RL algorithms and by their complexity, normally parameterized with thousands if not millions of parameters [17]. The present work intends to provide a non-exhaustive state-of-the-art review on explainable reinforcement learning, highlighting the main methods that we envision most promising. In the following, we will briefly recall some important concepts in XAI.While explainability starts being well developed for standard ML models and neural networks [14,15,16], the particular domain of RL has yet many intricacies to be better understood: both in terms of its functioning, and in terms of conveying the decisions of an RL model to different audiences. The difficulty lies in the very recent human-level performance of deep RL algorithms and by their complexity, normally parameterized with thousands if not millions of parameters [17]. The present work intends to provide a non-exhaustive state-of-the-art review on explainable reinforcement learning, highlighting the main methods that we envision most promising. In the following, we will briefly recall some important concepts in XAI.</p>
        <p>Explaining a Machine Learning model may involve different goals: trustworthiness, causality, transferability, informativeness, fairness, confidence, accessibility, interactivity and privacy awareness. These goals have to be taken into account while explaining a model because the expected type of explanation may differ, depending on the pursued objective. For example, a saliency map explaining what is recognized as a dog on an input image does not tell us much about privacy awareness. In addition, each goal may be a dimension of interest, but only for a certain audience (the public to whom the explanations will be addressed). Indeed, the transferability of a model can be significative for a developer, since he/she can save time by training only one model for different tasks, while the user will not be impacted, if not aware, by this aspect.Explaining a Machine Learning model may involve different goals: trustworthiness, causality, transferability, informativeness, fairness, confidence, accessibility, interactivity and privacy awareness. These goals have to be taken into account while explaining a model because the expected type of explanation may differ, depending on the pursued objective. For example, a saliency map explaining what is recognized as a dog on an input image does not tell us much about privacy awareness. In addition, each goal may be a dimension of interest, but only for a certain audience (the public to whom the explanations will be addressed). Indeed, the transferability of a model can be significative for a developer, since he/she can save time by training only one model for different tasks, while the user will not be impacted, if not aware, by this aspect.</p>
        <p>The understandability of an ML model therefore depends on its transparency (its capacity to be understandable by itself) but also on human understanding. According to these considerations, it is essential to take into account the concept of audience, as the intelligibility and comprehensibility of a model is dependant on the goals and the cognitive skills of its users. Barredo Arrieta et al. [18] discuss these aspects with additional details.The understandability of an ML model therefore depends on its transparency (its capacity to be understandable by itself) but also on human understanding. According to these considerations, it is essential to take into account the concept of audience, as the intelligibility and comprehensibility of a model is dependant on the goals and the cognitive skills of its users. Barredo Arrieta et al. [18] discuss these aspects with additional details.</p>
        <p>The broad concept of evaluation is based on metrics aiming to compare how well a technique performs compared to another. In the case of model explainability, metrics should evaluate how well a model fits the definition of explainable and how well performs in a certain aspect of explainability.The broad concept of evaluation is based on metrics aiming to compare how well a technique performs compared to another. In the case of model explainability, metrics should evaluate how well a model fits the definition of explainable and how well performs in a certain aspect of explainability.</p>
        <p>Explanation evaluation in XAI has proven to be quite a challenging task. First because the concept of explainability in Machine Learning is not well or uniformly accepted by the community and there is not a clear definition and thus, not a clear consensus on which metrics to use. Secondly because an explanation is relative to a specific audience, which is sometimes difficult to deal with (in particular when this specific audience is composed of domain experts who can be hard to involve in a testing phase). Thirdly, because the quality of an explanation is always qualitative and subjective, since it depends on the audience, the pursued goal and even the human variability as two people can have a different level of understandability for the same explanation. That is why user studies are so popular to evaluate explanations as it makes possible to convert qualitative evaluations into quantitative ones, by asking questions on the accuracy and clarity of the explanation such as "Does this explanation allow you to understand why the model predicted that this image is a dog? Did the context helped the model?"... etc. Generally in XAI, there is only a single model to explain at a time; however, it is more complicated in XRL, as we generally want to explain a policy, or "Why the agent took action x in state s?". Doshi-Velez et al. [19] propose an attempt to formulate some approaches to evaluate XAI methods. The authors introduce three main levels to evaluate the quality of the explanations provided by an XAI method, as summarized in Table 2.Explanation evaluation in XAI has proven to be quite a challenging task. First because the concept of explainability in Machine Learning is not well or uniformly accepted by the community and there is not a clear definition and thus, not a clear consensus on which metrics to use. Secondly because an explanation is relative to a specific audience, which is sometimes difficult to deal with (in particular when this specific audience is composed of domain experts who can be hard to involve in a testing phase). Thirdly, because the quality of an explanation is always qualitative and subjective, since it depends on the audience, the pursued goal and even the human variability as two people can have a different level of understandability for the same explanation. That is why user studies are so popular to evaluate explanations as it makes possible to convert qualitative evaluations into quantitative ones, by asking questions on the accuracy and clarity of the explanation such as "Does this explanation allow you to understand why the model predicted that this image is a dog? Did the context helped the model?"... etc. Generally in XAI, there is only a single model to explain at a time; however, it is more complicated in XRL, as we generally want to explain a policy, or "Why the agent took action x in state s?". Doshi-Velez et al. [19] propose an attempt to formulate some approaches to evaluate XAI methods. The authors introduce three main levels to evaluate the quality of the explanations provided by an XAI method, as summarized in Table 2.</p>
        <p>Table 2: Three main levels for evaluating the explanations provided by an XAI method (inspired from explanations provided in Doshi-Velez et al. [19]).Table 2: Three main levels for evaluating the explanations provided by an XAI method (inspired from explanations provided in Doshi-Velez et al. [19]).</p>
        <p>Put the explanation into the product and have it tested by the end user.Put the explanation into the product and have it tested by the end user.</p>
        <p>Simplified taskSimplified task</p>
        <p>Carry out the application level experiments with laypersons as it makes the experiment cheaper and it is easier to find more testers. Function levelCarry out the application level experiments with laypersons as it makes the experiment cheaper and it is easier to find more testers. Function level</p>
        <p>No human required Uses a proxy to evaluate the explanation quality. Works best when the model class used has already been evaluated by someone else in human level. For instance, a proxy for decision trees can be the depth of the tree.No human required Uses a proxy to evaluate the explanation quality. Works best when the model class used has already been evaluated by someone else in human level. For instance, a proxy for decision trees can be the depth of the tree.</p>
        <p>A common example of evaluation of an application level or human level task is to evaluate the quality of the mental model built by the user after seeing the explanation(s). Mental models can be described as internal representations, built upon experiences, and which allow to mentally simulate how something works in the real world. Hoffman et al. [20] propose to evaluate mental models by 1. Asking post-task questions on the behavior of the agent (such as "How does it work?" or "What does it achieve?") and 2. Asking the participants to make predictions on the agent's next action. These evaluations are often done using Likert scales.A common example of evaluation of an application level or human level task is to evaluate the quality of the mental model built by the user after seeing the explanation(s). Mental models can be described as internal representations, built upon experiences, and which allow to mentally simulate how something works in the real world. Hoffman et al. [20] propose to evaluate mental models by 1. Asking post-task questions on the behavior of the agent (such as "How does it work?" or "What does it achieve?") and 2. Asking the participants to make predictions on the agent's next action. These evaluations are often done using Likert scales.</p>
        <p>In this survey, we first introduce XAI and its main challenges in Section 1. We then review the recent literature in XAI applied to reinforcement learning in Section 2. In Section 3, we discuss the different approaches employed in the literature. Finally, we conclude in Section 4 with some directions for future research.In this survey, we first introduce XAI and its main challenges in Section 1. We then review the recent literature in XAI applied to reinforcement learning in Section 2. In Section 3, we discuss the different approaches employed in the literature. Finally, we conclude in Section 4 with some directions for future research.</p>
        <p>The key contributions of this paper are as follows:The key contributions of this paper are as follows:</p>
        <p>• A recent state of the art for Explainable Reinforcement Learning.• A recent state of the art for Explainable Reinforcement Learning.</p>
        <p>• An attempt to categorize XRL methods.• An attempt to categorize XRL methods.</p>
        <p>• Discussion and future work recommendations (provided at the end, in Sections 3 and 4).• Discussion and future work recommendations (provided at the end, in Sections 3 and 4).</p>
        <p>We hope that this work will give more visibility to existing XRL methods, while helping developing new ideas in this field.We hope that this work will give more visibility to existing XRL methods, while helping developing new ideas in this field.</p>
        <p>We reviewed the state of the art on XRL and summarized it in Table 3. This table presents, for each paper, the task(s) for which an explanation is provided, the employed RL algorithms (whose Algorithms glossary can be found in the 6), and the provided type of explanations, i.e.: based on images, diagrams (graphical components such as bar charts, plots or graphs), or text. We also present the level of the provided explanation (local if it explains only predictions, global if it explains the whole model), and the audience concerned by the explanation, as discussed in Section 1.1.We reviewed the state of the art on XRL and summarized it in Table 3. This table presents, for each paper, the task(s) for which an explanation is provided, the employed RL algorithms (whose Algorithms glossary can be found in the 6), and the provided type of explanations, i.e.: based on images, diagrams (graphical components such as bar charts, plots or graphs), or text. We also present the level of the provided explanation (local if it explains only predictions, global if it explains the whole model), and the audience concerned by the explanation, as discussed in Section 1.1.</p>
        <p>In Table 3 we summarized the literature focusing on explainable fundamental RL algorithms. However, we also reviewed articles about state of the art XAI techniques that can be used in the context of current RL which we did not include in Table 3. Next, we will describe the main ideas provided by these papers which can help bring explainability in RL. It is possible to classify all recent studies in two main categories: transparent methods and Post-Hoc explainability according to the XAI taxonomies in Barredo Arrieta et al. [18]. On the one hand, inherently transparent algorithms include by definition every algorithm which is understandable by itself, such as a decision-trees. On the other hand, Post-Hoc explainability includes all methods that provide explanations of an RL algorithm after its training, such as SHAP (SHapley Additive exPlanations) [15] or LIME [14] for standard ML models. Reviewed papers are referenced by type of explanation in Figure 2.In Table 3 we summarized the literature focusing on explainable fundamental RL algorithms. However, we also reviewed articles about state of the art XAI techniques that can be used in the context of current RL which we did not include in Table 3. Next, we will describe the main ideas provided by these papers which can help bring explainability in RL. It is possible to classify all recent studies in two main categories: transparent methods and Post-Hoc explainability according to the XAI taxonomies in Barredo Arrieta et al. [18]. On the one hand, inherently transparent algorithms include by definition every algorithm which is understandable by itself, such as a decision-trees. On the other hand, Post-Hoc explainability includes all methods that provide explanations of an RL algorithm after its training, such as SHAP (SHapley Additive exPlanations) [15] or LIME [14] for standard ML models. Reviewed papers are referenced by type of explanation in Figure 2.</p>
        <p>Saliency maps [31] Interaction data [32] [33] Transparent algorithms Hierarchical learning [28] [29] [27] [30] Simultaneous leaning [24] [25] [26] [36] Representation learningSaliency maps [31] Interaction data [32] [33] Transparent algorithms Hierarchical learning [28] [29] [27] [30] Simultaneous leaning [24] [25] [26] [36] Representation learning</p>
        <p>Figure 1: Taxonomy of the reviewed literature identified for bringing explainability to RL models. References in orange, purple, and light blue correspond to XAI techniques using images, text or diagrams, respectively.Figure 1: Taxonomy of the reviewed literature identified for bringing explainability to RL models. References in orange, purple, and light blue correspond to XAI techniques using images, text or diagrams, respectively.</p>
        <p>Transparent algorithms are well known and used in standard Machine Learning (e.g., linear regression, decision trees or rule-based systems). Their strength lie in the fact that they are designed to have a transparent architecture that makes them explainable by themselves, without the need of any external processing. However, it is quite different for RL, as standard DRL algorithms (e.g., DQN, PPO, DDPG, A2C...) are not transparent by nature. In addition, the large majority of studies related to transparency in XRL chose to build algorithms targeting only a specific task. Nonetheless, most of the time and contrary to standard Machine Learning models, transparent RL algorithms can achieve state of the art performance in these specific tasks [26,22,21].Transparent algorithms are well known and used in standard Machine Learning (e.g., linear regression, decision trees or rule-based systems). Their strength lie in the fact that they are designed to have a transparent architecture that makes them explainable by themselves, without the need of any external processing. However, it is quite different for RL, as standard DRL algorithms (e.g., DQN, PPO, DDPG, A2C...) are not transparent by nature. In addition, the large majority of studies related to transparency in XRL chose to build algorithms targeting only a specific task. Nonetheless, most of the time and contrary to standard Machine Learning models, transparent RL algorithms can achieve state of the art performance in these specific tasks [26,22,21].</p>
        <p>Representation learning algorithms focuses on learning abstract features that characterize data, in order to make it easier to extract useful information when building predictors [40,41]. These learned features have the advantage of having low dimensionality, which generally improves training speed and generalization of Deep Learning models [42,40,23].Representation learning algorithms focuses on learning abstract features that characterize data, in order to make it easier to extract useful information when building predictors [40,41]. These learned features have the advantage of having low dimensionality, which generally improves training speed and generalization of Deep Learning models [42,40,23].</p>
        <p>In the context of RL, learning representations of states, actions or policy can be useful to explain a RL algorithm, as these representations can bring some clues on the functioning of the algorithm. Indeed, State Representation Learning (SRL) [40] is a particular type of representation learning that aims at building a low-dimensional and meaningful representation of a state space, by processing high-dimensional raw observation data (e.g., learn a position (x, y) from raw image pixels). This enables to capture the variations in the environment influenced by the agent's actions and thus, extrapolate explanations. SRL can be especially useful in RL for robotics and control [37,23,43,44,45], and can help to understand how the agent interprets the observations and what is relevant to learn to act, i.e., actionable or controllable features [42]. Indeed, the dimensionality reduction induced by SRL, coupled with the link to the control and possible disentanglement of variation factors, could be highly beneficial to improve our understanding capacity of the decisions made by RL algorithms using a state representation method [40]. For example, SRL can be used to split the state representation [23] according to the different training objectives to be optimized before learning a policy. This allows to allocate room for encoding each necessary objective within the embedding state to be learned (in that case, reward prediction, a reconstruction objective and an inverse model). In this context, tools such as 
            <rs type="software">S-RL Toolbox</rs> [37] allow sampling from the embedding state space (learned through SRL) to allow a visual interpretation of the model's internal state, and pairing it with its associated input observation. Comprehensibility is thus enhanced, more easily observing if smoothness is preserved in the state space, as well as whether other invariants related to learning specific control task are guaranteed.
        </p>
        <p>There are several approaches employed for SRL: reconstructing the observations using autoencoders [46,47], training a forward model to predict next state [48,39], teach to an inverse model how to predict actions from previous state(s) [49,39] or using prior knowledge to constrain the state space [50,42].There are several approaches employed for SRL: reconstructing the observations using autoencoders [46,47], training a forward model to predict next state [48,39], teach to an inverse model how to predict actions from previous state(s) [49,39] or using prior knowledge to constrain the state space [50,42].</p>
        <p>Along the same lines, learning disentangled representations [51,52,53,54] is another interesting idea used for unsupervised learning, which decomposes (or disentangles) each feature into narrowly defined variables and encodes them as separate low-dimensional features (generally using a Variational Autoencoders [55]). It is also possible to make use of this concept, as well as lifelong learning to learn more interpretable representations on unsupervised classification tasks. In addition, one could argue that learning through life would allow compacting and updating old knowledge with new one while preventing catastrophic forgetting [56]. Thus, this is a key concept that could lead to more versatile RL agents, being able to learn new tasks without forgetting the previous ones. Information Maximizing Generative Adversarial Networks (InfoGAN) [57] is another model based on the principles of learning disentangled representations. The noise vector used in traditional GANs is decomposed into two parts: z: incompressible noise; and c: the latent code used to target the salient semantic features of the data distribution. The main idea is to feed z and c to the generator G, to maximize the mutual information between c and G(z, c), in order to assure that the information contained in c is preserved during the generation process. As a result, the InfoGAN model is able to create an interpretable representation via the latent code c (i.e., the values changing according to shape and features of the input data).Along the same lines, learning disentangled representations [51,52,53,54] is another interesting idea used for unsupervised learning, which decomposes (or disentangles) each feature into narrowly defined variables and encodes them as separate low-dimensional features (generally using a Variational Autoencoders [55]). It is also possible to make use of this concept, as well as lifelong learning to learn more interpretable representations on unsupervised classification tasks. In addition, one could argue that learning through life would allow compacting and updating old knowledge with new one while preventing catastrophic forgetting [56]. Thus, this is a key concept that could lead to more versatile RL agents, being able to learn new tasks without forgetting the previous ones. Information Maximizing Generative Adversarial Networks (InfoGAN) [57] is another model based on the principles of learning disentangled representations. The noise vector used in traditional GANs is decomposed into two parts: z: incompressible noise; and c: the latent code used to target the salient semantic features of the data distribution. The main idea is to feed z and c to the generator G, to maximize the mutual information between c and G(z, c), in order to assure that the information contained in c is preserved during the generation process. As a result, the InfoGAN model is able to create an interpretable representation via the latent code c (i.e., the values changing according to shape and features of the input data).</p>
        <p>Some work has been done to learn representations by combining symbolic AI with deep RL in order to facilitate the use of background knowledge, the exploitation of learnt knowledge, and to improve generalization [38,58,59,60]. Consequently, it also improves the explainability of the algorithms, while preserving state-ofthe-art performance.Some work has been done to learn representations by combining symbolic AI with deep RL in order to facilitate the use of background knowledge, the exploitation of learnt knowledge, and to improve generalization [38,58,59,60]. Consequently, it also improves the explainability of the algorithms, while preserving state-ofthe-art performance.</p>
        <p>Zambaldi et al. [21] propose making use of Inductive Logic Programming and self-attention to represent states, actions and policies using first order logic, using a mechanism similar to graph neural networks and more generally, message passing computations [61,62,63,64]. In these kind of models entity-entity relations are explicitly computed when considering the messages passed between connected nodes of the graph as shown in Fig. 2. Self-attention is used here as a method to compute interactions between these different entities (i.e. relevant pixels in a RGB image for the example from [21]), and thus perform non-local pairwise relational computations. This technique allows an expert to visualize the agent's attention weights associated to its available actions and interpret how to improve the understanding of its strategy.Zambaldi et al. [21] propose making use of Inductive Logic Programming and self-attention to represent states, actions and policies using first order logic, using a mechanism similar to graph neural networks and more generally, message passing computations [61,62,63,64]. In these kind of models entity-entity relations are explicitly computed when considering the messages passed between connected nodes of the graph as shown in Fig. 2. Self-attention is used here as a method to compute interactions between these different entities (i.e. relevant pixels in a RGB image for the example from [21]), and thus perform non-local pairwise relational computations. This technique allows an expert to visualize the agent's attention weights associated to its available actions and interpret how to improve the understanding of its strategy.</p>
        <p>Another work that aims to incorporate common sense to the agent, in terms of symbolic abstraction to represent the problem, is in [22]. This method subdivides the world state representation into many sub-states, with a degree of associated importance based on how far the object is from the agent. This helps understand the relevance of the actions taken by the agent by determining which sub-states were chosen.Another work that aims to incorporate common sense to the agent, in terms of symbolic abstraction to represent the problem, is in [22]. This method subdivides the world state representation into many sub-states, with a degree of associated importance based on how far the object is from the agent. This helps understand the relevance of the actions taken by the agent by determining which sub-states were chosen.</p>
        <p>While standard DRL algorithms struggle to provide explanations, those can be tweaked to learn simultaneously both policy and explanation. Thus, explanations become a learned component of the model. These methods are recommended on specific problems where it is possible to introduce knowledge, such classifying rewards by types, adding relationships between states, etc... Thus, tweaking the algorithm to introduce some task knowledge and to learn explanations generally also improves performance. A general notion is that the knowledge gained from the auxiliary task objective must be useful for downstream tasks. In this direction, Juozapaitis et al. [24] introduced reward decomposition, whose main principle is to decompose the reward function into a sum of meaningful reward types. Authors used reward decomposition to improve performance on Cliffworld and Starcraft II, where each action can be classified according to its type. This method consists of using a custom decomposed reward DQN by defining a vector-valued reward function, where each component is the reward for a certain type so that actions can be compared in terms of trade-offs among the types. In the same way, the Q-function is also vector valued and each component gives action values that account for only a reward type. The sum of each of those vector-valued functions gives the overall Q or reward function. Learning multiple Q-functions, one for each type of reward, allows the model to learn the best policy while also learning the explanations (i.e. the type of reward that the agent wanted to maximize by his action, illustrated on Fig. 4). They introduce the concept of Reward Difference Explanation (RDX, in Fig. 3) which enables to understand the reasons why an action has an advantage (or disadvantage) over another. They also define Minimal Sufficient Explanations (MSX, See Fig. 4), in order to help humans identify a small set of the most important reasons why the agent choose specific actions over another. MSX+ and MSX-are sets of critical positive and negative reasons (respectively) for the actions preferred by the agent. While reward decompositions help to understand the agent choice preferences between several actions, minimal sufficient explanations are used to help selecting the most important reward decompositions. Other works that facilitate the explainability of RL models by using reward-based losses for more interpretable RL are in [65,49,39]. fire-main-engine) for HRA in Lunar Lander before a crash. The RDX shows that noop is preferred to avoid penalties such as fuel cost. Reproduced with permission of Zoe Juozapaitis [24].While standard DRL algorithms struggle to provide explanations, those can be tweaked to learn simultaneously both policy and explanation. Thus, explanations become a learned component of the model. These methods are recommended on specific problems where it is possible to introduce knowledge, such classifying rewards by types, adding relationships between states, etc... Thus, tweaking the algorithm to introduce some task knowledge and to learn explanations generally also improves performance. A general notion is that the knowledge gained from the auxiliary task objective must be useful for downstream tasks. In this direction, Juozapaitis et al. [24] introduced reward decomposition, whose main principle is to decompose the reward function into a sum of meaningful reward types. Authors used reward decomposition to improve performance on Cliffworld and Starcraft II, where each action can be classified according to its type. This method consists of using a custom decomposed reward DQN by defining a vector-valued reward function, where each component is the reward for a certain type so that actions can be compared in terms of trade-offs among the types. In the same way, the Q-function is also vector valued and each component gives action values that account for only a reward type. The sum of each of those vector-valued functions gives the overall Q or reward function. Learning multiple Q-functions, one for each type of reward, allows the model to learn the best policy while also learning the explanations (i.e. the type of reward that the agent wanted to maximize by his action, illustrated on Fig. 4). They introduce the concept of Reward Difference Explanation (RDX, in Fig. 3) which enables to understand the reasons why an action has an advantage (or disadvantage) over another. They also define Minimal Sufficient Explanations (MSX, See Fig. 4), in order to help humans identify a small set of the most important reasons why the agent choose specific actions over another. MSX+ and MSX-are sets of critical positive and negative reasons (respectively) for the actions preferred by the agent. While reward decompositions help to understand the agent choice preferences between several actions, minimal sufficient explanations are used to help selecting the most important reward decompositions. Other works that facilitate the explainability of RL models by using reward-based losses for more interpretable RL are in [65,49,39]. fire-main-engine) for HRA in Lunar Lander before a crash. The RDX shows that noop is preferred to avoid penalties such as fuel cost. Reproduced with permission of Zoe Juozapaitis [24].</p>
        <p>In the same vein, Madumal et al. [25] use the way humans understand and represent knowledge through causal relationships and introduce an action influence model : a causal model which can explain the behaviour agents using causal explanations. Structural causal models [66] represent the world using random variables, some of which might have causal relationships, which can be described thanks to a set of structural equations.In the same vein, Madumal et al. [25] use the way humans understand and represent knowledge through causal relationships and introduce an action influence model : a causal model which can explain the behaviour agents using causal explanations. Structural causal models [66] represent the world using random variables, some of which might have causal relationships, which can be described thanks to a set of structural equations.</p>
        <p>In this work, structural causal models are extended to include actions as part of the causal relationships. An action influence model is a tuple represented by the state-actions ensemble and the corresponding set of structural equations. The whole process is divided into 3 phases:In this work, structural causal models are extended to include actions as part of the causal relationships. An action influence model is a tuple represented by the state-actions ensemble and the corresponding set of structural equations. The whole process is divided into 3 phases:</p>
        <p>• Defining the qualitative causal relationships of variables as an action influence model.• Defining the qualitative causal relationships of variables as an action influence model.</p>
        <p>• Learning the structural equations (as multivariate regression models during the training phase of the agent).• Learning the structural equations (as multivariate regression models during the training phase of the agent).</p>
        <p>• Generating explanations, called explanans, by traversing the action influence graph (see Figure 5) from the root to the leaf reward node.• Generating explanations, called explanans, by traversing the action influence graph (see Figure 5) from the root to the leaf reward node.</p>
        <p>This kind of models allow encoding cause-effect relations between events (actions and states) as shown by the graph featured in Figure 5. Thus, they can be used to generate explanations of the agent behaviour ("why" and "why not" questions), based on knowledge about how actions influence the environment. Their method was evaluated through a user study showing that, compared to video game playing without any explanations and relevant variable explanations, this model performs significantly better on 1) task prediction and 2) explanation goodness. However, trust was not shown to be significantly improved. The causal chain (explanation) for action As is depicted in bold arrows and the extracted explanan (subset of causes given the explanation) is shown as darkened nodes. The counterfactual action (why not A b ) explanan is shown as grayed node (B). Here, As is the explanandum, the action for which the user needs explanation. Thus, we can answer the question "Why not build_barrack (A b )?". Indeed, the explanation provided by the graph in bold arrows is: "Because it is more desirable to do action build_supply_depot (As) to have more Supply Depots as the goal is to have more Destroyed Units (Du) and Destroyed Buildings (D b )". Reproduced with permission of [25].This kind of models allow encoding cause-effect relations between events (actions and states) as shown by the graph featured in Figure 5. Thus, they can be used to generate explanations of the agent behaviour ("why" and "why not" questions), based on knowledge about how actions influence the environment. Their method was evaluated through a user study showing that, compared to video game playing without any explanations and relevant variable explanations, this model performs significantly better on 1) task prediction and 2) explanation goodness. However, trust was not shown to be significantly improved. The causal chain (explanation) for action As is depicted in bold arrows and the extracted explanan (subset of causes given the explanation) is shown as darkened nodes. The counterfactual action (why not A b ) explanan is shown as grayed node (B). Here, As is the explanandum, the action for which the user needs explanation. Thus, we can answer the question "Why not build_barrack (A b )?". Indeed, the explanation provided by the graph in bold arrows is: "Because it is more desirable to do action build_supply_depot (As) to have more Supply Depots as the goal is to have more Destroyed Units (Du) and Destroyed Buildings (D b )". Reproduced with permission of [25].</p>
        <p>Authors of [36] also learn explanations along with the model policy on pedestrians collision avoidance tasks. In this paper, an ensemble of LSTM networks was trained using Monte Carlo Dropout [67] and bootstrapping [68] to estimate collision probabilities and thus predict uncertainty estimates to detect novel observations. The magnitude of those uncertainty estimates was shown to reveal novel obstacles in a variety of scenarios, indicating that the model knows what it does not know. The result is a collision avoidance policy that can measure the novelty of an observation (via model uncertainty) and cautiously avoids pedestrians that exhibit unseen behavior. Measures of model uncertainty can also be used to identify unseen data during training or testing. Policies during simulation demonstrated to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline. This work also responds to the problem of safe reinforcement learning [69], whose goal is to ensure reasonable system performance and/or respect safety constraints also at the deployment phase. Some work has also been made to explain multiagent RL. Wang et al. [26] developed an approach named Shapley Q-values Deep Deterministic Policy Gradient (SQDDPG) to solve global reward games in a multiagent context based on Shapley values and DDPG. The proposed approach relies on distributing the global reward more efficiently across all agents. They show that integrating Shapley values into DDPG enables to share the global reward between all agents according to their contributions: the more the agent contributes, the more reward it will get. This contrasts to the classical shared reward approach, which could cause inefficient learning by assigning rewards to an agent who contributed poorly. The experiments showed that SQDDPG presents faster convergence rate and fairer credit assignment in comparison with other algorithms (i.e. IA2C, IDDPG, COMA and MADDPG). This method allows to plot credit assignment to each agent, which can explain how the global reward is divided during training and what agent contributed the most to obtain the global reward.Authors of [36] also learn explanations along with the model policy on pedestrians collision avoidance tasks. In this paper, an ensemble of LSTM networks was trained using Monte Carlo Dropout [67] and bootstrapping [68] to estimate collision probabilities and thus predict uncertainty estimates to detect novel observations. The magnitude of those uncertainty estimates was shown to reveal novel obstacles in a variety of scenarios, indicating that the model knows what it does not know. The result is a collision avoidance policy that can measure the novelty of an observation (via model uncertainty) and cautiously avoids pedestrians that exhibit unseen behavior. Measures of model uncertainty can also be used to identify unseen data during training or testing. Policies during simulation demonstrated to be more robust to novel observations and take safer actions than an uncertainty-unaware baseline. This work also responds to the problem of safe reinforcement learning [69], whose goal is to ensure reasonable system performance and/or respect safety constraints also at the deployment phase. Some work has also been made to explain multiagent RL. Wang et al. [26] developed an approach named Shapley Q-values Deep Deterministic Policy Gradient (SQDDPG) to solve global reward games in a multiagent context based on Shapley values and DDPG. The proposed approach relies on distributing the global reward more efficiently across all agents. They show that integrating Shapley values into DDPG enables to share the global reward between all agents according to their contributions: the more the agent contributes, the more reward it will get. This contrasts to the classical shared reward approach, which could cause inefficient learning by assigning rewards to an agent who contributed poorly. The experiments showed that SQDDPG presents faster convergence rate and fairer credit assignment in comparison with other algorithms (i.e. IA2C, IDDPG, COMA and MADDPG). This method allows to plot credit assignment to each agent, which can explain how the global reward is divided during training and what agent contributed the most to obtain the global reward.</p>
        <p>Methods based on Hierarchical RL [71] and sub-task decomposition [72] consist of a high level agent dividing the main goal into sub-goals for a low-level agent, which follows them one by one to perform the high-level task. By learning what sub-goals are optimal for the low-level agent, the high-level agent forms a representation of the environment that is interpretable by humans. Often, Hindsight Experience Replay (HER) [73] is used in order to ignore whether or not goals and sub-goals have been reached during an episode and to extract as much information as possible from past experience.Methods based on Hierarchical RL [71] and sub-task decomposition [72] consist of a high level agent dividing the main goal into sub-goals for a low-level agent, which follows them one by one to perform the high-level task. By learning what sub-goals are optimal for the low-level agent, the high-level agent forms a representation of the environment that is interpretable by humans. Often, Hindsight Experience Replay (HER) [73] is used in order to ignore whether or not goals and sub-goals have been reached during an episode and to extract as much information as possible from past experience.</p>
        <p>Beyret et al. [27] used this kind of methods along with HER for robotic manipulation (grasping and moving an item). The high level agent learns which sub-goals can make the low level agent reach the main goal while the low level agent learns to maximise the rewards for these sub-goals. The high-level agent provides a representation of the learned environment and the Q-values associated, which can be represented as heat maps as shown in Fig. 7.Beyret et al. [27] used this kind of methods along with HER for robotic manipulation (grasping and moving an item). The high level agent learns which sub-goals can make the low level agent reach the main goal while the low level agent learns to maximise the rewards for these sub-goals. The high-level agent provides a representation of the learned environment and the Q-values associated, which can be represented as heat maps as shown in Fig. 7.</p>
        <p>Based on the same ideas, Cideron et al. [28] proposed Textual Hierarchical Experience Replay (THER) which extends the HER explanation to a natural language setting, allowing to learn from past experiences and to map goals to trajectories without the need of an external expert. The mapping function labels unsuccessful trajectories by automatically predicting a substitute goal. THER is composed of two models: the instruction generator which outputs a language encoding of the final state, and an agent model which picks an action given the last observations and the language-encoded goal. The model learns to encode goals and states via natural language, and thus can be interpreted by a human operator (Fig. 8).Based on the same ideas, Cideron et al. [28] proposed Textual Hierarchical Experience Replay (THER) which extends the HER explanation to a natural language setting, allowing to learn from past experiences and to map goals to trajectories without the need of an external expert. The mapping function labels unsuccessful trajectories by automatically predicting a substitute goal. THER is composed of two models: the instruction generator which outputs a language encoding of the final state, and an agent model which picks an action given the last observations and the language-encoded goal. The model learns to encode goals and states via natural language, and thus can be interpreted by a human operator (Fig. 8).</p>
        <p>Another interesting work finds inspiration in human behaviour to improve generalization on a room navigation task, just like common sense and semantic understanding are used by humans to navigate unseen environments [29]. The entire model is composed of three parts: 1) a semantically grounded navigator used to predict the next action. The heat maps show the value of the different areas (highest in yellow) for the high-level agent to predict a sub-goal. Black squares represent the position of the cube, the red circle is the end goal. Thus, the low-level agent will have a succession of sub-goals (e.g. multiple actions that the robotic arm must perform such as moving or opening its pinch) that will ultimately lead to the achievement of the high-level goal (i.e. grasping the red ball). Reproduced with permission of [27].Another interesting work finds inspiration in human behaviour to improve generalization on a room navigation task, just like common sense and semantic understanding are used by humans to navigate unseen environments [29]. The entire model is composed of three parts: 1) a semantically grounded navigator used to predict the next action. The heat maps show the value of the different areas (highest in yellow) for the high-level agent to predict a sub-goal. Black squares represent the position of the cube, the red circle is the end goal. Thus, the low-level agent will have a succession of sub-goals (e.g. multiple actions that the robotic arm must perform such as moving or opening its pinch) that will ultimately lead to the achievement of the high-level goal (i.e. grasping the red ball). Reproduced with permission of [27].</p>
        <p>Figure 8: 
            <rs type="software">MiniGrid</rs> environment [74], where the agent is instructed through a textual string to pick up an object and place it next to another one. The model learns to represent the achieved goal (e.g. "Pick the purple ball") via language. As this achieved goal differs from the initial goal ("Pick the red ball"), the goal mapper relabels the episode, and both trajectories are appended to the replay buffer. Reproduced with permission of M. Seurin [28]. current one. 3) the semantic grounding module used to recognize rooms; it allows the detection of the current room and incorporates semantic understanding by generating questions about what the agent saw ("Did you see a bathroom?"). Self-supervision is then used for fine tuning on unseen environment. The explainability can be brought from the outputs of all parts of the entire model. An original idea proposed by Tasse et al. [30] consists of making an agent learn basic tasks and then allow it to perform new ones by composing the tasks previously learned in a boolean formula (i.e., with conjunctions, disjunctions and negations). The main strength of this method is that the agent is able to perform new tasks without the necessity of a learning phase. From an XRL point of view, the explainability comes from the fact that the agent is able to express its actions as boolean formulas, which are easily readable by humans.
        </p>
        <p>Post-Hoc explainability refers to explainability methods that rely on an analysis done after the RL algorithm finishes its training and execution. In other terms, it is a way of "enhancing" the considered RL algorithm from a black box to something that is somewhat explainable. Most Post-Hoc methods encountered were used in a perception context, i.e., when the data manipulated by the RL algorithm consisted of visual input such as images.Post-Hoc explainability refers to explainability methods that rely on an analysis done after the RL algorithm finishes its training and execution. In other terms, it is a way of "enhancing" the considered RL algorithm from a black box to something that is somewhat explainable. Most Post-Hoc methods encountered were used in a perception context, i.e., when the data manipulated by the RL algorithm consisted of visual input such as images.</p>
        <p>When an RL algorithm is learning from images, it can be useful to know which elements of those images hold the most relevant information (i.e., the salient elements). These elements can be detected using saliency methods that produce saliency maps [16,75]. In most cases, a saliency or heat map consists of a filter applied to an image that will highlight the areas salient for the agent.When an RL algorithm is learning from images, it can be useful to know which elements of those images hold the most relevant information (i.e., the salient elements). These elements can be detected using saliency methods that produce saliency maps [16,75]. In most cases, a saliency or heat map consists of a filter applied to an image that will highlight the areas salient for the agent.</p>
        <p>A major advantage of saliency maps is that it can produce elements that are easily interpretable by humans, even non-experts. Of course, the interpreting difficulty of a saliency map greatly depends on the saliency method used to compute that map and other parameters such as the color scheme or the highlighting technique. A disadvantage is that they are very sensitive to different input variations, and schemes to debug such visual explanation may not be straightforward [76].A major advantage of saliency maps is that it can produce elements that are easily interpretable by humans, even non-experts. Of course, the interpreting difficulty of a saliency map greatly depends on the saliency method used to compute that map and other parameters such as the color scheme or the highlighting technique. A disadvantage is that they are very sensitive to different input variations, and schemes to debug such visual explanation may not be straightforward [76].</p>
        <p>A very interesting example [31], introduces a new perturbation-based saliency computation method that produces crisp and easily interpretable saliency maps for RL agents playing OpenAI Gym environment Atari 2600 games with Asynchronous Actor Critic [77]. The main idea is to apply a perturbation on the considered image that will remove information from a specific pixel without adding new information (by generating an interpolation from a Gaussian blur of the same image). Indeed, this perturbation can be interpreted as adding spatial uncertainty to the region around its point of application. This spatial uncertainty can help understand how removing information in a specific area of the input image affects the agent's policy, and is quantified with a saliency metric S. The saliency map is then produced by computing S(i, j) for every pixel (i, j) of the input image, leading to images such as those in Fig. 9.A very interesting example [31], introduces a new perturbation-based saliency computation method that produces crisp and easily interpretable saliency maps for RL agents playing OpenAI Gym environment Atari 2600 games with Asynchronous Actor Critic [77]. The main idea is to apply a perturbation on the considered image that will remove information from a specific pixel without adding new information (by generating an interpolation from a Gaussian blur of the same image). Indeed, this perturbation can be interpreted as adding spatial uncertainty to the region around its point of application. This spatial uncertainty can help understand how removing information in a specific area of the input image affects the agent's policy, and is quantified with a saliency metric S. The saliency map is then produced by computing S(i, j) for every pixel (i, j) of the input image, leading to images such as those in Fig. 9.</p>
        <p>However, saliency methods are not a perfect solution in every situation, as pointed out in [79,80]. They need to respect a certain number of rules, such as implementation invariance or input invariance in order to be reliable, especially when it comes to their relation with either the model or the input data.However, saliency methods are not a perfect solution in every situation, as pointed out in [79,80]. They need to respect a certain number of rules, such as implementation invariance or input invariance in order to be reliable, especially when it comes to their relation with either the model or the input data.</p>
        <p>In a more generic way, the behaviour of an agent can be explained by gathering data from its interaction with the environment while running, and analysing it in order to extract key information. For instance, Caselles-Dupré et al demonstrate that symmetry-based disentangled representation learning requires interaction and not only static perception [81].In a more generic way, the behaviour of an agent can be explained by gathering data from its interaction with the environment while running, and analysing it in order to extract key information. For instance, Caselles-Dupré et al demonstrate that symmetry-based disentangled representation learning requires interaction and not only static perception [81].</p>
        <p>This idea is exploited by Sequeira et al. [33] where interaction is the core basis upon which their Interestingness Framework is built. This framework relies on introspection, conducted by the autonomous RL agents: the agent extracts interestingness elements that denote meaningful interactions from their history of [78] to the authors' perturbation-based approach (right) in an actor-critic model. Red indicates saliency for the critic; blue is saliency for the actor. Reproduced with permission of Sam Greydanus [31].This idea is exploited by Sequeira et al. [33] where interaction is the core basis upon which their Interestingness Framework is built. This framework relies on introspection, conducted by the autonomous RL agents: the agent extracts interestingness elements that denote meaningful interactions from their history of [78] to the authors' perturbation-based approach (right) in an actor-critic model. Red indicates saliency for the critic; blue is saliency for the actor. Reproduced with permission of Sam Greydanus [31].</p>
        <p>interaction with the environment. This is done using interaction data collected by the agent that is analysed using statistical methods organized in a three-level introspection analysis: level 0: Environment analysis, level 1: Interaction analysis; level 3: Meta-analysis. From these interestingness elements, it is then possible to generate visual explanations (in the form of videos compiling specific highlight situations of interest in the agent's behaviour), where the different introspection levels and their interconnections provide contextualized explanations.interaction with the environment. This is done using interaction data collected by the agent that is analysed using statistical methods organized in a three-level introspection analysis: level 0: Environment analysis, level 1: Interaction analysis; level 3: Meta-analysis. From these interestingness elements, it is then possible to generate visual explanations (in the form of videos compiling specific highlight situations of interest in the agent's behaviour), where the different introspection levels and their interconnections provide contextualized explanations.</p>
        <p>Figure 10: The interestingness framework. The introspection framework analyses interaction data collected by the agent and identifies interestingness elements of the interaction history. These elements are used by an explanation framework to expose the agent's behavior to a human user. Reproduced with permission from Pedro Sequeira [33].Figure 10: The interestingness framework. The introspection framework analyses interaction data collected by the agent and identifies interestingness elements of the interaction history. These elements are used by an explanation framework to expose the agent's behavior to a human user. Reproduced with permission from Pedro Sequeira [33].</p>
        <p>The authors applied their framework to the game Frogger and used it to generate video highlights of agents that were included in a user study. The latter showed that no summarizing technique among those used to generate highlight videos is adapted to all types of agents and scenarios. A related result is that agents having a monotonous, predictable performance will lack the variety of interactions needed by the interestingness framework to generate pertinent explanations. Finally, counter-intuitively, highlighting all different aspects of an agent's interactions is not the best course of action ,as it may confuse users by consecutively showing the best and poorest performances of an agent.The authors applied their framework to the game Frogger and used it to generate video highlights of agents that were included in a user study. The latter showed that no summarizing technique among those used to generate highlight videos is adapted to all types of agents and scenarios. A related result is that agents having a monotonous, predictable performance will lack the variety of interactions needed by the interestingness framework to generate pertinent explanations. Finally, counter-intuitively, highlighting all different aspects of an agent's interactions is not the best course of action ,as it may confuse users by consecutively showing the best and poorest performances of an agent.</p>
        <p>Some studies encountered do not fit in the above categories for the main reason that they are not linked to RL or do not directly provide explanations but nonetheless, they are interesting concepts that could contribute to the creation of new XRL methods in the future.Some studies encountered do not fit in the above categories for the main reason that they are not linked to RL or do not directly provide explanations but nonetheless, they are interesting concepts that could contribute to the creation of new XRL methods in the future.</p>
        <p>Although deep neural networks have exhibited superior performance in various tasks, their interpretability is always their Achilles' heel. Since CNNs are still considered black boxes, many recent research papers focus on providing different levels and notions of explanations to make them more explainable.Although deep neural networks have exhibited superior performance in various tasks, their interpretability is always their Achilles' heel. Since CNNs are still considered black boxes, many recent research papers focus on providing different levels and notions of explanations to make them more explainable.</p>
        <p>As many RL models harness visual input DL models (for instance, when processing pixel observations), they could profit from better explainability of these algorithms. That way, the complete block of a CNN associated to learn a policy, would be explainable as whole. In addition, some techniques used in the visual domain, such as representation disentanglement could be relevant to apply in RL. Among the approaches detailed by Zhang et al. [82], one of the most promising aims at creating disentangled (interpretable) representations of the conv-layers of these networks [83,84], as well as end-to-end learning of interpretable networks, working directly with comprehensible patterns, which are also a trending angle [85].As many RL models harness visual input DL models (for instance, when processing pixel observations), they could profit from better explainability of these algorithms. That way, the complete block of a CNN associated to learn a policy, would be explainable as whole. In addition, some techniques used in the visual domain, such as representation disentanglement could be relevant to apply in RL. Among the approaches detailed by Zhang et al. [82], one of the most promising aims at creating disentangled (interpretable) representations of the conv-layers of these networks [83,84], as well as end-to-end learning of interpretable networks, working directly with comprehensible patterns, which are also a trending angle [85].</p>
        <p>Explaining when, how, and under which conditions catastrophic forgetting [86] or memorizing of datasets occurs is another relevant aspect of life-long or continual learning [56] in DNNs yet not fully understood. An interesting method towards this vision is Learning Without Memorizing (LwM) [87], an extension of Learning Without Forgetting Multi-Class (LwF-MC) [88] applied to image classification. This model is able to incrementally learn new classes without forgetting classes previously learned and without storing data related them. The main idea is that at each step, a new model, the student, is trained to incrementally learn new classes, while the previous one, the teacher, only has knowledge of the base classes. By improving LwF-MC with the application of a new loss called Attention Distillation loss, LwM tries to preserve base classes knowledge across all models iterations. This new loss produces attention maps that can be studied by a human expert in order to interpret the model's logic by inspecting the areas that focus its attention.Explaining when, how, and under which conditions catastrophic forgetting [86] or memorizing of datasets occurs is another relevant aspect of life-long or continual learning [56] in DNNs yet not fully understood. An interesting method towards this vision is Learning Without Memorizing (LwM) [87], an extension of Learning Without Forgetting Multi-Class (LwF-MC) [88] applied to image classification. This model is able to incrementally learn new classes without forgetting classes previously learned and without storing data related them. The main idea is that at each step, a new model, the student, is trained to incrementally learn new classes, while the previous one, the teacher, only has knowledge of the base classes. By improving LwF-MC with the application of a new loss called Attention Distillation loss, LwM tries to preserve base classes knowledge across all models iterations. This new loss produces attention maps that can be studied by a human expert in order to interpret the model's logic by inspecting the areas that focus its attention.</p>
        <p>Another approach for scene analysis aimed to build a graph where each node represents an object detected in the scene and is capable of building a context-aware representation of itself by sending messages to the other nodes [89]. This makes it possible for the network to support relational reasoning, allowing it to be effectively transparent. Thus, users are able to make textual inquiries about relationships between objects (e.g., "Is the plate next to a white bowl?").Another approach for scene analysis aimed to build a graph where each node represents an object detected in the scene and is capable of building a context-aware representation of itself by sending messages to the other nodes [89]. This makes it possible for the network to support relational reasoning, allowing it to be effectively transparent. Thus, users are able to make textual inquiries about relationships between objects (e.g., "Is the plate next to a white bowl?").</p>
        <p>Compositionality is a universal concept stating that a complex (composed) problem can be decomposed into a set of simpler ones [90]. Thus, in the RL world, this idea can be translated into making an agent solve a complex task by hierarchically completing lesser ones (e.g. by first solving atomic ones as lesser tasks could also be complex) [91]. This provides reusability, enables quick initialization of policies and makes the learning process much faster by training an optimal policy for each reward and later combining them. Haarnoja et al. [34] showed that maximum entropy RL methods can produce much more composable policies. Empirical demonstrations were performed on a Sawyer robot trained to avoid a fix obstacle and to stack Lego blocks with both policies combined. They introduced the Soft Q-learning algorithm, based on maximum entropy RL [92] and energy-based models [93], as well as an extension of this algorithm that enables composition of learned skills. This kind of methods optimizing for compositionality does not provide a direct explanation tool; however compositionality can be qualitatively observed as self organized modules [94] and used to train multiple policies that benefit from being combined. Compositionality may also help better explain each policy along the training evolution in time, or each learned skill separately. However, it is also observed that compositionality may not emerge in the same manner as humans conceptually would understand it or expect it, e.g. based on symbolic abstract functionality modules. Some examples in language emergence in multi-agent RL settings show that generalization and acquisition speed [95] or language do not co-occur with compositionality, or that compositionality may not go hand in hand with language efficiency as in humans communication [96].Compositionality is a universal concept stating that a complex (composed) problem can be decomposed into a set of simpler ones [90]. Thus, in the RL world, this idea can be translated into making an agent solve a complex task by hierarchically completing lesser ones (e.g. by first solving atomic ones as lesser tasks could also be complex) [91]. This provides reusability, enables quick initialization of policies and makes the learning process much faster by training an optimal policy for each reward and later combining them. Haarnoja et al. [34] showed that maximum entropy RL methods can produce much more composable policies. Empirical demonstrations were performed on a Sawyer robot trained to avoid a fix obstacle and to stack Lego blocks with both policies combined. They introduced the Soft Q-learning algorithm, based on maximum entropy RL [92] and energy-based models [93], as well as an extension of this algorithm that enables composition of learned skills. This kind of methods optimizing for compositionality does not provide a direct explanation tool; however compositionality can be qualitatively observed as self organized modules [94] and used to train multiple policies that benefit from being combined. Compositionality may also help better explain each policy along the training evolution in time, or each learned skill separately. However, it is also observed that compositionality may not emerge in the same manner as humans conceptually would understand it or expect it, e.g. based on symbolic abstract functionality modules. Some examples in language emergence in multi-agent RL settings show that generalization and acquisition speed [95] or language do not co-occur with compositionality, or that compositionality may not go hand in hand with language efficiency as in humans communication [96].</p>
        <p>Distillation has also been used to learn task that are closely related and whose learning should improve speed up the learning of near tasks, in DisCoRL model [97], which helps transfer from simulation to real settings in navigation and goal based robotic tasks. We may then be able to further explain each policy along the training evolution timeline, or each learned skill separately.Distillation has also been used to learn task that are closely related and whose learning should improve speed up the learning of near tasks, in DisCoRL model [97], which helps transfer from simulation to real settings in navigation and goal based robotic tasks. We may then be able to further explain each policy along the training evolution timeline, or each learned skill separately.</p>
        <p>Imitation learning is a way of enabling algorithms to learn from human demonstrations, such as teaching robots to learn assembly skills [35,98]. While improving training time (compared to more traditional approaches [45]), this method also allows for better understanding of the agent's behaviour as it learns according to human expert actions [99]. It can also be a way to improve trust in the model, as it behaves seemingly as a human expert operator and can explain the basis of its decisions textually or verbally. Moreover, when encompassing human advice during training, it can be derived into advisable learning which further improves user trust as the model can understand human natural language and yields clear and precise explanations [100].Imitation learning is a way of enabling algorithms to learn from human demonstrations, such as teaching robots to learn assembly skills [35,98]. While improving training time (compared to more traditional approaches [45]), this method also allows for better understanding of the agent's behaviour as it learns according to human expert actions [99]. It can also be a way to improve trust in the model, as it behaves seemingly as a human expert operator and can explain the basis of its decisions textually or verbally. Moreover, when encompassing human advice during training, it can be derived into advisable learning which further improves user trust as the model can understand human natural language and yields clear and precise explanations [100].</p>
        <p>Transparency has been given multiple meanings over time, especially in robotics and AI Ethics. Theodorou et al. [101] freshly define it as a mechanism to expose decision making that could allow AI models to be debugged like traditional programs, as they will communicate information about their operation in real time. However, the relevance of this information should adapt to the user's technological background, from simple progress bars to complex debug logs. An interesting concept is that an AI system could be created using a visual editor that can help communicate which decision will be taken in which situation (very much like decision trees). These concepts have already been successfully implemented in an RL setup using Temporal Difference (TD) error to create an emotional model of an agent [102].Transparency has been given multiple meanings over time, especially in robotics and AI Ethics. Theodorou et al. [101] freshly define it as a mechanism to expose decision making that could allow AI models to be debugged like traditional programs, as they will communicate information about their operation in real time. However, the relevance of this information should adapt to the user's technological background, from simple progress bars to complex debug logs. An interesting concept is that an AI system could be created using a visual editor that can help communicate which decision will be taken in which situation (very much like decision trees). These concepts have already been successfully implemented in an RL setup using Temporal Difference (TD) error to create an emotional model of an agent [102].</p>
        <p>Despite explainable deep RL being still an emerging research field, we observed that numerous approaches were developed so far, as detailed in Section 2. However, there is no clear-cut method that serves all purposes. Most of the reviewed XRL methods are specifically designed to fit a particular task, often related to games or robotics and with no straight forward extension to other real-world RL applications. Furthermore, those methods cannot be generalized to other tasks or algorithms as they often make specific assumptions (e.g. on the MDP or environment properties). In fact in XRL there can be more than one model (as in Actor-Critic architectures) and different kinds of algorithms (DQN, DDPG, SARSA...) each with its own particularities. Moreover, there exists a wide variety of environments where each brings its own constraints. The necessity to adapt to the considered algorithm and environment means that it is hard to provide a holistic or generic explainability method. Thus, in our opinion, Shapley value-based methods [15,26] can be considered as an interesting lead to contribute to this goal. Shapley values could be used to explain the roles taken by agents when learning a policy to achieve a collaborative task but also to detect defects in training agents or in the data fed to the network. In addition, as a post-hoc explainability method, it may be possible to generalize Shapley value computation to numerous RL environments and models in the same way it was done with SHAP [15] for other black boxes Deep Learning classifiers or regressors.Despite explainable deep RL being still an emerging research field, we observed that numerous approaches were developed so far, as detailed in Section 2. However, there is no clear-cut method that serves all purposes. Most of the reviewed XRL methods are specifically designed to fit a particular task, often related to games or robotics and with no straight forward extension to other real-world RL applications. Furthermore, those methods cannot be generalized to other tasks or algorithms as they often make specific assumptions (e.g. on the MDP or environment properties). In fact in XRL there can be more than one model (as in Actor-Critic architectures) and different kinds of algorithms (DQN, DDPG, SARSA...) each with its own particularities. Moreover, there exists a wide variety of environments where each brings its own constraints. The necessity to adapt to the considered algorithm and environment means that it is hard to provide a holistic or generic explainability method. Thus, in our opinion, Shapley value-based methods [15,26] can be considered as an interesting lead to contribute to this goal. Shapley values could be used to explain the roles taken by agents when learning a policy to achieve a collaborative task but also to detect defects in training agents or in the data fed to the network. In addition, as a post-hoc explainability method, it may be possible to generalize Shapley value computation to numerous RL environments and models in the same way it was done with SHAP [15] for other black boxes Deep Learning classifiers or regressors.</p>
        <p>Meanwhile, the research community would benefit if more global-oriented approaches, which do not focus on a particular task or algorithm, were developed in the future, as it has already been done in general XAI, with for instance LIME [14] or SHAP [15].Meanwhile, the research community would benefit if more global-oriented approaches, which do not focus on a particular task or algorithm, were developed in the future, as it has already been done in general XAI, with for instance LIME [14] or SHAP [15].</p>
        <p>Moreover, some promising approaches to bring explainability to RL include representation learning related concepts such as Hindsight Experience Replay, Hierarchical RL and self-attention. However, despite the ability of those concepts to improve performance and interpretability in a mathematical sense (in particular representation learning), they somehow lack concrete explanations targeted to end users, as they mostly target technical domain experts and researchers. This is a key element to further develop and allow the deployment of RL in the real world and to make algorithms more trustable and understandable by the general public.Moreover, some promising approaches to bring explainability to RL include representation learning related concepts such as Hindsight Experience Replay, Hierarchical RL and self-attention. However, despite the ability of those concepts to improve performance and interpretability in a mathematical sense (in particular representation learning), they somehow lack concrete explanations targeted to end users, as they mostly target technical domain experts and researchers. This is a key element to further develop and allow the deployment of RL in the real world and to make algorithms more trustable and understandable by the general public.</p>
        <p>The state of the art shows there is still room for progress to be made to better explain deep RL models in terms of different invariants preservation and other common assumptions of disentangled representation learning [103,104].The state of the art shows there is still room for progress to be made to better explain deep RL models in terms of different invariants preservation and other common assumptions of disentangled representation learning [103,104].</p>
        <p>We reviewed and analyzed different state of the art approaches on RL and how XAI techniques can elucidate and inform their training, debugging and communication to different stakeholder audiences.We reviewed and analyzed different state of the art approaches on RL and how XAI techniques can elucidate and inform their training, debugging and communication to different stakeholder audiences.</p>
        <p>We focused on agent based RL in this work, however, explainability in RL involving humans (e.g. in collaborative problem solving [105]) should involve explainability methods to better assess when robots are able to perform the requested task, and when uncertainty is an indicator of better relying a task to a human. Equally important is to evaluate and explain other aspects in reinforcement learning, e.g. formally explaining the role of curriculum learning [106], quality diversity or other human-learning inspired aspects of open-ended learning [44,107,108]. Thus, more theoretic bases to serve explainable by design DRL are required. The future development of post-hoc XAI techniques should adapt to the requirements to build, train, and convey DRL models. Furthermore, it is worth noting that all presented methods decompose final prediction into additive components attributed to particular features [109], and thus interaction between features should be accounted for, and included in the explanation elaboration. Since most presented strategies to explain RL have mainly considered discrete model interpretations for explaining a model, as advocated in [110], continuous formulations of the proposed approaches (such as Integrated Gradients [111] based on the continuous extension of Shapley value, Aumann-Shapley value cost-sharing technique) should be devised in the future in RL contexts.We focused on agent based RL in this work, however, explainability in RL involving humans (e.g. in collaborative problem solving [105]) should involve explainability methods to better assess when robots are able to perform the requested task, and when uncertainty is an indicator of better relying a task to a human. Equally important is to evaluate and explain other aspects in reinforcement learning, e.g. formally explaining the role of curriculum learning [106], quality diversity or other human-learning inspired aspects of open-ended learning [44,107,108]. Thus, more theoretic bases to serve explainable by design DRL are required. The future development of post-hoc XAI techniques should adapt to the requirements to build, train, and convey DRL models. Furthermore, it is worth noting that all presented methods decompose final prediction into additive components attributed to particular features [109], and thus interaction between features should be accounted for, and included in the explanation elaboration. Since most presented strategies to explain RL have mainly considered discrete model interpretations for explaining a model, as advocated in [110], continuous formulations of the proposed approaches (such as Integrated Gradients [111] based on the continuous extension of Shapley value, Aumann-Shapley value cost-sharing technique) should be devised in the future in RL contexts.</p>
        <p>We believe the reviewed approaches and future extensions tackling the identified issues will likely be critical in the demanding future applications of RL. We advocate for the needs of targeting in the future more diverse audiences (developer, tester, end-user, general public) not yet approached in the development of XAI tools. Only this way we will produce actionable explanations and more comprehensive frameworks for explainable, trustable and responsible RL that can be deployed in practice.We believe the reviewed approaches and future extensions tackling the identified issues will likely be critical in the demanding future applications of RL. We advocate for the needs of targeting in the future more diverse audiences (developer, tester, end-user, general public) not yet approached in the development of XAI tools. Only this way we will produce actionable explanations and more comprehensive frameworks for explainable, trustable and responsible RL that can be deployed in practice.</p>
        <p>We thank Sam Greydanus, Zoe Juozapaitis, Benjamin Beyret, Prashan Madumal, Pedro Sequiera, Jianhong Wang, Mathieu Seurin and Vinicius Zambaldi for allowing us to use their original images for illustration purposes. We also would like to thank Frédéric Herbreteau and Adrien Bennetot for their help and support.We thank Sam Greydanus, Zoe Juozapaitis, Benjamin Beyret, Prashan Madumal, Pedro Sequiera, Jianhong Wang, Mathieu Seurin and Vinicius Zambaldi for allowing us to use their original images for illustration purposes. We also would like to thank Frédéric Herbreteau and Adrien Bennetot for their help and support.</p>
        <p>• A2C: Asynchronous Actor Critic [77] • AI: Artificial Intelligence• A2C: Asynchronous Actor Critic [77] • AI: Artificial Intelligence</p>
        <p>• COMA: Counterfactual multi-agent [112] • CNN: Convolutional Neural Network [113] • DDPG: Deep Deterministic Policy Gradient [114] • DL: Deep Learning• COMA: Counterfactual multi-agent [112] • CNN: Convolutional Neural Network [113] • DDPG: Deep Deterministic Policy Gradient [114] • DL: Deep Learning</p>
        <p>• GAN: Generative Adversarial Network [115] • HER: Hindsight Experience Replay [73] • HMM: Hidden Markov Model• GAN: Generative Adversarial Network [115] • HER: Hindsight Experience Replay [73] • HMM: Hidden Markov Model</p>
        <p>• HRA: Hybrid Reward Architecture [71] • HRL: Hierarchical Reinforcement Learning [72] • IDDPG: Independent DDPG [114] • MADDPG: Multiagent DDPG [70] • MDP: Markov Decision Process• HRA: Hybrid Reward Architecture [71] • HRL: Hierarchical Reinforcement Learning [72] • IDDPG: Independent DDPG [114] • MADDPG: Multiagent DDPG [70] • MDP: Markov Decision Process</p>
        <p>• Machine Learning: Machine Learning• Machine Learning: Machine Learning</p>
        <p>• POMDP: Partialy Observable Markov Decision Process• POMDP: Partialy Observable Markov Decision Process</p>
        <p>• PPO: Proximal Policy Optimization [116] • R-CNN: Region Convolutionnal Neural Network [117] • RL: Reinforcement Learning• PPO: Proximal Policy Optimization [116] • R-CNN: Region Convolutionnal Neural Network [117] • RL: Reinforcement Learning</p>
        <p>• SARSA: State Action Reward State Action [118] • SRL: State Representation Learning [40] • VAE: Variational Auto-Encoder [55] • XAI: Explainable Artificial Intelligence• SARSA: State Action Reward State Action [118] • SRL: State Representation Learning [40] • VAE: Variational Auto-Encoder [55] • XAI: Explainable Artificial Intelligence</p>
        <p>• XRL: Explainable Reinforcement Learning• XRL: Explainable Reinforcement Learning</p>
    </text>
</tei>

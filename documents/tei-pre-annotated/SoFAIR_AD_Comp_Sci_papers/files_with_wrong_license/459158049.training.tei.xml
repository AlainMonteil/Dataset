<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T11:31+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Please refer to published version for the most recent bibliographic citation information.</p>
        <p>The last decade has seen a tremendous increase in the use of data-driven approaches for the modeling of molecules and materials. Atomistic simulation has been a particularly fertile field of use; applications range from the analysis of large databases of materials properties, 1 to the design of molecules with the desired behavior for a given application. 2 Machine learning techniques have been applied to devise coarse-grained descriptions of complex molecular systems, [3][4][5][6][7][8][9] to build accurate and comparatively inexpensive interatomic potentials, [10][11][12][13][14][15][16][17][18] and more generally to predict, or rationalize, the relationship between a specific atomic configuration and the properties that can be computed by electronic-structure calculations [19][20][21][22][23][24][25][26] .</p>
        <p>All of these applications to atomic-scale systems share the need to map an atomic configuration Aidentified by the positions and chemical identity of its N atoms {r i , a i }, and possibly by the basis vectors of the periodic repeat unit h -into a more suitable representation. This mapping associates A with a point in a feature space, which is then used to construct a machine-learning model to regress (fit) a structureproperty relation, to cluster (group together) configurations that share similar structural patterns, or to further map the conformational landscape of a data set onto a low-dimensional visualization.</p>
        <p>The terms descriptor or fingerprint are used, usually interchangeably, in chemical and materials informatics to indicate heuristically-determined properties that are easier to compute than the quantities one ultimately wants to predict, but correlate strongly with them, facilitating the construction of transferable and accurate models. 27 Examples of descriptors include the fractional composition of a compound, the electronegativity of its atoms, a low-level-of-theory determination of the HOMO-LUMO gap of a molecule. In this review we focus on a more systematic class of mappings that use exclusively atomic composition and geometry as inputs, and aim to characterize precisely the instantaneous arrangement of the atoms, for which we use the term representation. We will be especially interested in those representations that apply geometric and algebraic manipulations to the Cartesian coordinates, to transform them in a way that fulfills physically-informed requirements: smoothness and symmetry with respect to isometries. Commonly used representations include atom-centered symmetry functions 10,28 , Coulomb matrices 19 , and the smooth overlap of atomic positions (SOAP) 29 . It is important to note that representations can be expressed using different mathematical entities. In the most straightforward realisation, the space of features takes the form of a vector space, in which each configuration is associated with a finite-dimensional vector whose entries are explicitly computed by the mapping procedure. Depending on the application, however, it may be simpler or more natural to describe the relationship between pairs of configurations. Such relationship can be expressed in terms of a kernel function k(A, A ) (e.g. the scalar product between feature vectors), or in terms of a distance between configurations d(A, A ) (e.g. the Euclidean distance between associated features). As we will see, distance or kernel-based formulations implicitly define a feature space, that in most cases can be expressed (at least approximately) in terms of a vector of features, and so can be seen as equivalent to a representation of individual structures, even in cases in which the distance or the kernel are not explicitly computed from a pair of feature vectors.</p>
        <p>While one can trace the origins of different representations to specific subfields of computational chemistry and materials science, the fact that representations should describe precisely the nature and positions of each atom means that they often are not specialized to a given application, but can be used with little modification for any atomistic system, from gasphase molecules to bulk solids [30][31][32] . This generality, however, does not mean that representations are completely abstract or disconnected from physical and chemical concepts. Over the past few years, it has become clear that representations that reflect more closely some fundamental principles -such as locality, the multi-scale nature of interactions, the similarities in the behavior of elements from the same group in the periodic table -usually yield models that are more robust, transferable and data-efficient. The link between a representation and the physical concepts it incorporates is usually mediated by the strategy one uses to fit the desired structure-property relations: it is often possible to show an explicit relationship between linear regression models built on the representation of a structure and well-known empirical forms of interatomic potentials (such as body-ordered, or multipole expansions), and more complex, non-linear machinelearning schemes built on the same features improve the flexibility in describing structure-property relations, albeit at the price of a less transparent interpretation of their behavior.</p>
        <p>Given the central role of structural representations in the application of data-driven methods to atomistic modeling, it is perhaps not surprising that considerable effort is being dedicated to understanding and improving their properties. These efforts follow several directions. First, the efficient, scalable, and parallel implementation of the construction of a given set of features is essential to ensure computational efficiency. Second, reduction in the number of features that is used to describe the system reduces the computational effort, and often improves the robustness of the model: feature selection aims at identifying the most expressive, yet concise, description of the system at hand. Third, it is often desirable to fine-tune a representation so that it facilitates training a model on a small number of reference structures, by incorporating more explicitly the available prior knowledge.</p>
        <p>This review aims to summarize recent work on the construction of efficient and mathematically sound representations of atomic and molecular structures, with a particular focus on the use for the regression of atomic-scale properties. It is part of a special issue that covers the many facets of the application of machine learning to chemical simulations, and the interested reader may find, among others, discussions of machine learning models based on Gaussian process regression, using some of the descriptors we discuss here 33 , of the construction of potentials for molecules 34,35 and materials 36 , the description of excited states 37 , and of unsupervised machine learning schemes 38 . Rather than focusing on a historical overview, we intend to provide a snapshot of the current insights on what makes a good representation, supporting our considerations with recent publications, and providing a perspective of the most promising research directions in the field.</p>
        <p>An atomic structure Ai An environment centered on the i-th atom of the structure A ri Position of the i-th atom rji</p>
        <p>Vector separating the i-th atom and its j-th neighbor, rjri Q Generic continuous index enumerating the components of an atomic representation q Generic discrete index enumerating the components of an atomic representation Q|A A representation of a structure A indexed by an unspecified label or set of labels Q ξ(Ai)</p>
        <p>Feature vector (with elements indexed by q) associated with an atom-centered environment, ξq(Ai) = q|Ai Ξ Feature matrix combining the features associated with multiple structures/environments χ q A column in a feature matrix, where (χ q )i = ξq(Ai) y(Ai) An atom-centered property, or its systematic approximation in terms of an atom-centered representation |Ai ỹ(ξ)</p>
        <p>A non-linear model that approximates y(Ai) using the feature vector ξ(Ai) k(A, A ) A (non-)linear kernel computed between two structures or environments, represented by the corresponding feature vectors ξ(A) d(A, A ) A distance computed between Even though this review has no intention of providing an exhaustive historical account of the development of descriptors for atomic structures, it is worth providing a brief overview. A "data-driven" philosophy emerged early in the field of chemical and molecular science, where the combinatorial extent of the space of possible molecules, 39 and the possibility of accessing this space with comparatively simple synthetic strategies, encouraged the development of quantitative structure/property relationships (QSPR) techniques, attempting to map 40 descriptors of molecular structure -based on cheminformatics fingerprints, 41,42 chemical-intuition driven descriptors 43 , molecular graphs, 44 or indicators obtained from quantum chemical calculations 45 -to the behavior of a selected compound, usually focusing on properties of direct applicative interest [46][47][48] such as solubility, toxicity, 49 or pharmacological activity. 50,51 This approach should be contrasted with that of "bottom-up" predictions, that aim to use models of the interactions between the atomic constituents of a material to simulate the behavior of the system on an atomic time and length scale. Starting from the early days of molecular simulations [52][53][54][55] the objective was to predict the energy, the forces, or any other observable of interest, for a specific molecular configuration, and use them to search for (meta-)stable configurations, or to simulate the evolution of the system by molecular dynamics 56,57 . In the absence of reliable reference values for the properties of specific atomic configurations, interatomic potentials (also called empirical force fields) were built using physically-inspired functional forms, combining harmonic terms to de- A schematic overview of the requirements for an effective structural representation. The mapping between structures and feature space should obey fundamental physical symmetries (equivalent structures should be mapped to the same features); should be complete (inequivalent structures should be mapped to distinct features); should be smooth (continuous deformations of a structure should map to a smooth deformation of the associated features). Furthermore, whenever dealing with datasets that are not homogeneous in molecular size, the representation should be additive: a structure should be decomposed in a sum of local environments (usually atomcentered), ensuring transferability and extensivity of predictions.</p>
        <p>scribe chemical bonds with Coulomb and 1/r 6 terms to describe electrostatics and dispersion. Their (few) parameters were determined by matching the values of experimental observables, such as cohesive energies, lattice vectors and elastic constants. The continuous increase in computational power, and the availability of electronic-structure techniques with a better costaccuracy ratio [58][59][60] has made it possible to compute extremely accurate energies and properties of specific configurations. This has opened the way to ab initio simulations of materials 55 , but also provided a viable alternative to empirical functional forms for the construction of interatomic potentials. Starting from the simplest compounds 61 , and then gradually increasing in complexity 62 , molecular potential energy surfaces fitted by interpolating between a comparatively small number of ab initio reference calculations provided the first practical applications of this idea. The possibility of combining very accurate calculations of the electronic structure of atomic systems with sampling of the statistics and dynamics of the nuclei on the electronic potential energy surface has allowed theoretical predictions that do not only agree with experimental results 61 -they can predict experiments 63 two decades before measurements become precise enough to verify the theoretical values. 64 Even though the ultimate goal of QSPR models and machine-learned potentials is the same -predicting scientifically and/or technologically relevant properties of molecules and materials -the approaches they follow to achieve this goal are quite different, which is reflected in the way an atomic structure is translated into an input for a machine-learning model. Cheminformatics descriptors, or fingerprints, are built ad hoc, incorporating both descriptors of molecular structure and composition, and easy-to-estimate molecular properties. They usually rely on a considerable amount of prior knowledge, are often system and problem specific, and are meant to label a compound rather than a specific configuration of its atoms. This is a logical consequence of the fact that QSPR aims for an end-to-end description of a thermodynamic property, which is not an attribute of an individual configuration, but of a thermodynamic state of matter. In the case of bottom-up modeling, instead, one aims first at building a very accurate surrogate model that is capable of reproducing precisely and inexpensively the outcome of quantum calculations for a specific configuration of the atoms. The end goal of predicting thermodynamic properties is achieved by coupling these prediction with statistical sampling methods 56,57,65 aimed at computing averages over the appropriate classical (or quantum 66,67 ) distribution of atomic configurations. As a consequence, the representations used as inputs of these surrogate quantum models are usually rather generic, constructed based exclusively on atomic coordinates and chemical species. They aim to establish a precise mapping between a specific structure and the associated atomicscale quantities, and for this reason have also proven very useful to analyze atomistic configurations [68][69][70] , an application we discuss in detail in Section VII.</p>
        <p>Even though we focus our discussion on this latter class of features, it is worth mentioning the recent, and rather successful, attempts to use descriptors that incorporate information from electronic-structure calculations, that we briefly summarize in Section IX G.</p>
        <p>In the rest of this section, we discuss the properties that are desirable for a representation used in atomistic machine learning, which are graphically summarized in Figure 1. The mapping between structures and features should be consistent with basic symmetries -i.e. reflect the fact that the properties associated with a structure do not change when the reference system or the labelling of identical atoms are modified; be smooth, so that models built on the features inherit a regular behavior with changing atomic coordinates; be complete, so that fundamentally distinct configurations are never mapped to the same set of features. Furthermore, many machine-learning tasks benefit greatly from being based on local features, which describe atoms or groups of atoms. Even though this is a less stringent requirement and, as we discuss below, global descriptors have been used very successfully, representations based on local en- Classes that appear as "leaves" of the tree are fully symmetric.</p>
        <p>vironments are usually associated with higher transferability, reflecting a "divide and conquer" approach to materials modeling 71,72 . Finally, less fundamental but not less important requirements are the numerical stability and computational efficiency of the structure-representation mapping, which we discuss in Section VIII.</p>
        <p>The Cartesian coordinates of the atoms encode all the information that is needed to reconstruct the geometry of a structure. Yet, it is obvious that they cannot be used directly as the input of a regression model. The fact that the Cartesian description of a molecule depends on its absolute position and orientation in space, and the order by which atoms are listed, means that configurations that are completely equivalent can be represented by many different Cartesian values, which makes any regression, classification or clustering scheme inefficient and potentially misleading. Over the years, many different approaches have been proposed by which translations, rotations, inversion and atom permutation symmetries can be enforced, which is reflected in the variety of alternative frameworks to achieve an effective representation to be used of the input of an atomistic machine-learning scheme. In fact, symmetry is such a central principle underpinning these efforts that it can be used to construct a "phylogenetic tree" of representations, organized according to the strategy that is used to incorporate symmetry in their construction, as shown in Figure 2.</p>
        <p>The need to remove the trivial symmetries, namely the dependency of the Cartesian coordinates on the origin and orientation of the reference system, has been recognized very early in the field of chemical and materials modeling. Different sets of internal coordinates 73 (bonds, angles, torsions) have been proposed, based on chemical intuition, as invariant descriptors of molecular geometry, and most of the molecular forcefields that have been so effective in the modeling of biological systems [74][75][76][77] rely on internal coordinates to define bonded interactions. A collection of internal coordinates that is sufficient to fully characterize the geometry of a structure, often referred-to as the Z-matrix, is a paradigmatic example of this class of representations. Even though the efficiency of this approach has often been questioned 78,79 , particularly because there is no unique way to define the Zmatrix, internal coordinates are still ubiquitous, and are effective whenever the system being studied has a well-defined, persistent bonding pattern (see Ref. 80 for a recent review). In these cases, internal coordinates can be seen as the initial step in the construction of discretized molecular representations, such as a molecular graph. Even though very widely used in chemical machine learning 2,81 , these graph based schemes are not meant to describe the exact arrangement of the atoms, but just their bonding pattern, and so fall outside the scope of this review.</p>
        <p>The limitations of an internal-coordinates description become most apparent when one wants to model a chemically-active system, as the bonding patterns can change during the course of a simulation, and therefore the invariance to atom index permutations becomes crucial to achieve a consistent model. The Empirical Valence Bond (EVB) method 82 has been used to simulate bond-breaking events, but the generality of the EVB approach is limited as the possible assignments need be pre-determined. This led to the development of representations that are intrinsically independent on the ordering of the atoms, such as permutation-invariant polynomials (PIPs) 11,[83][84][85][86] which are obtained by summing functions of the internal coordinates over all possible orderings. In their original implementation, the exponentially increasing cost of evaluating these sums limited their applicability to molecules with a small number of degrees of freedom. It is worth mentioning that the problem of fitting molecular potential energy surfaces, particularly for applications to gas-phase physical chemistry, has led to approaches that anticipate several of the ideas that have become central to modern machinelearning techniques: the need to symmetrize appropriately atomic structures, 87 the systematic fitting to databases of configurations computed with high levels of quantum chemistry, 61 and even the use of "neural network potentials" 88,89 are just a few examples of the pioneering contributions from this field.</p>
        <p>In the condensed phase, a similar pioneering role was played by the construction of systematic expansions of the potential energy of alloys 90 , and of bond order potentials based on the moments of the density of states [91][92][93] . Both anticipate the use of an atomcentered description of the energy, the role of symmetry, and the notion of building a systematic expansion of the target property in terms of a convergent hierarchy of terms of increasing complexity. The first successful attempt of explicitly bringing machine-learning ideas to the construction of interatomic potentials for condensed-phase materials can be attributed to Behler and Parrinello, who in Ref. 10 introduced the concept of atom-centered symmetry functions (ACSF), which rely on a local expansion of the energy and on the construction of a symmetric description of atomic environments. Similarly to PIPs, ACSF are translationally and rotationally invariant because they are functions of angles and distances, and permutationally invariant because they are summed over all possible atomic pairs and triplets within an atomic environment. The computational cost of ACSF is kept under control by restricting the range of interactions (which we discuss further in subsection III C) and the body order of the correlations considered. Despite these restrictions, ACSF models have been shown to achieve comparable accuracy to that reached by PIPs 94 . Indeed, the recently proposed atomic PIPs 95 use the same polynomial basis as global PIPs, but avoid the unfavorable scaling with increasing molecule size by combining locality (via a distance cutoff) and a truncation of the order of the expansion.</p>
        <p>Internal coordinates are also the fundamental building block of molecular matrix representations, which are based on functions of the interatomic distances within a structure. Coulomb matrices, which list the formal electrostatic interactions q i q j /r ji between each atomic pair in a structure, have been extensively explored in early applications of the machine learning of molecular properties 19 , with the main limitation being connected to the lack of permutation invariance 96 , which has also been tackled by approximate symmetrization, summing over a manageable number of randomized orderings of the atoms 97,98 . We discuss alternative approaches to symmetrizing Coulomb matrices, as well as other representations based on molecular matrices, in Subsection III B.</p>
        <p>The phylogenetic tree in Fig. 2 shows that a large number of existing representation take a different strategy to achieve symmetrization: rather than using internal coordinates that are inherently invariant to rotations and translations, they first -implicitly or explicitly -describe the system as an atom density i g(x -r i ), obtained by summing over localized functions centered on the positions r i of all atoms in the system. Such a density is naturally invariant to permutations, and only at a later stage one proceeds to symmetrize it over translations and rotations. We discuss in great detail this second approach in Section IV. It suffices to say, at this point, that even if the construction of symmetrized density representations is conceptually very different from those based on internal coordinates, there are many direct and indirect links between the two branches, sketched in Figure 2, which we will discuss when reviewing specific classes of representations.</p>
        <p>The overwhelming majority of atomic-scale properties are continuous, smooth functions of the atomic coordinates. Function regularity is crucial for creating efficient ML models, and is therefore one of the requirements for a good structural representation. Features constructed from a symmetrized atom density are naturally smooth functions of atomic coordinates, and it is usually not a problem to maintain this regular behavior upon symmetrization over translations and rotations. The level of smoothness can be adjusted by smearing the atomic density, or by expanding it on a smooth basis (effectively a Fourier smoothing), as we discuss more extensively in Section IV. Internal coordinates are also usually smooth, but the process of manipulating them to achieve a permutation invariant representation can affect the smoothness of the mapping.</p>
        <p>One way to obtain permutation invariance without incurring the exponential scaling of the cost associated with enumerating all possible permutations of atomic indices involves sorting the entries in a distance or Coulomb matrix 97,99 , an approach that has also been used with permutation invariant vectors (PIV) 100 , "bag of bonds" features (BoB) 101 . Similar descriptors based on sorted distances have been also used to identify recurring structures in structure optimization algorithms 102,103 , and more recently generalized to lexicographically-sorted lists of k-neighbors distances 104 . Computing the eigenvalues of (functions of) interatomic distances, which underlies the SPRINT method 105 as well the overlap matrix eigenvalue fingerprints 68,106 , also effectively achieves permutation invariance by similar means, since the vector of eigenvalues is taken to be sorted in ascending or descending order. The earliest implementation of the DeepMD scheme 107 also relied on sorting a local distance matrix. However, the sorting operation introduces derivative discontinuities in the mapping between Cartesian coordinates and features, because the order of the distance vector changes as atoms are displaced in the structure.</p>
        <p>Figure 3 illustrates the discontinuity of the derivatives of a function that is built from an ordered list of features. Consider a system of 3 atoms that is uniquely defined by the 3 interatomic distances r i , where the index i denotes the position of the interatomic distance r i in the ordered list of distances. We define a smooth function of the sorted distances, f = i c i r i -r 0 i 2 parameterized by c and r 0 . The function f is indeed invariant to the permutations of the atom order in the trimer, but at the price of introducing kinks in f and discontinuities in its derivative when the distance ordering changes. Fitting any smooth function of the trimer geometry by optimizing the parameters c and r 0 would necessarily lead to poor approximation accuracy.</p>
        <p>The lack of regularity has implications for the accuracy and stability of machine-learning models built on such features, as has been shown recently by using a Wasserstein metric to compare Coulomb matrices in a permutation-invariant manner 108 . In this context it is worth noting the remarkable connection linking the Euclidean distance between vectors of sorted distances and the Wasserstein distance between radial distribution functions (Section III.F in Ref. 109), which builds FIG. 3. Toy model demonstrating a non-smooth property (solid line) and its discontinuous derivative (dashed line) that are defined as functions of the ordered list of interatomic distances for a three-atom cluster.</p>
        <p>a formal bridge between conceptually unrelated families of atomic-scale representations.</p>
        <p>The overwhelming majority of empirical interatomic potentials are expressed as an additive combination of local terms, or of long-range pairwise contributions. Early models built to fit molecular potential energy surfaces were built explicitly as a function of the coordinates of all atoms in the system. 61,110,111 Besides the issues of computational cost, this approach is problematic, as it hinders the application of the potential to a molecule with a different number of atoms, or chemical composition. The work of Behler and Parrinello 10 did not only have the merit of emphasizing the importance of symmetries in atomistic machine learning, but it also applied to ML interatomic potentials an additive expansion of the molecular energy E(A), writing it as a sum of atom-centered contributions,</p>
        <p>The notion of an additive decomposition of properties, which is implicit in the functional forms of most interatomic potentials, has far reaching consequences in terms of the data efficiency of the model, as discussed in Subsection VIII C. Combined with the requirement that the atomic contributions only depend on the position of atoms within a finite range of distances, which is needed for the method to be computationally practical and is supported by fundamental physical principles 112 , the additivity assumption breaks down the problem of predicting the properties of a complex structure into simpler, shortrange problems. An additive decomposition is also the most straightforward way to ensure extensivity of predictions 113 , i.e. that the prediction of a property for two copies of a molecule at infinite distance from each other is equal to twice the prediction for a single molecule.</p>
        <p>It is not by chance that also in the field of molecular machine learning, for which many of the early representations aimed at a global description of a molecule 19,31,114,115 , most of the recent approaches have moved to additive, atomcentered representations 116,117 , that yield more accurate and transferable models, at least for extensive properties 118 . Oftentimes it is possible, and relatively straightforward, to modify a global representation to describe an atom-centered environment 68,95,119 , or to combine atom-centered representations to build a global description 69 , e.g. by summing or averaging the values of all the atom-centered features that are present in the structure, as we discuss in Section VII B. In fact, one could regard the list of atomcentered features for all the atoms in a structure as an equivariant global representation of the structure -one in which the entries in the feature vector transform according to the permutation of the atomic indices. This notion underlies for instance the concept of self-attention 120,121 , which has been very fruitfully applied in the construction of neural networks and models for cheminformatics. The connection between symmetry, locality, additivity, and the nature of the structure-property relation that one wants to model is essential to the construction of effective and transferable machine-learning models.</p>
        <p>The requirements of symmetry, smoothness and locality can be seen as geared towards reducing the complexity of the structural representation, eliminating redundant structures, reducing the resolution to the intrinsic length scale over which the target property exhibits substantial variations, and breaking down complicated compounds into simple fragments. This simplification should not, however, come at the expense of the completeness of the representation, meaning that the mapping between Cartesian and feature spaces should keep inequivalent structures distinct. For example, it has been known for some time that a histogram of interatomic distances (discarding the identity of the connected atoms) is insufficient to fully characterize a structure composed of more than three atoms 29,122,123 . More recently, counterexamples have emerged showing that atom-centered correlations -at least those of low order -are also insufficient to preserve the injectivity of the structure-feature mapping (see Ref. 124 and Section VI B for a more thorough discussion).</p>
        <p>Besides completeness in terms of the geometric structure-feature mapping, one should also consider whether for a chosen regression scheme the featureproperty mapping can be converged to arbitrary ac-curacy. More complex, non-linear models can often provide good results even when using a representation that involves excessive smoothing, or an highly truncated version of a family of features. The interplay between model and features is discussed in more detail in Section V, and the (largely open) problem of completeness in Section VI.</p>
        <p>As discussed in the previous Section, a multitude of representations have been introduced over the past decade, attempting to incorporate basic principles of symmetry and locality at the very core of atomistic machine learning. The differences between them are much less fundamental than it appears at a first glance, and in fact several works have recently pointed at the existence of a unified framework, in which an explicit formal connection can be established between the vast majority of representations. 109,[125][126][127] In this Section we summarize the construction of a class of features, that we refer to as "symmetrized atomic field representations", emphasizing the role played by symmetry and locality, as well as hint to the connection between this class of features and a linear mapping between structure and properties, which is discussed in more detail in Section V.</p>
        <p>We formalize a notation, that extends the one introduced in Refs. 109,125 and used in Ref. 128 to compare different kinds of local and global representations, which expresses the feature vectors associated with the representation of a structure in a way that mimics Dirac notation in quantum mechanics. At the most basic level, this notation can be seen as a way to indicate expressively the nature of the representation used, and to tidily enumerate the components of the associated feature vector. Much like in the quantum case, the real value of the formalism is that it emphasizes the basis-set independence of the class of representations we concentrate on, and that it provides visual cues that help recognizing at a glance the linear operations that occur in the construction and manipulation of the feature vectors, and of the models built on them. 129 We will use this notation consistently throughout this review as a neutral medium to express general results that reflect concepts shared by many of the most widespread representation, but occasionally make a link to the different notations that have become established to describe specific frameworks.</p>
        <p>Representations in bra-ket notation We use a ket |A to indicate an abstract feature vector associated Both the ket and the bra indices can (and will) be used with some looseness, to emphasize the most relevant elements of a representation while keeping the notation slim. For instance, as shown in Fig. 4, one can indicate explicitly multiple bra indices when their meaning in the definition of a representation is important, separating with a semicolon groups of indices that are conceptually related, or condense them in a compound index when the substructure is irrelevant. Occasionally, e.g. when juxtaposing different choices of basis functions, one may also include qualifiers in the bra, e.g. n; GTO| to indicate that Gaussian type orbitals are used as a basis. Moreover, when discussing the construction of a representation, the reference structure is not important, and so one may drop the structure index from the notation and write |α instead of |A; α . Conversely, when the representation of choice is well-established -e.g. when writing expressions that describe the regression scheme after having discussed the choice of representation -one may omit the specifics of the representation and write simply |A .</p>
        <p>The indices and the qualifiers that are associated with the structure index (typically in the ket) describe the essential nature of the representation and will be reflected in the architecture of a model built on it. The indices in the bra, instead, simply enumerate features that are of homogeneous nature, are usually manipulated together in the construction of the model, and can be transformed, contracted or sub-selected in a way that does not change the fundamental properties of the representation. In many cases, it is possible to describe the construction of a representation as a combination of kets, without indicating explicitly the use of a particular basis.</p>
        <p>This notation can be applied in a way that yields usage patterns that are very similar to those that are common in quantum mechanics, e.g. bra and ket can be interchanged using the convention A|Q = Q|A . However, much as in the case of the formalism we take inspiration from, a rigorous characterization of the mathematical relations between bras and kets is problematic 130 . It is better to see this notation as a form of symbolic calculus that facilitates memorizing and applying correctly recurring operations and transformations. Let us give a few examples, which also provide a reference of how the notation will be applied in this review.</p>
        <p>Change of basis. A change in the basis that is used to practically compute a representation can be written as a linear transformation,</p>
        <p>where T |Q indicates the coefficients that enact the change of basis. This kind of manipulations will be used in Section IV E to convert between a real-space description of the atom-centred density and one based on radial functions and spherical harmonics. 131 All of the expressions discussed here as integrals over a continuous index can be formulated as sums over (finitely or infinitely) countable, discrete indices</p>
        <p>Scalar product and kernels. The scalar product between the features of two structures A and A can FIG. 5. To obtain features that are invariant to inversion with respect to the vertical dotted line, Haar integration over the symmetry group in this case just corresponds to summing over two symmetry related images. Starting from two distinct functions |f (left panels, red) and |g (right panels, blue), the functions (full lines) and their mirror transformation (dotted lines) are summed to obtain invariant features (bottom row). Direct symmetrization is depicted in the central panels, yielding |f ⊗1 , while the external panels visualize the construction of tensor-product features, their symmetrization and summation, yielding |f ⊗2 .</p>
        <p>be written using a complete basis indexed by Q as</p>
        <p>where one recognizes an expression that is reminiscent of a completeness relation dQ |Q Q| = 1. This definition only holds for a complete, orthogonal basis and might entail an approximation when computed with a finite basis. The notation A|A can also be used to refer to a kernel k(A, A ) that expresses the similarity between two configurations; this is obvious when considering a linear kernel, but can also be used for non-linear kernels, keeping in mind that it might not be possible to write explicitly the features that correspond to the Hilbert space that reproduces the kernel. 132 Linear models. The bra-ket notation implicitly assumes linearity in the transformation between different choices of basis, and in the modeling of target properties. Even though the features can be used as an input of an arbitrarily complex nonlinear regression scheme (see Section V D), we will often investigate their behavior in the context of linear models, because they reveal more transparently how a given representation reflects structure-property relations. When using a representation |A; α to describe structures, a linear model for a property y(A) can be written as y(A) ≡ y|A ≈ dQ y; α|Q Q|A; α , (4) where y; α|Q indicates the regression weights for a model based on |A; α . Leaving aside (important) issues related to regularization, this expression emphasizes that one can transform simultaneously the weights and the features to a different basis, and the predicted value is unchanged. The expression y|A can also be seen as a hint of the fact that a collection of properties could be used as descriptors for a structure A, although this is an approach we only discuss briefly in this review.</p>
        <p>Tensor product. A pattern we use frequently in what follows, and that mimics a construction used in quantum mechanics, is the combination of multiple kets to build a tensor-product space, e.g.</p>
        <p>The construction of a tensor-product representation is well-defined even without indicating explicitly the basis used to describe either side of Eq. ( 5), and it is often possible to use either an explicit Cartesian product of the bases on the right-hand side, or a combined basis</p>
        <p>using only |A as a special case of Eq. ( 5) in which A ≡ A , and α ≡ α can be omitted.</p>
        <p>Operators and symmetry averages. Finally, we can consider the action of an "operator" on a ket, that is to be interpreted as a linear map that transforms the atomic structure. Taking for instance the operator î associated with inversion symmetry, î |A indicates the representation associated with structure A after the coordinates of all atoms have been reflected relative to the origin. Much as in quantum mechanics, the operator can also be applied to the bra, where it corresponds to a transformation of the basis. In terms of symmetry operations, this corresponds to the active or passive transformations, acting on the structure or on the reference frame. By summing over the operators associated with a symmetry group, an operation which is also referred to as Haar integration 133 , one can build symmetrized representations that are covariant under the actions of the elements of the group, e.g. for the C i point group,</p>
        <p>The index σ takes the value -1 for representations that change sign under inversion, and +1 for invariant features; in the invariant case, σ may be omitted. When the resulting symmetric representation is used often, and the symmetry group is clear from the context, we indicate the averaging with an overline and omit the explicit indication of the group it has been symmetrized over, e.g., 5 illustrates the notation and the Haar integration in one dimension. Two distinct functions, f and g are plotted using their usual real-space features, f (x) ≡ x|f and g(x) ≡ x|g . Applying inversion yields x| î |f = f (-x). An inversion-invariant feature can be created by symmetrizing: |f ⊗1 = |f + î |f , but our choice of f and g leads to a degenerate description, as |f ⊗1 = |g ⊗1 . A second order feature may be obtained by generating the tensor product of the functions, e.g. |f ⊗ |f , which in real space results in x 1 ; x 2 |f ⊗ f ≡ f (x 1 )f (x 2 ). Symmetrizing this tensor product yields features |g ⊗2 and |f ⊗2 that are also inversion-invariant, but are still able to distinguish between the two functions.</p>
        <p>An example: SOAP in bra-ket notation To give a concrete example of the use of this formalism, let us compare the functional notation used in Refs. 29,69 to indicate the components of a SOAP feature vector with the corresponding bra-ket notation. The reader who is unfamiliar with the SOAP construction will find the remainder of this Section, and in particular Section IV E, to give a very detailed account of this family of features, and might better skip this brief overview, that assumes knowledge of the derivation from Ref. 29. The SOAP power spectrum describes the two-point correlations between the atom density centered around the i-th atom of structure A, expanded in terms of atomic species (labeled by the indices a 1,2 ), radial basis functions (labeled by n 1,2 ) and angular momentum channels (labeled by l). The density expansion coefficients can be written as</p>
        <p>In this expression, ax|A; ρ i ≡ ρ i,a (x) indicates the atom-centred density, x|n ≡ R n (x) an orthonormal set of radial functions, and x|lm ≡ Y m l (x) the spherical harmonics.</p>
        <p>The SOAP features for the environment A i can be written as</p>
        <p>In the functional notation, one relies on the convention that c corresponds to the density expansion coefficients and p to the power spectrum, while the Dirac notation uses the more expressive symbols ρ i to indicate the i-centered atom density, and ρ ⊗2 i as a reminder that SOAP features can be derived as a symmetry-averaged 2-point correlation of |ρ i . This expanded notation is indicative of the place of the SOAP powerspectrum in the hierarchy of densitycorrelation features, and is useful to distinguish between different kinds of features (radial correlations, power spectrum, bispectrum . . . ). When it is clear that one is only using one type of representation, the compact (and generic) form |A i can be used instead. When it comes to the indices labelling different features, the functional notation mixes the indices (a) associated with the chemical species of the neighbors and the index i of the central atom, separating them from those associated with the radial channel (n). This reflects how SOAP was originally introduced to describe single-element systems. In the Dirac notation, on the other hand, the (a 1 n 1 ) and (a 2 n 2 ) indices are grouped together to indicate that they are conceptually linked in the construction as a tensor product of two densities, and the index indicating the identity of the central atom is associated with the ket.</p>
        <p>The starting point for the construction of a symmetry-adapted field representation is a field that describes the structure in terms of the distribution of its atoms -or, more generally, of points that are associated with the building blocks of the material, as one would have in a coarse-grained model. In the simplest possible case, one would take localized functions g centered on each atomic position r i and define</p>
        <p>where x|r i ; g ≡ g(x -r i ) is a localized function (e.g. a Gaussian) centered on the i-th atom, and the ρ in the ket indicates the kind of field used to describe the structure. As we discuss in more detail in Section IV E, the atomic density functions can be either finite-width Gaussians, which leads to representations akin to SOAP features 29 , or Dirac δ distributions, which recovers representations similar to the current implementation of moment tensor potentials 134</p>
        <p>In this form, Eq. ( 11) can be seen as an abstraction of the many real-space "voxel" representations of materials, 135,136 that are used often in the context of generative models and reinforcement learning 137 . The ket |A; ρ defined by expressions like (10) or (11) could be equally well expressed in a different basis, e.g. expanded in plane waves</p>
        <p>which also shows how the change of basis can be applied directly to the atom-centred density contributions. Eqs. (10) and (12) contain the same amount of information, and can be seen as special cases of a formal definition of the representation for the structure A as a sum of atomic representations,</p>
        <p>Even though the choice of a basis can be very important to simplify analytical derivations or practical implementation, representations can be regarded as abstract objects that can be defined independently of the basis set, much as it is the case for the wavefunction in quantum mechanics.</p>
        <p>One way to make x|ρ translationally invariant is to sum over the continuous translation group, d t x| t |ρ . Summing directly over the atom density eliminates all structural information, because d t x| t |r i ; g = dt g(t -r i ) = 1. Information loss is a usual issue with Haar integration, as exemplified in Figure 5. One can avoid or reduce it by summing over tensor products of the atom density field. Considering the case in which atoms are described only by their position and chemical identity, integrating over translations t yields a two-point density correlation function (14) where g indicates the cross-correlation of two of the localized density functions. In the case of a Gaussian density, g is simply a Gaussian with twice the variance, and outside this section we will use just g to indicate the atomic density both in |ρ and |ρ i . As a remindet that the representation has been obtained by averaging over translations the tensor product of two density fields, we use the superscript notation ρ ⊗2 , and we separate with a semicolon groups of feature indices that are associated with each factor in the tensor product, as discussed in Section IV A. Note that the representation in Eq. ( 14) has a large null space, as it depends only on x 1 -x 2 . One could then re-define it by labelling features using a single position vector, or transform it in a plane wave basis:</p>
        <p>where the second equality is a consequence of the convolution theorem. One sees that the translationallysymmetrized density is essentially equivalent to the diffraction pattern of the atomic structure I(k), that has been already used as a descriptor to classify crystalline configurations. 138 This construction can be taken as an inspiration to introduce an atom-centered representation</p>
        <p>where r ji = r j -r i . The fact that |A; ρ i is atom centered (and hence translationally invariant) is hinted at by the subscript notation ρ i , and so in what follows we only use this subscript to distinguish it from its nonsymmetrized counterpart (10) and simultaneously to indicate the central atom index. When expressing a representation centered around atom i without emphasis on its precise nature, we will use the notation |A i .</p>
        <p>Writing the symmetrized two-point density correlation in terms of Eq. ( 16) clarifies how an atomcentered representation is a natural consequence of the translational symmetrization: When building a linear model, this expression implies an additive decomposition of the target property, as well as the use of separate models depending on the nature of the central atomic species:</p>
        <p>Note that in this case we assume that only the regression weights depend on the nature of the central atom, but one might as well fine-tune the atom-centred features depending on the central atom. As discussed in Section V A, this expression can be taken as the prototype of all pair potentials, and higher-order of many-body interaction can be incorporated by taking higher tensor powers before symmetrization, or in the subsequent step of rotational averaging. Localization can be enforced by introducing a cutoff function in the definition (16). This is far from being an inconsequential operation, as it introduces an error: atomic energies and properties cannot depend on neighbors farther than this limit, as one can measure in terms of the locality of the response of forces to atomic displacements of neighbors 15 . However, introducing a relatively short-range cutoff often results in more robust models, which perform better in the data-poor regime. We discuss this in more detail in Section VIII C.</p>
        <p>The atom-centered representation ( 16) is translationally invariant, but does depend on the orientation of the structure. One should then proceed to perform Haar integration over the rotation group and (possibly) over inversion.</p>
        <p>We can define the (ν + 1)-body order symmetrized field representation as</p>
        <p>This can be expanded on an explicit position basis</p>
        <p>emphasizing that |ρ ⊗ν i corresponds to a symmetrized, ν-point correlation of the atom density centered on the i-th atom (Fig. 6) -a (ν + 1)-point correlation function, in the language used in statistical mechanics to describe the structure of liquids 139,140 . Similar to the case of Eq. ( 14), this object has a large null space (e.g. in the ν = 1 case it only depends on 109, one can choose a more concise enumeration of the real-space correlations in terms of distances and angles, that reduces in the limit g → δ to a sum over distances and angles between atoms. For instance, for the ν = 2 case one can write a 1 r 1 ; a 2 r 2 ; ω|δ ⊗2 i ∝ jj δ a1aj δ a2a j δ(r 1 -r ji )δ(r 2 -r j i )δ(ω-r ji •r j i ), (21) where we use ρ → δ to indicate that the correlation function is built on the Dirac-δ limit of the atom density field. Expressions of this kind reveal the close connection between symmetrized-field representations and atom-centered symmetry functions 10,20,141 , as well as equivalent constructions such as those used in the ANI 20 and DeepMD 142 frameworks, and the FCHL features 116,117 . Features that describe a chemical environment are written as a sum over tuples of neighbors of appropriate functions of their distances and angles, and can be seen as just a different choice of basis set for Eq. ( 21) (22) that demonstrates the connection between density correlations and atom-centered symmetry functions computed as a sum over groups of neighbors following the notation used in Ref. 141. Note that we choose to symmetrize the atomcentered description |ρ i -given that this is the procedure that recovers most of the existing representations -but one could as well proceed by averaging over tensor products of the translationally invariant representation of the full structure (23) as it was done for instance in Ref. 143. Doing so results in the appearance of cross terms involving correlations between densities centered on different atoms, which could be used to systematically incorporate in this framework machine-learning approaches based on convolutional, and message-passing, neural networks that combine information centered on neighboring atoms. 21,144 E. Density correlations in an angular momentum basis</p>
        <p>More concise (and easier to evaluate) expressions for the density correlation representations can be obtained with a change of basis. Using orthonormal radial functions R n (x) ≡ x|n and spherical harmonics Y m l (x) ≡ x|lm yields a discrete set of coefficients that transform as spherical harmonics anlm|A; ρ i = dx n|x lm|x ax|A; ρ i = j∈Ai δ aaj dx n|x lm|x xx|r ji ; g = j∈Ai δ aaj nlm|r ji ; g , (24) where nlm|r ji ; g corresponds to the expansion in radial functions and spherical harmonics of a Gaussian centered on the interatomic vector r ji . These expansion coefficients can be seen as functions of r ji , enumerated by the indices (n, l, m), that can be evaluated numerically or analytically, depending on the choice of basis (see Section VIII D for a few examples).</p>
        <p>The use of spherical harmonics |lm for the angular basis is natural, and makes it easy to evaluate the rotational integral of Eq. ( 19) analytically, because the matrix elements lm| R|l m = δ ll D l m m ( R) correspond to Wigner-D matrices, an irreducible representation of SO(3). Well-known results from the theory of angular momentum, 145 such as the orthonormality and the product reduction formula for Wigner-D matrices, allow deriving explicit expressions for the symmetrized field representations of order ν = 1, 2, 3</p>
        <p>where</p>
        <p>Much as it was the case for the real-space versions of the density correlation representations, there are several redundant indices in these expressions, resulting from the rotational averaging that leaves some of the m i as free parameters. We can then re-label the invariant features, in a way that emphasizes the connection to existing representations, by coupling the angular basis and absorbing some of the inconsequential constant factors. For the case ν = 1 one can define</p>
        <p>which corresponds to a discretized version of a pair correlation function</p>
        <p>in which we use the usual notation g a (r) to indicate the distribution of a atoms (although in this case it is restricted to an i-centered environment rather than averaged over an equilibrium distribution). For the ν = 2 case, Eq. ( 26) can be redefined as</p>
        <p>This corresponds -modulo irrelevant constants -to the rotation invariant 3D shape descriptor 146 and to the SOAP features, which would be written, in the notation of Refs. 29,69 as</p>
        <p>where c i,a nlm = anlm|ρ i indicate the density expansion coefficients following the same notation. The ν = 2 representation can also be written on a realspace basis as a 1 r 1 ; a 2 r 2 ; ω|ρ ⊗2 i , emphasizing its nature as three-body density correlation function that depends on two distances r 1 , r 2 and the cosine ω of the angle between the directions along which they are evaluated. The 4-body order invariant representation becomes</p>
        <p>corresponding to the SOAP bispectrum</p>
        <p>and closely related to the bispectrum used in the spectral neighbor analysis method 147,148 , which is essentially equivalent to a different choice of basis. As discussed in more detail in Ref. 149 and in the next sections, the relationship between the redundant expressions (25, 26, 27) that arise from the integral over rotations, and the more concise versions (28, 30, 32) can be seen as a transformation from the uncoupled to the coupled angular momentum basis, and starting from the ν = 4 additional indices k ν must be included to account for the different ways the coupling can be realized. A practical implementation of these higher body order features is given by the the atomic cluster expansion (ACE), which is usually computed based on the g → δ limit of ρ. The coefficients of the atom density are indicated as nlm|δ i ≡ A inlm following the notation of Ref. 126, and</p>
        <p>correspond to the features associated with ν-order neighbor clusters. Note that each The philosophy behind the density correlation features is different from that behind MTPs and ACE, in that these methods were at least originally thought of as bases for polynomial regression. While these basis functions can be equally used as symmetry-adapted features there are subtleties to be considered that we discuss in Sec. VI and in Sec. VIII B. Note that even though the contracted basis (a i n i l i k i ) i=1...ν | eliminates some of the redundant indices that are present in the tensor-product basis, the indices do not label a set of linearly independent features. Symmetries and selection rules -some of which, listed in Ref. 149, can be derived from results of angular momentum theory 151 -restrict greatly the number of independent entries that need to be computed. However, the non-trivial interaction between the radial and angular basis component makes this list incomplete. A mixed algebraic/numerical precomputation step can further reduce the required features 127 .</p>
        <p>Finally, the global SOAP-like descriptors introduced in Ref. 143, corresponding to Eq. ( 23), can be readily expressed in an angular momentum basis as</p>
        <p>where we recall that anlm|A; ρ ⊗2 = i∈A anlm|ρ i .</p>
        <p>A crucial point in comparing different representations is that with an appropriate discretization of the angular basis one can evaluate symmetrized highorder correlations as sum of products of the density coefficients defined in Eq. ( 24). This ensures that the cost of computing all coefficients of a given order ν, scales only linearly with the number of neighbors included within the cutoff around atom i, even though it scales exponentially with ν in terms of the number of basis functions, at least with a naive choice of basis. This is to be contrasted with atom-centered symmetry functions (ACSF), 20,141,142 and permutation invariant polynomials (PIP), 11 in which function are evaluated over all possible tuples composed of ν neighbors of the central atom (or on all the possible tuples in a structure to yield a global descriptor). In these frameworks, the cost depends linearly on the number of basis functions, but exponentially with ν in terms of the number of neighbors. This crucial difference makes density-expansion frameworks more convenient when one wants to ramp up the value of ν, and there are many neighbors. A-priori sparsification schemes, exemplified in (106), and feature selection schemes, discussed in Section VIII B, allow one to keep only the most important basis functions, and eliminate the exponential scaling with ν altogether.</p>
        <p>Despite this rather fundamental difference in philosophy and computational cost, the two families of representations compute entities that are essentially equivalent, which we see by writing explicitly Eq. ( 30) in the g → δ limit as a sum over neighbors j and j</p>
        <p>By using the addition formula of the spherical harmonics we get the equivalent formulation</p>
        <p>in which ω|l ≡ P l (ω) is a Legendre polynomial of order l. In Eq. ( 37), the ν = 2 density correlation coefficients are computed as a function of the distances and angles between triplets of atoms including the central atom i. By plugging this expression for a 1 n 1 ; a 2 n 2 ; l|δ ⊗2 i into Eq. ( 22), that evaluates the value of an arbitrary atom-centered symmetry function, one sees that this result is not specific to the choice of P l as angular functions: in the limit of a complete basis set, it is equally possible to compute any ACSF using a sum over neighbor tuples or a contraction of density coefficients, drawing an explicit link between the SOAP power spectrum features, 29,69 Behler-Parrinello symmetry functions, 20,152 the DeepMD framework, 142 and FCHL features 116 . Similar expressions could be derived for higher-order atom-centered symmetry functions, showing the complete equivalence -but dramatically different computational scaling with the number of neighbors -of the two frameworks.</p>
        <p>The previous construction is suitable to represent any rotationally-invariant atomic property. In many circumstances, however, one is interested in representing vector-valued or general tensorial quantities y. In this case, the prescribed transformations that the tensor undergoes under the symmetry operations of the O(3) group (e.g. y( RA) = Ry(A)) have to be incorporated into the atomic representation in the form of covariant, rather than simply invariant, features, so that the representation follows the same transformation as the target property, | RA = R |A . Equivariance (the general concept that indicates symmetry-adapted behavior, encompassing both invariance and covariance) can be enforced by comparing environments and defining the local contribution to the target relative to a pre-defined local reference frame, which has been used to build machine-learning models of tensorial properties in molecular systems [153][154][155][156] . A more general approach for achieving this goal consists in endowing the representation with the symmetries of spherical harmonics x|λµ = Y µ λ (x), as well as the desired parity under the action of the inversion operator î, which we associate with a ket |σ such that î |σ = σ |σ . The eigenvalue σ is 1 for polar tensors, and -1 for pseudotensors. Features that transform as |σ ⊗ |λµ can be achieved by including two additional fields 109,157 within the symmetrized tensor product of Eq. ( 19), i.e.,</p>
        <p>The operation is depicted in Fig. 7, showing how the λµ ket corresponds to the evaluation of a set of spherical harmonics that anchors the atom-centered density to a reference frame. The scalar and rotationallyinvariant case is recovered by taking |σ; λµ = |1; 00 . This construction represents a particularly convenient framework to target the prediction of any Cartesian tensor y in terms of its irreducible spherical components, 158 namely y σλ µ , that transform under rotation and inversion as</p>
        <p>Within a linear regression model, they can be written as the combination of equivariant representations of the proper order λ and parity σ with a set of rotationally-invariant weights Q|y; σ; λ :</p>
        <p>Each irreducible spherical component of y gives rise to a separate equivariant model, and the appropriate transformation rules are ensured by the fact that each equivariant feature Q|A; ρ ⊗ν i ; σ; λµ; separately transforms as the spherical harmonics |lm and the parity function |σ . Much like the case of invariant symmetrized fields features, Eq. ( 38) can be most effectively computed by first expanding the atomcentered field on a basis of spherical harmonics, and is equivalent to an equivariant extension of the atomic cluster expansion 150 or the moment tensor potentials, that are usually evaluated in the g → δ limit.</p>
        <p>A concrete example of these features is given by the density coefficients themselves: in fact, one can see that the ν = 1 equivariant reads simply</p>
        <p>Note how in the bra-ket notation the (λ, µ) indices on the two sides of this equation carry a different meaning. When used in the bra of the local density expansion nλµ|ρ i , they identify one of many components that are translationally invariant, but are not required to be rotationally equivariant; there is no explicit link to their behavior under rotation, and one could build a model by selecting only some of the µ values for a given (n, λ). When used in the ket of an equivariant feature n|ρ ⊗1 i ; σ; λµ , they label groups of features that should be taken together, because they transform in a specific way under the symmetries of the O(3) group. By using n|ρ ⊗1 i ; σ; λµ features in Eq. ( 40) one obtains a model that fulfills (39) (with the caveat that pseudotensors cannot be described by ν = 1 features) because acting on the spherical harmonics with R yields a product with the associated Wigner matrix</p>
        <p>The same covariant property applies to all densitycorrelation features,</p>
        <p>(43) Scalar products of these equivariant features generate matrix-valued kernels, that are suitable for symmetry-adapted Gaussian process regression -for example λ-SOAP kernels 159,160 . Each entry in the kernel describes the coupling between the µ channels associated with the two environments,</p>
        <p>The symmetry properties of the features translate into the a kernel that transforms under rotations of the environments as</p>
        <p>which generalizes the covariant property for kernels introduced by Glielmo et al. for the case of Cartesian vectors. 161 The fact that equivariant features of the form (38) follow O(3) transformation rules means that they can be combined using established relationships in the quantum theory of angular momentum. In particular, the coupled-basis representation used in the definition of Eqs. (28-32) can be formulated for an arbitrary value of ν, and in this form it is possible to express succintly 149 a recursive formula to evaluate |ρ ⊗ν i ; σ; λµ based on lower order terms:</p>
        <p>For ν = 2, the recursion yields the original expression for λ-SOAP equivariants 159</p>
        <p>Similar recursive expressions have been independently proposed to efficiently compute invariant features 127,134 , that can be obtained by taking |σ; λµ = |1; 00 in Eq. (46). The possibility of combining equivariant features using angular momentum rules is also exploited in the construction of covariant neural networks 144,162 One can also build models that are imbued with the appropriate transformation properties in an indirect fashion, by learning atom-centered scalars and combining them with the atomic positions to evaluate formal (or actual) molecular multipoles. This is easily seen for the case of the dipole moment of a neutral molecule, that can be computed as</p>
        <p>Models of this form have been used since the early days of the construction of molecular potential and dipole moment surfaces 62,164 , combined with neuralnetwork potentials to compute IR spectra in the condensed phases 165 , and more recently combined with tensorial models, to describe the interplay of atomic charges and polarization contributing to the total dipole moment 166 . Assigning constant formal charges q i to atoms has also been used to derive covariant kernels, in the so-called operator machine learning framework 167 , which is also similar in spirit to the tensorial embedded atom neural network 168 . The gist of the idea (although expressed in a feature rather than kernel (or NN) language) is that one can define a translationally-invariant representation that depends formally on an applied electric field, e.g.</p>
        <p>Deriving with respect to one of the components of E brings a dependency on the corresponding component of r ji , that upon rotational averaging (keeping in mind that R acts on atomic coordinates and not on the external field) plays the same role as |λµ in Eq. ( 38), providing a basis of features that can be used to learn vectors covariantly. The use of local interatomic vectors to build a covariant reference system is similar to the approach adopted in Ref. 169 to define a general atomic neighborhood fingerprint, and in Ref. 170 to learn the position of electronic Wannier centers. Despite the superficial similarity with the environmentdependent point-charge model of Eq. ( 48), this scheme more closely resembles a framework based on atomic dipoles, since its predictions can be decomposed as a sum of atom-centered equivariant terms.</p>
        <p>Introducing a cutoff in the definition of the local density is not only necessary to reduce the cost of evaluating the expansion coefficients, or the number of terms that have to be included to obtain a converged expansion of the density correlations. Increasing the range of the environment makes the model more complex, which often results in slower learning when limited training data is available. 30 The problem is particularly evident when studying systems with a prominent electrostatic component 159,173 , but long-range physics is ubiquitous 174 , and ultimately limits the accuracy and transferability of machinelearning models 24,166,173 . One pragmatic solution is to build models that explicitly incorporate a physicallymotivated functional form as a baseline, which could take the form of an existing model 175,176 , an electrostatic scheme based on machine-learned partial charges 165,177,178 or atomic multipoles 154,179 . Alternatively, one may attempt to construct representations that are multi-scale in nature, and are therefore suitable to describe, in a data-driven manner, properties that depend on multiple length scales. This idea has been implemented by combining local representations with different cutoffs 30 , scaling atomic contributions according to distance 116,125 (see also Section VIII C), treating separately intra-and intermolecular correlations 180,181 , as well as by building global structural representations based on an intrinsically multi-scale wavelet scattering transform 182 .</p>
        <p>A recently-proposed, more radical take to the problem, extends the symmetrized atomic field construction beyond the use of the atomic density as the starting point. In order to describe more naturally the long-range behavior that is typical of electrostatic interactions, it defines a Coulomb-like potential field based on the smoothed atomic density (Figure 8) This is a global operation, which can however be performed efficiently by transforming the density in plane waves, using one of the many different schemes that are routinely used to model electrostatics. Symmetrizing |V in the same way as for |ρ leads to an atomcentered potential, expanding it on an orthogonal basis of radial functions and spherical harmonics to obtain anlm|V i . One can then build features that are fully equivariant by averaging |V i over the symmetry operations of the O(3) group, leading to ν-point correlations analogous to those discussed above. Furthermore, one can combine local and long-range fields, as in Figure 10, constructing a family of multiscale long-distance equivariants (LODE) features 163 , that in the most general form can be written as</p>
        <p>The simplest multi-scale representation |ρ i ⊗ V i can be linked to physics-based models using an atomcentered multipole expansion of electrostatic interactions, as we discuss further in Section V C, but are effective to learn a multitude of long-ranged interactions, from permanent electrostatics, to polarization and dispersion. When trying to represent long-range interactions between molecular fragments, a model based on local |ρ ⊗2 i features produces a completely unphysical behavior, with the interaction reaching a plateau when the molecules are separated by more than the cutoff distance (Figure 9). Multi-scale LODE features, instead, can describe the asymptotic tail even when using a 3 Å cutoff in the definition of the atom-centered environments, and are capable of repre-senting interactions of very different chemical nature. Using a non-local field as the starting point of the symmetrization procedure provides interesting opportunities to incorporate long-range, many-body interactions in atomistic machine learning.</p>
        <p>Even though this review focuses on the problem of representing atomic structures in terms of a vector of features, one cannot ignore the intimate connection between the choice of features and how they are used to construct models of symmetric properties, such as site energies, which are then used in the context of regression schemes. 10,21,81,95,126,134,144,159,163,[183][184][185] The purpose of this section is therefore to discuss the interplay between representations and models. Given a set of symmetric features q|A i of an atomic environment A i , we explore how to use it to represent a symmetric property y(A i ). We discuss linear approximations,</p>
        <p>and show that the family of features we introduced in Section IV lead to natural generalisations of wellestablished models of interactions between atoms and molecules in terms of a body-ordered expansion. These relatively simple models put stringent requirements on the quality of the feature sets. We then go on to review how highly non-linear models may provide more flexibility in describing the relationship between a structure and its properties, and yield satisfactory results even with a rather simple, imperfect choice of features. Here, and in the following, we always understand implicitly that equality in these approximations can only be attained in the limit of an infinite cutoff radius and suitably converged parameterisation.</p>
        <p>An advantage of linear models is that they can often be connected to classical physics-inspired frameworks, and bring to light physical-chemical insights on the nature of the underlying representations An example of this connection involves the construction of interatomic potentials in terms of a body-ordered hierarchy of atom-centered energy terms</p>
        <p>in which each term can be written as a sum over ν neighbors of the central atom</p>
        <p>This kind of expansion underlies the vast majority of empirical force fields, that are customarily written as a combination of pair potentials, and short-range 2, 3, and 4-body bonded terms. Most potentials truncate this expansion at bodyorder three, i.e. ν = 2 -a notable exception being the dihedral angle potentials used in force fields, that are four-body but involve selected groups of atoms rather than a sum over all possible triplets. This is because the cost of a naive evaluation of the sum j1&lt;•••&lt;jν scales exponentially with the body order ν, i.e. as O(N ν i ) for an environment containing N i atoms. More sophisticated ways of symmetrizing the body-ordered terms, such as those discussed in Refs. 186 and 95, alleviate this behavior. In the following paragraphs we demonstrate, in particular, how this exponential scaling can be overcome by using the density correlation representations discussed in Section IV.</p>
        <p>The three-body case. It is illuminating to first discuss in full detail the representation of a 3-body site potential, written traditionally in internal coordinates, in the form (56) where ω ijj := rji • rj i . In order to connect to the atomic density correlations we first rewrite this as</p>
        <p>adding and subtracting a self-interaction from the 3body term. Approximating u (2) (r) in terms of a radial basis r|n ≡ R n (r) yields</p>
        <p>where |δ i is the g → δ limit of the atom-centered density |ρ i . As in Eq. ( 4), the use of the Dirac notation to express the pair potential highlights the fact that (atom-centered) properties can be seen as a type of representation, and that in this sense a linear model is nothing but an expansion in a discrete basis of u (2) |r ≡ u (2) (r).</p>
        <p>For the three-body term we revisit (22): first, we approximate u (3) in terms of the radial basis r|n ≡ R n (r) and the Legendre polynomials ω|l ≡ P l (ω), u (3) (r ji , r j i , ω ijj ) ≈ nn l u (3) |nn l n|r ji n |r j i l|ω ijj . (59) Applying Legendre's addition theorem to expand the P l in terms of spherical harmonics r|lm ≡ Y m l (r),</p>
        <p>absorbing the 4π 2l+1 into the weights u (3) nn l and reordering the summation yields</p>
        <p>Finally, we sum over all (j, j ) and reorder the summation to arrive at</p>
        <p>In summary, we have written an arbitrary 3-body site potential in terms of 1-and 2-correlations of the atomic density,</p>
        <p>Aside from connecting classical body-ordered interatomic potentials and ν-correlations of the atomic density this formulation has significant advantages in terms of computational complexity which we discuss below after generalising the argument to arbitrary body-order.</p>
        <p>General (ν + 1)-body order potentials. The systematic expansion to arbitrary body orders has been applied to the description of alloys in terms of a cluster expansion, a procedure that was very early shown to provide a complete description of the problem 90 , to the rationalization of fragment-based electronic structure methods 187 , and to the construction of last-generation potentials for water and aqueous systems 175 .</p>
        <p>We adopt the generalisation of ( 57) that includes self-interaction,</p>
        <p>which can be obtained from the more natural formulation ( 55) by incorporating the self-interaction terms into the ν-body-order energy similarly to Eq. ( 57).</p>
        <p>To connect (63) to the density correlations we represent the rotationally invariant (ν + 1)-body function u (ν+1) as</p>
        <p>where we use Q as a shorthand for (x 1 ; . . . x ν ), so that Q|r j1i , . . . ,</p>
        <p>The rotation can be made to act on the atomic positions or on the basis, depending on convenience. The (ν + 1)-order site energy is obtained by summing over clusters of neighbors</p>
        <p>The symmetrized sum can be reordered to show that it corresponds to the ν-point density correlation</p>
        <p>which is precisely Eq. ( 20) written in the g → δ limit. Thus we have explicitly represented E (ν+1) in terms of the symmetry-adapted density correlations. We emphasize again that this calculation required the inclusion of the self-interactions as the starting point (63) -even though, if one wishes so, they can be removed from the final result 188 .</p>
        <p>Linear completeness. For a practical implementation we can choose a finite, discrete basis, approximating E (ν+1) as</p>
        <p>Any complete implementation of ν-order density correlation features 126,127,134,149 provides a basis to expand u (ν+1) and approximate the (ν + 1)-order term, that contributes to the body-ordered expansion of E(A). The foregoing discussion shows that these bases are complete in the following sense. An (infinite) collection of symmetrized features { q|A i } q∈q total is a complete linear basis if there exists a sequence of finite subsets q ⊂ q total such that</p>
        <p>i.e. y q approximates y to within arbitrary accuracy in the limit as the number of features tends to infinity. We stress here that the weights y|q depend on the entire choice of feature set q and not just the single index q. Therefore the density correlation features provide a universal, complete linear basis to approximate body-ordered potentials and, more generally, body-ordered expansions of properties that can be meaningfully written as a sum of atom-centered contributions.</p>
        <p>For the specific choice</p>
        <p>Eq. ( 67) is the ACE model 126,127 . Note that the symmetrized correlations q|δ ⊗ν i can be efficiently and conveniently evaluated as already hinted at in Section IV E. Since MTPs provide an alternative basis set for the same space, they are complete as well, and in the same sense. We also emphasize that a rigorous proof of completeness of MTPs was already given by Shapeev 134 , and the essence of the idea can be traced back to the cluster expansion theory of alloys 90 . The "density trick", i.e., expanding in terms of the density correlations, ensures linear scaling in terms of the number of neighbors N i rather than the Ni ν scaling of the naive representation (55), which enables modeling very high body-orders. A recursive evaluation of the ν-correlations implemented by the MTP and ACE bases, or by the NICE formalism, avoids an unfavorable scaling of the evaluation of the high-order terms (see Section VIII D for a summary of these techniques).</p>
        <p>B. Density smearing.</p>
        <p>The real-space view of the density correlation features may be more intuitive when considering finite smearing of the atomic contributions to |ρ i , that gives rise to a smooth function that can be seen as a proxy for the electronic density, and is reminiscent of the atoms-in-molecules 189 description of the electronic structure of a molecule or a condensed-phase system as a collection of atom-centered densities. In the literature using SOAP features, the width of the atom-centrered Gaussians has been often indicated as a hyperparameter with an important influence on the robustness 190 and accuracy 191,192 of the resulting machine-learning models. Since we derived the link between density correlations and body-ordered potentials, and in particular the proof of the completeness of the linear expansion, only in the limit of a sharp density we now discuss whether a similar formal guarantee holds for a general |ρ i , admitting in particular smearing of the atomic contributions. With tensorproduct bases, all statements derived for higher correlation orders can eventually be reduced to a onedimensional description, that is sufficient to reveal the essential features of the problem. Note that the following discussion provides only theoretical guarantees; we explain below that excessive smearing creates severe numerical ill-conditioning which must be carefully considered in practical implementations.</p>
        <p>We begin by noting that the expansion of a smeared density in a basis x|n is identical to the expansion of a δ-like density in the corresponding smeared (a.k.a. mollified) basis x|n; g ≡ dx n|x g(x -x ):</p>
        <p>With this observation in hand showing that x|n; g inherits completeness from x|n is sufficient to ensure that all our results apply also to smeared densities.</p>
        <p>We first consider the case of standard monomials. Any continuous function f (x) can be expanded to within arbitrary accuracy into polynomials x n :</p>
        <p>We want to check whether we can also represent f in terms of smeared polynomials,</p>
        <p>For the particular choice of Gaussian smearing we can evaluate this expression explicitly and obtain</p>
        <p>i.e., p g n is in fact still a polynomial with leading-order term x n and this means it forms a basis. In particular we can now again represent f nmax (x) exactly as</p>
        <p>And in the limit n max → ∞ we recover f . In the more general case, suppose that we have an arbitrary complete basis x|j . Then we can approximate x n ≈ j b nj x|j . The smearing operator g * • is bounded, which allows us to write</p>
        <p>Given that p g n are dense, it follows that also the smeared basis functions x|j; g ≡ g * x|j are dense. From these arguments it is reasonable to conclude that the smeared density correlations also form a complete linear basis.</p>
        <p>As already mentioned above, this is a purely theoretical statement, and there is an important caveat: The inverse of the smearing operator is unbounded, which implies that the coefficients of the expansion of f in terms of the smoothed polynomial basis necessarily blow up when the size of the basis is increased, even if f has a stable expansion in a polynomial basis. Therefore, in practice, the smoothing of the density, the truncation of the basis, and the regularisation of the regression, must be carefully coordinated and adapted to the natural scale of the variations of the target function f , i.e. to its "natural" smoothness. Failure to do so may result in a representation that has insufficient resolution to describe the response of the target property to structural deformations, or vice versa to one that contains redundant information and is prone to overfitting.</p>
        <p>A similar formal correspondence with wellestablished functional forms of physical interactions can be derived when using (scalar) multiscale LODE features (52) within an additive, linear learning model, using as target the electrostatic energy U (A),</p>
        <p>(76) The fact that the representation is linear both in the density and in the potential fields allows one to derive rigorous asymptotic relationships for the interaction between two distant portions of the system, that resemble the electrostatic interactions between the multipoles of a localized charge density distribution and any other charge that is located arbitrarily far away. 163 Focusing only on the long-range contribution U &gt; to U (A i ), that is associated with the part of |A; V i generated by the far-field density, |A; V &gt; i , one can write</p>
        <p>In this expression, in which the reader can recognize the similarity with the multipole expansion of the electrostatic potential, 158 |ρ &gt; i indicates the atom density outside the cutoff, which is not computed explicitly but is encoded in the expansion of the local atomic potential (50). The coefficients lm|M &lt; i (U ) can be written as a combination of the regression weights r 1 r 2 l|U and the local density coefficients rlm|ρ &lt; i , and can be interpreted as adaptive multipole coefficients that depend in a general manner on the atomic distribution within the environment.</p>
        <p>Given that the atomic densities and potentials are not the physical charge density and electrostatic potential of the system, it is the role of the regression procedure to modulate the multipoles so as to reproduce the reference data for the electrostatic energy. In Fig. 11 we report an example where this is demonstrated by extrapolating the long-range interaction between a pair of rigid H 2 O and CO 2 molecules, upon training the multiscale LODE model on the longrange, yet not asymptotic, interaction profiles associated with 33 different reciprocal orientations of the two molecules. The figure compares the asymptotic extrapolation performance upon centering the representation on different atoms, as well as by truncating the angular expansion at different l max . It is apparent that the angular cutoff chosen reflects the number of multipoles introduced in the expansion of Eq. ( 77) and thus determines sharp crossovers of the prediction accuracy across critical l max values. For instance, a model that uses only features centered on the oxygen atom of H 2 O improves dramatically its performance when l max is increased from zero to one. A model using the carbon atom of CO 2 as the only environment shows a similar, sharp improvement in accuracy when going from l max = 1 to l max = 2. This is consistent with the primarily dipolar nature of the electrostatic field generated by a water molecule, and with the quadrupolar nature of the center-symmetric carbon dioxide. Even though this example showcases the link between a linear model based on |ρ i ⊗ V i and multipole electrostatics, the representation is sufficiently flexible to describe also other kinds of interactions, as demonstrated in Figure 9.</p>
        <p>Historically, linear representations used basis sets in internal coordinates (typically interatomic distances or simple transformations of them) that exploded in size with body order, see e.g. Refs. 87,186, and with exponential scaling in their computational cost of prediction due to the need to sum over all ν-clusters in a configuration or atomic environment. Moreover, it is clear that high body orders would be needed to obtain the desired accuracy, especially for models of materials. About a decade ago, nonlinear fits using low body order (ν = 2) descriptors appeared, with the surprising result that a few hundred degrees of freedom were enough to get good potentials 10,12 . Contrary to linear modeling where the symmetry-adapted features q|A i are used as a basis, in the context of non-linear regression they are best thought of as a coordinate transformation. In a linear setting the choice of a basis, and the details of the implementation, are a matter of computational performance but can be converged to a well-defined, basis-set independent limit. When taken as the input of a non-linear model, instead, the entries of the feature vector must always be precisely defined, because there is no complete basis set limit in which the models become equivalent. To emphasize that many of the formal manipulations that are possible in a linear context take on a different meaning when features are used for a non-linear model, we abandon the Dirac notation and indicate as ξ(A i ) the feature vector that describes the atom-centred environment A i , whose components are ξ q (A i ) = q|A i . If y(A i ) is a symmetric property such as a site energy, we aim to construct approximations of the general form</p>
        <p>The two most commonly used models for ỹ are artificial neural networks 20,21,35,36,107,152,165,184,193,194 (ANN) and kernel ridge regression 22,23,30,34,116,179,185,191,195,196 (KRR) models. In KRR models, 197 one builds a kernel matrix K with elements</p>
        <p>which provides a similarity measure between the environments A i and A j , measured in terms of the similarity between the corresponding feature vectors ξ(A i ) and ξ(A j ). Useful kernel functions, k, are nonlinear, e.g. polynomials, Gaussians, etc. 198 . The kernel inherits the symmetry of the feature vectors, and therefore a model for a symmetry-invariant property y(A i ) can be obtained as</p>
        <p>where, in the simplest setting, the M j are scattered interpolation points, but more generally are simply a collection of "centers" which induce a basis {k(•, ξ(M j ))} j in the symmetrized feature space.</p>
        <p>The weights b j are then obtained by a linear regression. Kernel models have two main advantages over "naive" linear regression using the same features. (1) They introduce implicitly a non-linear mapping between the inputs and a "reproducing kernel Hilbert space" |A i → |A i ; k , which has a larger (often infinite) dimensionality, allowing for a more flexible approximation of y(A i ). ( 2) Given that the basis is centered on the training points, it is adapted to the geometry of the data set in feature space. For example, if the centers |A i ; k in feature space fall on (or close to) a low-dimensional manifold then the KRR model naturally exploits this. For a comprehensive discussion of the use of kernel methods in atomistic modeling, see Ref. 33. In the context of body ordered features discussed above, the non-linearity in the kernel effectively increases the body order of the features used in the regression model, but in a rather special way: only those high body order terms are present that can be obtained as functions of low body order features. See Section VI on completeness for a more detailed discussion. While nonlinear models are by their very nature more flexible in representing complex highdimensional features, linear models come with different advantages. As we have shown in Section V A and Section V C, they tend to be more easily "interpretable", e.g. in terms of a body-ordered expansion of the target properties, or in terms of physicallymotivated asymptotic forms of the interactions. But are nonlinear models necessary to achieve high accuracy? This notion is challenged by the SNAP, 147,148 , the MTP, 134 , the ACE 126 and the NICE 149 representations: the "density trick" and its generalisations to higher body-orders, replacing polynomials with correlations of the atom-centered density, circumvents both the explicit symmetrization as well as the summation of all ν-clusters of traditional body ordered expansions. Particularly when using density correlations above ν = 2, it is critical to fully exploit the computational cost gains offered by permutation symmetric properties. Even if one were to initially specify a model in terms of the "natural" body-order expansion (55), one should convert it for computationally efficient evaluation to one of the many representation built in terms of ν-correlations. By employing the recursive evaluations introduced in Refs. 127,134,149, this transformation makes it possible to truncate at very high body-orders without significant penalty in computational cost, as discussed in more detail in Section VIII D. As an illustration of how a linear fit based on high-quality density-correlation representations can compete with non-linear models we show in Fig. 12 the learning curves resulting from the regression of the atomization energy for a very large and geometrically diverse database of CH 4 configura-tions (generated by randomly displacing the H atoms around the central carbon, in a sphere with a radius of 3.5 Å). The plot reflects a tradeoff between model complexity and the availability of training data. Saturation of the learning curves indicates that the model does not have sufficient flexibility to describe fully the underlying structure-property relations. 30,123 Thus, linear models based on NICE features incorporating higher and higher body order are capable of describing the structure-property relations to a higher degree of accuracy, which is apparent in the delayed saturation of the learning curve. One sees that a ν = 4 model starts saturating around n train = 10 5 , even though the system is composed of 5 atoms, and so the body-ordered expansion should be fully converged. This is because a linear model requires a complete basis, while here we select only a few 1000s invariants at each body order. A NN model can be designed to be more flexible and beat this saturation, at the expense, however, of performance in the small data set limitwhich, in a more chemically and structurally diverse regression exercise, usually translates to poorer transferability.</p>
        <p>Suppose we are given a finite collection of symmetry adapted features ξ(A) = { q|A } q which we wish to use as a descriptor for atomic structures or environments, for example symmetrized correlations of the density as described in the foregoing sections. In Section V we discussed two classes of models built from such equivariant features: linear models,</p>
        <p>for which the representation ξ(A) plays the role of a basis to expand the target property; and nonlinear models,</p>
        <p>where the representation plays the role of a coordinate transformation generating a finite-dimensional feature vector used as the argument of a non-linear function ỹ. In order to guarantee systematic convergence of these models to an arbitrary target, in suitable limits, we require that the employed set of features is complete. We already hinted in Section V D that these two scenarios lead to different requirements on the notion of completeness. In this section we provide a more in-depth discussion of the completeness issue in the nonlinear setting, and point out open problems.</p>
        <p>Recall from Section V D that for linear models the correct notion of completeness is the well-known and well-understood concept of a complete (linear) basis from linear algebra. In the context of a nonlinear model ỹ(ξ(A)) it is instructive to think of ỹ as a universal approximator in feature space (e.g., an ANN, GP, etc). We then ask the question whether (in a suitable limit) the model can represent an arbitrary symmetric property y(A), i.e., whether</p>
        <p>is achievable. This is the case if and only if the mapping A → ξ(A) is injective: this means that any two atomic configurations that are not related by symmetry are mapped to different descriptors. In particular knowledge of ξ would then enable us in principle to reconstruct the configuration A. When this is the case, we say that the descriptor ξ is geometrically complete.</p>
        <p>The ideal goal would be to have complete finite feature sets, that allow to approximate any symmetric function of the coordinates to arbitrary accuracy. As an elementary introduction to how such a construction might be achieved in principle, we consider a collection of N particles in 1D, {x i } N i=1 . As a concrete example, one can take two particles with positions (x 1 , x 2 ). In the absence of an angular component, we only need to consider the projection of the density ρ(x) = i δ(xx i ) onto the monomial basis x n :</p>
        <p>, etc. In this simple setting, one sees easily how the ν-point density correlations form a basis of symmetric polynomials</p>
        <p>which is complete (in the sense of a linear basis) because it contains all possible symmetrized monomials. In analogy to what we did in Section V A, we use the "self-interaction" formulation in which the sum extends over all the tuples of particle indices.</p>
        <p>For the case of two particles, linear combinations of</p>
        <p>2 are sufficient to write any symmetric polynomial of the particle positions.</p>
        <p>Thus, if we allow for algebraic operations on the n|ρ , it is clear that the ν = 1 coefficients provide a sufficient basis, because the elements of the linear basis (85) can be obtained as a product, e.g. n 1 n 2 |ρ ⊗2 = n 1 |ρ n 2 |ρ . In fact, well-established results from the theory of symmetric polynomials 199 allow making an even stronger statement. The first N power sum polynomials ( n|ρ ) N n=1 provide an algebraically-complete basis to write any symmetric polynomial function of the coordinates of N particles. For instance, for N = 2 we can express the n = 3 term as a polynomial of 1|ρ and 2|ρ</p>
        <p>This result implies, in general, that the mapping</p>
        <p>is injective: knowledge of the first N features n|ρ allows us to uniquely reconstruct the configuration (but not the index of the atoms). That is, this minimal feature set ξ(A) is indeed geometrically complete. It is not too difficult to construct similar complete and finite feature sets for finitely many particles in two and three dimensions as long as only permutational symmetry is considered. However, incorporating also rotational symmetry into the equivalence of particle configurations makes this much more challenging as we discuss next.</p>
        <p>In general, for three-dimensional atom configurations it is clear that taking all ν-correlations provides a complete set of features (after all, they are even complete in the sense of forming a complete linear basis), however, as we explained at the beginning of Sec. VI, this is not a practically useful property when considering nonlinear regression schemes. As we explain next, it remains an open problem how to construct a minimal complete feature set in this general setting.</p>
        <p>It is clear just based on dimensionality arguments that a descriptor that has fewer than 3N -6 components (the number of elements in the Cartesian position vectors, subtracting the degrees of freedom associated to translations and rotations) cannot be complete for N particles. On the other hand, the descriptors based on ν-point correlations have a number of components that scales with N ν . But having more than the necessary minimum number of components does not ensure that a descriptor is complete.</p>
        <p>Although it was appreciated for a long time that symmetrized two-correlations for entire structures are not complete, i.e. knowing the set of distances between points is not enough to reconstruct the point set 29,122,200 , it was not until recently that the connection to environment descriptors was made 124 . The fact that degenerate pairs of inequivalent environments mapping to the same descriptor exist for twocorrelation (distance-angle) representations came as a surprise because so many "successful" models for potential energy surfaces have been published based on such descriptors in the past decade 14,33 . An example of such "degenerate pair" is given in Figure 13.</p>
        <p>The construction involves an environment with four neighbors on the unit circle, with the two structures corresponding to the labels (1, 2, 3, 4) and (1, 2, 3, 4 ) being different, but having the same unordered list of distances and angles. The total number of degrees of freedom for this layout is three (because one neighbor can be fixed on the x axis), and there is one degree of freedom in the construction of the degenerate pair (the angle labelled α in Fig. 13). Thus, this manifold of pairs of degenerate configurations has a codimension of two, i.e. it has a dimensionality that involves two fewer degrees of freedom than the total. A more general construction, that yields a family of 3D degenerate pairs including an arbitrary number of neighbors, is discussed in Ref. 124. The fact that the degenerate pairs form a manifold does not mean that there is a degenerate manifold, i.e. a manifold of configurations all mapping to the same descriptor. This type of degeneracy occurs between pairs of configurations which are typically far from one another, and so this degeneracy problem differs from that of assessing the sensitivity of a representation to small atomic displacements 201,202 . As was shown in Ref. 124, in order to break this degeneracy, the correlation order has to be increased. Three-correlations ( |ρ ⊗3 i , equivalent to the unordered set of central tetrahedra, and the bispectrum of the atomic density) indeed distinguish environments such as those in Fig. 13. It is however possible to build pairs of environments, composed of 7 or more neighbors, which are distinct but have the same threecorrelations. This example, also discussed in Ref. 124, raises a number of open mathematical questions: (i) is the ν = 3 descriptor complete for N &lt; 7 neigh-bors, (ii) are all ν-correlations degenerate for sufficiently many neighbors, (iii) what is the codimension of the manifold of degenerate configurations for ν &gt; 2?</p>
        <p>The concept of completeness applies both to representing entire structures and to atomic environments, but the relationship between these two cases is subtle. Given an entire structure, it can be considered to be the "environment" of the point at the origin, and the same symmetries apply. However, specific representations appear differently in the two views. For example, the ν = 2 correlations around a central atom contain information on the full set of interparticle distances between the neighbors, and so any pair of environments that are degenerate in terms of |ρ ⊗2 i is also (removing the particle at the origin) a pair of structures with a degenerate description in terms of distances. 203 Note that the problem of completeness for entire structures is exactly the same as the problem of reconstructing point sets 122 .</p>
        <p>One way to break the degeneracy between the representations of two entire structures involves combining information on different environments. For instance, one can describe the entire structure using an additive combination of atom-centered features analogous to Eq. ( 17). Following the above reasoning, a pair of environments that are degenerate in terms of the list of distances and angles are also (removing the central atom) structures that are degenerate in terms of the list of distances. However, these structures are not necessarily degenerate in terms of the combined list of distance and angle histograms of each local environment. Thus, taking non-linear transformations of atom-centered features cannot resolve the environment-level degeneracies, but can provide a way to differentiate entire structures. 124 The construction of injective yet concise representations for environments and structures is still an open problem, whose solution may help to improve the accuracy and computational efficiency of machine-learning models.</p>
        <p>Note that in this discussion we are implicitly taking atomic structures related by symmetry as identical, and we focus on whether the injectivity holds for the domain of the descriptor map being the original atomic structures. The case of whether the same consideration hold for general scalar fields (e.g. those arising in the LODE construction) is a separate problem. For the case of translation symmetry (torus geometry) it is well-known that no finite correlation order suffices to reconstruct all signals 204 , however most signals can be reconstructed already from the bi-spectrum (ν = 3). To the best of our knowledge it is an open problem whether analogous results hold for the case of rotational symmetry of 3D spherical geometry 205,206 . See also Uhrin 207 for an excellent review connecting 3D signal processing and reconstruction of atomic configurations.</p>
        <p>As we explained above the set of all (N -1)correlations is complete for N particles, because it is equivalent to the completeness of polynomial basis sets such as MTP 134 , PIP 186 , aPIP 95 , ACE 126,208 and NICE 149 (see also Section V A). Any of these bases can be expressed in terms of the ν-correlations via a linear transformation, and vice-versa. Even for fixed maximum polynomial degree, these are enormous representations. Depending on how ν-correlation features are chosen their number might scale as rapidly as qmax+ν ν , where q max is the number of one-particle features.</p>
        <p>There is a class of much lower-dimensional descriptor maps based on the eigenspectra of overlap matrices 68,106 that lifts the degeneracy for the known examples, although their actual completeness is unknown. A simplified construction of these "spectral representations" proceeds as follows: First, one constructs an artificial overlap matrix based on the positions of atoms within the i-centered environment A i :</p>
        <p>where t : R → R. Then, one computes the ordered spectrum {τ k } N k=1 of T . If T is invariant (or covariant) {τ k } k is an invariant descriptor of A i . Due to eigenvalue crossings, the mapping A i → {τ k } k is nonsmooth, hence one may wish to project it on a smooth basis, e.g. polynomials,</p>
        <p>The spectral features (or, fingerprints as they are also called 106 ) { n|T } N n=1 correspond to the moments of the histogram of eigenvalues, and contain precisely the same information.</p>
        <p>An alternative way to write n|T is</p>
        <p>which is not computationally more efficient, but highlights the close connection between { n|T } n and the body-ordered features we discussed in previous sections. From (90) we observe that</p>
        <p>(91) and so forth. That is, n|T contains the projection of the histogram of n-simplices onto a single basis function. In other words, for n = 2, the cutoff function f cut and the overlap function t play the role of R n and P l in (37). More in general, n|T describes nneighbors correlations, and so it could be written, in principle, as a linear combination of a complete set of |ρ ⊗n i features. Thus, the n|T provide invariant high body-order features at relatively low computational cost, even though each scalar overlap matrix T contains information on a single feature per body order.</p>
        <p>If one takes t to be scalar (as we have done here) then there are at most N invariant features for N neighbors, but 3N -6 independent coordinates -so that the spectral features (89) must be grossly undercomplete. This source of incompleteness is easily lifted by simply taking multiple overlap matrices with different t functions, or taking t to be matrixvalued, as done in Ref. 106. However, even with that modification in mind, it is not at all understood whether these features are complete or can be made complete with limited modifications. For example it can be shown 127,149 that most high-body order features are actually polynomials of low body-order features, which means that they do not contain genuine high correlation information. This can be observed very easily with a seemingly trivial modification to the spectral representation construction. Consider N particles on the unit-circle at positions r ji , as in Fig. 13. In particular we then have only N -1 independent variables, which means that a scalar t is in principle sufficient to identify the configuration. However, choosing T jj := cos θ ijj it is straightforward to see that the two overlap matrices T for the two configurations of Figure 13 have eigenvalues {0, 0, 1, 3}. That is, this particular choice of spectral descriptor is unable to distinguish them nor any two configurations for different α.</p>
        <p>Even for a general atomic environment, T jj = r ji r j i cos θ ijj is the Gram matrix of the interatomic distance vectors, which has at most three non-zero eigenvalues -and hence the collection ( n|T ) N n=1 contains at most three independent features even though formally, n|T has body-order n. For a configuration in which the neighbors lie on a sphere, this case can be written as an overlap matrix by choosing an appropriate, monotonically decreasing t(r jj ), and for the general case with an appropriate (albeit contrived) choice of f cut and t. The purpose of these examples is to highlight that, although spectral descriptors offer some attractive features such as their computationally cheap high body-order nature, understanding under which conditions they are complete is subtle and requires a much deeper investigation.</p>
        <p>To conclude our discussion of completeness of representations we briefly review and contrast the two key notions of completeness that we introduced and also mention a third concept that we implicitly encountered in Sec. VI A. In the following, let ξ(A) = { q|A } q again denote a finite or infinite collection of equivariant features of a configuration or environment A.</p>
        <p>Complete linear basis: This is the correct notion of completeness of ξ for linear models, q y|q q|A , such as PIPs, aPIPs, MTP, ACE, NICE. It is now well-understood how to systematically generate such a complete linear basis in a variety of different ways. This is the strongest requirement one can make on a feature set.</p>
        <p>Geometric completeness: This is the correct notion of ξ completeness for nonlinear models, ỹ(ξ(A)), i.e., it is the minimal requirement to ensure systematic convergence of such a model. Ensuring only injectivity of the mapping A → ξ(A), means it is a much weaker requirement than being a complete linear basis. We therefore expect that complete feature vectors are generally significantly sparser, which is important for the performance of nonlinear regression schemes. At present, there is no systematic construction of minimal geometrically complete feature sets.</p>
        <p>Algebraic completeness: We say that ξ is algebraically complete if every element of a complete linear basis q|A; ρ ⊗ν i can be written as a polynomial of the entries of ξ, p q (ξ(A i )). This is precisely the concept we used to construct a geometrically complete feature set in the pedagogical example of Sec. VI A. The set of invariants used to construct PIP 186 and aPIP 95 potentials form a minimal algebraically complete descriptor. The concept was also proposed as part of the NICE framework 149 as a mechanism to reduce the size of descriptor set.</p>
        <p>In general, algebraic completeness is strictly stronger than geometric completeness and an algebraically complete feature set will be larger than a minimal geometrically complete one. It is nevertheless an interesting and useful concept: (i) it provides a stepping stone towards a theoretical understanding of geometric completeness; (ii) for the purpose of effective regression schemes it may in fact prove to be more important since it preserves polynomials, while inverting a minimal geometrically complete descriptor is likely to introduce singularities. Indeed, reducing algebraic dependence is a common technique in the signal processing literature. Uhrin 207 reviews those techniques and modifies them for the construction of descriptors with relatively few entries, that can in principle be made complete.</p>
        <p>A mathematical representation of the structure of an atomic configuration is not only useful as the starting point of supervised-learning algorithms, aimed at predicting its energy and properties. It can also be used, in combination with unsupervised learning schemes, to compare structures in search for repeating atomic patterns [210][211][212][213][214][215][216][217][218][219][220][221] , to obtain lowdimensional projections that help visualize complex datasets 1,4,6,[222][223][224][225][226] , and more generally to describe the lie of the land in (free)energy landscapes and interpret structure-property relationships in complex systems 38,[227][228][229][230] . There is a long-standing tradition of developing domain-specific descriptors to use in the automatic analysis of structural data. For instance, simulations of polypeptides have been interpreted in terms of backbone dihedral angles 231 , discrete secondary-structure categories 232,233 , as well as sophisticated continuous fingerprints of secondary structure and backbone chirality 234,235 . Simulations of clusters and condensed-phase systems have often used more general indicators, such as Steinhardt order parameters 236 , cubic harmonics 237,238 , radial distribution functions (either directly 239,240 or in the form of entropy-inspired fingerprints 241 ), histograms of coordination numbers 4 , that can be seen as precursors of the atom-density correlation representations that we discuss in Section IV D. More broadly, generalpurpose descriptors that can be understood, more or less transparently, as a special case of the densitycorrelation features |ρ ⊗ν i have been developed and used in unsupervised-learning contexts as much as in the context of regression models. A few examples include the diffraction-based fingerprints of Ziletti et al. 138 , the local order metric of Martelli et al. 242 , the spectral representations of Sadeghi et al. 68 , the Minkowski structure metric of Mickel et al. 243 (that closely resembles and anticipates the construction of the moment tensor potentials), and the use of SOAP features to analyze materials and molecules 69,70,244 .</p>
        <p>Understanding the way a representation converts the Cartesian coordinates of atoms into features is necessary to make sense of any subsequent analysis, because any explicit or implicit assumption made in the structure-feature map will be reflected in the unsupervised analyses based on those features 245 . An example of this is given in Figure 14, that shows the effect of using rotationally variant or invariant features (respectively, nlm|ρ i and n 1 n 2 l|ρ ⊗2 i ) to analyze a simulation of undercooled iron 209 . Atoms are colored according to a two-dimensional projection describing the associated environments, in this case obtained using a kernel principal component analysis 246 built on the feature vectors ξ(A i ). Using orientationdependent features makes it possible to distinguish more clearly the presence of multiple grains, and would be useful, for instance, to investigate the texture of the nanocrystalline sample, much like one would do with an electron backscattering diffraction analysis. Using invariant features highlights that all nanocrystals have the same structure, and makes it possible to recognize the disordered environments at the grain boundaries. This kind of analysis can also be used to elucidate the properties of different representations, investigating the effect of different choices on the unsupervised analysis of a well-understood system to better appreciate the relation between struc-ture and features.</p>
        <p>In this Section we summarize recent developments, and identify clear insights, related to the use of representations to determine the similarity between structures, to perform clustering and dimensionality reductions analyses, and to build models that go beyond the injective structure-property map that we have used this far.</p>
        <p>Before delving into the use of structural representations to visualize and classify atomic configurations, let us recall the link between feature vectors ξ(A i ), that are associated to structures or environments, and distances or kernels, that express the relationship between two of these entities. For example, given a feature vector ξ, it is possible to define a distance using e.g. a Euclidean metric,</p>
        <p>The opposite is also true: for a given set of configurations M , and any (negative definite) distance or (positive definite) kernel 247 it is possible to construct a set of features that generate the kernel by taking their scalar product -a practical implementation of the concept of reproducing kernel Hilbert space that underlies kernel methods. One only needs to construct the kernel matrix K ij = k(M i , M j ), and find its eigenvalues and eigenvectors Ku (j) = λ j u (j) . It is easy to see that the scalar product between the reproducing features</p>
        <p>computed for two members of the reference dataset yields exactly the value of the kernel function between the two configurations. 246 It is also possible to define a kernel-induced distance</p>
        <p>Even though different techniques may be formulated more naturally in terms of features, distances or kernels, it is always possible to translate -at least approximately -one description into another.</p>
        <p>Most unsupervised learning algorithms rely on the definition of a metric to tell apart structures depending on their similarity. A metric that is capable of identifying identical structures is extremely useful in all the applications that aim at automating the search of materials or molecules with desirable properties [248][249][250][251][252] . This is not an entirely trivial task: in molecular searches, a mismatch in the simple ordering of atomic indices can lead to the failure of metrics based on the alignment of conformers, such as the root mean square distance (RMSD), and the exact calculation of a permutation invariant version would involve combinatorially increasing computational effort. 68 . In the case of condensed phases, one needs to deal with the problem that the same periodic structure can be described by different choices of unit cell size and orientation. The requirements for a metric to compare atomic structures are similar to those discussed in Section III, and have been discussed in great detail in Ref. 68: a good metric needs to be invariant to rotations, translations, and permutations 253 , and still be capable of telling distinct structures apart 106 . The comparison between the resolving power of different metrics has been often determined using distancedistance correlation maps 68,69,124,202 , such as those shown in Figure 15, that compare the distance between pairs of structures in a reference dataset, as computed by two metrics. In the most extreme case, one observes pairs structures that are identical based on a metric, and distinct based on another -indicating the presence of a manifold of degenerate structures that are distinct, but cannot be told apart by one of the distances 124 .</p>
        <p>An important aspect when defining a metric for structural comparison is the fact one is often interested in measuring the dissimilarity between entire structures, d(A, A ). Most of the representations we discussed this far are designed to compare atomcentered environments, and therefore yield d(A i , A i ). As a practical example, we define d as the Euclidean distance between the feature vectors,</p>
        <p>Different ways of combining atom-centered representations to obtain a structure-level comparison are discussed and benchmarked in Ref. 69, using a construction based on the definition of global kernels. Here we present the same strategies, but express them directly in terms of distances. The two formulations are equivalent when using the kernel-induced distance.</p>
        <p>The simplest global distance can be defined as a mean over all environment pairs,</p>
        <p>Using the abstract notation |A i rather than ξ(A i ) to highlight the connection with the definition of the global representation |A; ρ ⊗2 as the sum of environmental |A; ρ i (see Section IV C) it is easy to see that</p>
        <p>i.e. that the average environment distance d2 (A, A ) can be computed by taking the Euclidean distance between the mean of the environment's features in the two structures. This construction is very natural, and consistent with an additive decomposition of properties in a regression model, but potentially lacks resolving power: two structures with very different environments could end up having a similar value of the average feature vector.</p>
        <p>An alternative way to determine a global metric involves finding the best match between the environments of the two structures, defining d2 (A, A ) = argmin</p>
        <p>where U N A ×N A is the set of N A × N A doublystochastic matrices, i.e. matrices with positive entries such that sums of rows and columns all equal 1/N A and 1/N A respectively. When N A = N A , the optimal P contains only zeros and 1/N A , and the problem can be construed as a linear assignment problem, and solved in O(N 3 A ) time using the Hungarian algorithm 257 . Much like the case of the use of sorted interatomic distances as a structural representation (Section III B), the process of matching entries in the environment distance matrix introduces discontinuities in the derivatives of the distance metric. One can solve this problem, obtaining at the same time a scheme with a cost that scales as O(N 2 A ) and that can be applied to the comparison of structures of different sizes, by introducing an entropy regularization in Eq. ( 97)</p>
        <p>controlled by the magnitude of the parameter γ. This approach was introduced in Ref. 258 for the general problem of solving optimal transport problems and of evaluating the Wasserstein distance between probability distributions, and was first applied in Ref. 69 to atomistic problems in terms of regularized entropy match (REMatch) kernels. By introducing a nonadditive combination of the environments, REMatch kernels and the associated distances offer an increased resolving power compared to the plain average distance (95), as demonstrated in Figure 16. The figure also shows that Eq. ( 98) interpolates between the average and the best-match metrics, to which it tends respectively for γ → ∞ and γ → 0.</p>
        <p>As stressed in the introduction of this Section, in performing cluster analysis or dimensionality reduction, the choice of featurization is not a neutral one, but introduces a bias that will be visible in the end result of the analysis. 245 . While sometimes this bias is desirable, such as in Fig. 14 in which a judicious choice of features makes it possible to emphasize, or ignore, the orientation of grains in a polycrystalline sample, one should resist the temptation to fine-tune parameters that do not have an obvious meaning to obtain a result that reflects a preconceived interpretation of the data. The top row of Fig. 17 shows how different choices of the hyperparameters of the SOAP powerspectrum (cutoff radius r cut , density smearing σ a , and the types of atoms that are used as environment centers) change unpredictably the distribution of the points on the 2D map obtained by principal components analysis of a dataset that consists in different polymorphs of a family of molecular materials 255 . In the first panel, in particular, one can recognize a de- gree of correlation between the position of the points, and intuitive structural and energetic properties, such as the number of H-bonds, and the lattice energy. The correlation is however far from perfect, and with other reasonable choices of hyperparameters it disappears almost completely.</p>
        <p>One possible approach to make unsupervised models less dependent on the details of the underlying featurization is to combine them with an element of supervised learning. This includes, for instance, combining or contrasting density-based clustering with (kernel) support vector machines classification 259 . Even more explicitly, one can combine a variancemaximization scheme analogous to PCA with the regression of a target property, as in principal covariates regression (PCovR) 260 . In PCovR one minimizes a loss built as a mixture of a PCA and a linear regression loss, weighted by a mixing parameter α</p>
        <p>The matrix P ΞT projects from the feature space to a low-dimensional latent space, P T Ξ reconstructs an approximation of the full-dimensional feature vector based on its latent-space embedding, and P T Y regresses the property matrix Y using the latent-space coordinates as inputs. By explicitly looking for a latent-space projection that allows to regress linearly a target property, one forces the dimensionality reduction to identify a subspace of the chosen features that correlates well with one or more quantities of interest.</p>
        <p>The lower row of Fig. 17 is obtained using a recent kernel extension of this method (KPCovR 256 ) attempting simultaneously to maximise the spread of data and the kernel regression of the lattice energy, giving equal weight to the two components (α = 0.5). Not only points on the resulting map correlate very well with the target: one observes that also structural parameters such as the H-bond counts are now clearly separated between different regions, and the appearance of further groups of well-clustered structures that correspond to similar isomers of azaphenacene 256 . What is perhaps more important, introducing an explicit supervised learning target leads to maps that are more consistent across different choices of hyperparameters. Thus, (K)PCovR reduces the arbitrariness of the description, and mitigates the risk of implicitly introducing an unknown bias by deliberate or accidental tuning of the hyperparameters of the representation.</p>
        <p>The unsupervised analysis of a dataset helps building an intuitive understanding of complicated structure-property relations for a material or a class of materials. Given the "black box" nature of many machine-learning models (and the fact that even the rigorously-defined density correlation features we focus on in this review have a high-dimensional nature and non-trivial relationship to the actual atomic structure) low-dimensional projections of the feature space can also be useful to gain a better understanding of the structure of feature space. For example, Fig. 18a tells us less about the QM7 dataset 254 (that contains small organic molecules containing C, H, N, O, S, Cl) than about the SOAP features that underlie the representation: the unsupervised analysis shows that the chemical composition is the most clear-cut differentiating characteristic when looking at this dataset through SOAP lenses. Fig. 18b visualizes the same QM7 data using a different representation, based on the Coulomb matrix, and shows how successive layers of a neural network transform these features into nonlinear combinations that correlate very well with the target properties. Thus, this visualization helps understand how a highly-nonlinear function transforms a description of the system into combinations that can be more easily used for regression, and diagnose the inner workings of the deep neural network.</p>
        <p>A final "introspective" application of this kind of analysis involves examining the structure of a dataset -not as a way to learn about the atomistic configurations it contains, but about its makeup, or the relationship with other datasets. An example is given in Fig. 19, showing the comparison between the chemical space covered by three databases of organic molecules, with QM9 and AA being mostly disjoint, and the more diverse OE molecules encompassing both the other FIG. 20. A schematic overview of the process of using atomic structure representations to predict properties that are not directly associated with the starting structure. (a) prediction of the properties of the minimum-energy configuration of a structure; the problem can be made wellposed by using a cheap approximate method to optimize the structure, and taking the representation of this approximate structure as the input to regress accurate energy and geometry. (b) prediction of a property that is associated with a thermodynamic average; the minimum energy structure can be taken as a proxy for the ensemble, but a more formally precise "ensemble representation" is also possible.</p>
        <p>sets. Other examples of this kind of analysis are discussed in Section IX.</p>
        <p>The one-to-one mapping between an atomic structure and its representation is one of the key requirements to achieve accurate "surrogate quantum models" of atomic-scale properties. However, it can also be a limitation whenever one wants to describe properties that are not strictly associated with the specific configuration at hand. For example, consider the databases of molecular properties (e.g. the QM9 dataset 262 ) that have been extensively used as a benchmark, and have been a powerful driving force behind the development of the representations we describe here. The typical benchmark involves taking a structure whose geometry has been optimized at the DFT level and use it to predict the DFT energy -an exercise that is manifestly of little practical utility. A more useful approach, instead, would be using a non-optimized structure to predict the properties of the nearest local configurational optimum. As shown in Fig. 20a, this is conceptually problematic, because we are now trying to achieve a many-to-one mapping. A possible solution is to map each distorted geometry to an idealized one, or to use a lower level of theory to determine an unique structure Ã0 . Thus, the many-to-one mapping is realized by the local optimization procedure, and the corresponding representation | Ã0 can be used to uniquely identify the entire basin of attraction of the local minimum. Only for the training structures, this geometry is optimized further at a higher level of theory, obtaining the structure A 0 for which properties are meant to be computed. When the model is fitted, the relationship between Ã0 and its high-quality counterpart is learned implicitly. This kind of "indirect" model has been used, for instance, in Ref. 30, where structures optimized at the semiempirical PM7 265 level were used to predict CCSD energetics computed for a DFT-optimized version of the same compound. While the error was almost twice as large as a model using directly |A 0 as input, chemical accuracy could be reached when discarding from the training set structures for which the DFT-optimized structure was too different from the PM7-optimized geometry.</p>
        <p>A similar conceptual problem arises when one wants to build models for properties that are associated with a thermodynamic state rather than a precise structure, such as a melting point or solubility of a material. The problem is very well understood in the context of cheminformatics, where moleculargraph descriptors can be thought as representing the entire set of molecular conformers. In the technique known as 4D-QSAR, "ensembles" of conformers are used to build fingerprints that encompass explicitly the structural variability of each compound 266 . These two approaches can also be applied while using the kind of representations discussed in the present review. Typically, and particularly if the ensemble con-sists in relatively small fluctuations around equilibrium, one might take a representative structure (e.g. the minimum energy configuration) and use its | Ã0 as a proxy of the thermodynamic state (Fig. 20b). The case in which the target property can be estimated as an ensemble average can be formulated very elegantly in the case of a linear model. Consider for instance the mean of a property y over the Boltzmann distribution at inverse temperature β, P (A) = e -βE(A) /Z,</p>
        <p>where Z = dA e -βE(A) is the canonical partition function. Exploiting the linear nature of the representation one can define an "ensemble ket"</p>
        <p>With this definition, one could use a linear model for y(A) with weights q|y and see that</p>
        <p>which is convenient because it allows using properties of configurations and of ensembles on the same footings -and possibly combining them in a single training exercise. The same approach can also be applied in a kernel setting, computing the ensemble average of the reproducing kernel Hilbert space vector associated with the structures.</p>
        <p>We have discussed in Section III how most of the existing choices of representations share profound similarities, and shown, in Section IV, that many alternative schemes can be formally related to each other by means of a linear transformation, smoothening or a limit operation. However, this is not to say that in practical applications they are entirely equivalent. The computational cost of evaluating them, and their performance in classifying structures, and in regressing their properties, is determined by the choice of basis functions. Even for formally equivalent representations, the condition number of the linear transformation between them and their corresponding bases have significant impact on the numerical behavior of the computed coefficients and the quantities derived from these coefficients.</p>
        <p>A preliminary question when comparing alternative choices of features for the description of atomic structures and/or environments is that of establishing an objective way of assessing their relative merits. The performance when used in the regression of useful atomic-scale properties is an obvious criterion, but such a comparison is intimately intertwined with the target property and the regression algorithm. 94,267,268 . Very recent efforts have attempted to characterize different representations in terms of their information content -for instance through the eigenvalue spectrum of the covariance or kernel matrix associated with a dataset, the decrease in accuracy when reducing the number of features 201 , or the sensitivity of the features to atomic displacements. This latter approach can be realized by directly comparing the separation in feature space against finite displacements of the atoms 201 , or through an analysis of the Jacobian J jk = ∂ k|A i /∂r j</p>
        <p>202 . The sensitivity of the features to small changes of the atomic positions indicates their usability and performance in regression of classification tasks. Onat et al. 201 analysed the effect of random perturbations in crystalline environments, finding that, for features based on atomic density correlations, displacements of atoms in the environment usually cause a linear response. One notable deviation from this trend are perturbations along some high-symmetry directions in atomic environments carved from perfect crystals, where the response to displacements is secondorder, implying that the representations cannot capture these types of deformations (Fig. 21). However, as discussed in reference 201 the types of symmetric deformations applied in the study correspond to reflection operations. Due to the body-correlation order considered, features are invariant to mirror symmetry, and so the observed loss of sensitivity is not unexpected. Analizing the response of the features to perturbations in terms of the Jacobian, as in Ref. 202, has the advantage of characterizing fully the sensitiv-ity at a given point. The Jacobian should have six zero principal values, corresponding to rigid rotations and translations of the environment. Additional zeros could be associated with the presence of a continuous manifold of degenerate structures. In some cases, as demonstrated by the finite-displacement deformation in Fig. 21b, high-symmetry configurations can result in directions with zero gradient that have no adverse effect on the accuracy of a model built on the density correlation features.</p>
        <p>Another comparison between different bases is to analyse the landscape defined by the similarity or distance between environments, d(A i , A i ) where the environment A i is kept fixed. The distance between the atom-centered environments A i and A i can be defined as the Eucledian distance between feature vectors, Eq. ( 94). Written as a function of the Cartesian coordinates of A i , d(A i , A i ) is a scalar field which will have a global minimum manifold where the field is exactly zero, corresponding to equivalent environments A i and A i that are related by symmetry operations. Whether there are other manifolds at exactly d(A i , A i ) = 0, corresponding to the same features resulting from symmetrically nonequivalent environments is related to the question of completeness (Section VI B). In practical applications, the shape of the global minimum manifold has also implications for the numerical evaluation. In particular, one could examine how different A i and A i may be for d(A i , A i ) &lt; where is a small number. Using a random search approach, the numerical sensitivity of the feature landscape has been analysed in Ref 29. Reference structures A i were perturbed and then reconstructed by minimising the distance d(A i , A i ), and the optimised structures compared to the reference ones. For small numbers of neighbors in the reference environment, all the examined representations performed similarly well, but only SOAP was capable of accurately reconstructing the reference environments of more than 12 neighbors. As we have seen in earlier sections, this differences can be attributed to the choice of basis functions other representations use, although it should be noted that SOAP distances and similarities converge in the limit of a complete basis, therefore the actual form of the basis might affect the convergence, and the computational cost of the representation, but does not impact its resolving power. A more explicit comparison between pairs of representations can be obtained by evaluating the error one incurs when using a set of features, arranged in a feature matrix Ξ in which each row corresponds to a sample in a reference dataset, to linearly reconstruct a second featurization of the same structures or environments Ξ , defining a global feature space reconstruction error GFRE(Ξ, Ξ ) = min P Ξ test -Ξ test P 2 /n test .</p>
        <p>(103) P is a linear regression weight matrix obtained on a training subset of the rows of Ξ and Ξ , and both sets of features are assumed to be standardised. 271 The GFRE can be extended to also incorporate nonlinearity in the mapping, either by a locally-linear approach, or by using a kernelized version. Loosely speaking, it measures the relative amount of information encoded by the two feature spaces, and is not symmetric. GFRE(Ξ, Ξ ) GFRE(Ξ , Ξ) indicates that the featurization underlying Ξ is more informative than that used to build Ξ , and vice versa. GFRE(Ξ, Ξ ) ≈ GFRE(Ξ , Ξ) ≈ 0 implies that the two featurizations contain similar information (Fig. 22a). A similar asymmetric measure of similarity between feature spaces can be defined by comparing the resolving power of the corresponding metrics 272 , translating the information that is present in distance-distance correlation plots (Sec. VII B) into a quantitative measure of information content.</p>
        <p>Having GFRE(Ξ, Ξ ) ≈ GFRE(Ξ , Ξ) ≈ 0 does not mean that Ξ and Ξ they are equivalent and can be used interchangeably. One could emphasize more some structural correlations than others: imagine for instance multiplying by a large constant the entries of one column. This kind of distortions, which can have a substantial impact on the performance of models built on Ξ or Ξ , can be measured by defining a global feature space distortion (GFRD)</p>
        <p>P is the same projection matrix that enters the definition of the GFRE (so that ΞP ≈ Ξ ), and Q is the unitary transformation that best aligns Ξ and the best linear approximation of Ξ .</p>
        <p>If both GFRE and GFRD are zero, then the linearly independent components of Ξ and Ξ are related by a unitary transformation, which implies that distances and scalar products between feature vectors are equal in Ξ and in Ξ . Figure 22 demonstrates the use of these measures to compare |ρ ⊗ν i features of different body order. The asymmetry is very clear, with higher-order features containing more information than their lower-order counterparts. Note that -in view of the linear nature of the mapping -this is not entirely obvious: formally, ν = 1 features are not linearly dependent on higher-ν features, and so these observations reflect the specific nature of the atom-density field whose correlations are being represented, and the nature of the structures in the benchmark datasets. The figure also includes invariants built with the N-body iterative contraction of equivariants (NICE) framework, that are designed to capture most of the information up to high body orders. The truncation of the expansion, that is necessary to keep the evaluation of ν = 4 order features affordable, leads to a small residual GFRE when reconstructing the full ν = 3 features. The GFRD is rather large between all featurizations, indicating that -even though higher-order features contain sufficient information to describe lower-order correlations -they weight the information differently, which is why it is often beneficial to treat different orders of correlation separately in the construction of interatomic potentials. 15,180,183,195 B. Feature selection Numerical feature vectors ξ(A i ) are the result of a basis set expansion of the abstract atom-centered representations, which are, for practical purposes, truncated. A concrete discretization of the symmetrized ν-correlations is obtained by choosing a finite subset from the set of all possible features,</p>
        <p>(A choice of (n α , l α ) α naturally induces a choice of m α and symmetrized features.) The role of the discretization q is very different for linear and nonlinear models and therefore warrants a brief comment: For nonlinear models we typically only require geometric completeness (see Section VI), which means that the feature set can be chosen to be minimal but in a way so that all possible configurations, or at least all configurations of interest (e.g. from a training set) can be distinguished in a stable and smooth way. While it is an open problem to characterize precisely what this entails, we generally expect that relatively small feature sets on the order of hundreds for single-species scenarios could be sufficient. On the other hand, converging a linear model requires eventually letting the discretisation q converge to the full feature set q total , which in practice leads to a much larger set q and in particular higher correlation-orders ν to achieve a desired accuracy, e.g. on the order O(10 000) features for single-species models. The additional cost in training and evaluating the features is of course offset by the fact there is no additional cost in evaluating the nonlinear models. Due to the large feature sets the selection of effective subset of q may be even more important in the linear setting. In particular it will be crucial to a priori choose sparse subsets of q total rather than tensor-product sets due to the combinatorial explosion of the number of features with high ν (curse of dimensionality). For example, a total-degree D discretisation,</p>
        <p>was used by Bachmayr et al. 127 , while closely related a priori sparsifications were used by Braams and Bowman 11 , Shapeev 134 in all cases demonstrating accuracy/performance competitive with or outperforming nonlinear models. Data-driven selections When using high-body order features, some of the components can be related by non-trivial linear dependencies, that can be enumerated numerically 127,149 . The construction does not ensure that there is no other linear dependence that is specific to a given dataset, meaning that feature vectors could potentially be compressed even further without noticeable deterioration in the quality of the representation. The benefits of the compression are clear: if only a few components need to be evaluated, significant efficiency gains may be realised both in computational effort and storage requirements.</p>
        <p>Thus, the objective of feature selection or truncation is to find a subset of features that retain the information content of the original, untruncated representation. This is to be contrasted with dimensionality reduction techniques, that apply a linear transformation on the full feature vector to generate a lower dimensional representation. These only reduce the computational cost of operations that are applied on the reduced feature vectors: the whole feature vector must be evaluated first, before being able to determine its projections.</p>
        <p>A simple example of a feature selection strategy is the farthest point sampling (FPS) technique 274 . One chooses an initial column χ c0 (indexed by c 0 ) of the feature matrix, and then iterates selecting the columns that maximize the Haussdorf distance to the previously selected columns</p>
        <p>effectively identifying the indices c of the features that have the most diverse values across the data set. FPS has also been used in a similar manner, but on the rows of Ξ in order to select a representative set of data points. 30,69,275 The CUR matrix decomposition 276 , instead, generates a low-rank approximation of the feature matrix Ξ, in the form</p>
        <p>Unlike singular value decomposition, CUR uses the actual columns (C) and rows (R) of Ξ. To make the selection, a leverage score is associated with each feature c</p>
        <p>based on the right singular vectors v i of the singular value decomposition of Ξ. k is usually taken to be the approximate rank of Ξ. Features may be selected in a probabilistic procedure or simply based on their score. Imbalzano et al. 277 argued that the scores associated with feature vector components which are linearly dependent are close, therefore the selection can easily result in a redundant set. Instead, in Ref. 277 a greedy algorithm based on the CUR decomposition was suggested, where features were selected iteratively. The feature with the highest score is selected, and the columns of Ξ are orthogonalised relative to the column corresponding to the selected feature. The scores are updated in each step, so the linear dependence of already selected features are removed. This iterative scheme often performs better when using a very small value of k in constructing the π c , Eq. ( 109).</p>
        <p>Fig. 23 shows that a data-driven selection of the most relevant/diverse features makes it possible to achieve models with an accuracy that approaches that of the full model while reducing the number of components by a factor of about 3 (for linear regression) or 10 (for KRR). Particularly for intermediate sizes of the selection, the improvement in accuracy with respect to a random selection can be dramatic. Both FPS and CUR methods can be improved further by incorporating information on the properties associated with the structures, 273 as in Eq. ( 99). Including a supervised component by setting α &lt; 1 in the feature selection usually leads to more performing models, as shown in Fig. 23. Feature selection methods can be applied to any flavor of density correlation features. Imbalzano et al. 277 used a reference data set on liquid water 278 and a large set of systematically generated ACSFs. Evaluating the RMSE of the predicted energies and forces revealed that automatic selections performed by a CUR or FPS approach may achieve similar performance to features selected based on chemical intuition and heuristics, while keeping approximately the selection size. A dramatic reduction in numbers of features is also possible for the SOAP power spectrum, and a data-driven selection of the most important components has quietly become commonplace to accelerate SOAP-based ML models 24,279,280 . A more systematic investigation of the effectiveness of feature selection for many commonly used atomic descriptors has been recently reported by Onat et al. 201 , who analysed how accurately the original feature vector can be reconstructed from the reduced set, as well as the performance on a practical regression task.</p>
        <p>As discussed in Section V D, non-linear models optimize the description of their inputs by generating new features that are best correlated with the target property, or that are adapted to the structure of the dataset. For instance, taking products of 2body features results in an effective representation that incorporates some, but not all, features of body order 3, 4. . . In some cases it is possible to find an expression for the effective representation associated with a kernel model 29,117,195 , while other cases (most notably deep neural network models) put less focus on the interpretability of the intermediate features, and act largely as data-driven 'black boxes'. Alternatively, feature optimization can be performed explicitly on the representations presented in Section IV E. Such optimization could take the form of the choice of basis functions. In the Behler-Parrinello framework, it is customary to select a small number of atomcentered symmetry functions based on experience and heuristics 141 . An optimization of the hyperparameters by gradient descent has also been proposed 281 to Learning curves for the atomization energy of molecules in the QM9 data set 262 . Four of the lines show the MAE on the test set for kernel regression models based on SOAP ( |ρ ⊗2 i ) features with different cutoff radii (dashed lines graduating from red to blue). The other lines show the MAE on the test set for the optimal radially-scaled (RS) and multiple-kernel (MK) SOAP models (black and grey lines respectively). In every model, the features were constructed with very converged hyperparameters, nmax = 12 and lmax = 9. The inset shows the radial-scaling function u(r) from r = 0 Å to r = 5 Å with the parameters that were found to minimize the tenfold cross validation MAE on the optimization set through a grid search, r0 = 2 Å and m = 7. The multiple-kernel model combines the rcut = 2, 3, 4 and RS kernels in the ratio 100'000 : 1 : 2 : 10'000, and the learning curve agrees with the RS result to within graphical accuracy. Error bars are omitted because they are as small as the data point markers. Note that errors are expressed on a per-atom basis. Error per molecule expressed in kcal/mol can be obtained approximately by multiplying the scale by 0.4147, that is computed based on the average size of a molecule in the QM9 database. Reproduced with permission from Ref. 125. Copyright 2018 PCCP Owner Societies.</p>
        <p>obtain more accurate models based on atom-centered symmetry functions.</p>
        <p>When considering systematically-convergent implementations of density-correlation features, the optimization of the basis set is less crucial, although one may want to reduce the size of the basis for the sake of computational efficiency, as discussed in Section VIII B. That is not to say that the details of the practical implementation of the features does not change the behavior of a model built upon them. Optimizing hyperparameters such as cutoff radius, density smearing, basis set cutoff, affects how naturally the features correlate with the target property, which is one of the factors determining how quickly a regression model becomes capable of performing accurate predictions 123 . For example, the smearing of the atom density, or the truncation of the basis set, should reflect the natural scale over which the target properties vary. Similarly, the size of the local environment de- termined by r cut relates to the typical decay length of interactions, as mentioned in Section III C, but it also changes the effective dimensionality of feature space, which affects the accuracy of the model in a non-trivial way. Consider the learning curves shown in Fig. 24, that report on the prediction accuracy, as a function of the train set size, for a kernel ridge regression model of molecular atomization energies, based on SOAP features that differ by the value of r cut . A very large cutoff r cut = 5 Å does not yield the best performance, despite providing information on a wider range of distances. In fact, one observes the need to balance the complexity of the model and the available data: a very short-range r cut = 2 Å yields the most effective description in the data-poor regime, but the accuracy of the corresponding model saturates due to lack of information on non-covalent interactions. Combining multiple representations in a "multi-kernel" model (which is effectively equivalent to concatenating multiple feature vectors, each scaled separately) yields consistently better performances 22,30 . The weighting of different components -that can be optimized by cross-validation -indicates the relative importance of correlations on various length-scales. The fact that large-r cut features carry low weight in the optimal combination suggests that an improvement of performance can be obtained by calibrating the distancedependent contributions of neighbors to the environment description. This can be achieved by introducing a radial scaling function (indicated as u(r) in Ref. 125, as f (r, r j , r cut ) in Ref. 191 and as ξ ν (d) in Ref. 116) that downweights the contributions of atoms in the far field. As shown in Fig. 25 for the FCHL representation 116 , the choice of the form of this scaling can change the accuracy of the model by more than a factor of 2. A similar effect is seen in Fig. 24 for the case of SOAP features. It is also worth noting that the cutoff function usually adopted in Behler-Parrinello-like frameworks decays rapidly well before reaching r cut -suggesting that a similar optimization is implicitly at play. 152 Optimization of a radial scaling function has become commonplace, and most recent applications based on the SOAP power spectrum rely on it to achieve consistently optimal performance in both the data-poor and data-rich regime.</p>
        <p>Rather than optimizing the correlations between geometric features and the target properties, one can attempt to build features that incorporate a notion of chemical similarity between different elements. The idea was introduced in terms of an alchemical similarity kernel in Ref. 69, that is also a core component of the FCHL framework 116 , but has been implemented in different forms in the context of atom-centered symmetry functions 283-285 and of generic atom-density correlation features 125 . In general terms, the idea is to achieve a reduction of the dimensionality of the chemical space, writing formally a (linear) projection of the elemental features ã| = a ã|a a| , (110) where the coefficients ã|a enact the projection between the elemental and the "alchemical" basis. The reduction in the dimensionality of the feature space can be substantial: for powerspectrum (ν = 2) features, the number of components scales quadratically with the number of species, and so even just halving the dimension of the chemical space reduces the Learning curves for a model of the cohesive energy of a database of elpasolite structures, each containing a random selection of four elements chosen among 39 main group elements. 282 The standard SOAP curve is shown in black, the best curve from Ref. Data-driven representations of the chemical space. (a) A 2D map of the elements contained in the elpasolite data set, 282 with the coordinates corresponding to 1|a and 2|a for the case with dJ = 2 (see also Fig. 26). Points are colored according to the group. (b) A periodic table colored according to the coordinates in the 2D chemical space. 1|a corresponds to the red channel and 2|a to the blue channel. (c) A periodic table colored according to 1|a (red channel) for a 1D chemical space. (d) A periodic table colored according to 4D chemical coordinates ( 1|a : red channel, 2|a : green channel, 3|a : blue channel, 4|a : hatches opacity) Reproduced with permission from Ref. 125 number of powerspectrum features by 75%:</p>
        <p>Figure 26 demonstrates how reducing the dimensionality of chemical space helps achieving a transferable, accurate model with a small number of training structures. By comparing learning curves with different degrees of compression, one sees that there is a similar data/complexity interplay as observed for radial correlations. A low-dimensional alchemical space is beneficial in the data-poor regime, as it allows the model to make an educated guess about the interactions of pairs of elements that are not represented in the training set. Learning curves with low chemical complexity, however, saturate in the limit of large training set, because they generate features that are not sufficiently flexible, and cannot describe the differences between elements.</p>
        <p>The optimization of both geometrical and compositional components of the density-correlation features can be construed as a linear transformation of the kets (or, when seen in terms of the linear kernels built on such features, as the action of a Hermitian operator 109 ). The requirement that such transformations do not affect the symmetry properties of the features restricts form they can take -for instance, they cannot mix different l or m dependent channels. These observations imply that (1) linear feature optimizations do not change the nature of the representations, and can be applied equally well to any implementation of |ρ ⊗ν i features, (2) as long as the linear transformation is full rank, there is no loss of information, which means that the observed change in performance is linked to the details of the regression scheme, such as regularization in linear or kernel models.</p>
        <p>As a final remark, let us mention that a critical analysis of a feature-optimization effort often reveals insights into the physical-chemical properties of the system being studied and the target properties. For instance comparing models of the energy using different r cut can be used to infer relationships between the length and energy scales 30 , and the inspection of the chemical mapping coefficients in Eq. ( 110) can be used to construct a data-driven periodic table of the elements (see Fig. 27). The use of interpretable, physicsinspired features can also be used to provide intuitive chemical insights by the construction of knockout models 286 in which for instance correlations are restricted to 2-bodies, the cutoff reduced to first or second neighbors. The impact of these artificial restrictions on the features information content, and therefore on the asymptotic performance of the model, indicates how important 3 or higher-body order interactions are, or how much long-range effects are relevant to determine the value of the target property. 230</p>
        <p>Despite encoding similar information content, the differences in formulation of competing structural representations may lead to large variations in implementation and performance. A first fundamental divide is between evaluation of features by summing over clusters of ν neighbors and computing ν tensor products of atomic densities (see Section IV F). Consider the case of evaluating "SOAP-like" ACSF by cluster sum (cost n max 2 l max n 2 neigh ) and by density expansion (cost n neigh n max l max 2 for the density, and n max 2 l max 2 for the SOAP evaluation). Despite the adverse scaling of ACSF computed as a sum over clusters of neighbors, these representations can be implemented efficiently 94,268,288 by relying on a careful selection of the features (discussed in Section VIII B), reuse of parts of the computations, parallelism and GPU acceleration. [289][290][291] In fact, when computing a linear model which is explicitly equivalent to a (ν +1)body order potential, the low-order terms can be more efficiently evaluated as a sum over neighbors 195,292 In line with the general focus of this review, we concentrate in particular on the efficient implementation of atom-density representations. As we shall see, roughly the same considerations apply to both those representations that are usually built on a smooth atom density, 109,125,149 that generalize the construction of the SOAP powerspectrum and bispectrum, 29 and those that are usually computed in a way that corresponds to a δ-like density, such as ACE 126,150 and MTP. 134,293 Indeed, both families of representations rely on three steps: (i) expansion of the local atom density on a suitable basis, e.g. Eq. ( 24), (ii) computation of ν tensor products of the expansion, and then (iii) contraction over the correlations to obtain equivariant features (Fig. 28). While these three steps have been implemented in different ways, their efficient implementation relies on similar considerations.</p>
        <p>Atomic density expansion Equation (20) provides the blueprints for a broad class of (ν + 1)-body atom-density representation. Practical implementations differ by the type of localized function used to construct the local atom density (see Eq. ( 16)), and by the radial and angular basis used for its expansion. As discussed in Section IV E, spherical harmonics are a natural angular basis, but other choices are possible. For instance, the MTP representation projects the atomic density onto a tensor product of direction vectors leading to the covariant moment tensor 134,243</p>
        <p>where P µ,ν is a radial function. Invariant components can be obtained by combining and contracting products of the elements of these tensors. of the MTP representation, 268 which relies on an efficient recursive evaluation of the basis functions 134 , are a testament to the effectiveness of this basis choice.</p>
        <p>As shown in Section IV E, the choice of an angular basis of spherical harmonics simplifies greatly the evaluation of Eq. ( 20), that can be written in terms of contractions of density coefficients nlm|ρ i (cf. Eq. ( 24)). If the environment-centered density is written in terms of a sum of density functions g(x -r ji ) ≡ x|r ji ; g , peaked at the neighbors positions, the expansion coefficients can be written as the accumulation</p>
        <p>of terms that correspond to an expansion over a basis of radial functions x|nl and spherical harmonics x|lm of contributions coming from Gaussians centered on each neighbor nlm|r ji ; g = dx nl|x lm|x x|r ji ; g . (114) In the g → δ limit, the contribution from the j-th neighbor amounts simply to a product of the radial and angular functions evaluated at r ji , nlm|r ji ; δ = nl|r ji lm|r ji .</p>
        <p>(</p>
        <p>Similar to the 1D case discussed in Sec. V B, for a given choice of radial basis the smearing of the density can be achieved by a mollification of the basis:</p>
        <p>= dx x |r ji ; δ lm|x nl; g|x ≡ nlm; g|r ji ; δ , (116) where we use nl; g| to indicate the radial term that results from the Gaussian convolution. Each of these terms can be computed very efficiently, exploiting in particular the fact that all orders of the spherical harmonics and their derivatives can be computed using recursion relations. 126,127,294 It might appear that using a smooth atom density g complicates substantially the evaluation of Eq. (114). However when g is a spherical Gaussian with standard deviation σ, the integral over d x can be computed analytically 295 d x lm|x xx|r ji ; g = x; l;|r ji ; g lm|r ji .</p>
        <p>(117) where the radial integral reads x; l;|r; g = 4πe -r 2 /2σ 2</p>
        <p>x 2 e -x 2 /2σ</p>
        <p>can be computed numerically for any form of the radial basis resulting in n max l max n grid evaluations of special functions. For instance, the original implementation of the SOAP representation uses a numerically orthogonalized, equispaced Gaussian basis. 29 Alternatively, this integral might also be performed analytically by using Gaussian type orbitals (GTO) as the radial basis 159,296 , x|nl; GTO . This choice makes it possible to compute the coefficients of the smeared density as easily as for the g → δ case nlm; GTO|r ji ; g = lm|r ji nl; GTO|r ji ; g , (121) where the only overhead comes from having to compute O(n max l max ) terms for the radial part and its orthonormalization. This is asymptotically cheaper than combining radial and angular terms -which requires O n max l max 2 multiplications per neighborbut can be substantial in practical cases, because the analytical integrals in Eqs. ( 117) and ( 120) yield nonstandard special functions.</p>
        <p>To reduce this overhead, one can choose a form of the atomic density that is symmetric about r i instead of r ji 191</p>
        <p>Together with a choice of radial functions that do not depend explicitly on l, this allows factorizing the radial integral (120) as dx n|x x; l|r ji ; ĝ = n|r ji ; ĝ l|r ji ; ĝ . (123) Coupled with the polynomial basis proposed in Ref. 29, these expansion coefficients can be computed efficiently using recurrence relations in the radial and angular coefficients. More in general, the cost of evaluating the radial integrals nl|r; g can be made negligible by using splines to approximate the value of the special functions resulting from the integrals, or the numerical integration of basis functions for which there is no analytical expression. Another aspect that does not affect the asymptotic scaling of the expansion, but can significantly influence the prefactor, involves the evaluation of spherical harmonics. 150,294 Several well-established techniques can be used to speed up the calculation of Y m l , including the use of real-valued spherical harmonics, the use of recurrence relations, and the use of formulations that are entirely written in terms of the Cartesian components of rji .</p>
        <p>Symmetrized n-body correlations The density coefficients anlm|ρ i are then combined to compute invariant (or covariant) features. Formally, the evaluation of the symmetry-adapted features -both those built using only local |ρ i features, and the multi-scale features that combine |ρ i and |V i -involves a tensor product of ν sets of density coefficients to yield density correlations in the uncoupled basis (a i n i l i m i ) ν i=1 |, and then a contraction along the m i indices, that generates the equivariant features expressed in the coupled basis (a i n i l i k i ) ν i=1 |. A technical difficulty one has to keep in mind when implementing the calculation of equivariant features is that the angular (l, m) indices have an irregular memory layout, with -l ≤ m ≤ l. Depending on the hardware architecture, it might be beneficial to store the coefficients in a regular (l max + 1) × (2l max + 1) array, padded with zeros.</p>
        <p>A more substantial challenge associated with the increase of the body order is that both the number of linearly independent features and the cost of evaluating each of them based on a naive contraction of the tensor products of density coefficients (e.g. based on the expressions in Ref. 126 ) increase exponentially with ν. Even though the exponential scaling is related to the expansion parameters l max and n max , and not on the number of neighbors, as it would be the case for the calculation of the features as a sum over clusters of ν atoms (see Sec. IV F and V A), it makes the enumeration of a complete linear basis prohibitively expensive. The recurrence relations 149 of Eq. ( 46) (or the equivalent ones for the invariant features proposed in Ref. 127 ) make it possible to evaluate individual equivariant features with a cost that scales only linearly with ν. To beat completely the exponential scaling, these recursive expressions should be combined with feature selection schemes such as those discussed in Section VIII B. For example, the n-body iterative contraction of equivariant (NICE) features incorporates a selection/contraction step at each level of the itera-tion. For each equivariant component q |ρ ⊗ν i ; σ; λµ , one determines (e.g. by principal component analysis, or just by dropping some components) a set of coefficients U ν;σλ q q that can be used to reduce the dimensionality of the features q ν;σλ |ρ ⊗ν i ; σ; λµ = q U ν;σλ qq q |ρ ⊗ν i ; σ; λµ . (124) Given that this operation only mixes features with the same equivariant behavior, it is then possible to perform an iteration equivalent to Eq. ( 46) to increase the body order further q|ρ ⊗(ν+1) i</p>
        <p>; σ; λµ ≡ q ν;τ k ; nlk|ρ</p>
        <p>Note that in the first line we use the loose definition of the indices in the bra-ket notation (Section IV A): the ν + 1 term can be indexed explicitly, with a notation that recalls the lower-order terms that are combined to obtain it; once it is computed, the granularity of the indexing becomes irrelevant, and a flat index can be used to streamline the notation. With this combination of expansion and contraction only the components that contribute significantly to the description of the structural diversity of the dataset, or to the prediction of the target properties, are retained to evaluate higher-order correlations.</p>
        <p>An alternative perspective for developing efficient implementations is to represent invariant or equivariant properties y(A i ) in terms of the unsymmetrized correlations,</p>
        <p>with the desired symmetries imposed through constraints on the coefficients y|n</p>
        <p>While this perspective imposes additional complexity on regression schemes it is convenient for fast evaluation of a fitted model (with coefficients now ensuring the correct symmetries) since the coupling coefficients need not be stored or evaluated anymore. An efficient evaluation now requires a recursion for the unsymmetrized correlations</p>
        <p>which is relatively straightforward to construct, 127 the key challenge being to retain only the (n α , l α , m α ) α features that give rise to non-zero coefficients.</p>
        <p>Splined GTO -no gradients The panels demonstrate the convergence of SOAP features as computed for 10'000 random CH 4 configurations 269 using the radial bases implemented in different codes, and measured in terms of the error one incurs when linearly predicting fully-converged features Ξ full (nmax = 24, lmax = 12, computed with the GTO implementation in 
            <rs type="software">librascal</rs>) using features with lmax = 8 and growing values of nmax (left) and with nmax = 14 (16 for 
            <rs type="software">librascal</rs>) and growing values of lmax (right). Top panels show the linear reconstruction error (GFRE) which measures the amount of information that cannot be linearly decoded from the coarser features. Bottom panels show the reconstruction distortion (GFRD) which measures the additional error one makes when limiting the reconstruction to an orthogonal transformation. 271
        </p>
        <p>To provide a practical example of the use of software to compute representations, we compare three packages, namely 
            <rs type="software">quippy</rs> 298 , 
            <rs type="software">dscribe</rs> 296 and 
            <rs type="software">librascal</rs> 287,297 , that are open source, and can be easily used in a Python 
            <rs type="software">code</rs>. We do not discuss the internals of the implementations, but show code snippets that can be readily used to evaluate descriptors of atomic structures, primarily focusing on the SOAP powerspectrum. All examples use the Atomic Simulation Environment 299 , and atomic structures are assumed to be stored in the variable structures, an instance of ASE's Atoms object. In all of these implementations, the descriptor vectors are returned as numpy.array objects, from which kernel values may be obtained by computing the dot products between descriptor vectors. We also do not discuss the computational efficiency of the different codes, which is still the subject of very active development. Fig. 30 provides some representative timings from 
            <rs type="software">librascal</rs>, and from a recent implementation of SOAP that uses non-Gaussian atomic densities 191 . The wildly different breakdown of the computational effort as a function of the basis set size, and the large overhead associated with the evaluation of the gradients of the features highlight some of the implementation challenges.
        </p>
        <p>The 
            <rs type="software">quippy</rs> python package is based on the QUIP suite with the GAP extension, which provides the descriptors module. QUIP must be downloaded and built using a Fortran compiler before quippy, which uses f90wrap to access the compiled functions in QUIP via python interfaces. In the 
            <rs type="software">GAP</rs> implementation, Gaussian radial basis functions are used, placed at equal intervals, and orthogonalized. from quippy.descriptors import Descriptor soap = Descriptor("soap cutoff=3.5 cutoff_transition_width=0.0 atom_sigma=0.3 n_max=8 l_max=6") # returns a dictionary containing the features and connectivity information features = soap.calc(structures) # features["data"] is a numpy array with the shape (\n_environments,n_features)
        </p>
        <p>The Descriptor object is initialised using a string containing the kernel parameters in a key=value format, with some keys being mandatory.</p>
        <p>The 
            <rs type="software">dscribe</rs> package 296 implements multiple descriptors, including SOAP, MBTR 31 and ACSF. A python interface is used to interact with calculator functions written in C/C++, ensuring efficient evaluation. The main difference between the SOAP implementation of quippy and 
            <rs type="software">dscribe</rs> are the choice of radial basis functions, which are spherical primitive Gaussian Type Orbitals (GTOs), orthogonalised using the method suggested by Löwdin 300 . Alternatively, cubic or higher order polynomials may also be chosen. In analogy with the definition of GTOs used in quantum chemistry, the radial basis has an explicit dependence on l. The the python object providing the descriptor is constructed from the class SOAP and specifying the parameters in the initialisation arguments. The package 
            <rs type="software">librascal</rs> also provides a variety of descriptors, but chiefly focuses on the calculation of density-based representations, including SOAP and the ν = 1 and ν = 3 correlations. The back-end, written in C++, can be accessed from python interfaces. Exploiting the spirit of the general construction of |ρ ⊗ν i features, 
            <rs type="software">librascal</rs> implements two kinds of radial functions, namely a family of GTO-like radial functions 157 as well as a discrete variable representation (DVR) basis, corresponding to a real-space evaluation of the symmetrized density using a Gauss-Legendre quadrature rule. The SphericalInvariants object uses the transform method to compute SOAP features, that are stored internally in a sparse format, in which each dense block corresponds to a (a, a ) pair of elemental densities. These features can be used to compute scalar-product kernels between two environments, or cast to a dense array through the get_features method.
        </p>
        <p>These three packages all compute "SOAP" features, but differ in the choice of basis functions. Much as with electronic structure codes, that often yield results that differ significantly despite performing nominally the same type of calculations, 301 one cannot expect to be able to combine the features computed by one package with the regression weights computed by another. It is however important to assess whether the features are equivalent in a less stringent sense, e.g. whether they contain analogous information, and whether they converge to the same limit when the expansion parameters (n max , l max ) are increased. Figure 31 demonstrates the convergence of the GFRE and GFRD (see Section VIII A and Ref. 271) between small-(n max , l max ) features and a highly converged Ξ full featurization. In all cases we consider GFRE(Ξ full , Ξ) is at least one order of magnitude smaller than 
            <rs type="software">GFRE</rs>(Ξ, Ξ full ). One sees that, reassuringly, in all cases the feature reconstruction errors converge towards zero. For n max = 16 all choices of radial bases are essentially converged, and the residual error is due to the convergence of the angular channels. Since all implementations use equivalent spherical harmonics expansions, the convergence with the angular cutoff l max is nearly identical. The convergence rate of the radial bases, however, is not the same. The GTO bases in 
            <rs type="software">librascal</rs> and 
            <rs type="software">dscribe</rs> have similar amounts of information (although they are not fully equivalent, as they are parameterized differently), and converge faster than the bases used in quippy and the librascal DVR implementation. The GFRD also converges to zero for most implementations -meaning that in the complete basis set limit the corresponding features become equivalent. The implementation in 
            <rs type="software">dscribe</rs> is an exception, with a GFRD saturating at approximately 0.1, suggesting that implementation details lead to persistent differences in the weighting of different kinds of correlations even when (n max , l max ) increase beyond the values that are typically used in practice.
        </p>
        <p>In this Section we report some representative applications that highlight different aspects of the representations discussed in this Review -demonstrating how an understanding of the nature and properties of the structure/features mapping can be used to construct efficient and insightful machine-learning models.</p>
        <p>Contrary to the problem of predicting interatomic potentials, or other extensive properties, the affinity between a protein and a small drug-like molecule does not fit well into the mold of an additive property model. The structure of the ligand must allow for the active portion of the molecule to fit in the binding pocket of the target protein, and the nature of the chemical groups in this "warhead" portion are more important to determine the strength of the interaction than peripheral portions of the molecule. Figure 32 shows the accuracy of a classifier based on SOAP features, that aims to distinguish active components from decoys for a given target protein. The targets and the ligands, as well as their "ground truth" binding behavior are taken from the database of useful decoys, enhanced (DUD-E) 51 . The performance of the classifier is represented in terms of the receiver operating characteristic (ROC) curves (the ROC curve of a perfect classifier would run along the left and top margins of the plot, while a classifier that is as good as random would run along the diagonal), and their area under the curve (AUC) (the AUC is the integral of the ROC, and roughly corresponds to the fraction of molecules that are classified correctly). The AUC plot, in the inset of Fig. 32 shows that a model based on an average metric -that describes each molecule as the average of its environments, Eq. ( 95) -performs rather poorly, which is unsurprising given the highly non-additive nature of the binding affinity. Using a "best-match" kernel (equivalent to the distance in Eq. 97, and implemented in practice as the smallγ limit of the REMatch kernel 69 ) improves dramatically the accuracy of the classifier, bringing the AUC to well above 0.95. A judicious choice of the training structures, based on farthest point sampling, accelerates even further the convergence of the classifier with train set size. This application provides an example of how local representations can be combined in a nonadditive way, resulting in a dramatic improvement of the machine-learning performance for a problem in which non-additive behavior is to be expected.</p>
        <p>Some of the early examples of machine-learning models leveraging covariant features focused on the prediction of dielectric response functions, such as the dipole moment µ (and the equivalent bulk quantity, polarization), polarizability α (and the closely-related electronic dielectric constant) as well as higher-order terms, such as the first hyperpolarizability β. We discuss the case of the static dipole polarizability α as a representative case that highlights many of the current ideas and applications. In its Cartesian form, α is a symmetric tensor, fully determined by six components (α xx , α yy , α zz , α xy , α xz , α yz ). In order to build a machine-learning model based on equivariant density correlation features, it is more convenient to apply a unitary transformation that casts it into its irreducible spherical components (ISCs). The spherically symmetric term, α (0) 0 , corresponds to the trace of the tensor, while the 5 anisotropic components, α</p>
        <p>{-2,-1,0,+1,+2} transform collectively as λ = 2 spherical harmonics, and can be computed using recursive relationships that are explicitly reported in Ref. 158. A clear advantage of this construction is that, unlike the components of the Cartesian tensor, the two ISCs of α can be independently represented by the equivariant density-based features corresponding to λ = 0 and λ = 2, relying on a linear prediction model similar to the one reported in Eq. ( 40). The inherent locality of the model means that the tensor prediction can be broken down in the sum of individual atomic contributions α = i α i . These local components can be combined to make predictions on larger, and more complex molecules than those included in the training set. This transferability was exploited in the 
            <rs type="software">AlphaML</rs> model 24,304 to fit against coupled-clusters (CCSD) reference values, computed on small organic molecules from the QM7b dataset 254,302 , and predict on 52 larger "showcase" molecules that are at the limit of what is computable with state-of-the-art quantum chemistry methods. On these molecules, the error of 
            <rs type="software">AlphaML</rs> against the CCSD reference (0.24 a.u./atom) was less than half the discrepancy between CCSD and DFT (0.57 a.u./atom).
        </p>
        <p>An additive model also provides predictions for the local contributions to α, which are represented in Fig. 33, in terms of ellipsoids aligned along the principal axes of α(A i ). Even though these components do not have to be physically meaningful -given that the only training target is given by total polarizabilities -the local α(A i ) reflect some chemical insights, e.g. the model predicts large components when centering the representation on the highly-polarizable sulfur atoms, as well as along the directions where the molecules are highly polarizable. Highly conjugated molecules are also interesting because they exhibit a non-additive behavior of the polarizability, due to the vanishing HOMO-LUMO gap. Due to the spatial nearsightedness of the representation, the model breaks down when asked to predict the polarizabil- ity of large polyenes and polyacenes based on the information learned on simpler and smaller molecular units. This is well represented in Fig. 34, where the prediction of α is tested for conjugated carbon-based molecules of increasing size, including fullerene 24 . The prediction of α using equivariant features can also be extended to the condensed phase and provides a crucial ingredient to compute Raman spectra. An example of this is reported in Ref. 160, where the polarizability of crystal polymorphs of paracetamol are predicted along a full molecular dynamics trajectory, thus allowing for the calculation of the Raman intensity in terms of the polarizability correlation spectrum. As shown in Fig. 35, given the local nature of the polarizability response in this kind of systems, accurate Raman intensities and lineshapes can be predicted for the entire range of frequencies. The low cost associated with computing dielectric response functions by ML models using symmetryadapted features makes it possible to routinely evaluate condensed-phases infrared and Raman spectra including also a description quantum mechanical nature of the nuclei 305 -a task that until very recently required enormous computational effort 306 .</p>
        <p>The clear breakdown of a ML model based on local features that is apparent in Fig. 34 is representative of a general limitation of density-based features. There are essentially two approaches one can take to tackle the issue of the non-locality of the structureproperty relations, both of which are illustrated in Figure 36. One approach is to learn a proxy of the target property, which has a more localized nature, and which can then be easily manipulated to obtain the end result. The top panel of Fig. 36, adapted from Ref. 159, is an example of this approach. The electronic dielectric response ∞ of bulk water is affected by a collective, macroscopic electrostatic effect that is captured, in the continuum limit, by wellknown expressions such as the Clausius-Mossotti relation, α = V ( -1)/( + 2), that links ∞ to an effective molecular polarizability. This effective α is more readily learnable by a local model, leading to better accuracy and transferability in predicting ∞ . A different approach is needed when there is no obvious transformation of the target property to a more local version, as is the case for the polarizability of conjugated hydrocarbons. In these cases, one needs a model that is able to describe arbitrary non-local correlations. Long-range representations such as multiscale LODE features (Section IV H and V C) are particularly attractive, in that they combine a long-range character (coming from the potential field) with an additive decomposition that provides the transferability needed to extend the prediction to systems of increasing size. This is demonstrated in Fig. 36, where the multiscale LODE model is tested for predicting the isotropic component of the polarizability of a series of polypeptides of increasing length. While the prediction at small peptides lengths share a similar accuracy as that obtained using a pure density-based representation, the inclusion of the potential field greatly decreases the prediction error when considering longer molecular chains. A large, overall improvement of the prediction accuracy is observed when adopting an optimized, weighted combination between local and LODE features. These result suggests that the inclusion of long-range features within the regression model provides a better description of the intermediate-range interactions, and that by adjusting the relative importance of local and delocalized terms the model can be trained only on small molecules, and extrapolate reliably across systems of increasing size. Very similar findings were reported on the transferability of models of molecular dipoles 166 -where however the splitting between local and long-range physics was achieved by combining different regression models rather than by different choices of features.</p>
        <p>Another relevant scenario where the data-driven prediction of a quantum property benefits from a representation that relies on the use of local and equivariant features is the electron density ρ(r) of an atomic structure. 307 The density is a scalar field, and has been modeled with some success by predicting its value at a specific point by an invariant representation centered on that point 308,309 . Given that (particularly in the case of an all-electron calculation) atomic nuclei are a natural vantage point to decompose the overall electron density, one may want instead to model ρ(r) as a sum of atom-centered contributions, ρ(r) = i ρ(A i ; r). These atom-centered terms can then be conveniently decomposed as a sum of local functions, at the price of adopting a multicentered non-orthogonal basis for the expansion i.e., ρ(A i , r)</p>
        <p>where R n represent some suitably optimized radial functions (for instance those used in resolution of the identity methods in quantum chemistry 310 ) and cnλµ (A i ) correspond to the non-orthogonal expansion coefficients, that depend on the arrangement of atoms in the environment A i . These coefficients must transform in a covariant fashion with a rotation of the environment, and each λ-component can be independently predicted using equivariant features of the corresponding order -for instance with a linear model cnλµ (A i ) ≈ q cnλ |Q Q|A i ; ρ ⊗ν i ; σ; λµ , (127) even though current implementations use a kernel regression scheme 23,311 . The non-orthogonality of the basis used to represent ρ(r), implies that the learning phase has now to be performed considering all the different density components at the same time 23,311 . While this may sound as a computational drawback of the model, it also improves the locality of the coefficients, which underlies its remarkable transferability across vast conformational and chemical spaces, since the electron density can be effectively learned as a collection of local contributions. This is well exemplified in Figs. 37, where the electron density prediction of C(8) hydrocarbons 23 is tested upon having trained the model on much smaller compounds, with only 4 carbon atoms. This approach has since been applied to more complex systems, such as oligopeptides 311 , and to the prediction of other scalar fields such as the on-top density 312 .</p>
        <p>As discussed in Section VII, the choice of a representation to describe atomic structures determines the "lens" through which they are interpreted, which in turns has a strong impact on the way unsupervised learning schemes, such as clustering and dimensionality reduction, bring to light recurring patterns, and structure-property relations. The potential of generalpurpose, atom-density correlation features for these tasks has been recognized rather early. Figure 38, adapted from Ref. 313 shows a classification of snapshots taken from simulations of different phases of water, based on Steinhardt order parameters 236 , which are closely related to |ρ ⊗2 i features, and make it possible to partly differentiate between phases. In the same study it is shown how a neural network based on atom-centered symmetry functions can be trained to achieve near-perfect classification accuracy. An even more comprehensive mapping of the phase diagram of water -in which crystalline and amorphous phases from across the phase diagram, as well as transition pathways between them were consideredwas produced in Ref. 218, using permutation-invariant vectors 316 as global descriptors for the different configurations. Abstract structural descriptors are particularly useful when applied to datasets that contain hypothetical structures, generated by a high-throughput procedure 1 . In combination with a dimensionalityreduction scheme 4 , and with a generalized convex hull construction that attempts to estimate the synthesizability of materials by considering jointly their pre- dicted stability, and the structural similarity to other potential candidates 314 , a SOAP representation has been able to rediscover all known (meta)stable ice phases, as well as to propose another 34 structures which might be also stabilizable by pressure, doping, or co-crystallization 315 (see Fig. 39).</p>
        <p>An incomplete list of applications that use generalpurpose features for structural analysis and classification includes: the construction of structure-property maps for small organic molecules 244,318 , molecular materials 255,319,320 , inorganic perovskites 321 , corrosion inhibitors 322 ; the identification and characterization of defects in solids [323][324][325] and self-assembled polymers 326,327 ; the classification of secondarystructure patterns in polypeptides 259 and the building blocks of zeolites 230,328 and porous materials 329 ; the classification of different phases in multi-phase materials 330,331 ; the characterization of amorphous systems 18,[332][333][334][335] ; the search of stable phases of materials 336 ; the determination of the convergence of microsolvation studies of the hydration free energy 337 .</p>
        <p>There are also several examples, besides those given in Section VII where low-dimensional maps have been used as a tool to understand the structure of a data set or the nature of a representation. In Ref. 98 a PCA map was used to understand the ef- fect of randomizing the atom ordering on the feature space associated with a Coulomb matrix description of molecules, emphasizing the information loss associated with sorting of the elements -an alternative route to achieve permutation invariance. In Ref. 69, maps based on different kinds of SOAP kernels provided an understanding of the effect of different approaches to combining environment-level kernels, and of different definitions of an alchemical kernel between chemical elements, on the similarity between molecules as measured by the representation. In Ref. 94, maps of a dataset of water oligomers were used to compare the performance of different ML scheme to build 2 and 3-body models of the energy of water clusters.</p>
        <p>The use of low-dimensional representations to visualize the structure of a dataset, showing the relationship between different kinds of training structures, identifying regions that are poorly sampled, and determining how new configurations relate to the data the ML model has been fitted, is also gaining traction. 30,69,114,115,280,[338][339][340] An example of such map is given in Fig. 40, showing the diversity of the structures used to train a transferable machinelearning potential for carbon. 317 Adopting the same type of representations used for the regression model as the basis of this kind of analysis ensures that the maps describe the same feature space that underlies the fit.</p>
        <p>Even though the focus of this review is on descriptors of the 3D structure of materials applied to the construction of surrogate models of quantum mechanical properties, there is also growing interest in their application to QSPR tasks. As we briefly discuss in Section III, the descriptors that have been traditionally used in cheminformatics are based on a collection of molecular properties, or on molecular graph descriptors that do not depend on the particular conformation. 341 From a conceptual point of view, their coarsness is an advantage, because it is compatible with the definition of thermodynamic properties that are not associated with a single specific configuration, such as solvation and ligand binding free energies. Nevertheless, there is growing evidence that the use of descriptors incorporating information on the 3D geometries can improve the accuracy of QSPR models, especially for difficult cases that involve very flexible molecules, 342,343 as well as for data analytics approaches for materials informatics 344 . One of the core challenges in these efforts is the determination of the conformer geometries that should be used to evaluate the 3D descriptors, an operation for which several strategies have been explored to enhance the accu-racy of QSPR models. 345,346 As we briefly discuss in Section VII E, one of the most promising research directions involves combining the high fidelity of density based representations with a well-principled construction of ensembles of features. This is still a very active subject of research, with very encouraging results having been recently demonstrated for the prediction of the solubility of small molecules 347 , the computational screening for antiviral drugs 348 , and the prediction of enantioselectivity of organocatalysts 349 .</p>
        <p>Another growing trend that is worth a brief mention involves the use of information from electronicstructure calculations in the construction of structural representations. The idea has been applied in different forms. At the simplest level, electronic-structurebased indicators of chemical similarity, obtained for bulk elements, have been used in the construction of elemental similarity kernels, 69 to obtain models that are more predictive across chemical space 350 . Alternatively, electronic-structure indicators, such as the local density of states, can be used side-by-side with purely structural representations, yielding a substantial improvement of the accuracy of the model 351,352 .</p>
        <p>Elements of an electronic structure calculation, such as the charge density 353 , the electron density of states 354 or the elements of the Fock matrix 355 can be used directly as the basis for a molecular representation. This approach requires an electronic structure calculation in order to make predictions for each new structure, which implies a substantial overhead in comparison with methods using as inputs only the atomic positions. However, the increase in the transferability of the models may well justified the greater computational effort, particularly when using descriptors based on low levels of quantum mechanical theory to predict high-end, accurate molecular properties. 356,357</p>
        <p>The description of atomic structures in terms of mathematically sound, computationally efficient, and physically-inspired representations has largely driven the extraordinarily successful application of machine-learning schemes to atomic-scale modeling. Independently-developed representations have undergone a process of convergent evolution to fulfill a concurrent set of requirements, such as symmetry with respect to translations and rotations, smoothness and injectivity -a clear indication of the importance of these criteria to obtain efficient machine-learning models. Over the past few years, a more systematic study of the problem of representing atomic structures has clarified the connections between most of the successful representations, and between these and wellestablished concepts in the statistical physics of liquids (ν-point density correlations) and of alloys (the cluster expansion), as well as with the construction of potential energy surfaces for molecules and the condensed phase.</p>
        <p>A formal treatment of symmetries enabled the development of equivariant features that are suitable to build models that automatically obey the same transformation rules as vectors and tensors, making it possible to learn efficiently properties such as dipole moments, polarizability and density fields. This equivariant formulation can also be used to iteratively increase the body order of a structural representation: An important open question is how to best treat these high-body-order terms, whether by linear models that explicitly include dedicated high-order features, or by non-linear models that generate (some of) them algebraically. The answer rests both on practical considerations and on the very fundamental, highly non-trivial issue of whether a representation of limited body order provides a complete (injective) description of an atomic structure. Even though it is possible to build systematically a complete basis to expand in a linear fashion any structure-property relation, it is not clear how to build a minimal set of features that guarantees an injective mapping when used as the input of a general non-linear model, or how to reduce in an effective manner the size of a complete linear basis. A better understanding of the mathematical properties of representations is likely to lead, in the near future, to more robust and better performing implementations, and might also help design better "deep" models, by identifying the algebraic manipulations that increase most effectively the expressive power of the features used as inputs.</p>
        <p>Another open challenge is how to deal with nonadditivity, and with properties that depend on longrange interactions between far-away atoms. Particularly promising is a long-distance equivariant framework, that can be formulated as as a rather straightforward extension of the same density-correlations scheme that underlies local features, and can be related to a multipole expansion of interactions. It is yet to be seen whether it can describe more subtle physical phenomena such as quantum delocalization, polarization and charge transfer, and how it compares with more explicitly physically-motivated "hybrid" models. A better control of the multi-scale nature of the interactions, including the use of "multi-resolution" features, is likely to be one of the focal point of featureengineering efforts, which may lead to an incremental -but nevertheless important -increase of the accuracy of ML models of matter. The optimization of features for a specific problem may however impact their general applicability, which is one of the critical advantages of the class of abstract, generic represen-tations we focus on in this review, that can be seen as the point of convergence of molecular potential energy surfaces and condensed-phase potentials. The quantitative assessment of the mutual information content of alternative descriptors, of their sensitivity to structural deformations, and to the degree to which they correlate with the target properties may serve as a guide to strike a balance between these conflicting goals, and to make better informed choices between alternative frameworks.</p>
        <p>One of the most recent research directions aims at extending even further the reach of the class of descriptors we discuss in this review, by resolving the divide between three dimensional continuous representations and discrete fingerprints, for applications to quantitative structure-property relations. The challenge here is to reconcile the superior resolving power of 3D, atom-density-correlation representations with the fact that traditional cheminformatics tasks aim to predict macroscopic properties, such as solubility or toxicity, that are not associated with an individual configuration, but rather with the ensemble of conformers corresponding to a specific thermodynamic state point. Another traditional application of cheminformatics is the inverse design of molecules with prescribed (or optimized) properties, and the construction for generative models. While one could envisage to use 3D representations for this task, a substantial hurdle would be the fact that the map between structure and density-correlation features is not bijective: there are feature vectors that do not correspond to any structure, and even feature vectors that cannot be obtained as a symmetrized correlation of an arbitrary scalar field. Thus, the unconstrained search for the "optimal feature vector" might result in a set of features that do not correspond to an actual structure. Until this issue is better understood, efforts to use atom-density representations for inverse design should rely on approaches that do not require an inverse feature map.</p>
        <p>In the quest for more accurate and efficient machine-learning models of the structure and properties of atomistic systems, physically-motivated concepts have been incorporated into the mathematical representation of atomic configurations, resulting in striking connections with traditional modeling frameworks. When treading the fine line between datadriven and physics-based approaches, the core question is how to achieve a natural description of wellunderstood phenomena without giving up the flexibility to model unexpected, complex effects -and how to build features that can be optimized for a specific application, while still being universally applicable. A definitive answer to this question is still lacking, but we believe that the general principles that we have summarized in this review may indicate the direction to follow, and provide some guidance to the practitioners who seek to make an informed choice among the ever increasing number of representations for atomicscale modeling.</p>
        <p>The authors would like to thank Yasushi Shibuta for providing the structures used in Fig. 14, and Stefan Goedecker for providing Fig. 15, and the many colleagues and friends who discussed with us about this review, and the ideas it summarizes. FM, MC and AG acknowledge support by the National Center of Competence in Research MARVEL, funded by the Swiss National Science Foundation.</p>
        <p>Félix Musil studied physics at the EPFL and received his MSc in applied physics in 2015, with a thesis on the modeling of plasma in a fusion reactor. For his PhD he joined in 2016 the group of Prof. Ceriotti at the EPFL to develop and apply methods to investigate structure-property relationships in materials using atomistic modeling and machine learning techniques.</p>
        <p>Andrea Grisafi studied chemistry at the University of Pisa and Scuola Normale Superiore of Pisa. In 2016, he received his MSc in physical chemistry with a thesis on the statistical mechanics of simple ionic liquids. Since then, he is a PhD student in the group of Prof. Michele Ceriotti at EPFL, where he works on the development of atomic-scale representations that are suitable to incorporate physical symmetries and long-range effects within machine-learning models of molecular and materials properties.</p>
        <p>Albert P. Bartók is an Assistant Professor at the University of Warwick. He earned his PhD degree in physics from the University of Cambridge in 2010, his research having been on developing interatomic potentials based on ab inito data using machine learning. He was a Junior Research Fellow at Magdalene College, Cambridge and later a Leverhulme Early Career Fellow. Before taking up his current position, he was a Research Scientist at the Science and Technology Facilities Council. His research focuses on developing theoretical and computational tools to understand atomistic processes.</p>
        <p>Christoph Ortner is Professor of Mathematics at the University of British Columbia (Canada). After obtaining his doctorate in numerical analysis in 2007 at the University of Oxford (UK) and remaining there as an RCUK fellow, he moved to the University of Warwick in 2011 and to UBC in 2020. His main interests revolve around mathematical and computational aspects of atomistic and multi-scale modeling.</p>
        <p>Gábor Csányi is Professor of Molecular Modelling at the University of Cambridge (UK). He obtained his doctorate in computational physics (2001) from the Massachusetts Institute of Technology (USA), having worked on electronic structure problems. He was in the group of Mike Payne in the Cavendish Laboratory before joining the faculty of the Engineering Laboratory at Cambridge. He is developing algorithms and data driven numerical methods for atomic scale problems in materials science and chemistry.</p>
        <p>Michele Ceriotti is Associate Professor at the Institute of Materials at the École Polytechnique Fédérale de Lausanne. He received his Ph.D. in Physics from ETH Zürich in 2010, under the supervision of Professor Michele Parrinello. He spent three years in Oxford as a Junior Research Fellow at Merton College, and joined EPFL in 2013, where he leads the laboratory for Computational Science and Modeling. His research interests focus on the development of methods for molecular dynamics and the simulation of complex systems at the atomistic level, as well as their application to problems in chemistry and materials science -using machine learning both as an engine to drive more accurate and predictive simulations, and as a conceptual tool to investigate the interplay between data-driven and physics-inspired modeling.</p>
    </text>
</tei>

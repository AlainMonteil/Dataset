<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:20+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>or visit the DOI to the publisher's website.or visit the DOI to the publisher's website.</p>
        <p>• The final author version and the galley proof are versions of the publication after peer review.• The final author version and the galley proof are versions of the publication after peer review.</p>
        <p>• The final published version features the final layout of the paper including the volume, issue and page numbers.• The final published version features the final layout of the paper including the volume, issue and page numbers.</p>
        <p>Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.</p>
        <p>• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal.• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal.</p>
        <p>If the publication is distributed under the terms of Article 25fa of the Dutch Copyright Act, indicated by the "Taverne" license above, please follow below link for the End UserIf the publication is distributed under the terms of Article 25fa of the Dutch Copyright Act, indicated by the "Taverne" license above, please follow below link for the End User</p>
        <p>to alternatives, the high information density of images, and the hardware can be used for multiple diseases and sites. 2,3 Medical imaging in its infancy generated analogue images, which underwent subjective interpretation based on visual inspection and verbal communication. By the end of the 20th century, information technology has brought radiology to the digital world, 4 although the interpretation of radiographs remained mostly qualitative. Humans excel at recognising patterns through visual inspection, however, they are often lacking when performing complex quantitative assessments. 5,6 In the early 1960s, researchers started to focus on computerised quantitative analysis of medical data for aiding clinical diagnosis, [7][8][9] what later came to be known as computer-aided diagnosis (CAD) systems. However, these systems were using a classical approach using statistical analysis and probability theories, and the volume of available data was low, so the results were often too inaccurate for clinical use. Later in the 1980s, further advances in theoretical computer science and digital imaging lead to the development of advanced machine learning and pattern recognition algorithms, which when integrated with CAD systems were able to generate clinically reliable results. 10,11 In recent decades, simple quantitative image analysis (QIA) has been adopted by clinicians (e.g. RECIST 12 ), and has been primarily focused on assisting qualitative observations. 13 For instance, CAD systems can be found in healthcare worldwide, aiding radiologists and clinicians in making diagnostic and theragnostic decisions. 14 One of the most typical applications of CAD systems is in recognising abnormalities during cancer screening. 15 Notable contributions are in the area of lung and breast cancer research. For example, there are many CAD studies which focus on detecting and diagnosing lung nodules 16,17 (as benign or malignant) on CT and chest radiographs. Similarly, many such studies have been conducted in breast mammography images for highlighting microcalcifications, 18 architectural distortions, and the prediction of mass type. 19,20 It is conceivable that the lack of quantitative information leads to increased follow-ups or invasive biopsies that would be deemed unnecessary given the unused information in medical images. 21 Even though there have been various developments in QIA, traditionally radiologists are trained to understand the behaviour of the underlying disease through visual inspection of radiographic images. 21 This partially explains why most of the developments in imaging technology are in optimising the visual representation of the generated images, with vendors competing to generate the highest quality images. With the exception of CT, with its semi-parametric calibrated Hounsfield Units, and some particular MRI sequences, individual voxel values do not correlate with the underlying biology without further calibration and modelling. Furthermore, qualitative analysis is not so dependent on reproducible voxel values, while machines on the other hand only process numerical values and rely on the standardisation of image acquisition and reconstruction to yield reproducible results. The lack of standardisation of medical images has been a major hurdle in the development of QIA in medical imaging. [22][23][24][25] However, in recent years, quantitative imaging is becoming more popular with the advent of, e.g. quantitative fludeoxyglucose-PET 26,27 or quantitative MRI 28,29 for treatment response assessment.to alternatives, the high information density of images, and the hardware can be used for multiple diseases and sites. 2,3 Medical imaging in its infancy generated analogue images, which underwent subjective interpretation based on visual inspection and verbal communication. By the end of the 20th century, information technology has brought radiology to the digital world, 4 although the interpretation of radiographs remained mostly qualitative. Humans excel at recognising patterns through visual inspection, however, they are often lacking when performing complex quantitative assessments. 5,6 In the early 1960s, researchers started to focus on computerised quantitative analysis of medical data for aiding clinical diagnosis, [7][8][9] what later came to be known as computer-aided diagnosis (CAD) systems. However, these systems were using a classical approach using statistical analysis and probability theories, and the volume of available data was low, so the results were often too inaccurate for clinical use. Later in the 1980s, further advances in theoretical computer science and digital imaging lead to the development of advanced machine learning and pattern recognition algorithms, which when integrated with CAD systems were able to generate clinically reliable results. 10,11 In recent decades, simple quantitative image analysis (QIA) has been adopted by clinicians (e.g. RECIST 12 ), and has been primarily focused on assisting qualitative observations. 13 For instance, CAD systems can be found in healthcare worldwide, aiding radiologists and clinicians in making diagnostic and theragnostic decisions. 14 One of the most typical applications of CAD systems is in recognising abnormalities during cancer screening. 15 Notable contributions are in the area of lung and breast cancer research. For example, there are many CAD studies which focus on detecting and diagnosing lung nodules 16,17 (as benign or malignant) on CT and chest radiographs. Similarly, many such studies have been conducted in breast mammography images for highlighting microcalcifications, 18 architectural distortions, and the prediction of mass type. 19,20 It is conceivable that the lack of quantitative information leads to increased follow-ups or invasive biopsies that would be deemed unnecessary given the unused information in medical images. 21 Even though there have been various developments in QIA, traditionally radiologists are trained to understand the behaviour of the underlying disease through visual inspection of radiographic images. 21 This partially explains why most of the developments in imaging technology are in optimising the visual representation of the generated images, with vendors competing to generate the highest quality images. With the exception of CT, with its semi-parametric calibrated Hounsfield Units, and some particular MRI sequences, individual voxel values do not correlate with the underlying biology without further calibration and modelling. Furthermore, qualitative analysis is not so dependent on reproducible voxel values, while machines on the other hand only process numerical values and rely on the standardisation of image acquisition and reconstruction to yield reproducible results. The lack of standardisation of medical images has been a major hurdle in the development of QIA in medical imaging. [22][23][24][25] However, in recent years, quantitative imaging is becoming more popular with the advent of, e.g. quantitative fludeoxyglucose-PET 26,27 or quantitative MRI 28,29 for treatment response assessment.</p>
        <p>The ubiquitous computer, vast amounts of data, and advanced algorithms have opened a new era in medical imaging. The high information density of images allows for many quantitative metrics since intricate pixel and voxel relationships can be captured by complex operations. Radiomics involves the process of extraction of quantifiable features from vast amounts of data that might correlate with the underlying biology or clinical outcomes using advanced machine learning analysis techniques. 30,31 Radiomics has two main arms, based on how imaging information is transformed into mineable data: handcrafted radiomics and deep learning. Handcrafted features are formulas mostly based on intensity histograms, shape attributes, and texture, that can be used to fingerprint phenotypical characteristics of the radiograph 32 while in deep learning a complex network "creates" its own features. Various statistical and machine learning models have been widely researched, and are envisioned to be complementary to best medical practice by aiding in making informed clinical decisions in both oncological and non-oncological diseases. [33][34][35][36] Since the 1990s predictions were being made that genomics, spearheaded by the Human Genome Project, would completely transform therapeutic medicine, heralding precision medicine. 37 Precision medicine, also termed personalised medicine, originally referred to the view that incorporating genomic information in the clinical workflow will lead to marked improvements in the prediction, diagnosis, and treatment of diseases. Recently, the scope of precision medicine has expanded to incorporate inputs beyond the genome. 38 Radiomics and other "-omic" developments, such as metabolomics and proteomics, are contributing to this a paradigm shift in medicine, where the focus has changed from standard clinical protocols based on trial populations to a personalised treatment tailored not only to the disease and site but also the patient, further enabling precision medicine.The ubiquitous computer, vast amounts of data, and advanced algorithms have opened a new era in medical imaging. The high information density of images allows for many quantitative metrics since intricate pixel and voxel relationships can be captured by complex operations. Radiomics involves the process of extraction of quantifiable features from vast amounts of data that might correlate with the underlying biology or clinical outcomes using advanced machine learning analysis techniques. 30,31 Radiomics has two main arms, based on how imaging information is transformed into mineable data: handcrafted radiomics and deep learning. Handcrafted features are formulas mostly based on intensity histograms, shape attributes, and texture, that can be used to fingerprint phenotypical characteristics of the radiograph 32 while in deep learning a complex network "creates" its own features. Various statistical and machine learning models have been widely researched, and are envisioned to be complementary to best medical practice by aiding in making informed clinical decisions in both oncological and non-oncological diseases. [33][34][35][36] Since the 1990s predictions were being made that genomics, spearheaded by the Human Genome Project, would completely transform therapeutic medicine, heralding precision medicine. 37 Precision medicine, also termed personalised medicine, originally referred to the view that incorporating genomic information in the clinical workflow will lead to marked improvements in the prediction, diagnosis, and treatment of diseases. Recently, the scope of precision medicine has expanded to incorporate inputs beyond the genome. 38 Radiomics and other "-omic" developments, such as metabolomics and proteomics, are contributing to this a paradigm shift in medicine, where the focus has changed from standard clinical protocols based on trial populations to a personalised treatment tailored not only to the disease and site but also the patient, further enabling precision medicine.</p>
        <p>In this review, we provide a broad overview and update on the fast-growing field of quantitative imaging research, focussing on the two arms "handcrafted radiomics and deep learning" describing some of its caveats and giving examples of the budding clinical implementation, the stepping stones towards precision medicine.In this review, we provide a broad overview and update on the fast-growing field of quantitative imaging research, focussing on the two arms "handcrafted radiomics and deep learning" describing some of its caveats and giving examples of the budding clinical implementation, the stepping stones towards precision medicine.</p>
        <p>Performing feature extraction of textures in medical imaging is nothing new and in fact serious research had begun in the early 1980s at Kurt Rossmann Laboratories for Radiologic Image Research in the Department of Radiology at the University of Chicago to develop CAD systems for the detection of lung nodules as well as detection of clustered microcalcifications in mammograms. 39,40 The first CAD patent was filed all the way back in 1987 using a method of pixel thresholding and contiguous pixel area thresholding. 40 The radiomic workflow begins with the medical image, which can be represented in two, three, or four dimensions. 32,41 Images contain quantitative data in the form of signals that are captured at different scales and variation across medical machines. 42,43 Normalisation techniques are used to distribute pixel intensities evenly across a data set evenly and within a standardised range. 42,43 44 . Next, a region of interest (ROI) is defined so that only information related to the lesion can be extracted, and the useful information that can be extracted are called features. There are competing methods to extract features both in twodimensional and three-dimensional. One such method is the manual segmentation of the lesion or the creation of a bounding box, as seen in Figure 2. 45,46 This can also be performed using automated segmentation algorithms. Methods for automated segmentation include deep learning architectures such as 
            <rs type="software">U-Net</rs>, or semi-automatic methods like click-and-grow algorithms. 45,46 Once the ROI is defined, the choice of features to be extracted depend on the information being sought. Shape features such as volume relate only to the definition of the ROI, and if this is manually created, suffer from inter-and intraobserver variability. 47 First-order features give insight into the distribution of pixel intensities, e.g. histograms of pixel intensities are quantified by a large number of statistical methods, including variance, skewness, and kurtosis. These features, however, are unable to quantify how pixels are positioned in relation to each other. Second and higher-order features may capture this relationship, with second-order features obtained based on the average relationship between two pixels/voxels, and higher-order features for more than two pixels/voxels. An example of a second-order feature extraction method is the grey level co-occurring matrix (GLCM). GLCMs are co-occurring pixels in each defined direction (Figure 3) and are counted and recorded (Figure 4) into a matrix. Statistical analysis such as contrast, correlation, and homogeneity, as well as tailored formulae can then be applied on the GLCM to extract independent features. 48 Features extracted in this manner are considered "handcrafted" features as they are features that are pre-defined by specially designed formulae. After features have been extracted from all the images in a database, a subset of features needs to be selected that go into the final model. To make a model generalisable, it is important to avoid finding spurious correlations in the data that do not generalise to other similar data sets, an occurrence termed overfitting. [49][50][51] If a model has learned to recognise noise, outliers, or other kinds of variance, it is unlikely to perform well when presented with new data. The larger the number of predictors, the larger the chance to find spurious correlations, a major problem in the realm of machine learning. 52 To detect overfitting, ideally, a model's performance is validated in external data sets with similar population and outcome distributions, but from different centres-if the model performs significantly better on the training set than on the validation set, overfitting is likely. 53,54 In the absence of an external validation data set, data can be split into different subsets, and the model trained in one group and validated on the other(s) in a process called cross-validation (Figure 5). 55 During this process, the model hyperparameters (settings within the model itself, e.g. degree of polynomial fitting) can be further tuned to increase performance in the training and validation sets. 56 A method to overcome overfitting is to reduce the number of predictors, in this case, imaging features. Feature selection is the process of reducing the number of predictors while retaining the core important information that correlates with outcomes or the underlying biology. 32 Many feature reduction methods exist, but none are known to work well on all kinds of data sets, and they can be combined in many ways. 32 This remains an active field of research. 57 Similar features can also be grouped to achieve dimensionality reduction, and methods such as principal component analysis and independent component analysis are employed to this end. 58 Once features are selected, the task is to correlate these features-individually or in groups-to diagnostic and prognostic outcomes or to the underlying biology. There are numerous methods to find and test such models, from simple linear regression and curve-fitting to advanced machine learning methods such as decision trees, support vector machines, random forests, boosted trees, or neural networks. 59 Ensembling is the combination of models that get trained on random samples of data from the training set called bags and then combined as a whole using a voting system. This is the basis for algorithms such as Random Forests, AdaBoost, and Gradient Boosting. 60 An intuitive explanation is that even though the individual models can show a large amount of variance due to being trained on small subsets of the data, their averaging or voting smooths out the variance while improving the ability to better generalise. 60 Once a generalisable model has been trained and externally validated, it might be desirable to expand the interoperability of the model to all hardware, acquisition, and reconstruction parameters found in general clinical practice. Instead of relying on the standardisation of images, the features themselves can be harmonised to a common frame-of-reference using combined batch methods such as ComBat, 44,60,61 originally developed for similar problems encountered in gene sequencing assays. 62 63 With the exception of unsupervised learning (such as autoencoders), deep learning architectures usually rely on information regarding the outcome in order to craft their features, and unlike in handcrafted radiomics, feature extraction and correlation are intertwined. 64 Also, unlike radiomics, there is generally no need for image segmentation, as the whole image can be presented to a deep learning model, both during training and in clinical routine.
        </p>
        <p>An ANN is able to use a collection of neurons and weights, one for each of the inputs preceding the neuron. 65 These weights get continuously updated, or corrected, in steps called epochs that work together to create a very complex function able to make predictions. The weights are inputs for each neuron and are multiplied and averaged, resulting in a transfer function, which is converted to an output via a function called an activation function. 66 These activation functions are often a sigmoidal function such as a hyperbolic tangent or sigmoid, or a function called a rectified linear unit that can be represented as the maximum of the product of the coefficient and zero or one. A representation of a single neuron, including the activation function, can be seen in Figure 6. 67 Multiple neurons can then be stacked to create a single layer referred to as a "hidden layer" and hidden layers (were inputs and outputs all connect) can be stacked to create larger networks, see Figure 7. 65 The term deep learning is used to describe a neural network that has many layers, which is considered deep. For a binary classifier or regression, the final layer should contain only a single neuron and use a sigmoid activation function to make a prediction with a binary outcome (zero or one). If the problem is categorical, the network's final layer should contain the same number of neurons as there are categories to be classified and the final activation will be a "softmax" function, which is the average of the exponentials of the inputs, 68 yielding the probabilities of each category. Deep learning for image vision employs convolutional neural networks (CNNs) which are a type of ANN that have an automated feature extractor designed specifically for images. 69 CNNs employ a filtering technique, which convolves the image with a kernel (sliding window), creating a new pixel/voxel value (and hence new image) by sliding a matrix of numbers over the image, see Figure 8. It is possible to make a variety of different filters using these types of convolutions, such as blurring, sharpening, edge detection, and gradient detection 69,70 , and CNNs are able to learn filters that are best suited to extracting features needed for making predictions.An ANN is able to use a collection of neurons and weights, one for each of the inputs preceding the neuron. 65 These weights get continuously updated, or corrected, in steps called epochs that work together to create a very complex function able to make predictions. The weights are inputs for each neuron and are multiplied and averaged, resulting in a transfer function, which is converted to an output via a function called an activation function. 66 These activation functions are often a sigmoidal function such as a hyperbolic tangent or sigmoid, or a function called a rectified linear unit that can be represented as the maximum of the product of the coefficient and zero or one. A representation of a single neuron, including the activation function, can be seen in Figure 6. 67 Multiple neurons can then be stacked to create a single layer referred to as a "hidden layer" and hidden layers (were inputs and outputs all connect) can be stacked to create larger networks, see Figure 7. 65 The term deep learning is used to describe a neural network that has many layers, which is considered deep. For a binary classifier or regression, the final layer should contain only a single neuron and use a sigmoid activation function to make a prediction with a binary outcome (zero or one). If the problem is categorical, the network's final layer should contain the same number of neurons as there are categories to be classified and the final activation will be a "softmax" function, which is the average of the exponentials of the inputs, 68 yielding the probabilities of each category. Deep learning for image vision employs convolutional neural networks (CNNs) which are a type of ANN that have an automated feature extractor designed specifically for images. 69 CNNs employ a filtering technique, which convolves the image with a kernel (sliding window), creating a new pixel/voxel value (and hence new image) by sliding a matrix of numbers over the image, see Figure 8. It is possible to make a variety of different filters using these types of convolutions, such as blurring, sharpening, edge detection, and gradient detection 69,70 , and CNNs are able to learn filters that are best suited to extracting features needed for making predictions.</p>
        <p>ANNs do have some drawbacks compared to using handcrafted features alongside other machine learning techniques. The main drawback is the intrinsic need for much larger datasets to train the models, since feature creation is contingent on the training data, as opposed to handcrafted radiomics. Another drawback to using ANNs is interpretability. ANNs build ultracomplex functions that can be extremely difficult for practitioners to make sense of. Although CNNs have performed very well in image recognition, they have been less successful learning texture features, since texture information inherently has a higher dimensionality compared to other types of data sets, making them more difficult for neural networks to master. 69,71 According to Basu et al, 71 a redesign of neural network architectures is required to extract features in a similar manner as GLCM and other features based on spatial correlation.ANNs do have some drawbacks compared to using handcrafted features alongside other machine learning techniques. The main drawback is the intrinsic need for much larger datasets to train the models, since feature creation is contingent on the training data, as opposed to handcrafted radiomics. Another drawback to using ANNs is interpretability. ANNs build ultracomplex functions that can be extremely difficult for practitioners to make sense of. Although CNNs have performed very well in image recognition, they have been less successful learning texture features, since texture information inherently has a higher dimensionality compared to other types of data sets, making them more difficult for neural networks to master. 69,71 According to Basu et al, 71 a redesign of neural network architectures is required to extract features in a similar manner as GLCM and other features based on spatial correlation.</p>
        <p>Currently, the main application of deep learning in the radiomics workflow still lies in the automated detection and localisation of organs and lesions, removing the major burden in data set curation. While there is no algorithm that can solve every problem, deep learning still has its place and is able to work as additional methods for delineation and feature extraction that compliments handcrafted radiomics. There is active research in combining both deep learning features and radiomics features that shows improved results. [72][73][74]Currently, the main application of deep learning in the radiomics workflow still lies in the automated detection and localisation of organs and lesions, removing the major burden in data set curation. While there is no algorithm that can solve every problem, deep learning still has its place and is able to work as additional methods for delineation and feature extraction that compliments handcrafted radiomics. There is active research in combining both deep learning features and radiomics features that shows improved results. [72][73][74]</p>
        <p>Radiomics has been widely studied for application in diagnosis and treatment prognosis/selection in oncology, primarily due to the existence of large imaging data sets used for staging, often containing delineations of tumours and organs at risk necessary for radiation treatment planning. These data sets can be used to train diagnostic and prognostic models for a variety of cancer types and sites. Using clinical reports, pathology/histology, and genetic information along with radiomics analysis can give a global outlook on the biology of the disease. 48 In this section, an overview of notable studies published in this area will be discussed.Radiomics has been widely studied for application in diagnosis and treatment prognosis/selection in oncology, primarily due to the existence of large imaging data sets used for staging, often containing delineations of tumours and organs at risk necessary for radiation treatment planning. These data sets can be used to train diagnostic and prognostic models for a variety of cancer types and sites. Using clinical reports, pathology/histology, and genetic information along with radiomics analysis can give a global outlook on the biology of the disease. 48 In this section, an overview of notable studies published in this area will be discussed.</p>
        <p>Lung cancer is by far the leading cause of cancer-related deaths among both males and females worldwide. 75 Recent studies have shown that radiomics can determine the risk of lung cancer from screening scans. [76][77][78] Radiomic features found to have a strong association to decode tumour heterogeneity for risk stratification, 79,80 concluding that patients with heterogeneous tumours tend to have a worse prognosis. In addition to that, Yoon et al were able to show the association of radiomic analysis with gene expression. 81 Radiomic features were also found to correlate with TNM staging for lung and head-and-neck cancer. 31,82 Later studies further validated the strong predictive power of radiomics for distant metastasis. [83][84][85] Radiomics may also play a role in lung cancer treatment planning by evaluating tumour response to a specific treatment. Several studies focused on analysing the tumour response to radiation therapy. 86,87 For instance, Mattonen et al developed a radiomics signature for treatment response to stereotactic ablative radiation therapy that was able to predict lung cancer recurrence posttherapy, 86 while Fave et al used multiple time point information referred to as delta-radiomic analysis to evaluate the change of radiomic features as a predictor for tumour response to radiation therapy. 87 The results suggest that δ radiomic features are in fact a good indicator of treatment response. Another interesting study by Mattonen et al found that radiomic analysis can identify features associated with local recurrence of lung cancer after Besides the traditional handcrafted feature extraction approach followed in the radiomics pipeline, deep learning radiomics is also gaining popularity among researchers. A deep learning-based approach followed by Shen et al yielded more accurate malignancy prediction of nodules compared to previous methods. 89 Pham et al used a two-step deep learning approach for evaluating lymph node metastases with accurate cancer detection. 90 Instead of using data from a single time point, deep recurrent convolutional network architectures can be used to analyse data from multiple time points to monitor treatment response. 91 Brain Brain tumours are usually graded based on clinical or pathological analysis to define their malignancy. Radiomics may be able to noninvasively perform grade assessment, as reported by Coroller et al in meningioma patients, suggesting a strong correlation between certain imaging features and histopathological grade. 92 Zhang et al were able to classify between low-grade gliomas and high-grade gliomas with high accuracy. 93 Chen et al investigated the prediction of brain metastases in T1 lung adenocarcinoma patients and found that the predictive performance for the radiomics model was significantly better compared to clinical models and could potentially be used for brain metastases screening. 94 Fetit et al performed radiomic analysis for the classification of brain tumours in childhood suggesting that radiomics can aid in the classification of tumour subtype. 95 However, the scalability of the techniques used in these studies needs to be assessed further by extensions to multicentric cohorts using different acquisition protocols and vendors.Lung cancer is by far the leading cause of cancer-related deaths among both males and females worldwide. 75 Recent studies have shown that radiomics can determine the risk of lung cancer from screening scans. [76][77][78] Radiomic features found to have a strong association to decode tumour heterogeneity for risk stratification, 79,80 concluding that patients with heterogeneous tumours tend to have a worse prognosis. In addition to that, Yoon et al were able to show the association of radiomic analysis with gene expression. 81 Radiomic features were also found to correlate with TNM staging for lung and head-and-neck cancer. 31,82 Later studies further validated the strong predictive power of radiomics for distant metastasis. [83][84][85] Radiomics may also play a role in lung cancer treatment planning by evaluating tumour response to a specific treatment. Several studies focused on analysing the tumour response to radiation therapy. 86,87 For instance, Mattonen et al developed a radiomics signature for treatment response to stereotactic ablative radiation therapy that was able to predict lung cancer recurrence posttherapy, 86 while Fave et al used multiple time point information referred to as delta-radiomic analysis to evaluate the change of radiomic features as a predictor for tumour response to radiation therapy. 87 The results suggest that δ radiomic features are in fact a good indicator of treatment response. Another interesting study by Mattonen et al found that radiomic analysis can identify features associated with local recurrence of lung cancer after Besides the traditional handcrafted feature extraction approach followed in the radiomics pipeline, deep learning radiomics is also gaining popularity among researchers. A deep learning-based approach followed by Shen et al yielded more accurate malignancy prediction of nodules compared to previous methods. 89 Pham et al used a two-step deep learning approach for evaluating lymph node metastases with accurate cancer detection. 90 Instead of using data from a single time point, deep recurrent convolutional network architectures can be used to analyse data from multiple time points to monitor treatment response. 91 Brain Brain tumours are usually graded based on clinical or pathological analysis to define their malignancy. Radiomics may be able to noninvasively perform grade assessment, as reported by Coroller et al in meningioma patients, suggesting a strong correlation between certain imaging features and histopathological grade. 92 Zhang et al were able to classify between low-grade gliomas and high-grade gliomas with high accuracy. 93 Chen et al investigated the prediction of brain metastases in T1 lung adenocarcinoma patients and found that the predictive performance for the radiomics model was significantly better compared to clinical models and could potentially be used for brain metastases screening. 94 Fetit et al performed radiomic analysis for the classification of brain tumours in childhood suggesting that radiomics can aid in the classification of tumour subtype. 95 However, the scalability of the techniques used in these studies needs to be assessed further by extensions to multicentric cohorts using different acquisition protocols and vendors.</p>
        <p>Radiation therapy can lead to necrosis, which is difficult to distinguish from tumour recurrence on imaging. Larroza et al were able to develop a high classification accuracy model to distinguish between brain metastasis and radiation necrosis using radiomic analysis. 96 Some radiomic studies successfully investigated the treatment response in recurrent glioblastoma patients with a radiomics approach. [97][98][99] An iterative study by radiomic researchers found strong evidence of radiomic features in predicting survival and treatment response of patients with glioblastoma using pre-treatment imaging data. [100][101][102] Deep learning has also made some other interesting contributions in this area. Chang et al used residual deep convolutional network for predicting the genotype in Grade II-IV glioma with high accuracy. 103 Deep learning can also be used complementary to traditional handcrafted radiomics studies. For example, studies 72,73 focused on using deep networks for segmentation, followed by radiomics analysis for survival prediction. Breast Among females, breast cancer is the second leading cause of death for cancer worldwide. 75 However, earlier diagnosis can lead to a better prognosis. Radiomics in the field of breast cancer has been applied to several imaging modalities including (PET)-MRI, (contrast-enhanced) mammography, ultrasound, and digital breast tomosynthesis focusing on tumour classification, molecular subtypes, tumour response prediction to neoadjuvant systemic therapy (NST), lymph node metastasis, overall survival, and recurrence risks. For example, a large number of radiomics studies have been used for the prediction of malignant breast cancers. [104][105][106][107] Besides the prediction of tumour malignancy, several radiomics studies examined the prediction of breast cancer molecular subtypes with the aim of leaving out liquid biopsies in the future. [108][109][110][111] Lymph node metastasis identification is an important prognostic factor and often determines treatment. In all clinically node negative patients, a sentinel lymph node procedure is the basis of the axillary treatment. 112 Dong et al was able to provide an alternative to this invasive approach by successfully applying radiomics for the prediction of lymph node metastasis in the sentinel lymph node using imaging data. 113 In addition to the prediction of breast tumour malignancy, tumour molecular subtypes and sentinel lymph node metastasis identification, radiomics studies have also made some significant contributions to treatment planning. Chan et al investigated the power of radiomics to discriminate between patients with low and high treatment failure risk on pre-treatment imaging data. 114 There are multiple studies that predict tumour response to NST using radiomic analysis. For instance, Braman et al found a combination of intratumoral and peritumoral radiomics features as a robust and strong indicator for pathologic complete tumour response using pre-treatment imaging data. 115 Two other studies 116,117 found similar evidence on serial imaging data containing follow-up scans. The use of multiparametric MRI for the prediction of tumour response to NST showed promising results. 118,119 Deep learning approaches have also been adopted in breast cancer research. The study of Huynh et al investigated tumour classification capacity of deep features extracted from convolutional networks trained on a different data set to analytically extracted features. 120 The results suggested a higher performance of deep features. Similarly, another study, 121 used deep learning for risk assessment and found higher performance compared to conventional texture analysis.Radiation therapy can lead to necrosis, which is difficult to distinguish from tumour recurrence on imaging. Larroza et al were able to develop a high classification accuracy model to distinguish between brain metastasis and radiation necrosis using radiomic analysis. 96 Some radiomic studies successfully investigated the treatment response in recurrent glioblastoma patients with a radiomics approach. [97][98][99] An iterative study by radiomic researchers found strong evidence of radiomic features in predicting survival and treatment response of patients with glioblastoma using pre-treatment imaging data. [100][101][102] Deep learning has also made some other interesting contributions in this area. Chang et al used residual deep convolutional network for predicting the genotype in Grade II-IV glioma with high accuracy. 103 Deep learning can also be used complementary to traditional handcrafted radiomics studies. For example, studies 72,73 focused on using deep networks for segmentation, followed by radiomics analysis for survival prediction. Breast Among females, breast cancer is the second leading cause of death for cancer worldwide. 75 However, earlier diagnosis can lead to a better prognosis. Radiomics in the field of breast cancer has been applied to several imaging modalities including (PET)-MRI, (contrast-enhanced) mammography, ultrasound, and digital breast tomosynthesis focusing on tumour classification, molecular subtypes, tumour response prediction to neoadjuvant systemic therapy (NST), lymph node metastasis, overall survival, and recurrence risks. For example, a large number of radiomics studies have been used for the prediction of malignant breast cancers. [104][105][106][107] Besides the prediction of tumour malignancy, several radiomics studies examined the prediction of breast cancer molecular subtypes with the aim of leaving out liquid biopsies in the future. [108][109][110][111] Lymph node metastasis identification is an important prognostic factor and often determines treatment. In all clinically node negative patients, a sentinel lymph node procedure is the basis of the axillary treatment. 112 Dong et al was able to provide an alternative to this invasive approach by successfully applying radiomics for the prediction of lymph node metastasis in the sentinel lymph node using imaging data. 113 In addition to the prediction of breast tumour malignancy, tumour molecular subtypes and sentinel lymph node metastasis identification, radiomics studies have also made some significant contributions to treatment planning. Chan et al investigated the power of radiomics to discriminate between patients with low and high treatment failure risk on pre-treatment imaging data. 114 There are multiple studies that predict tumour response to NST using radiomic analysis. For instance, Braman et al found a combination of intratumoral and peritumoral radiomics features as a robust and strong indicator for pathologic complete tumour response using pre-treatment imaging data. 115 Two other studies 116,117 found similar evidence on serial imaging data containing follow-up scans. The use of multiparametric MRI for the prediction of tumour response to NST showed promising results. 118,119 Deep learning approaches have also been adopted in breast cancer research. The study of Huynh et al investigated tumour classification capacity of deep features extracted from convolutional networks trained on a different data set to analytically extracted features. 120 The results suggested a higher performance of deep features. Similarly, another study, 121 used deep learning for risk assessment and found higher performance compared to conventional texture analysis.</p>
        <p>While cancers of the lung, brain, and breast have received wide attention from the radiomics research community, any site is open to QIA research. Diagnostic and prognostic radiomics research is ongoing for cancers of the head-and-neck, 122 ovaries, 38 prostate, 123 kidney, 124 liver, 125 colon and rectum, 126 and many other sites. The main requirements for a radiomics study are the presence of a radiologic phenotype which allows for the clustering of patients based on differences within that phenotype or some correlation to the underlying biology, and the availability of imaging and clinical data. While not nearly as prevalent, 127 this has meant that nononcological diseases which require medical imaging as part of the standard of care have also been the subject of radiomics analysis, such as in the fields of neurology, 35 ophthalmology, 128 and dentistry. 129 Limitations of radiomics and future directions towards precision medicine While radiomics facilitates new possibilities in the field of personalised medicine, some challenges remain. One of the primary obstacles is the lack of big and standardised clinical data. Although large amounts of medical imaging data are stored, these data are dispersed across different centres and acquired using different protocols. Access for research purposes is highly restricted by law and ethics. An exhaustive data curation and harmonisation process is still necessary to make it usable for research. Radiomics will potentially enable imagingbased clinical decision support systems, however, the current black box approach, particularly in deep learning, makes it less acceptable for clinical application. In certain cases, handcrafted radiomic features have already been correlated with biological processes, [130][131][132] but it is essential to work further in the direction of interpretable artificial intelligence (AI) to make it more accessible for clinical implementation. 33 In recent years, various countries have already adopted many measures to control variability in clinical trial protocols, data acquisition, and analysis. 133,134 For example, across Europe consistent protocol guidance was adopted with the help of European Association of Nuclear Medicine. 135 The Quantitative Imaging Biomarker Alliance initiative also aims to achieve the same task in a much broader level. 136,137 On the other hand, algorithmically, developments in deep learning allow for automated quality check, clustering of data, and automated detection and contouring of organs and lesions, vastly improving data curation times. Generative adversarial networks open up the possibility of generating synthetic data 138 or domain adaptive algorithms 139,140 might be able to deal with the shortage of standardised data. Techniques like distributed learning provide the ability to train machine learning models using distributed data without the data ever leaving their original locations. Distributed learning has already been applied across several medical institutions to build predictive and segmentation models. [141][142][143][144] Furthermore, this approach can be coupled with other technologies such as blockchain to trace back data provenance and monitor the use of the final models. 145 Various techniques to visualise deep features have already been put forward by researchers to generate an intuitive understanding. A completely new research area of AI called explainable AI aims to track the decisions made by the intelligent algorithms so that it can be better understood by humans. Companies like Google, IBM, Microsoft and Facebook are at the forefront in this research. This will not only help to build trust of AI systems among medical professionals but also unlocks new possibilities in understanding a disease. 146,147 The implementation of precision medicine itself has its own limitations and has drawn criticism due to the lack of a "transformation in therapeutic medicine" in the last two decades. 148 So far, life expectancies or other public health measures have not shown any dramatic improvements, regardless of the vast amounts of precision medicine research being conducted. Contentious points remain such as excessive costs (e.g. gene therapy), although new developments such as radiomics promise to reduce costs in the long run. Furthermore, the diagnostic and prognostic power of complex "omics-driven" models is still to be determined in specific populations, and evidence needs to be produced that such methods improve health outcomes. 149 Precision medicine is likely to mature and translate to clinical workflows over the next decade and will change the way health services are delivered and evaluated. Healthcare systems will need to adjust their methods and processes to accommodate for these changes. conclusion Radiomics, whether handcrafted or deep, is an emerging field that translates medical images into quantitative data to give biological information and enable phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Radiomics, in essence, allows personalised care by identifying features or signatures correlated with a disease or a treatment response with high precision and in a non-invasive way. Recent developments in genomics and deep learning have pushed radiomics researchers to focus more on extracting deep features and explore new possibilities in AI modelling. In the future, radiomics will be a valued addition to precision medicine workflows by facilitating earlier and more accurate diagnosis, providing prognostic information, aiding in treatment choice, monitoring disease and treatment non-invasively, and enabling routine dynamic treatment based on individual responses. But the road to this vision is long, and many technical, regulatory, and ethical problems still need to be solved.While cancers of the lung, brain, and breast have received wide attention from the radiomics research community, any site is open to QIA research. Diagnostic and prognostic radiomics research is ongoing for cancers of the head-and-neck, 122 ovaries, 38 prostate, 123 kidney, 124 liver, 125 colon and rectum, 126 and many other sites. The main requirements for a radiomics study are the presence of a radiologic phenotype which allows for the clustering of patients based on differences within that phenotype or some correlation to the underlying biology, and the availability of imaging and clinical data. While not nearly as prevalent, 127 this has meant that nononcological diseases which require medical imaging as part of the standard of care have also been the subject of radiomics analysis, such as in the fields of neurology, 35 ophthalmology, 128 and dentistry. 129 Limitations of radiomics and future directions towards precision medicine While radiomics facilitates new possibilities in the field of personalised medicine, some challenges remain. One of the primary obstacles is the lack of big and standardised clinical data. Although large amounts of medical imaging data are stored, these data are dispersed across different centres and acquired using different protocols. Access for research purposes is highly restricted by law and ethics. An exhaustive data curation and harmonisation process is still necessary to make it usable for research. Radiomics will potentially enable imagingbased clinical decision support systems, however, the current black box approach, particularly in deep learning, makes it less acceptable for clinical application. In certain cases, handcrafted radiomic features have already been correlated with biological processes, [130][131][132] but it is essential to work further in the direction of interpretable artificial intelligence (AI) to make it more accessible for clinical implementation. 33 In recent years, various countries have already adopted many measures to control variability in clinical trial protocols, data acquisition, and analysis. 133,134 For example, across Europe consistent protocol guidance was adopted with the help of European Association of Nuclear Medicine. 135 The Quantitative Imaging Biomarker Alliance initiative also aims to achieve the same task in a much broader level. 136,137 On the other hand, algorithmically, developments in deep learning allow for automated quality check, clustering of data, and automated detection and contouring of organs and lesions, vastly improving data curation times. Generative adversarial networks open up the possibility of generating synthetic data 138 or domain adaptive algorithms 139,140 might be able to deal with the shortage of standardised data. Techniques like distributed learning provide the ability to train machine learning models using distributed data without the data ever leaving their original locations. Distributed learning has already been applied across several medical institutions to build predictive and segmentation models. [141][142][143][144] Furthermore, this approach can be coupled with other technologies such as blockchain to trace back data provenance and monitor the use of the final models. 145 Various techniques to visualise deep features have already been put forward by researchers to generate an intuitive understanding. A completely new research area of AI called explainable AI aims to track the decisions made by the intelligent algorithms so that it can be better understood by humans. Companies like Google, IBM, Microsoft and Facebook are at the forefront in this research. This will not only help to build trust of AI systems among medical professionals but also unlocks new possibilities in understanding a disease. 146,147 The implementation of precision medicine itself has its own limitations and has drawn criticism due to the lack of a "transformation in therapeutic medicine" in the last two decades. 148 So far, life expectancies or other public health measures have not shown any dramatic improvements, regardless of the vast amounts of precision medicine research being conducted. Contentious points remain such as excessive costs (e.g. gene therapy), although new developments such as radiomics promise to reduce costs in the long run. Furthermore, the diagnostic and prognostic power of complex "omics-driven" models is still to be determined in specific populations, and evidence needs to be produced that such methods improve health outcomes. 149 Precision medicine is likely to mature and translate to clinical workflows over the next decade and will change the way health services are delivered and evaluated. Healthcare systems will need to adjust their methods and processes to accommodate for these changes. conclusion Radiomics, whether handcrafted or deep, is an emerging field that translates medical images into quantitative data to give biological information and enable phenotypic profiling for diagnosis, theragnosis, decision support, and monitoring. Radiomics, in essence, allows personalised care by identifying features or signatures correlated with a disease or a treatment response with high precision and in a non-invasive way. Recent developments in genomics and deep learning have pushed radiomics researchers to focus more on extracting deep features and explore new possibilities in AI modelling. In the future, radiomics will be a valued addition to precision medicine workflows by facilitating earlier and more accurate diagnosis, providing prognostic information, aiding in treatment choice, monitoring disease and treatment non-invasively, and enabling routine dynamic treatment based on individual responses. But the road to this vision is long, and many technical, regulatory, and ethical problems still need to be solved.</p>
    </text>
</tei>

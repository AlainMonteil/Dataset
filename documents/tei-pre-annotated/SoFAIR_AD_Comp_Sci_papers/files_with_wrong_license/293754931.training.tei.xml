<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:18+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>We study federated machine learning at the wireless network edge, where limited power wireless devices, each with its own dataset, build a joint model with the help of a remote parameter server (PS). We consider a bandwidth-limited fading multiple access channel (MAC) from the wireless devices to the PS, and propose various techniques to implement distributed stochastic gradient descent (DSGD) over this shared noisy wireless channel. We first propose a digital DSGD (D-DSGD) scheme, in which one device is selected opportunistically for transmission at each iteration based on the channel conditions; the scheduled device quantizes its gradient estimate to a finite number of bits imposed by the channel condition, and transmits these bits to the PS in a reliable manner. Next, motivated by the additive nature of the wireless MAC, we propose a novel analog communication scheme, referred to as the compressed analog DSGD (CA-DSGD), where the devices first sparsify their gradient estimates while accumulating error from previous iterations, and project the resultant sparse vector into a lowdimensional vector for bandwidth reduction. We also design a power allocation scheme to align the received gradient vectors at the PS in an efficient manner. Numerical results show that D-DSGD outperforms other digital approaches in the literature; however, in general the proposed CA-DSGD algorithm converges faster than the D-DSGD scheme, and reaches a higher level of accuracy. We have observed that the gap between the analog and digital schemes increases when the datasets of devices are not independent and identically distributed (i.i.d.). Furthermore, the performance of the CA-DSGD scheme is shown to be robust against imperfect channel state information (CSI) at the devices. Overall these results show clear advantages for the proposed analog over-the-air DSGD scheme, which suggests that learning and communication algorithms should be designed jointly to achieve the best end-to-end performance in machine learning applications at the wireless edge.We study federated machine learning at the wireless network edge, where limited power wireless devices, each with its own dataset, build a joint model with the help of a remote parameter server (PS). We consider a bandwidth-limited fading multiple access channel (MAC) from the wireless devices to the PS, and propose various techniques to implement distributed stochastic gradient descent (DSGD) over this shared noisy wireless channel. We first propose a digital DSGD (D-DSGD) scheme, in which one device is selected opportunistically for transmission at each iteration based on the channel conditions; the scheduled device quantizes its gradient estimate to a finite number of bits imposed by the channel condition, and transmits these bits to the PS in a reliable manner. Next, motivated by the additive nature of the wireless MAC, we propose a novel analog communication scheme, referred to as the compressed analog DSGD (CA-DSGD), where the devices first sparsify their gradient estimates while accumulating error from previous iterations, and project the resultant sparse vector into a lowdimensional vector for bandwidth reduction. We also design a power allocation scheme to align the received gradient vectors at the PS in an efficient manner. Numerical results show that D-DSGD outperforms other digital approaches in the literature; however, in general the proposed CA-DSGD algorithm converges faster than the D-DSGD scheme, and reaches a higher level of accuracy. We have observed that the gap between the analog and digital schemes increases when the datasets of devices are not independent and identically distributed (i.i.d.). Furthermore, the performance of the CA-DSGD scheme is shown to be robust against imperfect channel state information (CSI) at the devices. Overall these results show clear advantages for the proposed analog over-the-air DSGD scheme, which suggests that learning and communication algorithms should be designed jointly to achieve the best end-to-end performance in machine learning applications at the wireless edge.</p>
        <p>As the dataset sizes and model complexities grow, distributed machine learning (ML) is becoming the only viable alternative to centralized ML. In particular, with the increasing amount of information collected through wireless edge devices, such centralized solutions are becoming increasingly costly, due to the limited power and bandwidth available, and less desirable due to privacy concerns. Federated learning (FL) has been proposed as an alternative privacy-preserving distributed ML scheme, where each device participates in training using only locally available data with the help of M. Mohammadi Amiri was with the Department of Electrical and Electronic Engineering, Imperial College London. He is now with the Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA (email: mamiri@princeton.edu).As the dataset sizes and model complexities grow, distributed machine learning (ML) is becoming the only viable alternative to centralized ML. In particular, with the increasing amount of information collected through wireless edge devices, such centralized solutions are becoming increasingly costly, due to the limited power and bandwidth available, and less desirable due to privacy concerns. Federated learning (FL) has been proposed as an alternative privacy-preserving distributed ML scheme, where each device participates in training using only locally available data with the help of M. Mohammadi Amiri was with the Department of Electrical and Electronic Engineering, Imperial College London. He is now with the Department of Electrical Engineering, Princeton University, Princeton, NJ 08544, USA (email: mamiri@princeton.edu).</p>
        <p>D. Gündüz is with the Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2AZ, U.K. (e-mail: d.gunduz@imperial.ac.uk).D. Gündüz is with the Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2AZ, U.K. (e-mail: d.gunduz@imperial.ac.uk).</p>
        <p>This work was supported by the European Research Council (ERC) Starting Grant BEACON (grant agreement no. 677854). a parameter server (PS) [1]. In FL, devices exchange model parameters and their local updates with the PS, but the data never leaves the devices. In addition to privacy benefits, this is an attractive approach for wireless edge devices when dataset sizes are very large.This work was supported by the European Research Council (ERC) Starting Grant BEACON (grant agreement no. 677854). a parameter server (PS) [1]. In FL, devices exchange model parameters and their local updates with the PS, but the data never leaves the devices. In addition to privacy benefits, this is an attractive approach for wireless edge devices when dataset sizes are very large.</p>
        <p>ML problems often involve the minimization of the empirical loss functionML problems often involve the minimization of the empirical loss function</p>
        <p>where θ ∈ R d denotes the model parameters to be optimized, B is the training dataset of size |B| consisting of data samples and their labels, and f (•) is the loss function defined by the learning task. The minimization of F (θ) is typically carried out through iterative stochastic gradient descent (SGD) algorithm, in which the model parameter vector at iteration t, θ t , is updated with a stochastic gradientwhere θ ∈ R d denotes the model parameters to be optimized, B is the training dataset of size |B| consisting of data samples and their labels, and f (•) is the loss function defined by the learning task. The minimization of F (θ) is typically carried out through iterative stochastic gradient descent (SGD) algorithm, in which the model parameter vector at iteration t, θ t , is updated with a stochastic gradient</p>
        <p>which satisfies E [g (θ t )] = ∇F (θ t ), where η t is the learning rate. SGD can easily be implemented across multiple devices, each of which has access to only a small fraction of the dataset. In distributed SGD (DSGD), at each iteration, device m computes a gradient vector based on the global parameter vector with respect to its local dataset, denoted by B m , and sends the result to the PS, which updates the global parameter vector according towhich satisfies E [g (θ t )] = ∇F (θ t ), where η t is the learning rate. SGD can easily be implemented across multiple devices, each of which has access to only a small fraction of the dataset. In distributed SGD (DSGD), at each iteration, device m computes a gradient vector based on the global parameter vector with respect to its local dataset, denoted by B m , and sends the result to the PS, which updates the global parameter vector according to</p>
        <p>where M denotes the number of devices, and g m (θ t )where M denotes the number of devices, and g m (θ t )</p>
        <p>In FL, each device can carry out multiple local updates, and share the overall difference with respect to the previous global model with the PS [1].In FL, each device can carry out multiple local updates, and share the overall difference with respect to the previous global model with the PS [1].</p>
        <p>What distinguishes FL from conventional ML is the large number of devices that participate in the training, and the low-capacity and unreliable links that connect these devices to the PS. Therefore, there have been significant research efforts to reduce the communication requirements in FL [1]- [24]. However, these and follow-up studies ignore the physical layer aspects of wireless connections and consider interference-anderror-free links from the participating devices to the PS, even though FL has been mainly motivated for mobile devices.What distinguishes FL from conventional ML is the large number of devices that participate in the training, and the low-capacity and unreliable links that connect these devices to the PS. Therefore, there have been significant research efforts to reduce the communication requirements in FL [1]- [24]. However, these and follow-up studies ignore the physical layer aspects of wireless connections and consider interference-anderror-free links from the participating devices to the PS, even though FL has been mainly motivated for mobile devices.</p>
        <p>In this paper, we consider DSGD over-the-air; that is, we assume that learning takes place over a shared wireless medium over which the devices send their gradient estimates to the PS. To emphasize the limitations of the wireless medium, we note that the dimension of some of the recent ML models, which also determines the size of the gradient estimates or model updates that must be transmitted to the PS at each iteration, can be extremely large, e.g., the 50-layer ResNet network has ∼26 million weight parameters, while the VGGNet architecture has approximately 138 million parameters. On the other hand, available channel bandwidth is typically small due to the bandwidth and latency limitations; for example 1 LTE frame of 5MHz bandwidth and 10ms duration can carry only 6000 complex symbols. In principle, we can treat each iteration of the DSGD algorithm as a distributed over-the-air lossy computation problem. FL over a static Gaussian MAC is studied in [25], where both a digital scheme, which separates computation and communication, and an analog over-theair computation scheme are introduced. While the digital scheme exploits gradient quantization followed by independent channel coding at the participating wireless devices, the analog scheme exploits the additive nature of the wireless channel and gradient sparsification, and employs random linear projection for dimensionality reduction. In [26] the authors consider a fading MAC, and also apply analog transmission, where each entry of a gradient vector at each of the devices is scheduled for transmission depending on the corresponding channel condition. A multi-antenna PS is considered in [27], where receive beamforming is used to maximize the number of devices scheduled for transmission at each iteration.In this paper, we consider DSGD over-the-air; that is, we assume that learning takes place over a shared wireless medium over which the devices send their gradient estimates to the PS. To emphasize the limitations of the wireless medium, we note that the dimension of some of the recent ML models, which also determines the size of the gradient estimates or model updates that must be transmitted to the PS at each iteration, can be extremely large, e.g., the 50-layer ResNet network has ∼26 million weight parameters, while the VGGNet architecture has approximately 138 million parameters. On the other hand, available channel bandwidth is typically small due to the bandwidth and latency limitations; for example 1 LTE frame of 5MHz bandwidth and 10ms duration can carry only 6000 complex symbols. In principle, we can treat each iteration of the DSGD algorithm as a distributed over-the-air lossy computation problem. FL over a static Gaussian MAC is studied in [25], where both a digital scheme, which separates computation and communication, and an analog over-theair computation scheme are introduced. While the digital scheme exploits gradient quantization followed by independent channel coding at the participating wireless devices, the analog scheme exploits the additive nature of the wireless channel and gradient sparsification, and employs random linear projection for dimensionality reduction. In [26] the authors consider a fading MAC, and also apply analog transmission, where each entry of a gradient vector at each of the devices is scheduled for transmission depending on the corresponding channel condition. A multi-antenna PS is considered in [27], where receive beamforming is used to maximize the number of devices scheduled for transmission at each iteration.</p>
        <p>Here, we extend our previous works [25], [28], and study DSGD over a wireless fading MAC. While we focus on DSGD (also known as federated SGD), where each device sends its local gradient estimate at each iteration, the results can easily be extended by letting the devices send their model updates after several local SGD iterations. We first consider the separate computation and communication approach, and propose a digital DSGD (D-DSGD) scheme, in which only a single device is opportunistically scheduled for transmission at each iteration of DSGD based on the channel conditions from the devices to the PS. The scheduled device quantizes its gradient estimate to a finite number of bits using the gradient compression scheme in [18] while accumulating the error from previous iterations (this will be clarified later), and employs a channel code to transmit the bits over the available bandwidth-limited channel to the PS. For the MNIST classification task, it is shown that the proposed digital approach D-DSGD outperforms digital schemes that employ QSGD [8] or SignSGD [12] for gradient compression. We also observe that the proposed opportunistic scheduling scheme outperforms the scheme when all the devices participate in the transmission, with each device allocated orthogonal channel resources to communicate with the PS.Here, we extend our previous works [25], [28], and study DSGD over a wireless fading MAC. While we focus on DSGD (also known as federated SGD), where each device sends its local gradient estimate at each iteration, the results can easily be extended by letting the devices send their model updates after several local SGD iterations. We first consider the separate computation and communication approach, and propose a digital DSGD (D-DSGD) scheme, in which only a single device is opportunistically scheduled for transmission at each iteration of DSGD based on the channel conditions from the devices to the PS. The scheduled device quantizes its gradient estimate to a finite number of bits using the gradient compression scheme in [18] while accumulating the error from previous iterations (this will be clarified later), and employs a channel code to transmit the bits over the available bandwidth-limited channel to the PS. For the MNIST classification task, it is shown that the proposed digital approach D-DSGD outperforms digital schemes that employ QSGD [8] or SignSGD [12] for gradient compression. We also observe that the proposed opportunistic scheduling scheme outperforms the scheme when all the devices participate in the transmission, with each device allocated orthogonal channel resources to communicate with the PS.</p>
        <p>We then study analog transmission from the devices motivated by the signal-superposition property of the wireless MAC. At first, we extend the scheme in [26] by introducing error accumulation, which is shown to improve the performance. We then propose a novel scheme, inspired by the random projection used in [25] for dimensionality reduction, which we will refer to as the compressed analog DSGD (CA-DSGD). With CA-DSGD, we exploit the similarity in the sparsity patterns of the gradient estimates at different devices to speed up the computations, where each device projects its gradient estimate to a low-dimensional vector and transmits only the important gradient entries while accumulating the error. CA-DSGD provides the flexibility of adjusting the dimension of the gradient estimate sent by each device, which is particularly important for bandwidth-limited wireless channels, where the bandwidth may not be sufficient to send the entire gradient vector. A power allocation scheme is also designed, which aligns the vectors sent by different devices at the PS while satisfying the average power constraint. Numerical results show that the proposed CA-DSGD scheme improves upon the other analog and digital schemes under consideration with the same average power constraint and bandwidth resources, and the improvement is more significant when the datasets across devices are non-independent and identically distributed (i.i.d.). Its performance is also shown to be robust against imperfect channel state information (CSI) at the devices, whereas digital schemes are sensitive to accurate CSI, particularly if close to capacity operation is desired.We then study analog transmission from the devices motivated by the signal-superposition property of the wireless MAC. At first, we extend the scheme in [26] by introducing error accumulation, which is shown to improve the performance. We then propose a novel scheme, inspired by the random projection used in [25] for dimensionality reduction, which we will refer to as the compressed analog DSGD (CA-DSGD). With CA-DSGD, we exploit the similarity in the sparsity patterns of the gradient estimates at different devices to speed up the computations, where each device projects its gradient estimate to a low-dimensional vector and transmits only the important gradient entries while accumulating the error. CA-DSGD provides the flexibility of adjusting the dimension of the gradient estimate sent by each device, which is particularly important for bandwidth-limited wireless channels, where the bandwidth may not be sufficient to send the entire gradient vector. A power allocation scheme is also designed, which aligns the vectors sent by different devices at the PS while satisfying the average power constraint. Numerical results show that the proposed CA-DSGD scheme improves upon the other analog and digital schemes under consideration with the same average power constraint and bandwidth resources, and the improvement is more significant when the datasets across devices are non-independent and identically distributed (i.i.d.). Its performance is also shown to be robust against imperfect channel state information (CSI) at the devices, whereas digital schemes are sensitive to accurate CSI, particularly if close to capacity operation is desired.</p>
        <p>In addition to these benefits, we make the following observations:In addition to these benefits, we make the following observations:</p>
        <p>1) The improvement of analog over-the-air computation compared to the D-DSGD scheme is particularly striking in the low power regime. This is mainly due to the "beamforming" effect of simultaneously transmitting highly correlated gradient estimates. 2) While both the convergence speed and the accuracy of the D-DSGD scheme increase significantly with the available average power, the performance of the analog schemes improve marginally. This highlights the energy efficiency of over-the-air computation, and makes it particularly attractive for FL across low-power IoT sensors. 3) Increasing the number of devices improves the accuracy for all the schemes even if the total dataset size and total power consumption remain the same. This "diversity gain" is much more limited for the analog scheme, and diminishes further as the training duration increases. 4) We observe that the performance of the CA-DSGD scheme improves if we reduce the bandwidth used at each iteration, and increase the number of iterations instead. Notations: R and C represent the sets of real and complex values, respectively. For vectors x and y with the same dimension, x • y returns their Hadamard/entry-wise product. For a vector z ∈ C i , Re{z} ∈ R i and Im{z} ∈ R i return the entrywise real and imaginary components of z, respectively. Also, [v, w] represents the concatenation of two row vectors v and w. We denote a zero-mean normal distribution with variance σ 2 by N 0, σ 2 , and CN 0, σ 2 represents a complex normal distribution with independent real and imaginary terms each distributed according to N 0, σ 2 /2 . For positive integer i, we let [i] {1, . . . , i}. We denote the cardinality of set A by |A|, and l 2 norm of vector x by x 2 . The imaginary unit is represented by j.1) The improvement of analog over-the-air computation compared to the D-DSGD scheme is particularly striking in the low power regime. This is mainly due to the "beamforming" effect of simultaneously transmitting highly correlated gradient estimates. 2) While both the convergence speed and the accuracy of the D-DSGD scheme increase significantly with the available average power, the performance of the analog schemes improve marginally. This highlights the energy efficiency of over-the-air computation, and makes it particularly attractive for FL across low-power IoT sensors. 3) Increasing the number of devices improves the accuracy for all the schemes even if the total dataset size and total power consumption remain the same. This "diversity gain" is much more limited for the analog scheme, and diminishes further as the training duration increases. 4) We observe that the performance of the CA-DSGD scheme improves if we reduce the bandwidth used at each iteration, and increase the number of iterations instead. Notations: R and C represent the sets of real and complex values, respectively. For vectors x and y with the same dimension, x • y returns their Hadamard/entry-wise product. For a vector z ∈ C i , Re{z} ∈ R i and Im{z} ∈ R i return the entrywise real and imaginary components of z, respectively. Also, [v, w] represents the concatenation of two row vectors v and w. We denote a zero-mean normal distribution with variance σ 2 by N 0, σ 2 , and CN 0, σ 2 represents a complex normal distribution with independent real and imaginary terms each distributed according to N 0, σ 2 /2 . For positive integer i, we let [i] {1, . . . , i}. We denote the cardinality of set A by |A|, and l 2 norm of vector x by x 2 . The imaginary unit is represented by j.</p>
        <p>We consider FL across M wireless devices, each with its own local dataset, which employ DSGD with the help of a remote PS. We model the channel from the devices to the PS as a wireless fading MAC, and OFDM is employed for transmission. The system model is illustrated in Fig. 1. The parameter vector at iteration t is denoted by θ t , and we assume that it is delivered from the PS to the devices over an errorfree shared link. We denote the set of data samplesWe consider FL across M wireless devices, each with its own local dataset, which employ DSGD with the help of a remote PS. We model the channel from the devices to the PS as a wireless fading MAC, and OFDM is employed for transmission. The system model is illustrated in Fig. 1. The parameter vector at iteration t is denoted by θ t , and we assume that it is delivered from the PS to the devices over an errorfree shared link. We denote the set of data samples</p>
        <p>where µ n m (t) ∈ {0, 1} s is the entry-wise scheduling vector with the i-th entry µ n m,i (t) = 1, if m ∈ M n i (t), and µ n m,i (t) = 0, otherwise 1 , h n m (t) ∈ C s is the channel gains vector from device m to the PS with the i-th entry h n m,i (t) i.i.d. according to CN (0, σ 2 ), e.g., Rayleigh fading, and z n (t) ∈ C s is complex Gaussian noise vector with the i-th entry z n i (t) i.i.d. according to CN (0, 1). The channel input vector of device m at the n-th time slot of iteration t, n ∈ [N ], is a function of the channel gain vector h n m (t), current parameter vector θ t , the local dataset B m , and the current gradient estimate at device m, g m (θ t ), m ∈ [M ]. We assume that, at each time slot, the CSI is known by the devices and the PS. For a total of T iterations of the DSGD algorithm, the following total average transmit power constraint is imposed at device m:where µ n m (t) ∈ {0, 1} s is the entry-wise scheduling vector with the i-th entry µ n m,i (t) = 1, if m ∈ M n i (t), and µ n m,i (t) = 0, otherwise 1 , h n m (t) ∈ C s is the channel gains vector from device m to the PS with the i-th entry h n m,i (t) i.i.d. according to CN (0, σ 2 ), e.g., Rayleigh fading, and z n (t) ∈ C s is complex Gaussian noise vector with the i-th entry z n i (t) i.i.d. according to CN (0, 1). The channel input vector of device m at the n-th time slot of iteration t, n ∈ [N ], is a function of the channel gain vector h n m (t), current parameter vector θ t , the local dataset B m , and the current gradient estimate at device m, g m (θ t ), m ∈ [M ]. We assume that, at each time slot, the CSI is known by the devices and the PS. For a total of T iterations of the DSGD algorithm, the following total average transmit power constraint is imposed at device m:</p>
        <p>1 M n i (t) is a subset of the devices that will be specified for each of the schemes.1 M n i (t) is a subset of the devices that will be specified for each of the schemes.</p>
        <p>where the expectation is taken over the randomness of the channel gains.where the expectation is taken over the randomness of the channel gains.</p>
        <p>The goal is to recover 1 M M m=1 g m (θ t ) at the PS, which then updates the model parameter as in (3) after N time slots. However, due to the pre-processing performed at each device and the distortion caused by the wireless channel, the PS uses a noisy estimate to update the model parameter. Having definedThe goal is to recover 1 M M m=1 g m (θ t ) at the PS, which then updates the model parameter as in (3) after N time slots. However, due to the pre-processing performed at each device and the distortion caused by the wireless channel, the PS uses a noisy estimate to update the model parameter. Having defined</p>
        <p>T ] T , we have θ t+1 = φ(θ t , y(t)) for some update function φ : R d ×C N s → R d . The updated model parameter is then multicast to the devices by the PS through an error-free shared link, so the devices receive a consistent parameter vector for their computations in the next iteration.T ] T , we have θ t+1 = φ(θ t , y(t)) for some update function φ : R d ×C N s → R d . The updated model parameter is then multicast to the devices by the PS through an error-free shared link, so the devices receive a consistent parameter vector for their computations in the next iteration.</p>
        <p>Note that the goal is to recover the average of the local gradient estimates at the PS, which is a distributed lossy computation problem over a noisy MAC. We will consider both a digital approach based on separating computations and communication, and an analog approach, where the gradients are transmitted in an analog fashion, without being converted into bits first. Analog transmission has been well studied for image/ video multicasting over wireless channels in recent years [29]- [31], and here we employ the random projection technique proposed in [31] for image transmission over a bandwidth limited wireless channel.Note that the goal is to recover the average of the local gradient estimates at the PS, which is a distributed lossy computation problem over a noisy MAC. We will consider both a digital approach based on separating computations and communication, and an analog approach, where the gradients are transmitted in an analog fashion, without being converted into bits first. Analog transmission has been well studied for image/ video multicasting over wireless channels in recent years [29]- [31], and here we employ the random projection technique proposed in [31] for image transmission over a bandwidth limited wireless channel.</p>
        <p>We first consider DSGD with digital transmission of the gradient estimates by the devices over the wireless fading MAC, referred to as the digital DSGD (D-DSGS) scheme. For D-DSGD, we consider N = 1, i.e., the parameter vector is updated after each time slot, and drop the dependency on time slot parameter n.We first consider DSGD with digital transmission of the gradient estimates by the devices over the wireless fading MAC, referred to as the digital DSGD (D-DSGS) scheme. For D-DSGD, we consider N = 1, i.e., the parameter vector is updated after each time slot, and drop the dependency on time slot parameter n.</p>
        <p>The goal here is to schedule devices and employ power allocation across time slots such that devices can transmit to the PS their local gradient estimates as accurately as possible. A possible approach is to schedule all the devices at all the iterations; however, due to the interference among the devices this will result in each device sending a very coarse description of its local gradient estimate. Instead here we will schedule the devices opportunistically according to their channel states.The goal here is to schedule devices and employ power allocation across time slots such that devices can transmit to the PS their local gradient estimates as accurately as possible. A possible approach is to schedule all the devices at all the iterations; however, due to the interference among the devices this will result in each device sending a very coarse description of its local gradient estimate. Instead here we will schedule the devices opportunistically according to their channel states.</p>
        <p>In particular, with the knowledge of channel state information (CSI), at each iteration t, we select the device with the largest value ofIn particular, with the knowledge of channel state information (CSI), at each iteration t, we select the device with the largest value of</p>
        <p>Accordingly, the index of the transmitting device at iteration t is given by:Accordingly, the index of the transmitting device at iteration t is given by:</p>
        <p>We note that, due to the symmetry in the model, the probability of selecting a device at any time is the same, 1/M . The power allocated to device m at the t-th iteration is given by Pm (t), where Pm (t) = 0, if m = m * (t), and it should satisfyWe note that, due to the symmetry in the model, the probability of selecting a device at any time is the same, 1/M . The power allocated to device m at the t-th iteration is given by Pm (t), where Pm (t) = 0, if m = m * (t), and it should satisfy</p>
        <p>For the rate of transmission, we will use a capacity upper bound. The i-th entry of the channel output at the t-th iteration is given byFor the rate of transmission, we will use a capacity upper bound. The i-th entry of the channel output at the t-th iteration is given by</p>
        <p>which is equivalent to a wireless fast fading channel with a limited number of s channel uses, with CSI known at both the transmitter and receiver. In the following, we provide an upper bound on the capacity of this channel by treating it as s parallel Gaussian channels. This is equivalent to coding across infinitely many realizations of this s-dimensional channel.which is equivalent to a wireless fast fading channel with a limited number of s channel uses, with CSI known at both the transmitter and receiver. In the following, we provide an upper bound on the capacity of this channel by treating it as s parallel Gaussian channels. This is equivalent to coding across infinitely many realizations of this s-dimensional channel.</p>
        <p>For a transmit power P m * (t) (t), the capacity of this parallel Gaussian channel is obtained as the solution of the following optimization problem [32, Section 5.4.6]:For a transmit power P m * (t) (t), the capacity of this parallel Gaussian channel is obtained as the solution of the following optimization problem [32, Section 5.4.6]:</p>
        <p>subject tosubject to</p>
        <p>The optimization problem in ( 9) is solved through waterfilling, and the optimal power allocation is given byThe optimization problem in ( 9) is solved through waterfilling, and the optimal power allocation is given by</p>
        <p>where ζ is determined such thatwhere ζ is determined such that</p>
        <p>Having calculated P * 1 , . . . , P * s , the capacity of the wireless channel in ( 8) is given byHaving calculated P * 1 , . . . , P * s , the capacity of the wireless channel in ( 8) is given by</p>
        <p>which provides an upper bound on the capacity of the communication channel between device m * (t) and the PS. We would like to emphasize that this capacity upper bound can be quite loose especially for small s values.which provides an upper bound on the capacity of the communication channel between device m * (t) and the PS. We would like to emphasize that this capacity upper bound can be quite loose especially for small s values.</p>
        <p>We adopt the D-DSGD scheme proposed in [25, Section III], in which the gradient estimate g m (θ t ), computed at device m, is added to the error accumulated from previous iterations, denoted by ∆ m (t -1), where we set ∆ m (0) = 0, m ∈ [M ]. For the compression of the error compensated gradient vector g m (θ t )+∆ m (t-1), we employ the scheme in [18], where it is first sparsified by setting all but the highest q(t) positive and the smallest q(t) negative entries to zero, where q(t) ≤ d/2 (in practice, the goal is to have q(t) d, ∀t). Then, device m computes the mean value of the positive and negative entries of the resultant sparse vector, denoted byWe adopt the D-DSGD scheme proposed in [25, Section III], in which the gradient estimate g m (θ t ), computed at device m, is added to the error accumulated from previous iterations, denoted by ∆ m (t -1), where we set ∆ m (0) = 0, m ∈ [M ]. For the compression of the error compensated gradient vector g m (θ t )+∆ m (t-1), we employ the scheme in [18], where it is first sparsified by setting all but the highest q(t) positive and the smallest q(t) negative entries to zero, where q(t) ≤ d/2 (in practice, the goal is to have q(t) d, ∀t). Then, device m computes the mean value of the positive and negative entries of the resultant sparse vector, denoted by</p>
        <p>, device m sets all the negative entries of the sparse vector to zero and all the positive entries to q + m (t), and vice versa, if, device m sets all the negative entries of the sparse vector to zero and all the positive entries to q + m (t), and vice versa, if</p>
        <p>, the error accumulation vector, which maintains those entries of vector g m (θ t ) + ∆ m (t -1) that are not transmitted, is updated as follows:, the error accumulation vector, which maintains those entries of vector g m (θ t ) + ∆ m (t -1) that are not transmitted, is updated as follows:</p>
        <p>We note that, if user m is scheduled, the accumulated error at device m is the difference between g m (θ t ) + ∆ m (t -1) and its sparsified version ĝm (θ t ), m ∈ [M ]; on the other hand, if device m is not scheduled, we maintain vector g m (θ t ) + ∆ m (t -1) entirely as the accumulated error. For a sparsity level q(t), the D-DSGD scheme requires transmission of a total of [25,Equation (10)]We note that, if user m is scheduled, the accumulated error at device m is the difference between g m (θ t ) + ∆ m (t -1) and its sparsified version ĝm (θ t ), m ∈ [M ]; on the other hand, if device m is not scheduled, we maintain vector g m (θ t ) + ∆ m (t -1) entirely as the accumulated error. For a sparsity level q(t), the D-DSGD scheme requires transmission of a total of [25,Equation (10)]</p>
        <p>We assume that device m * (t) employs a capacity achieving channel code using the optimal value of the capacity upper bound in (11), and we set the sparsity level q(t) as the highest integer satisfying r(t) ≤ R(t).We assume that device m * (t) employs a capacity achieving channel code using the optimal value of the capacity upper bound in (11), and we set the sparsity level q(t) as the highest integer satisfying r(t) ≤ R(t).</p>
        <p>We highlight here that with the proposed D-DSGD algorithm, only a single device is scheduled for transmission according to (6). The PS updates the parameter vector after receiving the gradient estimate from the scheduled device and shares it with all the devices to continue their computations.We highlight here that with the proposed D-DSGD algorithm, only a single device is scheduled for transmission according to (6). The PS updates the parameter vector after receiving the gradient estimate from the scheduled device and shares it with all the devices to continue their computations.</p>
        <p>Remark 1. Alternatively, we can select the device with the highest capacity upperbound; however this would introduce an overhead as the PS will need to solve the waterfilling power allocation for all M devices. This will be prohibitive for large s and M values.Remark 1. Alternatively, we can select the device with the highest capacity upperbound; however this would introduce an overhead as the PS will need to solve the waterfilling power allocation for all M devices. This will be prohibitive for large s and M values.</p>
        <p>Remark 2. We can also schedule all or a subset of the devices, and allocate distinct subchannels to scheduled devices. In Section V we consider the so-called orthogonal digital DSGD (OD-DSGD) scheme, which schedules all the devices at each iteration, where each device is allocated s/M distinct subchannels. We have observed that OD-DSGD performs much worse than D-DSGD. It is worth noting that scheduling multiple devices reduces the number of subchannels allocated to each device, and forces the devices to transmit their information at shorter blocklengths. In practice, this would result in a higher error probability or reduced transmission rate [33]. An alternative approach is to code across time slots by allocating multiple time slots to a scheduled user. This requires the information about the future channel gains, which is not possible in our model since the channel gains are assumed i.i.d. across time slots and users.Remark 2. We can also schedule all or a subset of the devices, and allocate distinct subchannels to scheduled devices. In Section V we consider the so-called orthogonal digital DSGD (OD-DSGD) scheme, which schedules all the devices at each iteration, where each device is allocated s/M distinct subchannels. We have observed that OD-DSGD performs much worse than D-DSGD. It is worth noting that scheduling multiple devices reduces the number of subchannels allocated to each device, and forces the devices to transmit their information at shorter blocklengths. In practice, this would result in a higher error probability or reduced transmission rate [33]. An alternative approach is to code across time slots by allocating multiple time slots to a scheduled user. This requires the information about the future channel gains, which is not possible in our model since the channel gains are assumed i.i.d. across time slots and users.</p>
        <p>We will evaluate the performance of D-DSGD in Section V, and study in detail the impact of various system parameters, such as the power constraint and the number of devices on the performance. We will also compare D-DSGD with other compression schemes in the literature, as well as the analog transmission of local gradients, which we present next.We will evaluate the performance of D-DSGD in Section V, and study in detail the impact of various system parameters, such as the power constraint and the number of devices on the performance. We will also compare D-DSGD with other compression schemes in the literature, as well as the analog transmission of local gradients, which we present next.</p>
        <p>IV. ANALOG DSGD Analog DSGD is motivated by the fact that the PS is only interested in the average of the gradient vectors, and the underlying wireless MAC can provide the sum of the gradients if they are sent in an uncoded fashion. We first present a generalization of the over-the-air computation approach introduced in [26], referred to as entry-wise scheduled analog DSGD (ESA-DSGD), and then extend it by introducing error accumulation, referred to as error compensated ESA-DSGD (ECESA-DSGD). Finally, we propose a novel analog scheme, built upon our previous work [25], referred to as compressed analog DSGD (CA-DSGD).IV. ANALOG DSGD Analog DSGD is motivated by the fact that the PS is only interested in the average of the gradient vectors, and the underlying wireless MAC can provide the sum of the gradients if they are sent in an uncoded fashion. We first present a generalization of the over-the-air computation approach introduced in [26], referred to as entry-wise scheduled analog DSGD (ESA-DSGD), and then extend it by introducing error accumulation, referred to as error compensated ESA-DSGD (ECESA-DSGD). Finally, we propose a novel analog scheme, built upon our previous work [25], referred to as compressed analog DSGD (CA-DSGD).</p>
        <p>With the ESA-DSGD scheme studied in [26], each device sends its gradient estimate entirely after applying power allo-cation, which is to satisfy the average power constraint. At the t-th iteration of the DSGD, device m, m ∈ [M ], transmits its local gradient estimate g m (θ t ) ∈ R d over N = d/2s time slots by utilizing both the real and imaginary components of the available s subchannels. We define, for n ∈With the ESA-DSGD scheme studied in [26], each device sends its gradient estimate entirely after applying power allo-cation, which is to satisfy the average power constraint. At the t-th iteration of the DSGD, device m, m ∈ [M ], transmits its local gradient estimate g m (θ t ) ∈ R d over N = d/2s time slots by utilizing both the real and imaginary components of the available s subchannels. We define, for n ∈</p>
        <p>where g m,i (θ t ) is the i-th entry of g m (θ t ), and we zero-pad g m (θ t ) to dimension 2sN . We note that, according to (13),where g m,i (θ t ) is the i-th entry of g m (θ t ), and we zero-pad g m (θ t ) to dimension 2sN . We note that, according to (13),</p>
        <p>wherewhere</p>
        <p>, where α e,n m (t) ∈ C s is the power allocation vector, which is set to satisfy the average transmit power constraint. Thus, after N time slots, each device sends its gradient estimate of dimension d entirely. The i-th entry of the power allocation vector α e,n m (t) is set as follows:, where α e,n m (t) ∈ C s is the power allocation vector, which is set to satisfy the average transmit power constraint. Thus, after N time slots, each device sends its gradient estimate of dimension d entirely. The i-th entry of the power allocation vector α e,n m (t) is set as follows:</p>
        <p>for some γ e,n m (t), λ e (t) ∈ R, set to satisfy the average transmit power constraint. According to (15), each entry of a gradient vector is transmitted if its corresponding channel gain is over a threshold. The set of devices selected to transmit the i-th entry of the channel input vector at the n-th time slot is given by,for some γ e,n m (t), λ e (t) ∈ R, set to satisfy the average transmit power constraint. According to (15), each entry of a gradient vector is transmitted if its corresponding channel gain is over a threshold. The set of devices selected to transmit the i-th entry of the channel input vector at the n-th time slot is given by,</p>
        <p>In the following, we analyze the average transmit power of the ESA-DSGD scheme based on the power allocation design given in (15). We set the parameters γ e,n m (t) and λ e (t) to obtain the same average transmit powerIn the following, we analyze the average transmit power of the ESA-DSGD scheme based on the power allocation design given in (15). We set the parameters γ e,n m (t) and λ e (t) to obtain the same average transmit power</p>
        <p>According to (15), ∀m ∈ [M ], we have, for n ∈According to (15), ∀m ∈ [M ], we have, for n ∈</p>
        <p>We highlight that the entries of the gradient vector g n m (θ t ) are independent of the channel gains h n m,i (t), ∀i, n, m. Since the power allocation vector α e,n m (t) is a function ofWe highlight that the entries of the gradient vector g n m (θ t ) are independent of the channel gains h n m,i (t), ∀i, n, m. Since the power allocation vector α e,n m (t) is a function of</p>
        <p>Note that h n m,i (t) 2 follows an exponential distribution with mean σ 2 , ∀i, n, m. Thus, we haveNote that h n m,i (t) 2 follows an exponential distribution with mean σ 2 , ∀i, n, m. Thus, we have</p>
        <p>wherewhere</p>
        <p>where we define P e,n m (t)where we define P e,n m (t)</p>
        <p>. Given the threshold value λ e (t), we set, for m ∈. Given the threshold value λ e (t), we set, for m ∈</p>
        <p>which we note that it does not differ significantly across devices, since values of P e,n m (t), ∀m, are not too different. We assume that, before transmittingwhich we note that it does not differ significantly across devices, since values of P e,n m (t), ∀m, are not too different. We assume that, before transmitting</p>
        <p>m (t) to the PS in an error-free fashion using an error correcting code, and the PS computesm (t) to the PS in an error-free fashion using an error correcting code, and the PS computes</p>
        <p>This factor is used by the PS to scale down the received signal.This factor is used by the PS to scale down the received signal.</p>
        <p>Here we analyze the received signal at the PS. By substituting x n m (θ t ) and α e,n m (t) into (4), it follows that, for iHere we analyze the received signal at the PS. By substituting x n m (θ t ) and α e,n m (t) into (4), it follows that, for i</p>
        <p>, using its noisy observation y n i (t), given in (24), as, using its noisy observation y n i (t), given in (24), as</p>
        <p>and estimatesand estimates</p>
        <p>T is then used to update the parameter vector as θ t+1 = θ t -η t ĝe (θ t ).T is then used to update the parameter vector as θ t+1 = θ t -η t ĝe (θ t ).</p>
        <p>Remark 3. We remark here that the scheme in [26] imposes a stricter average power constraint P per iteration of the DSGD, i.e., at device m we should haveRemark 3. We remark here that the scheme in [26] imposes a stricter average power constraint P per iteration of the DSGD, i.e., at device m we should have</p>
        <p>For fairness in our comparisons we relax this power constraint, and impose the one in (5), which constrains the average power over all the iterations.For fairness in our comparisons we relax this power constraint, and impose the one in (5), which constrains the average power over all the iterations.</p>
        <p>With the ESA-DSGD scheme, entries of the gradient vectors that are not sent due to poor channel conditions are completely forgotten. The proposed ECESA-DSGD scheme modifies ESA-DSGD by incorporating error accumulation technique to retain the accuracy of local gradients.With the ESA-DSGD scheme, entries of the gradient vectors that are not sent due to poor channel conditions are completely forgotten. The proposed ECESA-DSGD scheme modifies ESA-DSGD by incorporating error accumulation technique to retain the accuracy of local gradients.</p>
        <p>We denote the error accumulation vector of device m at the n-th time slot of the t-iteration by ∆ v,n m (t) ∈ C s , and set ∆ v,n m (t) = 0, ∀n, t, m. Similarly to the ESA-DSGD scheme, with ECESA-DSGD, each device sends its entire gradient estimate of dimension d through N = d/2s time slots, where the gradient estimates at the devices are zeropadded to dimension 2sN . After computing g m (θ t ) and obtaining g n m (θ t ) according to (13), device m, m ∈ [M ], updates its gradient estimate with the accumulated error asWe denote the error accumulation vector of device m at the n-th time slot of the t-iteration by ∆ v,n m (t) ∈ C s , and set ∆ v,n m (t) = 0, ∀n, t, m. Similarly to the ESA-DSGD scheme, with ECESA-DSGD, each device sends its entire gradient estimate of dimension d through N = d/2s time slots, where the gradient estimates at the devices are zeropadded to dimension 2sN . After computing g m (θ t ) and obtaining g n m (θ t ) according to (13), device m, m ∈ [M ], updates its gradient estimate with the accumulated error as</p>
        <p>, where α v,n m (t) ∈ C s is the power allocation vector, whose i-th entry is given by:, where α v,n m (t) ∈ C s is the power allocation vector, whose i-th entry is given by:</p>
        <p>for some γ v,n m (t), λ v (t) ∈ R. Device m, m ∈ [M ], then updates the i-th entry of vector ∆ v,n m (t) as follows:for some γ v,n m (t), λ v (t) ∈ R. Device m, m ∈ [M ], then updates the i-th entry of vector ∆ v,n m (t) as follows:</p>
        <p>where 1(•) is the indicator function, and g n m,i (θ t ) denotes the i-th entry of g n m (θ t ), for i ∈ [s], n ∈ [N ]. Thus, the i-th entry of vector g v,n m (θ t ) is given by, for i ∈where 1(•) is the indicator function, and g n m,i (θ t ) denotes the i-th entry of g n m (θ t ), for i ∈ [s], n ∈ [N ]. Thus, the i-th entry of vector g v,n m (θ t ) is given by, for i ∈</p>
        <p>According to (29), each entry of the gradient vector g v,n m (θ t ) that is not transmitted due to the power allocation given in (28), is retained in the error accumulation vector ∆ v,n m (t) for possible transmission in the next iteration.According to (29), each entry of the gradient vector g v,n m (θ t ) that is not transmitted due to the power allocation given in (28), is retained in the error accumulation vector ∆ v,n m (t) for possible transmission in the next iteration.</p>
        <p>Here we provide the power analysis of the ECESA-DSGD scheme. For fairness, we set the parameters γ v,n m (t) and λ v (t) yielding an average transmit power P n (t) at device m, m ∈ [M ], in time slot n, n ∈ [N ], of iteration t, satisfying the constraint in (17). Since the power allocation vector of ECESA-DSGD, given in (28), is similar to that of the ESA-DSGD, by following a similar procedure we obtain the following average power at device m for ECESA-DSGD:Here we provide the power analysis of the ECESA-DSGD scheme. For fairness, we set the parameters γ v,n m (t) and λ v (t) yielding an average transmit power P n (t) at device m, m ∈ [M ], in time slot n, n ∈ [N ], of iteration t, satisfying the constraint in (17). Since the power allocation vector of ECESA-DSGD, given in (28), is similar to that of the ESA-DSGD, by following a similar procedure we obtain the following average power at device m for ECESA-DSGD:</p>
        <p>where we define P v,n m (t)where we define P v,n m (t)</p>
        <p>shared with the PS in an error-free manner, through which the PS computesshared with the PS in an error-free manner, through which the PS computes</p>
        <p>From the power allocation in (28), it follows that, for i ∈ [s], n ∈ [N ],From the power allocation in (28), it follows that, for i ∈ [s], n ∈ [N ],</p>
        <p>where we havewhere we have</p>
        <p>Having perfect CSI, the PS's goal is to recoverHaving perfect CSI, the PS's goal is to recover</p>
        <p>m,i (θ t ), the real and imaginary terms of which provide estimates for 1m,i (θ t ), the real and imaginary terms of which provide estimates for 1</p>
        <p>and estimates 1 M M m=1 g n m,(2n-1)s+i (θ t ) throughand estimates 1 M M m=1 g n m,(2n-1)s+i (θ t ) through</p>
        <p>T is then used to update the parameter vector as θ t+1 = θ t -η t ĝv (θ t ).T is then used to update the parameter vector as θ t+1 = θ t -η t ĝv (θ t ).</p>
        <p>As opposed to ESA-DSGD and ECESA-DSGD, which aim to transmit all the gradient entries to the PS at each DSGD iteration, i.e., N = d/2s , the CA-DSGD scheme in Algorithm 1 reduces the transmission bandwidth by a linear projection. Each device projects its gradient estimate to dimension s = 2sN , which can then be transmitted through N time slots, for some N ∈ [ d/2s ].As opposed to ESA-DSGD and ECESA-DSGD, which aim to transmit all the gradient entries to the PS at each DSGD iteration, i.e., N = d/2s , the CA-DSGD scheme in Algorithm 1 reduces the transmission bandwidth by a linear projection. Each device projects its gradient estimate to dimension s = 2sN , which can then be transmitted through N time slots, for some N ∈ [ d/2s ].</p>
        <p>We describe the CA-DSGD scheme for an arbitrary number of time slots N ∈ [ d/2s ] per iteration of DSGD, which is determined later. At each iteration the devices sparsify their gradient estimates as described below. They employ error accumulation [7], where the accumulated error vector at device m until iteration t is denoted by ∆ c m (t -1) ∈ R d , where we set ∆ c m (0) = 0, ∀m ∈ [M ]. After computing g m (θ t ), device m updates its estimate with the accumulated error asWe describe the CA-DSGD scheme for an arbitrary number of time slots N ∈ [ d/2s ] per iteration of DSGD, which is determined later. At each iteration the devices sparsify their gradient estimates as described below. They employ error accumulation [7], where the accumulated error vector at device m until iteration t is denoted by ∆ c m (t -1) ∈ R d , where we set ∆ c m (0) = 0, ∀m ∈ [M ]. After computing g m (θ t ), device m updates its estimate with the accumulated error as</p>
        <p>Next, the devices apply gradient sparsification, where device m setsNext, the devices apply gradient sparsification, where device m sets</p>
        <p>• devices do:• devices do:</p>
        <p>3:3:</p>
        <p>for m = 1, . . . , M in parallel do 4:for m = 1, . . . , M in parallel do 4:</p>
        <p>Compute g m (θ t ) with respect to local dataset B m 5:Compute g m (θ t ) with respect to local dataset B m 5:</p>
        <p>g sp m (θ t ) = sparse k (g ec m (θ t ))g sp m (θ t ) = sparse k (g ec m (θ t ))</p>
        <p>7:7:</p>
        <p>gm (θ t ) = Ag sp m (θ t )gm (θ t ) = Ag sp m (θ t )</p>
        <p>9:9:</p>
        <p>for n = 1, . . . , N do 10:for n = 1, . . . , N do 10:</p>
        <p>end for 12:end for 12:</p>
        <p>end for • PS does:end for • PS does:</p>
        <p>To transmit the sparse vectors over the limited-bandwidth channel, devices employ a random projection matrix.To transmit the sparse vectors over the limited-bandwidth channel, devices employ a random projection matrix.</p>
        <p>A pseudo-random matrix A ∈ R s×d , with each entry i.i.d. according to N (0, 1/s), is generated and shared between the PS and the devices, where s = 2sN , for an arbitrary N ∈ [ d/2s ]. At each iteration t, device m computes gm (θ t ) Ag sp m (θ t ) ∈ R s, and aims to transmit it to the PS over N = s/2s time slots. We define, for n ∈A pseudo-random matrix A ∈ R s×d , with each entry i.i.d. according to N (0, 1/s), is generated and shared between the PS and the devices, where s = 2sN , for an arbitrary N ∈ [ d/2s ]. At each iteration t, device m computes gm (θ t ) Ag sp m (θ t ) ∈ R s, and aims to transmit it to the PS over N = s/2s time slots. We define, for n ∈</p>
        <p>where gm,i (θ t ) is the i-th entry of gmwhere gm,i (θ t ) is the i-th entry of gm</p>
        <p>, where α c,n m (t) ∈ C s is the power allocation vector. The i-th entry of the power allocation vector α c,n m (t) is set as follows:, where α c,n m (t) ∈ C s is the power allocation vector. The i-th entry of the power allocation vector α c,n m (t) is set as follows:</p>
        <p>for some γ c,n m (t), λ c (t) ∈ R. The set of devices scheduled to transmit the i-th entry of the channel input vector at the n-th time slot is given by,for some γ c,n m (t), λ c (t) ∈ R. The set of devices scheduled to transmit the i-th entry of the channel input vector at the n-th time slot is given by,</p>
        <p>Similarly to ESA-DSGD and ECESA-DSGD, we set the average transmit power at device m, m ∈ [M ], in time slot n, n ∈ [N ], of iteration t for CA-DSGD to P n (t),Similarly to ESA-DSGD and ECESA-DSGD, we set the average transmit power at device m, m ∈ [M ], in time slot n, n ∈ [N ], of iteration t for CA-DSGD to P n (t),</p>
        <p>where we definewhere we define</p>
        <p>and the PS computesand the PS computes</p>
        <p>after receiving γ c,n m (t). By substituting x n m (θ t ) and α c (t) into ( 4), it follows that,after receiving γ c,n m (t). By substituting x n m (θ t ) and α c (t) into ( 4), it follows that,</p>
        <p>where a T i denotes the i-th row of measurement matrix A, and we note that gm,i (θ t ) = a T i g sp m (θ t ), i ∈ [s]. The PS wants to recover 1 M M m=1 g sp m (θ t ) from its noisy observations in (44). For this, using its knowledge of matrix A and the CSI, PS employs the approximate message passing (AMP) algorithm [34]. The AMP algorithm is represented by AMP A in Algorithm 1. The PS first obtains, for i ∈where a T i denotes the i-th row of measurement matrix A, and we note that gm,i (θ t ) = a T i g sp m (θ t ), i ∈ [s]. The PS wants to recover 1 M M m=1 g sp m (θ t ) from its noisy observations in (44). For this, using its knowledge of matrix A and the CSI, PS employs the approximate message passing (AMP) algorithm [34]. The AMP algorithm is represented by AMP A in Algorithm 1. The PS first obtains, for i ∈</p>
        <p>ŷ(2n-1)s+i (t) =ŷ(2n-1)s+i (t) =</p>
        <p>and then estimatesand then estimates</p>
        <p>where we define ŷ(t) [ŷ 1 (t), • • • , ŷs (t)] T . If ŷ(t) = 0, ĝc (θ t ) is used to update the parameter vector as θ t+1 = θ t -η t ĝc (θ t ). On the other hand, if ŷ(t) = 0, the previous parameter vector is simply used as the new one, i.e., θ t+1 = θ t . Remark 4. For N = d/2s , in which the entire gradient vectors are transmitted to the PS at each iteration, the CA-DSGD scheme reduces to the ECESA-DSGD scheme.where we define ŷ(t) [ŷ 1 (t), • • • , ŷs (t)] T . If ŷ(t) = 0, ĝc (θ t ) is used to update the parameter vector as θ t+1 = θ t -η t ĝc (θ t ). On the other hand, if ŷ(t) = 0, the previous parameter vector is simply used as the new one, i.e., θ t+1 = θ t . Remark 4. For N = d/2s , in which the entire gradient vectors are transmitted to the PS at each iteration, the CA-DSGD scheme reduces to the ECESA-DSGD scheme.</p>
        <p>Remark 5. We remark that k is a design parameter which can take different values limited to k &lt; s. For relatively small k values, Remark 6. We note that, even though each device transmits a sparse vector g sp m (θ t ), their sum received over the channel does not need to be sparse. However, when the datasets are i.i.d. across devices and B is large, we expect the gradient estimates across devices to be statistically uniform, and thus, have similar sparsity patterns. Note, however, that the proposed CA-DSGD scheme does not require data to be independent across devices. As it will be shown in Fig. 2, the CA-DSGD scheme converges even when the local datasets are biased, in which case the sparsity patterns are expected to be more diverse. We observe that the transmissions from multiple devices still align on a small number of coordinates thanks to the superposition property of analog transmission. Thus, AMP still manages to recover the average gradient with reasonable accuracy, and the DSGD process converges, albeit more slowly compared to the IID scenario. We will see in Fig. 2 that CA-DSGD outperforms alternative analog and digital schemes with even a higher performance gap in the non-IID scenario.Remark 5. We remark that k is a design parameter which can take different values limited to k &lt; s. For relatively small k values, Remark 6. We note that, even though each device transmits a sparse vector g sp m (θ t ), their sum received over the channel does not need to be sparse. However, when the datasets are i.i.d. across devices and B is large, we expect the gradient estimates across devices to be statistically uniform, and thus, have similar sparsity patterns. Note, however, that the proposed CA-DSGD scheme does not require data to be independent across devices. As it will be shown in Fig. 2, the CA-DSGD scheme converges even when the local datasets are biased, in which case the sparsity patterns are expected to be more diverse. We observe that the transmissions from multiple devices still align on a small number of coordinates thanks to the superposition property of analog transmission. Thus, AMP still manages to recover the average gradient with reasonable accuracy, and the DSGD process converges, albeit more slowly compared to the IID scenario. We will see in Fig. 2 that CA-DSGD outperforms alternative analog and digital schemes with even a higher performance gap in the non-IID scenario.</p>
        <p>Remark 7. With ESA-DSGD, each device transmits only the entries of its estimated gradient whose corresponding channel conditions are sufficiently good. Thus, the gradient vector is inherently sparsified, but only based on the channel gains, regardless of the importance of the gradient entries. Then the entire gradient vector is sent over the bandwidth-limited wireless MAC over orthogonal time periods. On the other hand, with CA-DSGD, each device sends only k ≤ s important gradient entries, where the magnitude of each entry is regarded as the importance metric, by projecting the sparse gradient vector to a low-dimensional vector of length s ≤ d. We further highlight the error accumulation technique incorporated into ECESA-DSGD and CA-DSGD, whereas with ESA-DSGD, entries of the gradient vectors that are not sent are forgotten. Remark 8. We highlight that, thanks to the wireless MAC providing a noisy version of the average of the gradient estimates, the analog schemes can potentially help preserve the privacy as well. This is particularly compelling for the proposed CA-DSGD scheme, where the gradient estimates are compressed through linear projection before transmission.Remark 7. With ESA-DSGD, each device transmits only the entries of its estimated gradient whose corresponding channel conditions are sufficiently good. Thus, the gradient vector is inherently sparsified, but only based on the channel gains, regardless of the importance of the gradient entries. Then the entire gradient vector is sent over the bandwidth-limited wireless MAC over orthogonal time periods. On the other hand, with CA-DSGD, each device sends only k ≤ s important gradient entries, where the magnitude of each entry is regarded as the importance metric, by projecting the sparse gradient vector to a low-dimensional vector of length s ≤ d. We further highlight the error accumulation technique incorporated into ECESA-DSGD and CA-DSGD, whereas with ESA-DSGD, entries of the gradient vectors that are not sent are forgotten. Remark 8. We highlight that, thanks to the wireless MAC providing a noisy version of the average of the gradient estimates, the analog schemes can potentially help preserve the privacy as well. This is particularly compelling for the proposed CA-DSGD scheme, where the gradient estimates are compressed through linear projection before transmission.</p>
        <p>Here we compare the performances of the presented wireless edge learning schemes for the task of image classification. We run experiments on the MNIST dataset [35] with 60000 training and 10000 test samples, and train a single layer neural network with d = 7850 parameters utilizing 
            <rs type="software">ADAM</rs> optimizer [36]. Throughout the experiments, we consider σ 2 = 1, and s = d/20 parallel subchannels, which results in N = 10 for ESA-DSGD and ECESA-DSGD; and for any s of the CA-DSGD scheme, we set the sparsity level to k = s/2.5 . For a fair comparison between the analog DSGD schemes, we set λ x (t) = λ, ∀x ∈ {e, v, c}, and for average transmit power P n (t) at the n-th time slot, n ∈ [N ], of the t-th iteration, t ∈ [T ], we calculate values of γ e,n m (t), γ v,n m (t) and γ c,n m (t) for ESA-DSGD, ECESA-DSGD and CA-DSGD through (22), (32) and (42), respectively. Also, we consider P n (t) = P , ∀n, t, for the analog schemes, and Pm * (t) (t) = P , ∀t, for the digital schemes. The performance is measured as the accuracy with respect to the test data samples, called test accuracy, versus the normalized time N t.
        </p>
        <p>We consider two scenarios to model the data distribution across the devices: in IID data distribution, B randomly selected training data samples are assigned to each device at the beginning of training; while in non-IID data distribution, each device has B training data samples, where half of them are selected at random from only one class/label; that is, for each device, we first select two classes/labels at random, and then randomly select B/2 data samples from each of the two classes/labels. At each iteration, devices use all the B local data samples to compute their gradient estimates, i.e., the batch size is equal to the size of the local datasets.We consider two scenarios to model the data distribution across the devices: in IID data distribution, B randomly selected training data samples are assigned to each device at the beginning of training; while in non-IID data distribution, each device has B training data samples, where half of them are selected at random from only one class/label; that is, for each device, we first select two classes/labels at random, and then randomly select B/2 data samples from each of the two classes/labels. At each iteration, devices use all the B local data samples to compute their gradient estimates, i.e., the batch size is equal to the size of the local datasets.</p>
        <p>For numerical comparison, we also consider the error-free shared link approach, where at each time slot the PS receives the average of the gradient estimates, 1 M M m=1 g m (θ t ), in a noiseless fashion. We note that, for the error-free shared link approach, we have N = 1. We consider three alternative digital schemes employing sparse binary compression [18], QSGD [8] and SignSGD [12] algorithms for gradient compression, respectively. When we refer to D-DSGD it refers to using the sparse binary compression technique. For a fair comparison, we apply QSGD and SignSGD to a limited number of gradient entries such that the final number of bits does not exceed the capacity of the underlying fading MAC. To be more precise, considering the device scheduling policy given in (6), q S (t) and q Q (t) gradient entries with highest magnitudes are selected for transmission for SignSGD and QSGD, respectively, while all the other entries are set to zero. With the SignSGD algorithm [12], the scheduled device transmits only the signs of the q S (t) selected entries, and a total of r S (t) = log 2 d q S (t) + q S (t) bits, ∀t,For numerical comparison, we also consider the error-free shared link approach, where at each time slot the PS receives the average of the gradient estimates, 1 M M m=1 g m (θ t ), in a noiseless fashion. We note that, for the error-free shared link approach, we have N = 1. We consider three alternative digital schemes employing sparse binary compression [18], QSGD [8] and SignSGD [12] algorithms for gradient compression, respectively. When we refer to D-DSGD it refers to using the sparse binary compression technique. For a fair comparison, we apply QSGD and SignSGD to a limited number of gradient entries such that the final number of bits does not exceed the capacity of the underlying fading MAC. To be more precise, considering the device scheduling policy given in (6), q S (t) and q Q (t) gradient entries with highest magnitudes are selected for transmission for SignSGD and QSGD, respectively, while all the other entries are set to zero. With the SignSGD algorithm [12], the scheduled device transmits only the signs of the q S (t) selected entries, and a total of r S (t) = log 2 d q S (t) + q S (t) bits, ∀t,</p>
        <p>are required to send the sign and location of each selected entry, and q S (t) is set as the largest integer satisfying r S (t) ≤ R(t). With the QSGD algorithm [8], the scheduled device transmits a quantized version of each of the q Q (t) selected entries with a quantization level of 2 l Q , the l 2 -norm of the resultant vector with q Q (t) non-zero entries, and the locations of the non-zero entries. Thus, a total of sent over the wireless fading MAC, where q Q (t) is set as the largest integer satisfying r Q (t) ≤ R t . Here we consider a quantization level l Q = 2 for QSGD.are required to send the sign and location of each selected entry, and q S (t) is set as the largest integer satisfying r S (t) ≤ R(t). With the QSGD algorithm [8], the scheduled device transmits a quantized version of each of the q Q (t) selected entries with a quantization level of 2 l Q , the l 2 -norm of the resultant vector with q Q (t) non-zero entries, and the locations of the non-zero entries. Thus, a total of sent over the wireless fading MAC, where q Q (t) is set as the largest integer satisfying r Q (t) ≤ R t . Here we consider a quantization level l Q = 2 for QSGD.</p>
        <p>We further consider the OD-DSGD scheme, where each device has access to s/M distinct subchannels to perform digital transmission without interfering with other devices. Due to symmetry across devices, we allocate subchannels (m -1) s/M + 1 to m s/M to device m, m ∈ [M ]. Similarly to the D-DSGD scheme, we use the capacity upperbound to determine the number of bits each user can convey to the PS at each iteration. This bound is computed by waterfilling across the s/M channels available to each device as in ( 9)- (11). We use sparse binary compression with the OD-DSGD scheme as well with the sparsity level q O m (t) set as the largest integer satisfying log 2 d q O m (t) + 33 ≤ R O m (t), ∀t. In Fig. 2, we compare the performances of different analog and digital schemes for IID and non-IID data distribution scenarios, for M = 25 devices, B = 1000 training data samples and average transmit power constraint P = 20. We consider s = 2s = d/10, i.e., N = 1 for CA-DSGD, and we set the threshold value to λ = 10 -3 . Observe that for both IID and non-IID data distribution cases CA-DSGD outperforms all other analog and digital schemes with the improvement substantially larger for non-IID data distribution, which shows its robustness to bias in the data distribution. CA-DSGD has a smaller convergence speed in non-IID case which is due to the reduction in the similarity of the sparsity patterns of the gradients across devices, although it does converge much faster and to a much higher accuracy level compared to the other schemes under consideration. The gap between the errorfree shared link approach and CA-DSGD is relatively small for the IID case, and the final test accuracies of the two approaches are also similar for the non-IID case, although CA-DSGD converges more slowly in this case. Unlike the digital schemes, CA-DSGD benefits from the superposition property of the underlying wireless MAC by aligning the transmit powers to dominate the noise. We further highlight that the main reasons for the degradation of the ESA-DSGD over CA-DSGD are i) scheduling gradient entries for transmission only based on the channel gains; ii) transmitting the entire gradient vectors of relatively huge dimensions (compared to the channel bandwidth); iii) ignoring the gradient entries which have not been transmitted due to the poor conditions of their corresponding channels. We note that ECESA-DSGD resolves the last issue by utilizing error accumulation technique, which provides some gains with respect to ESA-DSGD, but we observe that better scheduling and more efficient utilization of the bandwidth through linear projection provide significant gains, especially for the non-IID case, where the performances of ESA-DSGD and ECESA-DSGD significantly degrade in terms of the test accuracy, as well as the convergence speed. Also, D-DSGD provides a better accuracy than SignSGD and QSGD for both data distribution scenarios, and the performance of all the digital schemes under consideration deteriorate substantially in the non-IID case; this performance loss is more severe for QSGD.We further consider the OD-DSGD scheme, where each device has access to s/M distinct subchannels to perform digital transmission without interfering with other devices. Due to symmetry across devices, we allocate subchannels (m -1) s/M + 1 to m s/M to device m, m ∈ [M ]. Similarly to the D-DSGD scheme, we use the capacity upperbound to determine the number of bits each user can convey to the PS at each iteration. This bound is computed by waterfilling across the s/M channels available to each device as in ( 9)- (11). We use sparse binary compression with the OD-DSGD scheme as well with the sparsity level q O m (t) set as the largest integer satisfying log 2 d q O m (t) + 33 ≤ R O m (t), ∀t. In Fig. 2, we compare the performances of different analog and digital schemes for IID and non-IID data distribution scenarios, for M = 25 devices, B = 1000 training data samples and average transmit power constraint P = 20. We consider s = 2s = d/10, i.e., N = 1 for CA-DSGD, and we set the threshold value to λ = 10 -3 . Observe that for both IID and non-IID data distribution cases CA-DSGD outperforms all other analog and digital schemes with the improvement substantially larger for non-IID data distribution, which shows its robustness to bias in the data distribution. CA-DSGD has a smaller convergence speed in non-IID case which is due to the reduction in the similarity of the sparsity patterns of the gradients across devices, although it does converge much faster and to a much higher accuracy level compared to the other schemes under consideration. The gap between the errorfree shared link approach and CA-DSGD is relatively small for the IID case, and the final test accuracies of the two approaches are also similar for the non-IID case, although CA-DSGD converges more slowly in this case. Unlike the digital schemes, CA-DSGD benefits from the superposition property of the underlying wireless MAC by aligning the transmit powers to dominate the noise. We further highlight that the main reasons for the degradation of the ESA-DSGD over CA-DSGD are i) scheduling gradient entries for transmission only based on the channel gains; ii) transmitting the entire gradient vectors of relatively huge dimensions (compared to the channel bandwidth); iii) ignoring the gradient entries which have not been transmitted due to the poor conditions of their corresponding channels. We note that ECESA-DSGD resolves the last issue by utilizing error accumulation technique, which provides some gains with respect to ESA-DSGD, but we observe that better scheduling and more efficient utilization of the bandwidth through linear projection provide significant gains, especially for the non-IID case, where the performances of ESA-DSGD and ECESA-DSGD significantly degrade in terms of the test accuracy, as well as the convergence speed. Also, D-DSGD provides a better accuracy than SignSGD and QSGD for both data distribution scenarios, and the performance of all the digital schemes under consideration deteriorate substantially in the non-IID case; this performance loss is more severe for QSGD.</p>
        <p>In the following experiments, we only consider IID data distribution. In Fig. 3, we compare the performances of different analog and digital algorithms for two different average transmit power values P = 10 and P = 30. We consider M = 25 and B = 10, and we set s = 2s = d/10, i.e., N = 1 for CA-DSGD, and λ = 5 × 10 -3 . As it can be seen, CA-DSGD continues to outperform all the other schemes, with a relatively small gap to the error-free shared link approach. By comparing Figures 3a and3b, it can be seen that the performances of all the schemes improve with P , but the improvement is more significant for the digital schemes in terms of both the accuracy and the convergence speed, except QSGD which only improves in terms of accuracy. This shows that the analog schemes are less sensitive to a reduction in the average transmit power than the digital ones. This is because, thanks to the signal-superposition property, the system continues to operate in a relatively high effective signal-to-noise ratio (SNR) regime despite reduction in the transmission power of individual devices.In the following experiments, we only consider IID data distribution. In Fig. 3, we compare the performances of different analog and digital algorithms for two different average transmit power values P = 10 and P = 30. We consider M = 25 and B = 10, and we set s = 2s = d/10, i.e., N = 1 for CA-DSGD, and λ = 5 × 10 -3 . As it can be seen, CA-DSGD continues to outperform all the other schemes, with a relatively small gap to the error-free shared link approach. By comparing Figures 3a and3b, it can be seen that the performances of all the schemes improve with P , but the improvement is more significant for the digital schemes in terms of both the accuracy and the convergence speed, except QSGD which only improves in terms of accuracy. This shows that the analog schemes are less sensitive to a reduction in the average transmit power than the digital ones. This is because, thanks to the signal-superposition property, the system continues to operate in a relatively high effective signal-to-noise ratio (SNR) regime despite reduction in the transmission power of individual devices.</p>
        <p>In It is again evident that CA-DSGD outperforms all the other schemes with the improvement over ESA-DSGD and ECESA-DSGD more noticeable for the higher M value. As it can be seen, the performances of the analog schemes improve with M , since increasing M provides additional power introduced by each device and increases the robustness of the estimation against noise. We note that this improvement is larger for CA-DSGD, which is due to the more efficient utilization of the gradient estimates computed by the devices. Digital schemes also gain from increasing M , which is due to the additional power allocated to the selected device, as the devices are less frequently scheduled for transmission. We note that the superiority of the ECESA-DSGD over ESA-DSGD reduces with M , which shows that error accumulation is less effective for higher M values when M B is fixed. This is because for larger M , the chance of receiving more estimates for each entry of the actual gradient vector is higher (each gradient entry is estimated more accurately at the PS), and it is less likely that no estimate of any gradient entry is received by the PS. Accordingly, the benefit of error accumulation becomes less significant for higher number of devices.In It is again evident that CA-DSGD outperforms all the other schemes with the improvement over ESA-DSGD and ECESA-DSGD more noticeable for the higher M value. As it can be seen, the performances of the analog schemes improve with M , since increasing M provides additional power introduced by each device and increases the robustness of the estimation against noise. We note that this improvement is larger for CA-DSGD, which is due to the more efficient utilization of the gradient estimates computed by the devices. Digital schemes also gain from increasing M , which is due to the additional power allocated to the selected device, as the devices are less frequently scheduled for transmission. We note that the superiority of the ECESA-DSGD over ESA-DSGD reduces with M , which shows that error accumulation is less effective for higher M values when M B is fixed. This is because for larger M , the chance of receiving more estimates for each entry of the actual gradient vector is higher (each gradient entry is estimated more accurately at the PS), and it is less likely that no estimate of any gradient entry is received by the PS. Accordingly, the benefit of error accumulation becomes less significant for higher number of devices.</p>
        <p>In Fig. 5, we compare the performance of D-DSGD with that of OD-DSGD for different P values, P ∈ {20, 100}, when M = 25 and B = 1000. For both power values we observe that D-DSGD significantly outperforms OD-DSGD in terms of accuracy and convergence speed, while the superiority is more highlighted for the higher P value. This shows that opportunistically allocating all the available bandwidth to only a single device is better than sharing it equally among all the devices which indicates that it is better to receive an accurate gradient estimate from a single device at each iteration, instead of receiving coarse estimates from all the devices. This improvement is more significant when P increases.In Fig. 5, we compare the performance of D-DSGD with that of OD-DSGD for different P values, P ∈ {20, 100}, when M = 25 and B = 1000. For both power values we observe that D-DSGD significantly outperforms OD-DSGD in terms of accuracy and convergence speed, while the superiority is more highlighted for the higher P value. This shows that opportunistically allocating all the available bandwidth to only a single device is better than sharing it equally among all the devices which indicates that it is better to receive an accurate gradient estimate from a single device at each iteration, instead of receiving coarse estimates from all the devices. This improvement is more significant when P increases.</p>
        <p>In Fig. 6 we investigate the impact of s on the performance of CA-DSGD. We consider s ∈ {2s, 4s} = {d/10, d/5} for CA-DSGD, in which s = 2s and s = 4s are equivalent to N = 1 and N = 2, respectively. We have M = 15, B = 1000 and P = 1, ans we set λ = 5 × 10 -3 . We highlight the superiority of the CA-DSGD scheme for both s values under consideration over ESA-DSGD. The ECESA-DSGD scheme, which is equivalent to CA-DSGD for s = d, also outperforms ESA-DSGD slightly. However, as it can be seen, the performance of CA-DSGD degrades as s increases, which indicates that transmitting more sparse versions of the gradient estimates while using the available channel bandwidth for further iterations results in a higher accuracy. The flexibility in choosing the dimension of the transmitted gradient estimates makes the proposed CA-DSGD scheme particularly compelling for learning at the wireless edge under strict bandwidth limit. In Fig. 7, we consider the impact of imperfect CSI on the performance of analog schemes CA-DSGD and ECESA-DSGD for M = 25, B = 1000 and P = 10. We set λ = 5 × 10 -3 , and s = 2s = d/10, which results in N = 1 for CA-DSGD. We assume a noisy CSI at device m given by ĥn m,i (t) = h n m,i (t) + hn m,i (t), ∀m, n, i, t, where hn m,i (t) is i.i.d. according to CN (0, 1), i.e., a complex normal random variable with the same variance as the actual channel gain h n m,i (t). We note that all the processing at the devices, such as power allocation, finding the set M n i (t), and obtaining γ v,n m (t) and γ c,n m (t), and consequently γv,n (t) and γc,n (t) for CA-DSGD and ECESA-DSGD, respectively, are performed based on the imperfect CSI ĥn m,i (t), ∀m, n, t. As it can be seen, both the CA-DSGD and ECESA-DSGD are robust against the imperfect CSI. To be more precise, after 2250 time slots (2250 SGD iterations for CA-DSGD and 225 SGD iterations for ECESA-DSGD), the final test accuracy reduction for CA-DSGD due to the imperfect CSI is 0.67%, and that of ECESA-DSGD is 0.76%. We highlight that, with imperfect CSI, even though the users will allocate higher or lower power to each subchannel than the optimal one, the cumulative effect becomes negligible since these variations across users are averaged out thanks to the superposition property.In Fig. 6 we investigate the impact of s on the performance of CA-DSGD. We consider s ∈ {2s, 4s} = {d/10, d/5} for CA-DSGD, in which s = 2s and s = 4s are equivalent to N = 1 and N = 2, respectively. We have M = 15, B = 1000 and P = 1, ans we set λ = 5 × 10 -3 . We highlight the superiority of the CA-DSGD scheme for both s values under consideration over ESA-DSGD. The ECESA-DSGD scheme, which is equivalent to CA-DSGD for s = d, also outperforms ESA-DSGD slightly. However, as it can be seen, the performance of CA-DSGD degrades as s increases, which indicates that transmitting more sparse versions of the gradient estimates while using the available channel bandwidth for further iterations results in a higher accuracy. The flexibility in choosing the dimension of the transmitted gradient estimates makes the proposed CA-DSGD scheme particularly compelling for learning at the wireless edge under strict bandwidth limit. In Fig. 7, we consider the impact of imperfect CSI on the performance of analog schemes CA-DSGD and ECESA-DSGD for M = 25, B = 1000 and P = 10. We set λ = 5 × 10 -3 , and s = 2s = d/10, which results in N = 1 for CA-DSGD. We assume a noisy CSI at device m given by ĥn m,i (t) = h n m,i (t) + hn m,i (t), ∀m, n, i, t, where hn m,i (t) is i.i.d. according to CN (0, 1), i.e., a complex normal random variable with the same variance as the actual channel gain h n m,i (t). We note that all the processing at the devices, such as power allocation, finding the set M n i (t), and obtaining γ v,n m (t) and γ c,n m (t), and consequently γv,n (t) and γc,n (t) for CA-DSGD and ECESA-DSGD, respectively, are performed based on the imperfect CSI ĥn m,i (t), ∀m, n, t. As it can be seen, both the CA-DSGD and ECESA-DSGD are robust against the imperfect CSI. To be more precise, after 2250 time slots (2250 SGD iterations for CA-DSGD and 225 SGD iterations for ECESA-DSGD), the final test accuracy reduction for CA-DSGD due to the imperfect CSI is 0.67%, and that of ECESA-DSGD is 0.76%. We highlight that, with imperfect CSI, even though the users will allocate higher or lower power to each subchannel than the optimal one, the cumulative effect becomes negligible since these variations across users are averaged out thanks to the superposition property.</p>
        <p>We have studied FL at the wireless edge, where M devices with limited transmit power and datasets communicate with the PS over a bandwidth-limited fading MAC to minimize a loss function by performing DSGD. The PS updates the parameter vector, and shares it with the devices over an errorfree shared link. We first presented a digital approach that treats computation and communication separately. At each iteration of the proposed D-DSGD scheme, one device is selected depending on the channel states, and the selected device first quantizes its gradient estimate, and transmits the quantized bits to the PS using a capacity-achieving channel code. Then we studied an alternative analog transmission approach, which does not employ quantization or channel coding, and exploits the superposition property of the wireless MAC, rather than orthogonalizing the transmissions from different devices. We have proposed the CA-DSGD scheme, where each device employs gradient sparsification with error accumulation followed by linear projection to reduce the typically very large parameter vector dimension to the limited channel bandwidth.We have studied FL at the wireless edge, where M devices with limited transmit power and datasets communicate with the PS over a bandwidth-limited fading MAC to minimize a loss function by performing DSGD. The PS updates the parameter vector, and shares it with the devices over an errorfree shared link. We first presented a digital approach that treats computation and communication separately. At each iteration of the proposed D-DSGD scheme, one device is selected depending on the channel states, and the selected device first quantizes its gradient estimate, and transmits the quantized bits to the PS using a capacity-achieving channel code. Then we studied an alternative analog transmission approach, which does not employ quantization or channel coding, and exploits the superposition property of the wireless MAC, rather than orthogonalizing the transmissions from different devices. We have proposed the CA-DSGD scheme, where each device employs gradient sparsification with error accumulation followed by linear projection to reduce the typically very large parameter vector dimension to the limited channel bandwidth.</p>
        <p>t ), m ∈ [M ]. This k-level sparsification is represented by function sparse k in Algorithm 1, i.e., g sp mt ), m ∈ [M ]. This k-level sparsification is represented by function sparse k in Algorithm 1, i.e., g sp m</p>
        <p>We have also designed a power allocation scheme to align the received vectors at the PS while satisfying the average power constraints at the devices. The CA-DSGD scheme allows a much more efficient use of the limited channel bandwidth, and benefits from the "beamforming effect" thanks to the similarity in the patterns of the gradient estimates across devices. The impact of various system parameters on the performance is studied numerically considering MNIST classification across edge devices as an example. Numerical results show that CA-DSGD outperforms D-DSGD and other state-of-the-art analog schemes consistently, while this improvement is even more significant for the non-IID data distribution scenario.We have also designed a power allocation scheme to align the received vectors at the PS while satisfying the average power constraints at the devices. The CA-DSGD scheme allows a much more efficient use of the limited channel bandwidth, and benefits from the "beamforming effect" thanks to the similarity in the patterns of the gradient estimates across devices. The impact of various system parameters on the performance is studied numerically considering MNIST classification across edge devices as an example. Numerical results show that CA-DSGD outperforms D-DSGD and other state-of-the-art analog schemes consistently, while this improvement is even more significant for the non-IID data distribution scenario.</p>
    </text>
</tei>

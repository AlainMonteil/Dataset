<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:22+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Perhaps the most straightforward classifier in the arsenal or Machine Learning techniques is the Nearest Neighbour Classifierclassification is achieved by identifying the nearest neighbours to a query example and using those neighbours to determine the class of the query. This approach to classification is of particular importance because issues of poor run-time performance is not such a problem these days with the computational power that is available. This paper presents an overview of techniques for Nearest Neighbour classification focusing on; mechanisms for assessing similarity (distance), computational issues in identifying nearest neighbours and mechanisms for reducing the dimension of the data. This paper is the second edition of a paper previously published as a technical report [16]. Sections on similarity measures for time-series, retrieval speed-up and intrinsic dimensionality have been added. An Appendix is included providing access to Python 
            <rs type="software">code</rs> for the key methods.
        </p>
        <p>CCS Concepts: • Computing methodologies → Machine learning; Machine learning.CCS Concepts: • Computing methodologies → Machine learning; Machine learning.</p>
        <p>The basic idea is as shown in Figure 1 which depicts a 3-Nearest Neighbour Classifier on a two-class problem in a two-dimensional feature space. In this example the decision for 𝑞 1 is straightforward -all three of its nearest neighbours are of class 𝑂 so it is classified as an 𝑂. The situation for 𝑞 2 is a bit more complicated at it has two neighbours of class 𝑋 and one of class 𝑂. This can be resolved by simple majority voting or by distance weighted voting (see below). So 𝑘-NN training example is labelled with a class label 𝑦 𝑗 ∈ 𝑌 . Our objective is to classify an unknown example q. For each 𝑥 𝑖 ∈ 𝐷 we can calculate the distance between q and x 𝑖 as follows:The basic idea is as shown in Figure 1 which depicts a 3-Nearest Neighbour Classifier on a two-class problem in a two-dimensional feature space. In this example the decision for 𝑞 1 is straightforward -all three of its nearest neighbours are of class 𝑂 so it is classified as an 𝑂. The situation for 𝑞 2 is a bit more complicated at it has two neighbours of class 𝑋 and one of class 𝑂. This can be resolved by simple majority voting or by distance weighted voting (see below). So 𝑘-NN training example is labelled with a class label 𝑦 𝑗 ∈ 𝑌 . Our objective is to classify an unknown example q. For each 𝑥 𝑖 ∈ 𝐷 we can calculate the distance between q and x 𝑖 as follows:</p>
        <p>This is a summation over all the features in 𝐹 with 𝑤 𝑓 the weight for each feature. There are a large range of possibilities for this distance metric; a basic version for continuous and discrete attributes would be:This is a summation over all the features in 𝐹 with 𝑤 𝑓 the weight for each feature. There are a large range of possibilities for this distance metric; a basic version for continuous and discrete attributes would be:</p>
        <p>The 𝑘 nearest neighbours are selected based on this distance metric. Then there are a variety of ways in which the 𝑘 nearest neighbours can be used to determine the class of q. The most straightforward approach is to assign the majority class among the nearest neighbours to the query.The 𝑘 nearest neighbours are selected based on this distance metric. Then there are a variety of ways in which the 𝑘 nearest neighbours can be used to determine the class of q. The most straightforward approach is to assign the majority class among the nearest neighbours to the query.</p>
        <p>It will often make sense to assign more weight to the nearer neighbours in deciding the class of the query. A fairly general technique to achieve this is distance weighted voting where the neighbours get to vote on the class of the query case with votes weighted by the inverse of their distance to the query.It will often make sense to assign more weight to the nearer neighbours in deciding the class of the query. A fairly general technique to achieve this is distance weighted voting where the neighbours get to vote on the class of the query case with votes weighted by the inverse of their distance to the query.</p>
        <p>Thus the vote assigned to class 𝑦 𝑗 by neighbour 𝑥 𝑐 is 1 divided by the distance to that neighbour, i.e. 1(𝑦 𝑗 , 𝑦 𝑐 ) returns 1Thus the vote assigned to class 𝑦 𝑗 by neighbour 𝑥 𝑐 is 1 divided by the distance to that neighbour, i.e. 1(𝑦 𝑗 , 𝑦 𝑐 ) returns 1</p>
        <p>if the class labels match and 0 otherwise. In equation 3 𝑝 would normally be 1 but values greater than 1 can be used to further reduce the influence of more distant neighbours.if the class labels match and 0 otherwise. In equation 3 𝑝 would normally be 1 but values greater than 1 can be used to further reduce the influence of more distant neighbours.</p>
        <p>Manuscript submitted to ACM Another approach to voting is based on Shepard's work [48] and uses an exponential function rather than inverse distance, i.e:Manuscript submitted to ACM Another approach to voting is based on Shepard's work [48] and uses an exponential function rather than inverse distance, i.e:</p>
        <p>𝑒 𝑑 (q,x 𝑐 ) 1(𝑦 𝑗 , 𝑦 𝑐 )𝑒 𝑑 (q,x 𝑐 ) 1(𝑦 𝑗 , 𝑦 𝑐 )</p>
        <p>It is worth mentioning that 𝑘-NN can also be effective for regression [2]. In regression the dependant variable 𝑦 is a real number (𝑦 ∈ R) so the predicted value ŷ can be the mean or weighted mean of the 𝑦 value for the neighbours. The weighted mean would be defined as follows:It is worth mentioning that 𝑘-NN can also be effective for regression [2]. In regression the dependant variable 𝑦 is a real number (𝑦 ∈ R) so the predicted value ŷ can be the mean or weighted mean of the 𝑦 value for the neighbours. The weighted mean would be defined as follows:</p>
        <p>The objective for this paper is to present a comprehensive tutorial resource on the use of 𝑘-NN. While other 𝑘-NN resources exist, most notably the 1991 book by Dasarathy [17] and the more recent survey by Bhatia [11], our focus is on the implementation of a wide variety of 𝑘-NN methods and we provide supporting code in Python. To this end, we consider three important factors that must be considered with the use of 𝑘-NN. In the next section we look at the core issue of similarity and distance measures and explore some exotic (dis)similarity measures to illustrate the generality of the 𝑘-NN idea. In section 3 we look at computational complexity issues and review some speed-up techniques for 𝑘-NN. In section 4 we look at dimension reduction -both feature selection and sample selection. Dimension reduction is of particular importance with 𝑘-NN as it has a big impact on computational performance and accuracy. The paper concludes with a summary of the advantages and disadvantages of 𝑘-NN.The objective for this paper is to present a comprehensive tutorial resource on the use of 𝑘-NN. While other 𝑘-NN resources exist, most notably the 1991 book by Dasarathy [17] and the more recent survey by Bhatia [11], our focus is on the implementation of a wide variety of 𝑘-NN methods and we provide supporting code in Python. To this end, we consider three important factors that must be considered with the use of 𝑘-NN. In the next section we look at the core issue of similarity and distance measures and explore some exotic (dis)similarity measures to illustrate the generality of the 𝑘-NN idea. In section 3 we look at computational complexity issues and review some speed-up techniques for 𝑘-NN. In section 4 we look at dimension reduction -both feature selection and sample selection. Dimension reduction is of particular importance with 𝑘-NN as it has a big impact on computational performance and accuracy. The paper concludes with a summary of the advantages and disadvantages of 𝑘-NN.</p>
        <p>While the terms similarity metric and distance metric are often used colloquially to refer to any measure of affinity between two objects, the term metric has a formal meaning in mathematics. A metric must conform to the following four criteria (where 𝑑 (𝑥, 𝑦) refers to the distance between two objects 𝑥 and 𝑦):While the terms similarity metric and distance metric are often used colloquially to refer to any measure of affinity between two objects, the term metric has a formal meaning in mathematics. A metric must conform to the following four criteria (where 𝑑 (𝑥, 𝑦) refers to the distance between two objects 𝑥 and 𝑦):</p>
        <p>(1) 𝑑 (𝑥, 𝑦) ≥ 0; non-negativity It is possible to build a 𝑘-NN classifier that incorporates an affinity measure that is not a proper metric, however there are some performance optimisations to the basic 𝑘-NN algorithm that require the use of a proper metric [10,46,59,59].(1) 𝑑 (𝑥, 𝑦) ≥ 0; non-negativity It is possible to build a 𝑘-NN classifier that incorporates an affinity measure that is not a proper metric, however there are some performance optimisations to the basic 𝑘-NN algorithm that require the use of a proper metric [10,46,59,59].</p>
        <p>In brief, these techniques can identify the nearest neighbour of an object without comparing that object to every other object but the affinity measure must be a metric, in particular it must satisfy the triangle inequality.In brief, these techniques can identify the nearest neighbour of an object without comparing that object to every other object but the affinity measure must be a metric, in particular it must satisfy the triangle inequality.</p>
        <p>The basic distance metric described in equations 1 and 2 is a special case of the Minkowski Distance metric -in fact it is the 1-norm (𝐿 1 ) Minkowski distance. The general formula for the Minkowski distance isThe basic distance metric described in equations 1 and 2 is a special case of the Minkowski Distance metric -in fact it is the 1-norm (𝐿 1 ) Minkowski distance. The general formula for the Minkowski distance is</p>
        <p>The 𝐿 The other important Minkowski distance is the 𝐿 ∞ or Chebyshev distance.The 𝐿 The other important Minkowski distance is the 𝐿 ∞ or Chebyshev distance.</p>
        <p>This is simply the distance in the dimension in which the two examples are most different; it is sometimes referred to as the chessboard distance as it is the number of moves it takes a chess king to reach any square on the board.This is simply the distance in the dimension in which the two examples are most different; it is sometimes referred to as the chessboard distance as it is the number of moves it takes a chess king to reach any square on the board.</p>
        <p>While the Euclidean and Manhattan distances are probably the most popular 𝑘-NN distance measures, much of the usefulness of 𝑘-NN derives from the potential to work with metrics that are specific to the data under analysis.While the Euclidean and Manhattan distances are probably the most popular 𝑘-NN distance measures, much of the usefulness of 𝑘-NN derives from the potential to work with metrics that are specific to the data under analysis.</p>
        <p>In the next subsections we will look at the merits of Cosine Similarity and (Pearson) Correlation. Then we will look at some more complex distance measures that are specialised for particular data types, i.e. Earth Mover Distance for image data and Dynamic Time Warping for time-series data. These were chosen because they enable the application of machine learning on data that is not in a feature vector format. This section concludes with a brief introduction to Metric Learning whereby a metric can be induced from the data.In the next subsections we will look at the merits of Cosine Similarity and (Pearson) Correlation. Then we will look at some more complex distance measures that are specialised for particular data types, i.e. Earth Mover Distance for image data and Dynamic Time Warping for time-series data. These were chosen because they enable the application of machine learning on data that is not in a feature vector format. This section concludes with a brief introduction to Metric Learning whereby a metric can be induced from the data.</p>
        <p>Like Minkowski distance, Cosine Similarity works with feature vector data. However, similarity is based on the angles between the feature vectors -see Figure 2. While C would be the closer example to Q based on Euclidean distance, D is closer to Q when the angles between the features vectors is considered. The Cosine similarity between a query q and x 𝑖 is as follows:Like Minkowski distance, Cosine Similarity works with feature vector data. However, similarity is based on the angles between the feature vectors -see Figure 2. While C would be the closer example to Q based on Euclidean distance, D is closer to Q when the angles between the features vectors is considered. The Cosine similarity between a query q and x 𝑖 is as follows:</p>
        <p>Figure 3 shows a scenario where correlation would be the appropriate similarity measure. While B is the more similar example to the query Q in terms of feature values, the pattern in A better correlates with Q. Sometimes this correlation is is the key to the underlying similarity. This would be appropriate where the feature values reflect resource allocation (for example household expenditure on 10 categories (X0 -X9)). The magnitudes might be quite different but the allocation pattern could be the same. The two most popular correlation coefficients are the Pearson and Spearman measures [20]. The Pearson is applicable for features that are normally distributed. When reference is made to a correlation coefficient without specifying which one, it is probably the Pearson. The Pearson correlation between a query q and a sample x 𝑖 is defined as follows:Figure 3 shows a scenario where correlation would be the appropriate similarity measure. While B is the more similar example to the query Q in terms of feature values, the pattern in A better correlates with Q. Sometimes this correlation is is the key to the underlying similarity. This would be appropriate where the feature values reflect resource allocation (for example household expenditure on 10 categories (X0 -X9)). The magnitudes might be quite different but the allocation pattern could be the same. The two most popular correlation coefficients are the Pearson and Spearman measures [20]. The Pearson is applicable for features that are normally distributed. When reference is made to a correlation coefficient without specifying which one, it is probably the Pearson. The Pearson correlation between a query q and a sample x 𝑖 is defined as follows:</p>
        <p>where x𝑖 and 𝑠 𝑥 are the mean and standard deviation of x 𝑖 . This is the dot product of the mean-adjusted q and x 𝑖 vectors divided by their standard deviations. This mean adjustment makes the measure insensitive to variations in scale.where x𝑖 and 𝑠 𝑥 are the mean and standard deviation of x 𝑖 . This is the dot product of the mean-adjusted q and x 𝑖 vectors divided by their standard deviations. This mean adjustment makes the measure insensitive to variations in scale.</p>
        <p>In circumstances where the features are not normally distributed the Spearman (rank) correlation can be used. The feature values are ranked and the statistic is calculated using ranks rather than the original values.In circumstances where the features are not normally distributed the Spearman (rank) correlation can be used. The feature values are ranked and the statistic is calculated using ranks rather than the original values.</p>
        <p>Correlation scores range from [-1, 1]. A score of 1 represents a perfect correlation, 0 is no correlation and -1 means the samples are anti-correlated. A correlation is a similarity score and so can be converted to a distance in the same manner as for Cosine -see equation 8.Correlation scores range from [-1, 1]. A score of 1 represents a perfect correlation, 0 is no correlation and -1 means the samples are anti-correlated. A correlation is a similarity score and so can be converted to a distance in the same manner as for Cosine -see equation 8.</p>
        <p>The Minkowski distance defined in (6) is a very general metric that can be used in a 𝑘-NN classifier for any data that is represented as a feature vector. When working with image data a convenient representation for the purpose of calculating distances is a colour histogram. An image can be considered as a grey-scale histogram 𝐻 of 𝑁 levels or bins where ℎ 𝑖 is the number of pixels that fall into the interval represented by bin 𝑖 (this vector ℎ is the feature vector). The Minkowski distance formula (6) can be used to compare two images described as histograms. 𝐿 1 , 𝐿 2 and less often 𝐿 ∞ norms are used. Other popular measures for comparing histograms are the Kullback-Leibler divergence (10) [32] and the 𝜒 2 statistic (11) [45].The Minkowski distance defined in (6) is a very general metric that can be used in a 𝑘-NN classifier for any data that is represented as a feature vector. When working with image data a convenient representation for the purpose of calculating distances is a colour histogram. An image can be considered as a grey-scale histogram 𝐻 of 𝑁 levels or bins where ℎ 𝑖 is the number of pixels that fall into the interval represented by bin 𝑖 (this vector ℎ is the feature vector). The Minkowski distance formula (6) can be used to compare two images described as histograms. 𝐿 1 , 𝐿 2 and less often 𝐿 ∞ norms are used. Other popular measures for comparing histograms are the Kullback-Leibler divergence (10) [32] and the 𝜒 2 statistic (11) [45].</p>
        <p>where 𝐻 and 𝐾 are two histograms, ℎ and 𝑘 are the corresponding vectors of bin values and 𝑚 𝑖 = ℎ 𝑖 +𝑘 𝑖 2 . While these measures have sound theoretical support in information theory and in statistics they have some significant drawbacks. The first drawback is that they are not metrics in that they do not satisfy the symmetry requirement.where 𝐻 and 𝐾 are two histograms, ℎ and 𝑘 are the corresponding vectors of bin values and 𝑚 𝑖 = ℎ 𝑖 +𝑘 𝑖 2 . While these measures have sound theoretical support in information theory and in statistics they have some significant drawbacks. The first drawback is that they are not metrics in that they do not satisfy the symmetry requirement.</p>
        <p>However, this problem can easily be overcome by defining a modified distance between 𝑥 and 𝑦 that is in some way an average of 𝑑 (𝑥, 𝑦) and 𝑑 (𝑦, 𝑥) -see [45] for the Jeffrey divergence which is a symmetric version of the Kullback-Leibler divergence.However, this problem can easily be overcome by defining a modified distance between 𝑥 and 𝑦 that is in some way an average of 𝑑 (𝑥, 𝑦) and 𝑑 (𝑦, 𝑥) -see [45] for the Jeffrey divergence which is a symmetric version of the Kullback-Leibler divergence.</p>
        <p>A more significant drawback is that these measures are prone to errors due to bin boundaries. The distance between an image and a slightly darker version of itself can be great if pixels fall into an adjacent bin as there is no consideration of adjacency of bins in these measures.A more significant drawback is that these measures are prone to errors due to bin boundaries. The distance between an image and a slightly darker version of itself can be great if pixels fall into an adjacent bin as there is no consideration of adjacency of bins in these measures.</p>
        <p>Earth Mover Distance The Earth Mover Distance (EMD) is a distance measure that overcomes many of these problems that arise from the arbitrariness of binning. As the name implies, the distance is based on the notion of the amount of effort required to convert one image to another based on the analogy of transporting mass from one distribution to another. If we think of two images as distributions and view one distribution as a mass of earth in space and the other distribution as a hole (or set of holes) in the same space then the EMD is the minimum amount of work involved in filling the holes with the earth.Earth Mover Distance The Earth Mover Distance (EMD) is a distance measure that overcomes many of these problems that arise from the arbitrariness of binning. As the name implies, the distance is based on the notion of the amount of effort required to convert one image to another based on the analogy of transporting mass from one distribution to another. If we think of two images as distributions and view one distribution as a mass of earth in space and the other distribution as a hole (or set of holes) in the same space then the EMD is the minimum amount of work involved in filling the holes with the earth.</p>
        <p>In their analysis of the EMD Rubner et al. argue that a measure based on the notion of a signature is better than one based on a histogram. A signature {s 𝑗 = m 𝑗 , 𝑤 m 𝑗 } is a set of 𝑗 clusters where m 𝑗 is a vector describing the mode of cluster 𝑗 and 𝑤 m 𝑗 is the fraction of pixels falling into that cluster. Thus a signature is a generalisation of the notion of a histogram where boundaries and the number of partitions are not set in advance; instead 𝑗 should be 'appropriate' to the complexity of the image [45].In their analysis of the EMD Rubner et al. argue that a measure based on the notion of a signature is better than one based on a histogram. A signature {s 𝑗 = m 𝑗 , 𝑤 m 𝑗 } is a set of 𝑗 clusters where m 𝑗 is a vector describing the mode of cluster 𝑗 and 𝑤 m 𝑗 is the fraction of pixels falling into that cluster. Thus a signature is a generalisation of the notion of a histogram where boundaries and the number of partitions are not set in advance; instead 𝑗 should be 'appropriate' to the complexity of the image [45].</p>
        <p>The example in Figure 4 illustrates this idea. We can think of the clustering as a quantization of the image in some colour space so that the image is represented by a set of cluster modes and their weights. In the figure the source image is represented in a 2D space as two points of weights 0.6 and 0.4; the target image is represented by three points with weights 0.5, 0.3 and 0.2. In this example the EMD is calculated to be the sum of the amounts moved (0.2, 0.2, 0.1 and 0.5) multiplied by the distances they are moved. Calculating the EMD involves discovering an assignment that minimizes this amount.The example in Figure 4 illustrates this idea. We can think of the clustering as a quantization of the image in some colour space so that the image is represented by a set of cluster modes and their weights. In the figure the source image is represented in a 2D space as two points of weights 0.6 and 0.4; the target image is represented by three points with weights 0.5, 0.3 and 0.2. In this example the EMD is calculated to be the sum of the amounts moved (0.2, 0.2, 0.1 and 0.5) multiplied by the distances they are moved. Calculating the EMD involves discovering an assignment that minimizes this amount.</p>
        <p>For two images described by signatures 𝑆 = {m 𝑗 , 𝑤 m 𝑗 } 𝑛 𝑗=1 and 𝑄 = {p 𝑘 , 𝑤 p 𝑘 } 𝑟 𝑘=1 we are interested in the work required to transfer from one to the other for a given flow pattern F:For two images described by signatures 𝑆 = {m 𝑗 , 𝑤 m 𝑗 } 𝑛 𝑗=1 and 𝑄 = {p 𝑘 , 𝑤 p 𝑘 } 𝑟 𝑘=1 we are interested in the work required to transfer from one to the other for a given flow pattern F:</p>
        <p>Fig. 4. An example of the EMD between two 2D signatures with two points (clusters) in one signature and three in the other (based on example in [44]).Fig. 4. An example of the EMD between two 2D signatures with two points (clusters) in one signature and three in the other (based on example in [44]).</p>
        <p>where 𝑑 𝑗𝑘 is the distance between clusters m 𝑗 and p 𝑘 and 𝑓 𝑗𝑘 is the flow between m 𝑗 and p 𝑘 that minimises overallwhere 𝑑 𝑗𝑘 is the distance between clusters m 𝑗 and p 𝑘 and 𝑓 𝑗𝑘 is the flow between m 𝑗 and p 𝑘 that minimises overall</p>
        <p>cost. An example of this in a 2D colour space is shown in Figure 4. Once the transportation problem of identifying the flow that minimises effort is solved (using dynamic programming) the EMD is defined to be:cost. An example of this in a 2D colour space is shown in Figure 4. Once the transportation problem of identifying the flow that minimises effort is solved (using dynamic programming) the EMD is defined to be:</p>
        <p>Efficient algorithms for the EMD are described in [45] however this measure is expensive to compute with cost increasing more than linearly with the number of clusters. Nevertheless, it is an effective measure for capturing similarity between images.Efficient algorithms for the EMD are described in [45] however this measure is expensive to compute with cost increasing more than linearly with the number of clusters. Nevertheless, it is an effective measure for capturing similarity between images.</p>
        <p>In recent years the idea of basing a similarity metric on compression has received a lot of attention. [28,34]. Indeed Li et al. [34], refer to this as The similarity metric. The basic idea is quite straight-forward; if two documents are very similar then the compressed size of the two documents concatenated together will not be much greater than the compressed size of a single document. This will not be true for two documents that are very different. Slightly more formally, the difference between two documents 𝐴 and 𝐵 is related to the compressed size of document 𝐵 when compressed using the codebook produced when compressing document 𝐴.In recent years the idea of basing a similarity metric on compression has received a lot of attention. [28,34]. Indeed Li et al. [34], refer to this as The similarity metric. The basic idea is quite straight-forward; if two documents are very similar then the compressed size of the two documents concatenated together will not be much greater than the compressed size of a single document. This will not be true for two documents that are very different. Slightly more formally, the difference between two documents 𝐴 and 𝐵 is related to the compressed size of document 𝐵 when compressed using the codebook produced when compressing document 𝐴.</p>
        <p>The theoretical basis of this metric is in the field of Kolmogorov complexity, specifically in conditional Kolmogorov complexity.The theoretical basis of this metric is in the field of Kolmogorov complexity, specifically in conditional Kolmogorov complexity.</p>
        <p>where 𝐾𝑣 (𝑥 |𝑦) is the length of the shortest program that computes 𝑥 when 𝑦 is given as an auxiliary input to the program and 𝐾𝑣 (𝑥𝑦) is the length of the shortest program that outputs 𝑦 concatenated to 𝑥. While this is an abstract idea it can be approximated using compression:where 𝐾𝑣 (𝑥 |𝑦) is the length of the shortest program that computes 𝑥 when 𝑦 is given as an auxiliary input to the program and 𝐾𝑣 (𝑥𝑦) is the length of the shortest program that outputs 𝑦 concatenated to 𝑥. While this is an abstract idea it can be approximated using compression:</p>
        <p>𝐶 (𝑥) is the size of data 𝑥 after compression, and 𝐶 (𝑥 |𝑦) is the size of 𝑥 after compressing it with the compression model built for 𝑦. If we assume that 𝐾𝑣 (𝑥 |𝑦) = 𝐾𝑣 (𝑥𝑦) -𝐾𝑣 (𝑦) then we can define a normalised compression distance:𝐶 (𝑥) is the size of data 𝑥 after compression, and 𝐶 (𝑥 |𝑦) is the size of 𝑥 after compressing it with the compression model built for 𝑦. If we assume that 𝐾𝑣 (𝑥 |𝑦) = 𝐾𝑣 (𝑥𝑦) -𝐾𝑣 (𝑦) then we can define a normalised compression distance:</p>
        <p>It is important that 𝐶 (.) should be an appropriate compression metric for the data. Delany and Bridge [21] show that compression using Lempel-Ziv (GZip) is effective for text. They show that this compression based metric is more accurate in 𝑘-NN classification than distance based metrics on a bag-of-words representation of the text.It is important that 𝐶 (.) should be an appropriate compression metric for the data. Delany and Bridge [21] show that compression using Lempel-Ziv (GZip) is effective for text. They show that this compression based metric is more accurate in 𝑘-NN classification than distance based metrics on a bag-of-words representation of the text.</p>
        <p>Time-series data can be as diverse as human activity measured by wearable sensors [37] or measurements coming from a manufacturing process. There is a long history of machine learning research on time-series analysis and 1-NN is the base-line metric for time-series classification [5]. However, the special characteristics of time-series do present challenges for 𝑘-NN. Consider a query time-series q and a target x:Time-series data can be as diverse as human activity measured by wearable sensors [37] or measurements coming from a manufacturing process. There is a long history of machine learning research on time-series analysis and 1-NN is the base-line metric for time-series classification [5]. However, the special characteristics of time-series do present challenges for 𝑘-NN. Consider a query time-series q and a target x:</p>
        <p>x = 𝑥 1 , 𝑥 2 , ..., 𝑥 𝑗 , ..., 𝑥 𝑛x = 𝑥 1 , 𝑥 2 , ..., 𝑥 𝑗 , ..., 𝑥 𝑛</p>
        <p>While both time-series are vectors, the Euclidean distance between these two vectors may be quite large even if they have the same general shape (see Figure 5). Furthermore the two time-series might be of different lengths. To complicate things further, similarity might depend on specific features (motifs) in the time-series rather than similarity across the time-series as a whole.While both time-series are vectors, the Euclidean distance between these two vectors may be quite large even if they have the same general shape (see Figure 5). Furthermore the two time-series might be of different lengths. To complicate things further, similarity might depend on specific features (motifs) in the time-series rather than similarity across the time-series as a whole.</p>
        <p>A number of methods for scoring similarity between time-series have been developed that allow 𝑘-NN to work with time-series data. Three popular methods are:A number of methods for scoring similarity between time-series have been developed that allow 𝑘-NN to work with time-series data. Three popular methods are:</p>
        <p>• Dynamic Time Warping (DTW): Because two time series may be fundamentally similar but offset or slightly distorted, DTW allows the time axis to be warped to identify underlying similarities [29].• Dynamic Time Warping (DTW): Because two time series may be fundamentally similar but offset or slightly distorted, DTW allows the time axis to be warped to identify underlying similarities [29].</p>
        <p>The idea with SAX is to discretize the time series so that it can be represented as a sequence of symbols [35]. Then methods for scoring sequence similarity can be applied.The idea with SAX is to discretize the time series so that it can be represented as a sequence of symbols [35]. Then methods for scoring sequence similarity can be applied.</p>
        <p>• Symbolic Fourier Approximation (SFA): SFA is like SAX except the sequence representation is produced from a discrete Fourier transform representation of the signal rather than a discretization of the signal itself. So SFA is a frequency domain rather than a time domain representation of the signal [47].• Symbolic Fourier Approximation (SFA): SFA is like SAX except the sequence representation is produced from a discrete Fourier transform representation of the signal rather than a discretization of the signal itself. So SFA is a frequency domain rather than a time domain representation of the signal [47].</p>
        <p>By far the most popular of these is DTW so we will provide some detail here on how DTW works. As the name suggests, the idea is to allow the time-series to be stretched (warped) to find the best mapping. The DTW distance is defined as follows:By far the most popular of these is DTW so we will provide some detail here on how DTW works. As the name suggests, the idea is to allow the time-series to be stretched (warped) to find the best mapping. The DTW distance is defined as follows:</p>
        <p>where 𝜋 = [𝜋 1 , ..., 𝜋 𝑙 , ..., 𝜋 𝐿 ] is the optimum path (mapping) having the following properties:where 𝜋 = [𝜋 1 , ..., 𝜋 𝑙 , ..., 𝜋 𝐿 ] is the optimum path (mapping) having the following properties:</p>
        <p>The DTW path for the two time-series in Figure 5 is shown on the right. It starts at the top left (1,1) and finishes at the bottom right (𝑚, 𝑛). Each point (𝑖, 𝑗) on the path indicates the mapping between 𝑞 𝑖 and 𝑥 𝑗 . The extent of the deviation from the main diagonal reflects the warping. In practice, the path may be restricted to a band around the main diagonal to restrict warping. The computational complexity of DTW is 𝑂 (𝑛, 𝑚) because it entails a search through the matrix Manuscript submitted to ACM Fig. 5. The image on the left illustrates the main challenge in quantifying similarity between time-series. The two series are similar but the Euclidean distance between them is large. The image on the right shows the DTW mapping between the two time-series (produced using 
            <rs type="software">tslearn</rs> [51]).
        </p>
        <p>shown on the right is Figure 5. This is effectively 𝑂 (𝑛 2 ) in the length of the time-series -so DTW is computationally expensive.shown on the right is Figure 5. This is effectively 𝑂 (𝑛 2 ) in the length of the time-series -so DTW is computationally expensive.</p>
        <p>Finally, DTW is not a proper metric because it fails two of the criteria laid out at the beginning of this section.Finally, DTW is not a proper metric because it fails two of the criteria laid out at the beginning of this section.</p>
        <p>𝐷𝑇𝑊 (q, x) = 0 ⇏ x = x and the triangle inequality may not hold. This means that speed-up mechanisms such as Ball Trees (section 3.2) that work for proper similarity metrics cannot be applied. Neither can mechanisms that work for vector space representations, i.e. Kd-Trees (section 3.1) and Random Projection Trees (section 3.3.2).𝐷𝑇𝑊 (q, x) = 0 ⇏ x = x and the triangle inequality may not hold. This means that speed-up mechanisms such as Ball Trees (section 3.2) that work for proper similarity metrics cannot be applied. Neither can mechanisms that work for vector space representations, i.e. Kd-Trees (section 3.1) and Random Projection Trees (section 3.3.2).</p>
        <p>So far we have considered scenarios where the system designer selects a metric that is considered appropriate based on their insight into the data. It is also possible to induce a metric from the data. This Metric Learning has been the subject of extensive research [53,54]. In this section we briefly outline two general strategies:So far we have considered scenarios where the system designer selects a metric that is considered appropriate based on their insight into the data. It is also possible to induce a metric from the data. This Metric Learning has been the subject of extensive research [53,54]. In this section we briefly outline two general strategies:</p>
        <p>• Linear Discriminant Analysis (LDA) is a linear projection method similar to PCA (see section 4.1). However LDA is supervised in the sense that the objective is to discover a projection that does a good job of separating the classes. Thus LDA can be used as the basis of a learned metric for 𝑘-NN. • The Mahalanobis Distance is defined to be:• Linear Discriminant Analysis (LDA) is a linear projection method similar to PCA (see section 4.1). However LDA is supervised in the sense that the objective is to discover a projection that does a good job of separating the classes. Thus LDA can be used as the basis of a learned metric for 𝑘-NN. • The Mahalanobis Distance is defined to be:</p>
        <p>where S is the covariance matrix of the data. If we replace S with any Positive Semi-Definite Matrix M we have a very general metric that can be learned from the data -this is the Generalized Mahalanobis distance [53]:where S is the covariance matrix of the data. If we replace S with any Positive Semi-Definite Matrix M we have a very general metric that can be learned from the data -this is the Generalized Mahalanobis distance [53]:</p>
        <p>Of these two approaches, strategies based on Generalized Mahalanobis distance have received the most attention [53]. If the data is described by 𝑚 features then M is an 𝑚 × 𝑚 matrix so the learned metric has considerable expressive Manuscript submitted to ACM power. Appendix I provides a link to Python 
            <rs type="software">code</rs> presenting an example of metric learning based on the large margin nearest neighbor method [54]. Code for metric learning is not included in 
            <rs type="software">scikit-learn</rs> but a dedicated metric learning extension for 
            <rs type="software">scikit-learn</rs> is available [19].
        </p>
        <p>Computationally expensive metrics such as the Earth-Mover's Distance and compression based (dis)similarity metrics focus attention on the computational issues associated with k-NN classifiers. Basic 𝑘-NN classifiers that use a simple Minkowski distance will have a time behaviour that is 𝑂 (𝑑𝑛) where 𝑛 is the number of data samples and 𝑑 is the number of features that describe the data, i.e. the distance metric is linear in the number of features and the comparison process increases linearly with the amount of data. The computational complexity of the EMD and compression metrics is more difficult to characterise but a 𝑘-NN classifier that incorporates an EMD metric is likely to be 𝑂 (𝑛𝑐 3 log𝑐) where 𝑐 is the number of clusters [45].Computationally expensive metrics such as the Earth-Mover's Distance and compression based (dis)similarity metrics focus attention on the computational issues associated with k-NN classifiers. Basic 𝑘-NN classifiers that use a simple Minkowski distance will have a time behaviour that is 𝑂 (𝑑𝑛) where 𝑛 is the number of data samples and 𝑑 is the number of features that describe the data, i.e. the distance metric is linear in the number of features and the comparison process increases linearly with the amount of data. The computational complexity of the EMD and compression metrics is more difficult to characterise but a 𝑘-NN classifier that incorporates an EMD metric is likely to be 𝑂 (𝑛𝑐 3 log𝑐) where 𝑐 is the number of clusters [45].</p>
        <p>For these reasons there has been considerable research on editing down the training data and on reducing the number of features used to describe the data (see section 4). There has also been considerable research on alternatives to the exhaustive search strategy (brute force) that is used in the standard 𝑘-NN algorithm. In the remainder of this section we review Kd-Trees and Cover Trees, the two speedup strategies included in 
            <rs type="software">Scikit-learn</rs>. We also review some approximate 𝑘-NN algorithms that don't guarantee to retrieve nearest neighbours but offer dramatic speedup with little loss of accuracy. In the final sub-section a simple comparison of Kd-Trees and Cover Trees against brute force search is presented.
        </p>
        <p>Kd-Trees represent the longest established strategy for speedup in 𝑘-NN [6]. It is best to think of Kd-Trees as a general strategy rather than a single algorithm. The general idea is that a binary tree is used to successively partition the dataset with training samples sorted to the leaves of the tree. This offers the potential for retrieval time that is 𝑂 (𝑑 log(𝑛))Kd-Trees represent the longest established strategy for speedup in 𝑘-NN [6]. It is best to think of Kd-Trees as a general strategy rather than a single algorithm. The general idea is that a binary tree is used to successively partition the dataset with training samples sorted to the leaves of the tree. This offers the potential for retrieval time that is 𝑂 (𝑑 log(𝑛))</p>
        <p>rather than 𝑂 (𝑑𝑛).rather than 𝑂 (𝑑𝑛).</p>
        <p>A sample Kd-Tree is shown in Figure 6. The data is described by two features so it can be represented as a 2DA sample Kd-Tree is shown in Figure 6. The data is described by two features so it can be represented as a 2D</p>
        <p>plot. The plot on the left corresponds to the binary tree on the right. The Kd-Tree always partitions the data along hyperplanes (lines in the 2D case) that are perpendicular to the axes.plot. The plot on the left corresponds to the binary tree on the right. The Kd-Tree always partitions the data along hyperplanes (lines in the 2D case) that are perpendicular to the axes.</p>
        <p>The figure shows a query point 𝑄 (2, 5). The search for nearest neighbours for 𝑄 will locate it to the appropriate node in the tree 𝐺 (2, 6). It can be seen in the plot that the nearest neighbour for 𝑄 is not guaranteed to be located in the hypercube represented by 𝐺. However the distance between 𝐺 and 𝑄 gives us an upper bound on the distance to the nearest neighbour. It is clear that the grey box (hypercube) needs also to be considered; the rest of the tree can be bounded out from consideration. It is this potential to bound out large parts of the data that yields the 𝑂 (𝑑 log(𝑛))The figure shows a query point 𝑄 (2, 5). The search for nearest neighbours for 𝑄 will locate it to the appropriate node in the tree 𝐺 (2, 6). It can be seen in the plot that the nearest neighbour for 𝑄 is not guaranteed to be located in the hypercube represented by 𝐺. However the distance between 𝐺 and 𝑄 gives us an upper bound on the distance to the nearest neighbour. It is clear that the grey box (hypercube) needs also to be considered; the rest of the tree can be bounded out from consideration. It is this potential to bound out large parts of the data that yields the 𝑂 (𝑑 log(𝑛))</p>
        <p>performance.performance.</p>
        <p>Some other aspects of Kd-Trees that need to be considered are as follows:Some other aspects of Kd-Trees that need to be considered are as follows:</p>
        <p>• Constructing Kd-Trees entails a straightforward binary partitioning of the data and sorting the data to leaf nodes so the construction process is comparatively quick. The partition is typically at the median value for the selected feature. • At each step in the building of the tree a decision has to be made on feature selection. The policy could be to cycle through the features in order or to select the feature in which the variance (spread) in the data is highest.• Constructing Kd-Trees entails a straightforward binary partitioning of the data and sorting the data to leaf nodes so the construction process is comparatively quick. The partition is typically at the median value for the selected feature. • At each step in the building of the tree a decision has to be made on feature selection. The policy could be to cycle through the features in order or to select the feature in which the variance (spread) in the data is highest.</p>
        <p>Manuscript submitted to ACM Fig. 6. A Kd-Tree based on the example in the original paper by Bentley [7]). The partitioning of the 2D feature space shown on the left corresponds to the tree on the right.Manuscript submitted to ACM Fig. 6. A Kd-Tree based on the example in the original paper by Bentley [7]). The partitioning of the 2D feature space shown on the left corresponds to the tree on the right.</p>
        <p>• The query time will increase with the number of neighbours required (𝑘). For very large 𝑘 query time will exceed that for brute force search.• The query time will increase with the number of neighbours required (𝑘). For very large 𝑘 query time will exceed that for brute force search.</p>
        <p>• The 𝑂 (𝑑 log(𝑛)) retrieval time depends on a balanced tree. If the tree is not well balanced some retrieval times will be poor [6].• The 𝑂 (𝑑 log(𝑛)) retrieval time depends on a balanced tree. If the tree is not well balanced some retrieval times will be poor [6].</p>
        <p>• The main drawback with Kd-Trees is that the curse of dimensionality still applies. The benefits of using a binary tree as an indexing structure cease to apply when 𝑑 is large (say &gt; 20). Kleinberg [30] points out that when 𝑑 &gt; 𝑙𝑜𝑔(𝑛), 𝑂 (𝑑 log(𝑛)) is no better than 𝑂 (𝑑𝑛).• The main drawback with Kd-Trees is that the curse of dimensionality still applies. The benefits of using a binary tree as an indexing structure cease to apply when 𝑑 is large (say &gt; 20). Kleinberg [30] points out that when 𝑑 &gt; 𝑙𝑜𝑔(𝑛), 𝑂 (𝑑 log(𝑛)) is no better than 𝑂 (𝑑𝑛).</p>
        <p>Figure 6 shows that a Kd-Tree indexes the data by partitioning the feature space. By contrast a Ball Tree is a 'metric tree' in the sense that it is based on a metric defined on pairs of samples [23,59]. The construction of the ball tree is akin to a hierarchical clustering problem that can be tackled top-down or bottom-up:Figure 6 shows that a Kd-Tree indexes the data by partitioning the feature space. By contrast a Ball Tree is a 'metric tree' in the sense that it is based on a metric defined on pairs of samples [23,59]. The construction of the ball tree is akin to a hierarchical clustering problem that can be tackled top-down or bottom-up:</p>
        <p>• Bottom-up: Initially each data point is a point sized bounding ball. At each step, select the closest pair of balls, the pair that have the smallest bounding ball that covers them. Join these balls. Continue until the top-level bounding ball is reached.• Bottom-up: Initially each data point is a point sized bounding ball. At each step, select the closest pair of balls, the pair that have the smallest bounding ball that covers them. Join these balls. Continue until the top-level bounding ball is reached.</p>
        <p>• Top-down: At each step, two data points are chosen that have the maximum distance between them. The remaining points are partitioned by allocating to the closer or these. This process is repeated recursively until a stopping criterion is met, e.g. number of samples at a leaf node.• Top-down: At each step, two data points are chosen that have the maximum distance between them. The remaining points are partitioned by allocating to the closer or these. This process is repeated recursively until a stopping criterion is met, e.g. number of samples at a leaf node.</p>
        <p>In contrast to Kd-Trees, the construction of a Ball tree depends on a metric defined on the data rather than a feature space representation. However, it should be noted that the distance measure must be a metric so a Ball Tree cannot be applied for measures such as Earth Mover Distance or Dynamic Time Warping. Compared with Kd-Trees, Ball Trees have the potential to perform better for high-dimension data, for example in image analysis [33].In contrast to Kd-Trees, the construction of a Ball tree depends on a metric defined on the data rather than a feature space representation. However, it should be noted that the distance measure must be a metric so a Ball Tree cannot be applied for measures such as Earth Mover Distance or Dynamic Time Warping. Compared with Kd-Trees, Ball Trees have the potential to perform better for high-dimension data, for example in image analysis [33].</p>
        <p>Brute force search for 𝑘-NN is 𝑂 (𝑑𝑛). As we have seen in the preceeding sections we can get over the linear dependence on 𝑛 but high dimension data is still a problem. Fortunately, for many applications, it is not essential to retrieve the absolute nearest neighbours. For instance, in recommender systems, the most similar item is not necessarily required -indeed items that are reasonably close may offer some serendipitous discovery. In 𝑘-NN classification, neighbours that are close (but not necessarily closest) are probably of the correct class. So in this section we review the two most popular strategies for Approximate 𝑘-NN, these are Locality Sensitive Hashing and Random Projection Trees [36].Brute force search for 𝑘-NN is 𝑂 (𝑑𝑛). As we have seen in the preceeding sections we can get over the linear dependence on 𝑛 but high dimension data is still a problem. Fortunately, for many applications, it is not essential to retrieve the absolute nearest neighbours. For instance, in recommender systems, the most similar item is not necessarily required -indeed items that are reasonably close may offer some serendipitous discovery. In 𝑘-NN classification, neighbours that are close (but not necessarily closest) are probably of the correct class. So in this section we review the two most popular strategies for Approximate 𝑘-NN, these are Locality Sensitive Hashing and Random Projection Trees [36].</p>
        <p>Hashing. With Locality Sensitive Hashing (LSH) the objective is to map similar items into the same 'buckets' with high probability. This contrasts with conventional hashing where the objective of minimising hashing collisions means that similar items will have very different hashes. Given that LSH maps similar items to the same buckets it can be used to implement approximate nearest neighbour search. The strategy is to use a number of variants of LSH algorithms to retrieve a candidate set of nearest neighbours. This candidate set is the union of the items in the buckets returned by the LSH algorithms. Then the similarity metric can be applied to these candidates to find nearest neighbours that will be near-optimal [27,36].Hashing. With Locality Sensitive Hashing (LSH) the objective is to map similar items into the same 'buckets' with high probability. This contrasts with conventional hashing where the objective of minimising hashing collisions means that similar items will have very different hashes. Given that LSH maps similar items to the same buckets it can be used to implement approximate nearest neighbour search. The strategy is to use a number of variants of LSH algorithms to retrieve a candidate set of nearest neighbours. This candidate set is the union of the items in the buckets returned by the LSH algorithms. Then the similarity metric can be applied to these candidates to find nearest neighbours that will be near-optimal [27,36].</p>
        <p>Trees. In section 3.1 we saw that exact nearest neighbour search is a two stage process. First the query is located to the correct leaf node in the tree and candidate nearest neighbours are identified. Then there is a backtracking process that finds better candidates or bounds out sections of the tree from consideration. The retrieval of nearest neighbours is guaranteed without explicitly measuring against all data points. Random Projection Trees depends on two extensions to this basic kd-tree idea:Trees. In section 3.1 we saw that exact nearest neighbour search is a two stage process. First the query is located to the correct leaf node in the tree and candidate nearest neighbours are identified. Then there is a backtracking process that finds better candidates or bounds out sections of the tree from consideration. The retrieval of nearest neighbours is guaranteed without explicitly measuring against all data points. Random Projection Trees depends on two extensions to this basic kd-tree idea:</p>
        <p>• Defeatist Search: The query item is located to the correct leaf node but the backtracking process to ensure optimality is dropped or at least greatly curtailed [36]. The search gives up early which might be considered a bit defeatist.• Defeatist Search: The query item is located to the correct leaf node but the backtracking process to ensure optimality is dropped or at least greatly curtailed [36]. The search gives up early which might be considered a bit defeatist.</p>
        <p>• Multiple Trees: If the search returns without backtracking the prospect of finding good neighbours can be improved by repeating with multiple trees. Different variants of the tree can be produced by including a random element in the Kd-Tree generation process [49]. Since there is a risk that there will not be great variety in the Kd-Tree variants, it is common to produce different trees by randomly projecting the data into a different space (i.e. perform a simple linear transformation on the data) [30].• Multiple Trees: If the search returns without backtracking the prospect of finding good neighbours can be improved by repeating with multiple trees. Different variants of the tree can be produced by including a random element in the Kd-Tree generation process [49]. Since there is a risk that there will not be great variety in the Kd-Tree variants, it is common to produce different trees by randomly projecting the data into a different space (i.e. perform a simple linear transformation on the data) [30].</p>
        <p>In the next section we provide a demonstration of the effectiveness of Approximate 𝑘-NN on a number of datasets.In the next section we provide a demonstration of the effectiveness of Approximate 𝑘-NN on a number of datasets.</p>
        <p>The results show significant speed-up with little or no loss of accuracy. The caveat is that it only works for feature vector data.The results show significant speed-up with little or no loss of accuracy. The caveat is that it only works for feature vector data.</p>
        <p>The objective in this section is to show the potential speed-up that is possible with these methods, it is not meant as a comprehensive evaluation. In our first evaluation we assess the three options available in the 𝑘-NN implementation in 
            <rs type="software">scikit-learn</rs> (
            <rs type="url">scikit-learn.org</rs>). These are brute-force search, Kd-Trees and Ball Trees. The evaluation covers the four datasets summarised in Table 1. Two of these datasets are low dimension (&lt; 10). The Credit dataset would be considered high-dimension with 23 features.
        </p>
        <p>We present two sets of results (see Figure 7), one using 2-fold cross validation and one using 10-fold. The objective is to show the impact of the tree building phase; while both cross validations use all the data for testing, the 10-fold cross validation incurs the tree building overhead 10 times instead of twice.We present two sets of results (see Figure 7), one using 2-fold cross validation and one using 10-fold. The objective is to show the impact of the tree building phase; while both cross validations use all the data for testing, the 10-fold cross validation incurs the tree building overhead 10 times instead of twice.</p>
        <p>The bar charts show the processing time divided by the time for brute force search. It is clear that significant speed-up is possible for the low-dimension datasets. The Kd-Tree results are slightly better than Ball Tree in all cases.The bar charts show the processing time divided by the time for brute force search. It is clear that significant speed-up is possible for the low-dimension datasets. The Kd-Tree results are slightly better than Ball Tree in all cases.</p>
        <p>Manuscript submitted to ACM However, the performance for the Credit dataset is worse than brute-force search. This is to be expected given that it is high-dimension. This poor performance on high-dimension data shows that the curse of dimensionality cannot be avoided in exact nearest neighbour search. Figure 8 shows the speed-up that can be achieved with Approximate Nearest Neighbour. The method we evaluate is called Annoy (github.com/spotify/annoy) and uses Random Projection Trees [9]. Annoy stands for 'Approximate Nearest Neighbor Oh Yeah' but the name probably stems from the fact that the method is 'annoyingly' effective. The results in Figure 8 show dramatic speed-up with almost no loss of accuracy except for the Letter dataset.Manuscript submitted to ACM However, the performance for the Credit dataset is worse than brute-force search. This is to be expected given that it is high-dimension. This poor performance on high-dimension data shows that the curse of dimensionality cannot be avoided in exact nearest neighbour search. Figure 8 shows the speed-up that can be achieved with Approximate Nearest Neighbour. The method we evaluate is called Annoy (github.com/spotify/annoy) and uses Random Projection Trees [9]. Annoy stands for 'Approximate Nearest Neighbor Oh Yeah' but the name probably stems from the fact that the method is 'annoyingly' effective. The results in Figure 8 show dramatic speed-up with almost no loss of accuracy except for the Letter dataset.</p>
        <p>When more trees are added the accuracy reaches that achievable with exact 𝑘-NN with a four-fold improvement in processing time.When more trees are added the accuracy reaches that achievable with exact 𝑘-NN with a four-fold improvement in processing time.</p>
        <p>Given the high dimension nature of the data, Dimension Reduction is a core research topic in Machine Learning.Given the high dimension nature of the data, Dimension Reduction is a core research topic in Machine Learning.</p>
        <p>to this is to transform the data into a representation with less features). It is important to emphasise that dimension reduction through feature selection can be achieved without loss of information because the intrinsic dimension of the data may be considerably less than the number of features. This notion of intrinsic dimension is discussed in section 4.1. Dimension reduction as achieved by supervised feature selection is described in section 4.2. Unsupervised feature transformation using Principle Component Analysis (PCA) [15] can be used as a preprocessing step for 𝑘-NN [4]. PCA is discussed in the next section in the context of intrinsic dimension. However there is no evidence that PCA can be combined with 𝑘-NN without sacrificing accuracy so PCA will not be covered in this paper. The other aspect of dimension reduction is the deletion of redundant or noisy instances in the training data -this is reviewed in section 4.3.to this is to transform the data into a representation with less features). It is important to emphasise that dimension reduction through feature selection can be achieved without loss of information because the intrinsic dimension of the data may be considerably less than the number of features. This notion of intrinsic dimension is discussed in section 4.1. Dimension reduction as achieved by supervised feature selection is described in section 4.2. Unsupervised feature transformation using Principle Component Analysis (PCA) [15] can be used as a preprocessing step for 𝑘-NN [4]. PCA is discussed in the next section in the context of intrinsic dimension. However there is no evidence that PCA can be combined with 𝑘-NN without sacrificing accuracy so PCA will not be covered in this paper. The other aspect of dimension reduction is the deletion of redundant or noisy instances in the training data -this is reviewed in section 4.3.</p>
        <p>Colloquially we can think of the intrinsic dimension as the minimum number of features required to provide a 'good' representation of the data. This notion of a 'good' representation can be considered in terms of Principle Component Analysis (PCA). We can represent a dataset 𝐷 as a rectangular matrix D of dimension 𝑛 × 𝑝, that is 𝑛 samples described by 𝑝 features. If we perform PCA on D we get:Colloquially we can think of the intrinsic dimension as the minimum number of features required to provide a 'good' representation of the data. This notion of a 'good' representation can be considered in terms of Principle Component Analysis (PCA). We can represent a dataset 𝐷 as a rectangular matrix D of dimension 𝑛 × 𝑝, that is 𝑛 samples described by 𝑝 features. If we perform PCA on D we get:</p>
        <p>The PCA provides a linear mapping of the data into a lower dimension representation T. The PCA also provides a ranking of the principle components (PCs) in terms of the variance in the data that they capture. We can select the top 𝑠 PCs that together capture (1 -𝜖) fraction of the variance in the data. This 𝑠 is an approximation of the intrinsic dimension of the data.The PCA provides a linear mapping of the data into a lower dimension representation T. The PCA also provides a ranking of the principle components (PCs) in terms of the variance in the data that they capture. We can select the top 𝑠 PCs that together capture (1 -𝜖) fraction of the variance in the data. This 𝑠 is an approximation of the intrinsic dimension of the data.</p>
        <p>The variance captured by the first four PCs for the HTRU and Shuttle datasets is shown in Figure 9. The four PCs capture almost all of the variance for the HTRU data but less than 80% for Shuttle. We can think of four as a reasonable assessment of the intrinsic dimension of the HTRU data, whereas the intrinsic dimension for Shuttle is greater than four.The variance captured by the first four PCs for the HTRU and Shuttle datasets is shown in Figure 9. The four PCs capture almost all of the variance for the HTRU data but less than 80% for Shuttle. We can think of four as a reasonable assessment of the intrinsic dimension of the HTRU data, whereas the intrinsic dimension for Shuttle is greater than four.</p>
        <p>This PCA inspired notion of intrinsic dimension is a global approximation and there may be parts of the space where the intrinsic dimension is locally less than 𝑠. Imagine a neighbourhood of radius 𝑟 around a point q (e.g. among the 𝑘 nearest neighbours), (1 -𝜖) fraction of the variance will be covered by 𝑠 ′ features, where 𝑠 ′ &lt; 𝑝.This PCA inspired notion of intrinsic dimension is a global approximation and there may be parts of the space where the intrinsic dimension is locally less than 𝑠. Imagine a neighbourhood of radius 𝑟 around a point q (e.g. among the 𝑘 nearest neighbours), (1 -𝜖) fraction of the variance will be covered by 𝑠 ′ features, where 𝑠 ′ &lt; 𝑝.</p>
        <p>Manuscript submitted to ACM Dasgupta &amp; Freund [18] provide an insightful example to explain intrinsic dimension. Imagine a motion capture system with 13 markers attached to a person to facilitate processing (see Figure 10). In a 2D image these markers can be represented by 26 𝑥, 𝑦 coordinates. So the dimension of the motion capture data will be 26. However, it is clear fromManuscript submitted to ACM Dasgupta &amp; Freund [18] provide an insightful example to explain intrinsic dimension. Imagine a motion capture system with 13 markers attached to a person to facilitate processing (see Figure 10). In a 2D image these markers can be represented by 26 𝑥, 𝑦 coordinates. So the dimension of the motion capture data will be 26. However, it is clear from</p>
        <p>When the objective is to reduce the number of features used to describe data there are two strategies that can be employed. Techniques such as Principle Components Analysis (PCA) may be employed to transform the data into a lower dimension represention. Alternatively feature selection may be employed to discard some of the features. In using 𝑘-NN with high dimension data there are several reasons why it is useful to perform feature selection:When the objective is to reduce the number of features used to describe data there are two strategies that can be employed. Techniques such as Principle Components Analysis (PCA) may be employed to transform the data into a lower dimension represention. Alternatively feature selection may be employed to discard some of the features. In using 𝑘-NN with high dimension data there are several reasons why it is useful to perform feature selection:</p>
        <p>-For many distance measures, the retrieval time increases directly with the number of features (see section 3). -Noisy or irrelevant features can have the same influence on retrieval as predictive features so they will impact negatively on accuracy.-For many distance measures, the retrieval time increases directly with the number of features (see section 3). -Noisy or irrelevant features can have the same influence on retrieval as predictive features so they will impact negatively on accuracy.</p>
        <p>-Things look more similar on average the more features used to describe them (see Figure 11).-Things look more similar on average the more features used to describe them (see Figure 11).</p>
        <p>Feature Selection techniques typically incorporate a search strategy for exploring the space of feature subsets, including methods for determining a suitable starting point and generating successive candidate subsets, and an evaluation criterion to rate and compare the candidates, which serves to guide the search process. The evaluation schemes can be divided into two broad categories:Feature Selection techniques typically incorporate a search strategy for exploring the space of feature subsets, including methods for determining a suitable starting point and generating successive candidate subsets, and an evaluation criterion to rate and compare the candidates, which serves to guide the search process. The evaluation schemes can be divided into two broad categories:</p>
        <p>-Filter approaches attempt to remove irrelevant features from the feature set prior to the application of the learning algorithm. Initially, the data is analysed to identify those dimensions that are most relevant for describing its structure. The chosen feature subset is subsequently used to train the learning algorithm. Feedback regarding an algorithm's performance is not required during the selection process, though it may be useful when attempting to gauge the effectiveness of the filter.-Filter approaches attempt to remove irrelevant features from the feature set prior to the application of the learning algorithm. Initially, the data is analysed to identify those dimensions that are most relevant for describing its structure. The chosen feature subset is subsequently used to train the learning algorithm. Feedback regarding an algorithm's performance is not required during the selection process, though it may be useful when attempting to gauge the effectiveness of the filter.</p>
        <p>-Wrapper methods for feature selection make use of the learning algorithm itself to choose a set of relevant features. The wrapper conducts a search through the feature space, evaluating candidate feature subsets by estimating the predictive accuracy of the classifier built on that subset. The goal of the search is to find the subset that maximises this criterion.-Wrapper methods for feature selection make use of the learning algorithm itself to choose a set of relevant features. The wrapper conducts a search through the feature space, evaluating candidate feature subsets by estimating the predictive accuracy of the classifier built on that subset. The goal of the search is to find the subset that maximises this criterion.</p>
        <p>It is worth mentioning at this point that some other classification techniques perform implicit feature selection. For instance the process of building a decision tree will very often not select all the features for use in the tree. Features not used in the tree have no role then in classification.It is worth mentioning at this point that some other classification techniques perform implicit feature selection. For instance the process of building a decision tree will very often not select all the features for use in the tree. Features not used in the tree have no role then in classification.</p>
        <p>Filter Techniques Central to the Filter strategy for feature selection is the criterion used to score the predictiveness of the features. In recent years Information Gain (IG) has become perhaps the most popular criterion for feature selection.Filter Techniques Central to the Filter strategy for feature selection is the criterion used to score the predictiveness of the features. In recent years Information Gain (IG) has become perhaps the most popular criterion for feature selection.</p>
        <p>The Information Gain of a feature is a measure of the amount of information that a feature brings to the training set [41]. It is defined as the expected reduction in entropy caused by partitioning the training set 𝐷 using the feature 𝑓 as shown in Equation 24where 𝐷 𝑣 is that subset of the training set 𝐷 where feature 𝑓 has value 𝑣. Entropy is a measure of how much randomness or impurity there is in the data set. It is defined in Equation 14where 𝑐 equals the number of classes in the training set and pi is the proportion of class i in the data -entropy is highest when the proportions are equal.The Information Gain of a feature is a measure of the amount of information that a feature brings to the training set [41]. It is defined as the expected reduction in entropy caused by partitioning the training set 𝐷 using the feature 𝑓 as shown in Equation 24where 𝐷 𝑣 is that subset of the training set 𝐷 where feature 𝑓 has value 𝑣. Entropy is a measure of how much randomness or impurity there is in the data set. It is defined in Equation 14where 𝑐 equals the number of classes in the training set and pi is the proportion of class i in the data -entropy is highest when the proportions are equal.</p>
        <p>In binary classification, the 𝐸𝑛𝑡𝑟𝑜𝑝𝑦 (𝐷)can be simplified to 𝐸𝑛𝑡𝑟𝑜𝑝𝑦 (𝐷) = -𝑝 + log 2 𝑝 + -𝑝 -log 2 𝑝 -where 𝑝 + represents the class and 𝑝 -the non-class. For comparison purposes we will also consider Odds Ratio (OR) [39] which is an alternative filtering criterion. For binary classification OR calculates the ratio of the odds of a feature occurring in the class to the odds of the feature occurring in the non-class.In binary classification, the 𝐸𝑛𝑡𝑟𝑜𝑝𝑦 (𝐷)can be simplified to 𝐸𝑛𝑡𝑟𝑜𝑝𝑦 (𝐷) = -𝑝 + log 2 𝑝 + -𝑝 -log 2 𝑝 -where 𝑝 + represents the class and 𝑝 -the non-class. For comparison purposes we will also consider Odds Ratio (OR) [39] which is an alternative filtering criterion. For binary classification OR calculates the ratio of the odds of a feature occurring in the class to the odds of the feature occurring in the non-class.</p>
        <p>Where a specific feature does not occur in a class, it can be assigned a small fixed value so that the OR can still be calculated. For feature selection, the features can be ranked according to their OR with high values indicating features that are very predictive of the class. The same can be done for the non-class to highlight features that are predictive of the non-class.Where a specific feature does not occur in a class, it can be assigned a small fixed value so that the OR can still be calculated. For feature selection, the features can be ranked according to their OR with high values indicating features that are very predictive of the class. The same can be done for the non-class to highlight features that are predictive of the non-class.</p>
        <p>We can look at the impact of these feature selection criteria in an email spam classification task. In this experiment we selected the 𝑛 2 𝑛 features with the highest IG value and 𝑛 features each from 𝑂𝑅(𝑓 , 𝑠𝑝𝑎𝑚) and 𝑂𝑅(𝑓 , 𝑛𝑜𝑛𝑠𝑝𝑎𝑚) sets. The results, displayed in Figure 12, show that IG performed significantly better than OR. The reason for this is that OR is inclined to select features that occur rarely but are very strong indicators of the class. This means that some objects (emails) are described by no features and thus have no similarity to any cases in the case base. In this experiment this occurs in 8.8% of cases with OR compared with 0.2% for the IG technique. This shows a simple but effective strategy for feature selection in very high dimension data. IG can be used to rank features, then a cross validation process can Manuscript submitted to ACM Fig. 13. The Feature Subspace.We can look at the impact of these feature selection criteria in an email spam classification task. In this experiment we selected the 𝑛 2 𝑛 features with the highest IG value and 𝑛 features each from 𝑂𝑅(𝑓 , 𝑠𝑝𝑎𝑚) and 𝑂𝑅(𝑓 , 𝑛𝑜𝑛𝑠𝑝𝑎𝑚) sets. The results, displayed in Figure 12, show that IG performed significantly better than OR. The reason for this is that OR is inclined to select features that occur rarely but are very strong indicators of the class. This means that some objects (emails) are described by no features and thus have no similarity to any cases in the case base. In this experiment this occurs in 8.8% of cases with OR compared with 0.2% for the IG technique. This shows a simple but effective strategy for feature selection in very high dimension data. IG can be used to rank features, then a cross validation process can Manuscript submitted to ACM Fig. 13. The Feature Subspace.</p>
        <p>be employed to identify the number of features above which classification accuracy is not improved. This evaluation suggests that the top 350 features as ranked by IG are adequate.be employed to identify the number of features above which classification accuracy is not improved. This evaluation suggests that the top 350 features as ranked by IG are adequate.</p>
        <p>While this is an effective strategy for feature selection it has the drawback that features are considered in isolation so redundancies or dependancies are ignored. Two strongly correlated features may both have high IG scores but one may be redundant once the other is selected. More sophisticated filter techniques that address these issues using Mutual Information to score groups of features have been researched by Novovičová et al. [40] and have been shown to be more effective than these simple Filter techniques.While this is an effective strategy for feature selection it has the drawback that features are considered in isolation so redundancies or dependancies are ignored. Two strongly correlated features may both have high IG scores but one may be redundant once the other is selected. More sophisticated filter techniques that address these issues using Mutual Information to score groups of features have been researched by Novovičová et al. [40] and have been shown to be more effective than these simple Filter techniques.</p>
        <p>The obvious criticism of the Filter approach to feature selection is that the filter criterion is separate from the induction algorithm used in the classifier. This is overcome in the Wrapper approach by using the performance of the classifier to guide search in feature selection -the classifier is wrapped in the feature selection process [31]. In this way the merit of a feature subset is the generalisation accuracy it offers as estimated using crossvalidation on the training data. If 10-fold cross validation is used then 10 classifiers will be built and tested for each feature subset evaluated -so the wrapper strategy is very computationally expensive. If there are 𝑝 features under consideration then the search space is of size 2 𝑝 so it is an exponential search problem.The obvious criticism of the Filter approach to feature selection is that the filter criterion is separate from the induction algorithm used in the classifier. This is overcome in the Wrapper approach by using the performance of the classifier to guide search in feature selection -the classifier is wrapped in the feature selection process [31]. In this way the merit of a feature subset is the generalisation accuracy it offers as estimated using crossvalidation on the training data. If 10-fold cross validation is used then 10 classifiers will be built and tested for each feature subset evaluated -so the wrapper strategy is very computationally expensive. If there are 𝑝 features under consideration then the search space is of size 2 𝑝 so it is an exponential search problem.</p>
        <p>A simple example of the search space for feature selection where 𝑝 = 4 is shown in Figure 13. Each node is defined by a feature mask; the node at the top of the figure has no features selected while the node at the bottom has all features selected. For large values of 𝑝 an exhaustive search is not practical because of the exponential nature of the search.A simple example of the search space for feature selection where 𝑝 = 4 is shown in Figure 13. Each node is defined by a feature mask; the node at the top of the figure has no features selected while the node at the bottom has all features selected. For large values of 𝑝 an exhaustive search is not practical because of the exponential nature of the search.</p>
        <p>The two most popular strategies are:The two most popular strategies are:</p>
        <p>-Forward Selection which starts with no features selected, evaluates all the options with just one feature, selects the best of these and considers the options with that feature plus one other, etc.-Forward Selection which starts with no features selected, evaluates all the options with just one feature, selects the best of these and considers the options with that feature plus one other, etc.</p>
        <p>-Backward Elimination starts with all features selected, considers the options with one feature deleted, selects the best of these and continues to eliminate features. These strategies will terminate when adding (or deleting) a feature will not produce an improvement in classification accuracy as assessed by cross validation. Both of these are greedy search strategies and so are not guaranteed to discover the best feature subset. More sophisticated search strategies can be employed to better explore the search space; however, Reunanen [42] cautions that more intensive search strategies are more likely to overfit the training data.-Backward Elimination starts with all features selected, considers the options with one feature deleted, selects the best of these and continues to eliminate features. These strategies will terminate when adding (or deleting) a feature will not produce an improvement in classification accuracy as assessed by cross validation. Both of these are greedy search strategies and so are not guaranteed to discover the best feature subset. More sophisticated search strategies can be employed to better explore the search space; however, Reunanen [42] cautions that more intensive search strategies are more likely to overfit the training data.</p>
        <p>The second aspect of dimension reduction is instance selection, reducing the size of the training set by removing redundant or noisy instances while maintaining or even improving performance. This aspect of dimension reduction is explored and researched in two different areas, nearest-neighbour classification and case-based reasoning (CBR). It is known as Instance Selection or Prototype Selection by those who focus on nearest neighbour classification [24] and Case-base Editing or Case-base Maintenance by the CBR community [38].The second aspect of dimension reduction is instance selection, reducing the size of the training set by removing redundant or noisy instances while maintaining or even improving performance. This aspect of dimension reduction is explored and researched in two different areas, nearest-neighbour classification and case-based reasoning (CBR). It is known as Instance Selection or Prototype Selection by those who focus on nearest neighbour classification [24] and Case-base Editing or Case-base Maintenance by the CBR community [38].</p>
        <p>Instance selection techniques can be categorised as competence preservation or competence enhancement techniques [12].Instance selection techniques can be categorised as competence preservation or competence enhancement techniques [12].</p>
        <p>Competence preservation corresponds to redundancy reduction, removing superfluous instances that do not contribute to classification competence. Competence enhancement is effectively noise reduction, removing noisy or corrupt instances from the training set. Figure 14 Noise reduction on the other hand aims to remove noisy or corrupt instances but can remove exceptional or border instances which may not be distinguishable from true noise, so a balance of both can be useful. Techniques which combine a balance of both redundancy reduction and noise removal are known as hybrid approaches.Competence preservation corresponds to redundancy reduction, removing superfluous instances that do not contribute to classification competence. Competence enhancement is effectively noise reduction, removing noisy or corrupt instances from the training set. Figure 14 Noise reduction on the other hand aims to remove noisy or corrupt instances but can remove exceptional or border instances which may not be distinguishable from true noise, so a balance of both can be useful. Techniques which combine a balance of both redundancy reduction and noise removal are known as hybrid approaches.</p>
        <p>Editing strategies normally operate in one of two ways; incremental which involves adding selected instances from the training set to an initially empty edited set, and decremental which involves contracting the training set by removing selected instances.Editing strategies normally operate in one of two ways; incremental which involves adding selected instances from the training set to an initially empty edited set, and decremental which involves contracting the training set by removing selected instances.</p>
        <p>Early Techniques An early competence preservation technique is Hart's Condensed Nearest Neighbour (CNN) [26].Early Techniques An early competence preservation technique is Hart's Condensed Nearest Neighbour (CNN) [26].</p>
        <p>CNN is an incremental technique which adds to an initially empty edited set any instance from the training set that cannot be classified correctly by the edited set. This technique is very sensitive to noise and to the order of presentation of the training set instances, in fact CNN by definition will tend to preserve noisy instances. Improvements on CNN included the Selective NN (SNN) [43] which imposes the rule that every instance in the training set must be closer to an instance of the same class in the edited set than to any other training instance of a different class. The Reduced NN Rule [25] took the opposite, decremental, approach removing a instance from the training set where its removal does not cause any other instance to be misclassified. This technique will allow for the removal of noisy cases but is sensitive to the order of presentation of cases. Both make multiple passes over the training set, the former repeating the ENN algorithm until no further eliminations can be made from the training set and the latter using incrementing values of 𝑘. These techniques focus on noisy or exceptional instances and do not result in the same storage reduction gains as the competence preservation approaches.CNN is an incremental technique which adds to an initially empty edited set any instance from the training set that cannot be classified correctly by the edited set. This technique is very sensitive to noise and to the order of presentation of the training set instances, in fact CNN by definition will tend to preserve noisy instances. Improvements on CNN included the Selective NN (SNN) [43] which imposes the rule that every instance in the training set must be closer to an instance of the same class in the edited set than to any other training instance of a different class. The Reduced NN Rule [25] took the opposite, decremental, approach removing a instance from the training set where its removal does not cause any other instance to be misclassified. This technique will allow for the removal of noisy cases but is sensitive to the order of presentation of cases. Both make multiple passes over the training set, the former repeating the ENN algorithm until no further eliminations can be made from the training set and the latter using incrementing values of 𝑘. These techniques focus on noisy or exceptional instances and do not result in the same storage reduction gains as the competence preservation approaches.</p>
        <p>Hybrid techniques were introduced with a series of instance based learning IBn algorithms [1]. IB2 is similar to CNN adding only instances that cannot be classified correctly by the reduced training set. IB2's susceptibility to noise is handled by IB3 which records how well instances are classifying and only keeps those that classify correctly to a statistically significant degree. Other researchers have provided variations on the IB𝑛 algorithms [13,14,58].Hybrid techniques were introduced with a series of instance based learning IBn algorithms [1]. IB2 is similar to CNN adding only instances that cannot be classified correctly by the reduced training set. IB2's susceptibility to noise is handled by IB3 which records how well instances are classifying and only keeps those that classify correctly to a statistically significant degree. Other researchers have provided variations on the IB𝑛 algorithms [13,14,58].</p>
        <p>Competence-Based Case-Base Editing Approaches to case-base editing build a competence model of the training data and use the competence properties of the cases to determine which cases to include in the edited set. Measuring and using case competence to guide case-base maintenance was first introduced by Smyth and Keane [50] and developed by Zhu and Yang [59]. Smyth and Keane [50] introduce two important competence properties, the reachability and coverage sets for a case in a case-base. The reachability set of a case 𝑐 is the set of all cases that can successfully classify 𝑐, and the coverage set of a case 𝑐 is the set of all cases that 𝑐 can successfully classify. The coverage and reachability sets represent the local competence characteristics of a case and are used as the basis of a number of editing techniques.Competence-Based Case-Base Editing Approaches to case-base editing build a competence model of the training data and use the competence properties of the cases to determine which cases to include in the edited set. Measuring and using case competence to guide case-base maintenance was first introduced by Smyth and Keane [50] and developed by Zhu and Yang [59]. Smyth and Keane [50] introduce two important competence properties, the reachability and coverage sets for a case in a case-base. The reachability set of a case 𝑐 is the set of all cases that can successfully classify 𝑐, and the coverage set of a case 𝑐 is the set of all cases that 𝑐 can successfully classify. The coverage and reachability sets represent the local competence characteristics of a case and are used as the basis of a number of editing techniques.</p>
        <p>A family of competence-guided editing methods for case-bases combine both incremental and decremental strategies using a combination of rules [38]:A family of competence-guided editing methods for case-bases combine both incremental and decremental strategies using a combination of rules [38]:</p>
        <p>(1) an ordering policy for the presentation of the cases that is based on the competence characteristics of the cases, (2) an addition rule to determine the cases to be added to the edited set, (3) a deletion rule to determine the cases to be removed from the training set and (4) an update policy which indicates whether the competence model is updated after each editing step.(1) an ordering policy for the presentation of the cases that is based on the competence characteristics of the cases, (2) an addition rule to determine the cases to be added to the edited set, (3) a deletion rule to determine the cases to be removed from the training set and (4) an update policy which indicates whether the competence model is updated after each editing step.</p>
        <p>One of these algorithms, Conservative Redundancy Removal (CRR) [22] is included in the assessment in section 4.3.1. This algorithm is similar in concept to the FCNN rule [3] which can be applied to huge collections of data. Other approaches also use the coverage and reachability properties of cases. Iterative Case Filtering (ICF) [12] is a decremental strategy contracting the training set by removing those cases 𝑐, where the number of other cases that can correctly classify 𝑐 is higher that the number of cases that 𝑐 can correctly classify. This strategy focuses on removing cases far from class borders. After each pass over the training set, the competence model is updated and the process repeated until no more cases can be removed. ICF includes a pre-processing noise reduction stage, effectively RENN, to remove noisy cases. McKenna and Smyth compared their family of algorithms to ICF and concluded that the overall best algorithm of the family delivered improved accuracy (albeit marginal, 0.22%) with less than 50% of the cases needed by the ICF edited set [38].One of these algorithms, Conservative Redundancy Removal (CRR) [22] is included in the assessment in section 4.3.1. This algorithm is similar in concept to the FCNN rule [3] which can be applied to huge collections of data. Other approaches also use the coverage and reachability properties of cases. Iterative Case Filtering (ICF) [12] is a decremental strategy contracting the training set by removing those cases 𝑐, where the number of other cases that can correctly classify 𝑐 is higher that the number of cases that 𝑐 can correctly classify. This strategy focuses on removing cases far from class borders. After each pass over the training set, the competence model is updated and the process repeated until no more cases can be removed. ICF includes a pre-processing noise reduction stage, effectively RENN, to remove noisy cases. McKenna and Smyth compared their family of algorithms to ICF and concluded that the overall best algorithm of the family delivered improved accuracy (albeit marginal, 0.22%) with less than 50% of the cases needed by the ICF edited set [38].</p>
        <p>Wilson and Martinez [57] present a series of reduction techniques called DROP1 to DROP51 which, although published before the definitions of coverage and reachability, could also be considered to use a competence model. They define the set of associates of a case 𝑐 which is comparable to the coverage set of McKenna and Smyth except that the associates set will include cases of a different class from case 𝑐 whereas the coverage set will only include cases of the same class as 𝑐.Wilson and Martinez [57] present a series of reduction techniques called DROP1 to DROP51 which, although published before the definitions of coverage and reachability, could also be considered to use a competence model. They define the set of associates of a case 𝑐 which is comparable to the coverage set of McKenna and Smyth except that the associates set will include cases of a different class from case 𝑐 whereas the coverage set will only include cases of the same class as 𝑐.</p>
        <p>The DROP𝑛 algorithms use a decremental strategy. Their comprehensive evaluation found DROP3 to be the best mix of generalisation accuracy and storage requirements, performing consistently well in comparison with other reduction techniques. A comparison of ICF against DROP3 found that neither algorithm consistently out performed the other and both represented the "cutting edge in instance set reduction techniques" [12]. on training set size and generalisation accuracy. The evaluation shows that, at least for some datasets, the training set size can be dramatically reduced with almost no impact on generalisation accuracy. If there is a lot of redundancy in the training data, dramatic speedup can be achieved through instance selection without any significant impact on accuracy.The DROP𝑛 algorithms use a decremental strategy. Their comprehensive evaluation found DROP3 to be the best mix of generalisation accuracy and storage requirements, performing consistently well in comparison with other reduction techniques. A comparison of ICF against DROP3 found that neither algorithm consistently out performed the other and both represented the "cutting edge in instance set reduction techniques" [12]. on training set size and generalisation accuracy. The evaluation shows that, at least for some datasets, the training set size can be dramatically reduced with almost no impact on generalisation accuracy. If there is a lot of redundancy in the training data, dramatic speedup can be achieved through instance selection without any significant impact on accuracy.</p>
        <p>With the large number of decisions to be made in setting up a 𝑘-NN classifier, model selection and hyper-parameter tuning is a key part of the process. Model selection is recognised as fundamental to Machine Learning and toolkits such as 
            <rs type="software">Scikit-learn</rs> include extensive support for model selection and parameter tuning. The fundamental strategy is to use cross-validation to evaluate the model/hyper-parameter combinations [8].
        </p>
        <p>Even a simple 𝑘-NN deployment will require some model choices, for example:Even a simple 𝑘-NN deployment will require some model choices, for example:</p>
        <p>• What is the best value for 𝑘?• What is the best value for 𝑘?</p>
        <p>• What distance measure is best for the data, e.g. Cosine, Euclidean, Correlation?• What distance measure is best for the data, e.g. Cosine, Euclidean, Correlation?</p>
        <p>• Does it help to include distance weighting?• Does it help to include distance weighting?</p>
        <p>The normal practice is to use grid-search to explore this hyper-parameter space as shown in Figure 16. If five alternatives for 𝑘 are considered and three alternative distance measures then a grid of 3 × 5 needs to be explored. When two instance weighting alternatives are added the grid expands to 3 × 5 × 2. These alternatives can be tested using cross validation on the training data and the generalization accuracy of the best combination can be assessed using a hold-out set. A link to Python 
            <rs type="software">code</rs> for grid search is provided in the Appendix.
        </p>
        <p>𝑘-NN is very simple to understand and easy to implement. So it should be considered in seeking a solution to any classification problem. Some advantages of 𝑘-NN are as follows (many of these derive from its simplicity and interpretability):𝑘-NN is very simple to understand and easy to implement. So it should be considered in seeking a solution to any classification problem. Some advantages of 𝑘-NN are as follows (many of these derive from its simplicity and interpretability):</p>
        <p>-Because the process is transparent, it is easy to implement and debug.-Because the process is transparent, it is easy to implement and debug.</p>
        <p>-𝑘-NN can be applied to data that cannot be described as a feature vector provided a similarity measure is available.-𝑘-NN can be applied to data that cannot be described as a feature vector provided a similarity measure is available.</p>
        <p>Thus 𝑘-NN can be used in situations where other ML mechanisms will not be applicable.Thus 𝑘-NN can be used in situations where other ML mechanisms will not be applicable.</p>
        <p>-In situations where an explanation of the output of the classifier is useful, 𝑘-NN can be very effective if an analysis of the neighbours is useful as explanation.-In situations where an explanation of the output of the classifier is useful, 𝑘-NN can be very effective if an analysis of the neighbours is useful as explanation.</p>
        <p>-There are some noise reduction techniques that work only for 𝑘-NN that can be effective in improving the accuracy of the classifier [22].-There are some noise reduction techniques that work only for 𝑘-NN that can be effective in improving the accuracy of the classifier [22].</p>
        <p>-In some circumstances, speed-up mechanisms such as Kd-Trees or Ball Trees can improve retrieval times without any loss of accuracy.-In some circumstances, speed-up mechanisms such as Kd-Trees or Ball Trees can improve retrieval times without any loss of accuracy.</p>
        <p>-Approximate Nearest Neighbour techniques can greatly improve retrieval times, sometimes with minimal impact on accuracy [30].-Approximate Nearest Neighbour techniques can greatly improve retrieval times, sometimes with minimal impact on accuracy [30].</p>
        <p>These advantages of 𝑘-NN, particularly those that derive from its interpretability, should not be underestimated. On the other hand, some significant disadvantages are as follows:These advantages of 𝑘-NN, particularly those that derive from its interpretability, should not be underestimated. On the other hand, some significant disadvantages are as follows:</p>
        <p>-Because all the work is done at run-time, 𝑘-NN can have poor run-time performance if the training set is large.-Because all the work is done at run-time, 𝑘-NN can have poor run-time performance if the training set is large.</p>
        <p>-𝑘-NN is very sensitive to irrelevant or redundant features because all features contribute to the similarity (see Eq. 1) and thus to the classification. This can be ameliorated by careful feature selection or feature weighting.-𝑘-NN is very sensitive to irrelevant or redundant features because all features contribute to the similarity (see Eq. 1) and thus to the classification. This can be ameliorated by careful feature selection or feature weighting.</p>
        <p>Manuscript submitted to ACM -On very difficult classification tasks, 𝑘-NN may be outperformed by more exotic techniques such as Support Vector Machines or Neural Networks.Manuscript submitted to ACM -On very difficult classification tasks, 𝑘-NN may be outperformed by more exotic techniques such as Support Vector Machines or Neural Networks.</p>
        <p>Manuscript submitted to ACMManuscript submitted to ACM</p>
        <p>Manuscript submitted to ACM k-Nearest Neighbour Classifiers -A TutorialManuscript submitted to ACM k-Nearest Neighbour Classifiers -A Tutorial</p>
        <p>Three of these algorithms were originally published in[56] as Reduction Techniques (RT1 to RT3).Manuscript submitted to ACMThree of these algorithms were originally published in[56] as Reduction Techniques (RT1 to RT3).Manuscript submitted to ACM</p>
        <p>This work was funded by Science Foundation Ireland through I-From: The SFI Centre for Advance Manufacturing Research (16/RC/3872) and the SFI Centre for Research Training in Machine Learning (Grant No. 18/CRT/6183).This work was funded by Science Foundation Ireland through I-From: The SFI Centre for Advance Manufacturing Research (16/RC/3872) and the SFI Centre for Research Training in Machine Learning (Grant No. 18/CRT/6183).</p>
        <p>The GitHub repository 2 associated with this paper contains the following Python Notebooks:The GitHub repository 2 associated with this paper contains the following Python Notebooks:</p>
        <p>• kNN-Basic: Code for a basic 𝑘-NN classifier in 
            <rs type="software">scikit</rs>-learn.
        </p>
        <p>• kNN-Correlation: How to use correlation as the 𝑘-NN metric in scikit-learn (see section 2.2).• kNN-Correlation: How to use correlation as the 𝑘-NN metric in scikit-learn (see section 2.2).</p>
        <p>• kNN-Cosine: How to use Cosine as the 𝑘-NN metric in scikit-learn. Using Cosine similarity for text classification (section 2.1).• kNN-Cosine: How to use Cosine as the 𝑘-NN metric in scikit-learn. Using Cosine similarity for text classification (section 2.1).</p>
        <p>• kNN-DTW: Using the 
            <rs type="software">tslearn</rs> library for time-series classification using DTW (section 2.4).
        </p>
        <p>• kNN-MetricLearn: Using the metric-learn library to learn a similarity metric.• kNN-MetricLearn: Using the metric-learn library to learn a similarity metric.</p>
        <p>• kNN-Speedup: 
            <rs type="software">scikit-learn</rs> provides some options for speeding up nearest neighbour retrieval. This notebook tests the speedup on four datasets (section 3.4).
        </p>
        <p>• kNN-Annoy: Testing the speedup offered by the Approximate Nearest Neighbour implementation in annoy (section 3.4).• kNN-Annoy: Testing the speedup offered by the Approximate Nearest Neighbour implementation in annoy (section 3.4).</p>
        <p>• kNN-PCA: Some code to use PCA to estimate the intrinsic dimension of the four datasets.• kNN-PCA: Some code to use PCA to estimate the intrinsic dimension of the four datasets.</p>
    </text>
</tei>

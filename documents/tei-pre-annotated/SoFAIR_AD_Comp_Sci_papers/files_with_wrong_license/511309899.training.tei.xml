<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:15+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Mammogram inspection in search of breast tumors is a tough assignment that radiologists must carry out frequently. Therefore, image analysis methods are needed for the detection and delineation of breast tumors, which portray crucial morphological information that will support reliable diagnosis. In this paper, we proposed a conditional Generative Adversarial Network (cGAN) devised to segment a breast tumor within a region of interest (ROI) in a mammogram. The generative network learns to recognize the tumor area and to create the binary mask that outlines it. In turn, the adversarial network learns to distinguish between real (ground truth) and synthetic segmentations, thus enforcing the generative network to create binary masks as realistic as possible.Mammogram inspection in search of breast tumors is a tough assignment that radiologists must carry out frequently. Therefore, image analysis methods are needed for the detection and delineation of breast tumors, which portray crucial morphological information that will support reliable diagnosis. In this paper, we proposed a conditional Generative Adversarial Network (cGAN) devised to segment a breast tumor within a region of interest (ROI) in a mammogram. The generative network learns to recognize the tumor area and to create the binary mask that outlines it. In turn, the adversarial network learns to distinguish between real (ground truth) and synthetic segmentations, thus enforcing the generative network to create binary masks as realistic as possible.</p>
        <p>The cGAN works well even when the number of training samples are limited. As a consequence, the proposed method outperforms several state-of-the-art approaches. OurThe cGAN works well even when the number of training samples are limited. As a consequence, the proposed method outperforms several state-of-the-art approaches. Our</p>
        <p>Breast cancer is the most common diagnosed cause of death from cancer in women in the world Siegel et al. (2017). Mammography is a world recognized tool that has been proven effective to reduce the mortality rate, since it allows early detection of breast diseases Lauby-Secretan et al. (2015).Breast cancer is the most common diagnosed cause of death from cancer in women in the world Siegel et al. (2017). Mammography is a world recognized tool that has been proven effective to reduce the mortality rate, since it allows early detection of breast diseases Lauby-Secretan et al. (2015).</p>
        <p>Breast masses are the most important findings among diverse types of breast abnormalities, such as micro-calcification and architectural distortion. All these findings may point out the presence of carcinomas Rangayyan et al. (2010). Moreover, morphological information of tumor shape (irregular, lobular, oval and round) and margin type (circumscribed, ill defined, spiculated and obscured) also play crucial roles in the diagnosis of tumor malignancy Tang et al. (2009).Breast masses are the most important findings among diverse types of breast abnormalities, such as micro-calcification and architectural distortion. All these findings may point out the presence of carcinomas Rangayyan et al. (2010). Moreover, morphological information of tumor shape (irregular, lobular, oval and round) and margin type (circumscribed, ill defined, spiculated and obscured) also play crucial roles in the diagnosis of tumor malignancy Tang et al. (2009).</p>
        <p>Computer aided diagnosis (CAD) systems are highly recommended to assist radiologists in detecting breast tumors and outlining their borders. However, breast tumor segmentation and classification are still challenges due to low signal-to-noise ratio and variability of tumors in shape, size, appearance, texture and location. Recently, many studies based on deep representation of breast images and combining features have been proposed to improve performance on breast mass classification Jiao et al. (2018) .Computer aided diagnosis (CAD) systems are highly recommended to assist radiologists in detecting breast tumors and outlining their borders. However, breast tumor segmentation and classification are still challenges due to low signal-to-noise ratio and variability of tumors in shape, size, appearance, texture and location. Recently, many studies based on deep representation of breast images and combining features have been proposed to improve performance on breast mass classification Jiao et al. (2018) .</p>
        <p>In addition, based on mammographic images, it is very complicated for an expert radiologist to discern the molecular subtypes, i.e., Luminal-A, Luminal-B, HER-2 (Human Epidermal growth factor receptor 2) and Basal-like (triple negative), which are key for prescribing the best oncological treatment Cho (2016), Liu et al. (2016a), Tamaki et al. (2011). However, recent studies point out some loose correlations between visual tumor features (e.g., texture and shape) and molecular subtypes. Recently, a Convolutional Neural Network (CNN) was used to classify molecular subtypes using texture patches extracted from mammography Singh et al. (2017), which yielded an overall accuracy of 67%. However, depending only on texture feature is not sufficient to classify the breast cancer molecular subtypes from mammograms Tamaki et al. (2011). Thus, some studies attempt to use morphological information of tumor shape in classifying breast cancer molecular subtypes. In this paper, we propose a method based on two main stages, one for breast tumor segmentation and another for tumor shape classification, as shown in Figure 1. Before applying our segmentation approach, the Single Shot Detector (SSD) Liu et al. (2016b) is used to locate the tumor and then our method computes the proper coordinates to crop the ROI. Afterwards, the first stage segments the breast tumor, contained in the ROI, as a binary mask. In the second stage, the binary mask is classified to a shape type (irregular, lobular, oval and round). Unlike traditional object classifiers Kisilev et al. (2015), Kim et al. (2018) that use texture, intensity or edge information, our method is forced to learn only morphological features from the binary masks. The current proposal is a thorough improvement of our previous work Singh et al. (2018b). The major contributions of this paper are as follows:In addition, based on mammographic images, it is very complicated for an expert radiologist to discern the molecular subtypes, i.e., Luminal-A, Luminal-B, HER-2 (Human Epidermal growth factor receptor 2) and Basal-like (triple negative), which are key for prescribing the best oncological treatment Cho (2016), Liu et al. (2016a), Tamaki et al. (2011). However, recent studies point out some loose correlations between visual tumor features (e.g., texture and shape) and molecular subtypes. Recently, a Convolutional Neural Network (CNN) was used to classify molecular subtypes using texture patches extracted from mammography Singh et al. (2017), which yielded an overall accuracy of 67%. However, depending only on texture feature is not sufficient to classify the breast cancer molecular subtypes from mammograms Tamaki et al. (2011). Thus, some studies attempt to use morphological information of tumor shape in classifying breast cancer molecular subtypes. In this paper, we propose a method based on two main stages, one for breast tumor segmentation and another for tumor shape classification, as shown in Figure 1. Before applying our segmentation approach, the Single Shot Detector (SSD) Liu et al. (2016b) is used to locate the tumor and then our method computes the proper coordinates to crop the ROI. Afterwards, the first stage segments the breast tumor, contained in the ROI, as a binary mask. In the second stage, the binary mask is classified to a shape type (irregular, lobular, oval and round). Unlike traditional object classifiers Kisilev et al. (2015), Kim et al. (2018) that use texture, intensity or edge information, our method is forced to learn only morphological features from the binary masks. The current proposal is a thorough improvement of our previous work Singh et al. (2018b). The major contributions of this paper are as follows:</p>
        <p>1. We believe this is the first adaptation of cGAN in the area of breast tumor seg-mentation in mammograms. The adversarial network yields more reliable learning than other state-of-the-art algorithms since training data is scarce (i.e., mammograms with labeled breast tumor boundaries), while it does not increase the computational complexity at prediction time.1. We believe this is the first adaptation of cGAN in the area of breast tumor seg-mentation in mammograms. The adversarial network yields more reliable learning than other state-of-the-art algorithms since training data is scarce (i.e., mammograms with labeled breast tumor boundaries), while it does not increase the computational complexity at prediction time.</p>
        <p>2. The application of a multi-class CNN architecture to predict the four breast tumor shapes (i.e., irregular, lobular, oval and round) using the binary mask segmented in the previous stage (cGAN output).2. The application of a multi-class CNN architecture to predict the four breast tumor shapes (i.e., irregular, lobular, oval and round) using the binary mask segmented in the previous stage (cGAN output).</p>
        <p>3. An in-depth evaluation of our system's performance using two public (1,274 images) and one private (300 images) databases. The obtained results outperform current state-of-the-art in both tumor segmentation and shape classification.3. An in-depth evaluation of our system's performance using two public (1,274 images) and one private (300 images) databases. The obtained results outperform current state-of-the-art in both tumor segmentation and shape classification.</p>
        <p>4. A study of the correlation between the tumor shape predicted by our automatic method with respect to the ground-truth molecular subtypes of breast cancer, which reasonably matches with other clinical studies like Boisserie-Lacroix et al.4. A study of the correlation between the tumor shape predicted by our automatic method with respect to the ground-truth molecular subtypes of breast cancer, which reasonably matches with other clinical studies like Boisserie-Lacroix et al.</p>
        <p>This paper is organized as follows. Section II provides the related work of both tumor segmentation and shape classification. The proposed architectures for tumor segmentation (using cGAN) and shape classification (using CNN) are described in Section III. In Section IV, extensive experiments are performed on the two stages of the proposed method and the obtained results are compared with the state-of-the-art results.This paper is organized as follows. Section II provides the related work of both tumor segmentation and shape classification. The proposed architectures for tumor segmentation (using cGAN) and shape classification (using CNN) are described in Section III. In Section IV, extensive experiments are performed on the two stages of the proposed method and the obtained results are compared with the state-of-the-art results.</p>
        <p>In addition, the limitations of the proposed models are explained in Section IV. Finally, Section V concludes our work and suggests some future lines of research.In addition, the limitations of the proposed models are explained in Section IV. Finally, Section V concludes our work and suggests some future lines of research.</p>
        <p>In the following paragraphs we point out some works mainly focused on breast tumor segmentation and shape classification in mammography, as well as generic image analysis methods highly related with our field of interest.In the following paragraphs we point out some works mainly focused on breast tumor segmentation and shape classification in mammography, as well as generic image analysis methods highly related with our field of interest.</p>
        <p>Convolutional Neural Networks (CNNs) can automatically learn features from the given images to represent objects at different scales and orientations. By increasing the number of layers (depth of CNN model) more detailed features can be obtained, which play crucial part in solving different computer vision problems, such as object detection, classification and segmentation. Thus, numerous methods has been proposed to solve the image segmentation problem based on deep learning approaches Schmidhuber (2015).Convolutional Neural Networks (CNNs) can automatically learn features from the given images to represent objects at different scales and orientations. By increasing the number of layers (depth of CNN model) more detailed features can be obtained, which play crucial part in solving different computer vision problems, such as object detection, classification and segmentation. Thus, numerous methods has been proposed to solve the image segmentation problem based on deep learning approaches Schmidhuber (2015).</p>
        <p>One of the well-known architectures for semantic segmentation is the Fully Convolutional Network (FCN) Long et al. (2015), which is based on encoding (convolutional)One of the well-known architectures for semantic segmentation is the Fully Convolutional Network (FCN) Long et al. (2015), which is based on encoding (convolutional)</p>
        <p>and decoding (deconvolutional) layers. This approach gets rid of the fully connected layers of CNNs to convert the image classification networks into image filtering net-and decoding (deconvolutional) layers. This approach gets rid of the fully connected layers of CNNs to convert the image classification networks into image filtering net-</p>
        <p>works. An improvement of this scheme was proposed by the U-Net architecture Ronneberger et al. (2015), where skip connections between encoding and decoding layers are added to retain significant information from the input features. Later on, a new variation of FCN was proposed Badrinarayanan et al. (2017) named SegNet, which consists of hierarchy of decoders, each one corresponding to each encoder. The decoder network uses the max-pooling indices received from the corresponding encoder to perform non-linear upsampling of their input feature maps.works. An improvement of this scheme was proposed by the U-Net architecture Ronneberger et al. (2015), where skip connections between encoding and decoding layers are added to retain significant information from the input features. Later on, a new variation of FCN was proposed Badrinarayanan et al. (2017) named SegNet, which consists of hierarchy of decoders, each one corresponding to each encoder. The decoder network uses the max-pooling indices received from the corresponding encoder to perform non-linear upsampling of their input feature maps.</p>
        <p>Since semantic segmentation has achieved great progress with deep learning, there is recent popularity in applying such models to medical imaging, such as for skin lesions segmentation (Litjens et al. (2017), Sarker et al. (2018)), and for fundus photography of the rear of an eye ( Fu et al. (2018), Singh et al. (2018a)).Since semantic segmentation has achieved great progress with deep learning, there is recent popularity in applying such models to medical imaging, such as for skin lesions segmentation (Litjens et al. (2017), Sarker et al. (2018)), and for fundus photography of the rear of an eye ( Fu et al. (2018), Singh et al. (2018a)).</p>
        <p>For breast tumor detection, segmentation and classification, many medical image analysis methods have been proposed so far, such as Yassin et al. (2018) and Hamidinekoo et al. (2018).For breast tumor detection, segmentation and classification, many medical image analysis methods have been proposed so far, such as Yassin et al. (2018) and Hamidinekoo et al. (2018).</p>
        <p>A tumor classification and segmentation method was proposed Rouhi et al. (2015) using an automated region growing algorithm whose threshold was obtained by a trained Artificial Neural Network (ANN) and Cellular Neural Network (CeNN). In turn, to reduce the computational complexity and increase the robustness, a quantized and non-linear CeNN for breast tumor segmentation was proposed in Liu et al. (2018).A tumor classification and segmentation method was proposed Rouhi et al. (2015) using an automated region growing algorithm whose threshold was obtained by a trained Artificial Neural Network (ANN) and Cellular Neural Network (CeNN). In turn, to reduce the computational complexity and increase the robustness, a quantized and non-linear CeNN for breast tumor segmentation was proposed in Liu et al. (2018).</p>
        <p>After segmenting the breast tumor region, a Multilayer Perceptron Classifier was used for tumor classification as benign or malignant.After segmenting the breast tumor region, a Multilayer Perceptron Classifier was used for tumor classification as benign or malignant.</p>
        <p>Furthermore, Dhungel et al. (2015b) segmented breast tumors using Structured Support Vector Machines (SSVM) and Conditional Random Fields (CRF). Both graphical models minimize a loss function build on pixel probabilities provided by a CNN and Deep Belief Network, a Gaussian Mixture Model (GMM) and shape prior. The SSVM is based on graph cuts and the CRF relies on tree re-weighted belief propagation with truncated fitting training Dhungel et al. (2015a). Cardoso et al. (2015Cardoso et al. ( , 2017) ) tackled the same problem by employing a closed contour fitting in the mammogram and minimizing a cost function depending on the radial derivative of the tumor contour. A measure of regularity of the gray pixel values inside and outside the tumor was also included in Cardoso et al. (2017).Furthermore, Dhungel et al. (2015b) segmented breast tumors using Structured Support Vector Machines (SSVM) and Conditional Random Fields (CRF). Both graphical models minimize a loss function build on pixel probabilities provided by a CNN and Deep Belief Network, a Gaussian Mixture Model (GMM) and shape prior. The SSVM is based on graph cuts and the CRF relies on tree re-weighted belief propagation with truncated fitting training Dhungel et al. (2015a). Cardoso et al. (2015Cardoso et al. ( , 2017) ) tackled the same problem by employing a closed contour fitting in the mammogram and minimizing a cost function depending on the radial derivative of the tumor contour. A measure of regularity of the gray pixel values inside and outside the tumor was also included in Cardoso et al. (2017).</p>
        <p>In turn, Zhu et al. (2018) proposed an FCN concatenated to a CRF layer to impose the compactness of the segmentation output taking into account pixel position. This approach was trained end-to-end, since the CRF and FCN can exchange data in the forward-backward propagation. An adversarial term was introduced to prevent the samples with the worst perturbation in the loss function, which reduced the overfitting and provided a robust learning with few training samples. In addition, Al-antari et al.In turn, Zhu et al. (2018) proposed an FCN concatenated to a CRF layer to impose the compactness of the segmentation output taking into account pixel position. This approach was trained end-to-end, since the CRF and FCN can exchange data in the forward-backward propagation. An adversarial term was introduced to prevent the samples with the worst perturbation in the loss function, which reduced the overfitting and provided a robust learning with few training samples. In addition, Al-antari et al.</p>
        <p>(2018) proposed a CAD system consisting of three deep learning stages for detecting, segmenting and classifying the tumors in mammographic images. To locate tumors in a full mammogram, the YOLO network proposed in Redmon et al. (2016) was used.(2018) proposed a CAD system consisting of three deep learning stages for detecting, segmenting and classifying the tumors in mammographic images. To locate tumors in a full mammogram, the YOLO network proposed in Redmon et al. (2016) was used.</p>
        <p>A Full resolution Convolutional Network (FrCN) was then used for segmenting the located tumor region. Finally, a CNN network was used for classifying segmented ROI as either benign or malignant.A Full resolution Convolutional Network (FrCN) was then used for segmenting the located tumor region. Finally, a CNN network was used for classifying segmented ROI as either benign or malignant.</p>
        <p>We believe that Yang et al. (2017) is the first work that exploits GAN Goodfellow et al. (2014) for medical image segmentation. In particular, they performed three-dimensional (3D) liver segmentations using abdominal Computerized Tomography (CT) scans. In Singh et al. (2018b), we adapted a cGAN image-to-image translation algorithm Isola et al. (2017) to address the tumor segmentation in two-dimensional (2D) mammograms. With that method, we achieved state-of-the-art performance on both public and private databases.We believe that Yang et al. (2017) is the first work that exploits GAN Goodfellow et al. (2014) for medical image segmentation. In particular, they performed three-dimensional (3D) liver segmentations using abdominal Computerized Tomography (CT) scans. In Singh et al. (2018b), we adapted a cGAN image-to-image translation algorithm Isola et al. (2017) to address the tumor segmentation in two-dimensional (2D) mammograms. With that method, we achieved state-of-the-art performance on both public and private databases.</p>
        <p>In the literature, many approaches used traditional computer vision techniques to extract hand-craft features and subsequently classify them. For instance, Matos et al. (2018) To date, numerous shape classification methods are applied for medical image analysis Singh et al. (2018b), andKim et al. (2018). An automated method for textual description of anatomical breast tumor lesions was proposed by Kisilev et al. (2015), which performs joint semantic estimation from image measurements to classify the tumor shape. In addition, Kisilev et al. (2016) also presented a multi-task fast regionbased CNN Ren et al. (2015) to classify three tumor shapes: irregular, oval and round.In the literature, many approaches used traditional computer vision techniques to extract hand-craft features and subsequently classify them. For instance, Matos et al. (2018) To date, numerous shape classification methods are applied for medical image analysis Singh et al. (2018b), andKim et al. (2018). An automated method for textual description of anatomical breast tumor lesions was proposed by Kisilev et al. (2015), which performs joint semantic estimation from image measurements to classify the tumor shape. In addition, Kisilev et al. (2016) also presented a multi-task fast regionbased CNN Ren et al. (2015) to classify three tumor shapes: irregular, oval and round.</p>
        <p>Furthermore, the work in Kim et al. (2018) utilized a GAN to diagnose and classify tumors in mammograms into four shapes: irregular, lobular, oval and round. Previously, Singh et al. (2018b) proposed a multi-class CNN to categorize the tumor shapes into four classes as in Kim et al. (2018) from the public dataset DDSM1 .Furthermore, the work in Kim et al. (2018) utilized a GAN to diagnose and classify tumors in mammograms into four shapes: irregular, lobular, oval and round. Previously, Singh et al. (2018b) proposed a multi-class CNN to categorize the tumor shapes into four classes as in Kim et al. (2018) from the public dataset DDSM1 .</p>
        <p>The proposed CAD system shown in Fig. 1 is divided into two stages: breast tumor segmentation and shape classification.The proposed CAD system shown in Fig. 1 is divided into two stages: breast tumor segmentation and shape classification.</p>
        <p>Before feeding an image to the first stage, our optimal workflow applies the Single Shot Detector (SSD) Liu et al. (2016b) Redmon et al. (2016) and Faster R- CNN Ren et al. (2015). Empirically, the SSD detector yields the best results since it is able to detect small tumor regions and provides an overall accuracy of 97%. We are not targeting object sizes less than 7×7 pixels because those objects are really hard to be identified as tumors. Indeed, they may correspond to other types of findings, such as calcifications. We are considering only mammograms with tumors, since our main goal is tumor shape classification following tumor segmentation. Therefore, we have not applied SSD on normal mammograms (no tumor), although the SSD method is capable of dealing with this case as well.Before feeding an image to the first stage, our optimal workflow applies the Single Shot Detector (SSD) Liu et al. (2016b) Redmon et al. (2016) and Faster R- CNN Ren et al. (2015). Empirically, the SSD detector yields the best results since it is able to detect small tumor regions and provides an overall accuracy of 97%. We are not targeting object sizes less than 7×7 pixels because those objects are really hard to be identified as tumors. Indeed, they may correspond to other types of findings, such as calcifications. We are considering only mammograms with tumors, since our main goal is tumor shape classification following tumor segmentation. Therefore, we have not applied SSD on normal mammograms (no tumor), although the SSD method is capable of dealing with this case as well.</p>
        <p>To obtain the proper cropping area, our best framing method, so-called "loose frame", expands the original bounding box coordinates by adding extra space around, so that the cropped ROI always encompasses the tumor as well as some surrounding area containing healthy tissue (30% and 70% for tumor and healthy tissues, respectively). The computed coordinates are shifted to make the ROI frame fit inside the mammogram image. Besides, both sides of the frame are set equal in order to preserve the original aspect ratio of tumors. Last adjustments required to make the image square sometimes cause the tumor be out of the ROI center. However, this does not preclude the segmentation and classification due to the position-independent nature of convolutional filters.To obtain the proper cropping area, our best framing method, so-called "loose frame", expands the original bounding box coordinates by adding extra space around, so that the cropped ROI always encompasses the tumor as well as some surrounding area containing healthy tissue (30% and 70% for tumor and healthy tissues, respectively). The computed coordinates are shifted to make the ROI frame fit inside the mammogram image. Besides, both sides of the frame are set equal in order to preserve the original aspect ratio of tumors. Last adjustments required to make the image square sometimes cause the tumor be out of the ROI center. However, this does not preclude the segmentation and classification due to the position-independent nature of convolutional filters.</p>
        <p>Moreover, ROI images are scaled to 256×256 pixels, which is the optimal cGAN input size found experimentally. After scaling, they are pre-processed for noise removal as proposed in Kshema et al. (2017) (Gaussian filter with σ = 0.5 yields the best segmentation results) and then contrast is enhanced using histogram equalization, similarly to Cheng et al. (2003). Then, we apply a normalization for rescaling the pixel values between [0..1].).Moreover, ROI images are scaled to 256×256 pixels, which is the optimal cGAN input size found experimentally. After scaling, they are pre-processed for noise removal as proposed in Kshema et al. (2017) (Gaussian filter with σ = 0.5 yields the best segmentation results) and then contrast is enhanced using histogram equalization, similarly to Cheng et al. (2003). Then, we apply a normalization for rescaling the pixel values between [0..1].).</p>
        <p>The prepared data is then fed to the cGAN to obtain a binary mask of the breast tumor, which is post-processed using morphological operations (we used filter sizes of 3×3 for closing, 2×2 for erosion, and 3×3 for dilation) to remove small speckles, as proposed in Hazarika and Mahanta (2018). In the second stage, the output binary mask is downsampled into 64×64 pixels, which is then fed to a multi-class CNN shape descriptor to categorize it into four classes: irregular, lobular, oval and round. The reason of this downsampling is that our shape classification CNN does not need a high resolution image to extract the core morphological features for each class, since the tumors are represented with flat white areas in front of a black background. Hence, the changes in the image present very low frequencies. 1. The Generator G network of the cGAN is an FCN composed of encoding and decoding layers, which learn the intrinsic features (gray-level, texture, gradients, edges, shape, etc.) of healthy and unhealthy (tumor) breast tissue, and generate a binary mask according to these features.The prepared data is then fed to the cGAN to obtain a binary mask of the breast tumor, which is post-processed using morphological operations (we used filter sizes of 3×3 for closing, 2×2 for erosion, and 3×3 for dilation) to remove small speckles, as proposed in Hazarika and Mahanta (2018). In the second stage, the output binary mask is downsampled into 64×64 pixels, which is then fed to a multi-class CNN shape descriptor to categorize it into four classes: irregular, lobular, oval and round. The reason of this downsampling is that our shape classification CNN does not need a high resolution image to extract the core morphological features for each class, since the tumors are represented with flat white areas in front of a black background. Hence, the changes in the image present very low frequencies. 1. The Generator G network of the cGAN is an FCN composed of encoding and decoding layers, which learn the intrinsic features (gray-level, texture, gradients, edges, shape, etc.) of healthy and unhealthy (tumor) breast tissue, and generate a binary mask according to these features.</p>
        <p>2. The Discriminative D network of the cGAN assesses if a given binary mask is likely to be a realistic segmentation or not. Therefore, including the adversarial score in the computation of the generator loss strengthens its capability to provide a correct segmentation.2. The Discriminative D network of the cGAN assesses if a given binary mask is likely to be a realistic segmentation or not. Therefore, including the adversarial score in the computation of the generator loss strengthens its capability to provide a correct segmentation.</p>
        <p>The combination of G and D networks allows robust learning with few training samples. Since the ROI image is a conditioning input for both G and D, the segmentation result is better fitted to the tumor appearance. Otherwise, regular (unconditional) GAN Goodfellow et al. (2014) will infer the segmentation just from random noise, which will require more training iterations compared to the cGAN to obtain an acceptable segmentation result. the loss function of G is defined as:The combination of G and D networks allows robust learning with few training samples. Since the ROI image is a conditioning input for both G and D, the segmentation result is better fitted to the tumor appearance. Otherwise, regular (unconditional) GAN Goodfellow et al. (2014) will infer the segmentation just from random noise, which will require more training iterations compared to the cGAN to obtain an acceptable segmentation result. the loss function of G is defined as:</p>
        <p>where z is introduced as dropout in the decoding layers Dn 1 , Dn 2 and Dn 3 at both training and testing phases, which provides stochasticity to generalize the learning processes and avoid overfitting.where z is introduced as dropout in the decoding layers Dn 1 , Dn 2 and Dn 3 at both training and testing phases, which provides stochasticity to generalize the learning processes and avoid overfitting.</p>
        <p>The optimization process of G will try to minimize both expected values, i.e., the D values should approach to 1.0 (correct tumor segmentations), and the dice loss Dice should approach to 0.0 (generated masks are equal to ground truth). Both terms of generator loss enforce the proper optimization of G: the dice loss term fosters a rough prediction of the mask shape (central tumor area) while the adversarial term fosters an accurate prediction of the mask outline (tumor borders). Neglecting one of the two terms may lead to either very poor segmentation results or slow learning speed.The optimization process of G will try to minimize both expected values, i.e., the D values should approach to 1.0 (correct tumor segmentations), and the dice loss Dice should approach to 0.0 (generated masks are equal to ground truth). Both terms of generator loss enforce the proper optimization of G: the dice loss term fosters a rough prediction of the mask shape (central tumor area) while the adversarial term fosters an accurate prediction of the mask outline (tumor borders). Neglecting one of the two terms may lead to either very poor segmentation results or slow learning speed.</p>
        <p>In addition, Dice (y, G(x, z)) is the dice loss of the predicted mask with respect to ground truth, which is defined as:In addition, Dice (y, G(x, z)) is the dice loss of the predicted mask with respect to ground truth, which is defined as:</p>
        <p>where • is the pixel wise multiplication of the two images and |.| is the total sum of pixel values of a given image. If inputs are binary images, then each pixel can be considered as a boolean value (white is 1 / black is 0 ). The formulation in ( 2) is equivalent to the dice coefficient i.e., 2 × T P T P+FN+T P+FP , but it must be subtracted from 1.0 because the loss function will be minimized. Let A be the ground truth of the ROI and B the segmented region. Then the true positive degree (TP) is defined as T P = A ∩ B, which is the area of the segmented region common in both A and B. The false positive degree (FP) is defined as A ∩ B, which is the segmented area not belonging to A. Similarly, the false negative degree (FN) is defined as A ∩ B, which is the true area missed by the proposed segmentation method.where • is the pixel wise multiplication of the two images and |.| is the total sum of pixel values of a given image. If inputs are binary images, then each pixel can be considered as a boolean value (white is 1 / black is 0 ). The formulation in ( 2) is equivalent to the dice coefficient i.e., 2 × T P T P+FN+T P+FP , but it must be subtracted from 1.0 because the loss function will be minimized. Let A be the ground truth of the ROI and B the segmented region. Then the true positive degree (TP) is defined as T P = A ∩ B, which is the area of the segmented region common in both A and B. The false positive degree (FP) is defined as A ∩ B, which is the segmented area not belonging to A. Similarly, the false negative degree (FN) is defined as A ∩ B, which is the true area missed by the proposed segmentation method.</p>
        <p>In our previous work Singh et al. (2018b), the generator network loss was formulated by combining the logistic Binary Cross Entropy (BCE) loss and the L1-norm. In this work, we replace the L1-norm loss with the dice loss as shown in Fig. 4. L1-norm loss minimizes the sum of absolute differences between the ground truth label y and estimated binary mask G(x, z) obtained from the generator network, which takes all pixels into account. In turn, dice loss is highly dependent on TP predictions, which is the most influential term in foreground segmentation. Fig. 5 shows that the dice loss achieves lower values (more optimal) than the L1-norm loss. Moreover, the loss function of D is defined in (3):In our previous work Singh et al. (2018b), the generator network loss was formulated by combining the logistic Binary Cross Entropy (BCE) loss and the L1-norm. In this work, we replace the L1-norm loss with the dice loss as shown in Fig. 4. L1-norm loss minimizes the sum of absolute differences between the ground truth label y and estimated binary mask G(x, z) obtained from the generator network, which takes all pixels into account. In turn, dice loss is highly dependent on TP predictions, which is the most influential term in foreground segmentation. Fig. 5 shows that the dice loss achieves lower values (more optimal) than the L1-norm loss. Moreover, the loss function of D is defined in (3):</p>
        <p>The optimizer will fit D to maximize the loss values for ground truth masks (by minimizinglog(D(x, y))) and minimize the loss values for generated masks (by minimizinglog(1-D(x, G(x, z))). These two terms compute BCE loss using both masks, assuming that the expected class for ground truth and generated masks is 1 and 0, respectively.The optimizer will fit D to maximize the loss values for ground truth masks (by minimizinglog(D(x, y))) and minimize the loss values for generated masks (by minimizinglog(1-D(x, G(x, z))). These two terms compute BCE loss using both masks, assuming that the expected class for ground truth and generated masks is 1 and 0, respectively.</p>
        <p>The optimization of G and D is done concurrently, i.e., one optimization step for both networks at each iteration, where G learns how to compute a valid tumor segmentations and D learns how to differentiate between synthetic and real segmentations.The optimization of G and D is done concurrently, i.e., one optimization step for both networks at each iteration, where G learns how to compute a valid tumor segmentations and D learns how to differentiate between synthetic and real segmentations.</p>
        <p>In this work, we experimented on different hyper-parameters to improve the segmentation accuracy of our previous contribution in Singh et al. (2018b). Besides introducing the dice loss, we have reduced the number of filters of each network from 64 to 32. We also explored different learning rates and loss optimizers (SGD, 
            <rs type="software">AdaGrad</rs>, Adadelta, 
            <rs type="software">RMSProp</rs> and Adam), finding 
            <rs type="software">Adam</rs> with β 1 = 0.5, β 2 = 0.999 and initial learning rate = 0.0002 with batch size 8 the best combination. In (1), the dice loss weighting factor λ = 150 was found to be the best choice. Finally, the best results
        </p>
        <p>were achieved by training both G and D from scratch for 150 epochs.were achieved by training both G and D from scratch for 150 epochs.</p>
        <p>In the literature, various approaches for tumor shape classification have found that texture and intensity features are relevant for their proposals. However, in this proposal we attempt to use only shape context to classify the tumor shapes. Specifically, we propose a multi-class CNN architecture for breast tumor shape classification (i.e., irregular, lobular, oval and round) using the binary masks obtained from the 
            <rs type="software">cGAN</rs>.
        </p>
        <p>In the literature, most methods attempted to directly categorize the shape using breast tumor intensity, texture, boundary, etc. (Kisilev et al. (2015(Kisilev et al. ( , 2016)); Ren et al. (2015); Kim et al. (2018)), which increase computational complexity. We simplify the problem by extracting morphological features from binary masks.In the literature, most methods attempted to directly categorize the shape using breast tumor intensity, texture, boundary, etc. (Kisilev et al. (2015(Kisilev et al. ( , 2016)); Ren et al. (2015); Kim et al. (2018)), which increase computational complexity. We simplify the problem by extracting morphological features from binary masks.</p>
        <p>We have evaluated the performance of proposed models on two public mammography datasets and one private dataset:We have evaluated the performance of proposed models on two public mammography datasets and one private dataset:</p>
        <p>It is a publicly available database containing a total of 115 cases (410 mammograms), which include: masses, calcifications, asymmetries and distortions. However, only 106 out of 410 mammograms have their corresponding ground truth of binary masks. Thus, we only used this 106 mammograms to test our detection and segmentation model.It is a publicly available database containing a total of 115 cases (410 mammograms), which include: masses, calcifications, asymmetries and distortions. However, only 106 out of 410 mammograms have their corresponding ground truth of binary masks. Thus, we only used this 106 mammograms to test our detection and segmentation model.</p>
        <p>It is a publicly available digital database for screening mammography containing 2, 620 mammography studies. In this work, 1, 168 cases of breast tumors with their corresponding ground truths are used for shape classification, where 504, 473, 115 and 76 tumors are labeled as irregular, lobular, oval and round, respectively. The remaining images are excluded since they do not provide shape labels. We have used 75% of the images for training and rest for testing the tumor shape classification model.It is a publicly available digital database for screening mammography containing 2, 620 mammography studies. In this work, 1, 168 cases of breast tumors with their corresponding ground truths are used for shape classification, where 504, 473, 115 and 76 tumors are labeled as irregular, lobular, oval and round, respectively. The remaining images are excluded since they do not provide shape labels. We have used 75% of the images for training and rest for testing the tumor shape classification model.</p>
        <p>It is our private dataset that contains 300 malignant tumors (123 Luminal-A, 107It is our private dataset that contains 300 malignant tumors (123 Luminal-A, 107</p>
        <p>Luminal-B, 33 Her-2 and 37 Basal-like) with their respective ground truth binary masks obtained by radiologists. The SSD detector and proposed cGAN segmentation model is trained and tested using 220 and 80 images, respectively. The duty of confidentiality and security measures were fully complied, in accordance with the current legislation on the Protection of Personal Data (article 7.1 of the Organic Law 15/1999, 13th of December).Luminal-B, 33 Her-2 and 37 Basal-like) with their respective ground truth binary masks obtained by radiologists. The SSD detector and proposed cGAN segmentation model is trained and tested using 220 and 80 images, respectively. The duty of confidentiality and security measures were fully complied, in accordance with the current legislation on the Protection of Personal Data (article 7.1 of the Organic Law 15/1999, 13th of December).</p>
        <p>The proposed method was implemented using python with 
            <rs type="software">Pytorch</rs>
            <rs type="version">3</rs> running on a 64-bit Ubuntu operating system using a 3.4 GHz Intel Core-i7 with 16 GB of RAM and 
            <rs type="software">Nvidia</rs> GTX 1070 GPU with 8 GB of video RAM. The SSD method yields the best results, with the highest TPR and lowest FPR. In turn, YOLO, Faster R-CNN and Dhungel et al. ( 2017) models have properly detected masses in the input mammograms, but with slightly worse quantitative results. Consequently, we have chosen the SSD model in order to locate tumors in mammograms.
        </p>
        <p>The proposed breast tumor segmentation method is compared with the state-of-theart methods and evaluated both quantitatively and qualitatively. For the quantitative analysis, segmentation accuracy is computed using Dice coefficient (F1 score) and Jaccard index (IoU). In turn, for the qualitative analysis, segmentation results with the their respective ground truth binary masks are compared visually.The proposed breast tumor segmentation method is compared with the state-of-theart methods and evaluated both quantitatively and qualitatively. For the quantitative analysis, segmentation accuracy is computed using Dice coefficient (F1 score) and Jaccard index (IoU). In turn, for the qualitative analysis, segmentation results with the their respective ground truth binary masks are compared visually.</p>
        <p>These experiments have been carried using three different framing of the tumor ROI: full mammogram, loose and tight frames (see Fig. 7). The ideal CAD system should be able to automatically segment the breast tumor from a full mammogram.These experiments have been carried using three different framing of the tumor ROI: full mammogram, loose and tight frames (see Fig. 7). The ideal CAD system should be able to automatically segment the breast tumor from a full mammogram.</p>
        <p>However, this is a very difficult task due to high similarity between gray level pixel distributions of healthy and tumorous tissue. The results depicted in Table 2 are divided in two sections, one for our private dataset and another for the INbreast dataset. Note that all models are trained on the private dataset, and then tested using our private dataset as well as the INbreast dataset without fine tuning. The results of all tested methods are after the post processing explained in sub-section 3.1.However, this is a very difficult task due to high similarity between gray level pixel distributions of healthy and tumorous tissue. The results depicted in Table 2 are divided in two sections, one for our private dataset and another for the INbreast dataset. Note that all models are trained on the private dataset, and then tested using our private dataset as well as the INbreast dataset without fine tuning. The results of all tested methods are after the post processing explained in sub-section 3.1.</p>
        <p>According to the results, our method outperforms the compared state-of-the-art methods in all cases except for the IoU computed on tight crops of our private dataset.According to the results, our method outperforms the compared state-of-the-art methods in all cases except for the IoU computed on tight crops of our private dataset.</p>
        <p>The SLSDeep approach yielded the best IoU (79.93%), whereas our method yielded the second best result (79.87%) with a very small difference of 0.06%. The postprocessing improved the results of our proposed model by 1% with the three framing inputs.The SLSDeep approach yielded the best IoU (79.93%), whereas our method yielded the second best result (79.87%) with a very small difference of 0.06%. The postprocessing improved the results of our proposed model by 1% with the three framing inputs.</p>
        <p>All models yielded their worst segmentation results with full mammograms compared to other framing inputs, which is logical taking into account the difficulties stated earlier in this section. Most of the models have obtained their best results for the tight frame crops except for CRFCNN and our proposal, which yielded their best results for loose frame crops. However, the good results for tight crops may be due to the imbalance of tumor/non-tumor pixels, since the former class is present in more than 90% of the image area. The learning can be biased towards this class, which makes rough solutions (almost everything is tumor) to provide very high ranks of performance. Loose frame crops, on the contrary, have a more balanced proportion of pixels for both classes, which makes them ideal to learn and evaluate the model on a realistic situation: it is more convenient for radiologists to provide a fast frame drawing around the breast tumor rather than a tight frame.All models yielded their worst segmentation results with full mammograms compared to other framing inputs, which is logical taking into account the difficulties stated earlier in this section. Most of the models have obtained their best results for the tight frame crops except for CRFCNN and our proposal, which yielded their best results for loose frame crops. However, the good results for tight crops may be due to the imbalance of tumor/non-tumor pixels, since the former class is present in more than 90% of the image area. The learning can be biased towards this class, which makes rough solutions (almost everything is tumor) to provide very high ranks of performance. Loose frame crops, on the contrary, have a more balanced proportion of pixels for both classes, which makes them ideal to learn and evaluate the model on a realistic situation: it is more convenient for radiologists to provide a fast frame drawing around the breast tumor rather than a tight frame.</p>
        <p>Comparing the general results for both datasets, most methods performed better onComparing the general results for both datasets, most methods performed better on</p>
        <p>INbreast rather than on private dataset with loose and tight framing. This effect can be explained by the fact that INbreast provides more detailed ground truths, which leads to better testing results, despite all network training has been conducted on our private dataset.INbreast rather than on private dataset with loose and tight framing. This effect can be explained by the fact that INbreast provides more detailed ground truths, which leads to better testing results, despite all network training has been conducted on our private dataset.</p>
        <p>In general, our proposal , with and without post-processing, has performed well Nevertheless, even in this second segmentation, there is a very high rate of black pixels (TN), which indicates that the model easily recognizes non-tumor areas.In general, our proposal , with and without post-processing, has performed well Nevertheless, even in this second segmentation, there is a very high rate of black pixels (TN), which indicates that the model easily recognizes non-tumor areas.</p>
        <p>In the loose frame segmentations (middle row), specially with example (2), the results contain very few FN and FP pixels. For example (1), a modest amount of green pixels indicate that our model expands the tumor segmentation beyond its respective ground truth. In the tight frame crops (bottom row), besides the green areas, our model also has missed some tumor areas i.e., the red pixels (FN). The mistaken areas (red and green) are mostly around the tumor borders, since these areas have a mixture of healthy and unhealthy cells. At the same time, the inner part of the tumor as well as the image regions outside of tumors are properly classified, which indicates the stability of our model. the loose frame cases (four top rows), our method clearly outperforms the rest for all tumors except for the second one, where the majority of models provided a similar degree of accuracy. In these four tumors, UNet-VGG16 and CRFCNN provided the worst results. Moreover, cGAN-ResNet101 also performed bad in the fourth example.In the loose frame segmentations (middle row), specially with example (2), the results contain very few FN and FP pixels. For example (1), a modest amount of green pixels indicate that our model expands the tumor segmentation beyond its respective ground truth. In the tight frame crops (bottom row), besides the green areas, our model also has missed some tumor areas i.e., the red pixels (FN). The mistaken areas (red and green) are mostly around the tumor borders, since these areas have a mixture of healthy and unhealthy cells. At the same time, the inner part of the tumor as well as the image regions outside of tumors are properly classified, which indicates the stability of our model. the loose frame cases (four top rows), our method clearly outperforms the rest for all tumors except for the second one, where the majority of models provided a similar degree of accuracy. In these four tumors, UNet-VGG16 and CRFCNN provided the worst results. Moreover, cGAN-ResNet101 also performed bad in the fourth example.</p>
        <p>For the tight frame cases (four bottom rows), our method also provides the lowest degrees of FN and FP compared to the rest of the models. Our cGAN and the cGAN-ResNet101 model yield irregular borders compared to FCN-ResNet101 and SLSDeep, since GAN models strive for higher accuracy on edges. However, in the third tight frame sample (seventh row), both cGAN-ResNet101 and our proposal generated an irregular border that slightly differs from the smooth ground truth border, which results in lower segmentation accuracy around the edges. Although the rest of the models generate smoother borders, the resulting segmentations may differ from the ground truth significantly.For the tight frame cases (four bottom rows), our method also provides the lowest degrees of FN and FP compared to the rest of the models. Our cGAN and the cGAN-ResNet101 model yield irregular borders compared to FCN-ResNet101 and SLSDeep, since GAN models strive for higher accuracy on edges. However, in the third tight frame sample (seventh row), both cGAN-ResNet101 and our proposal generated an irregular border that slightly differs from the smooth ground truth border, which results in lower segmentation accuracy around the edges. Although the rest of the models generate smoother borders, the resulting segmentations may differ from the ground truth significantly.</p>
        <p>From the experimental results, it can be concluded that the proposed breast tumor segmentation method is the most effective to date compared to the currently available state-of-the-art methods. However, our method needs a loose crop around the tumor to obtain a proper segmentation, which can be done by the SSD model. Our segmentation model contains about 13, 607, 043 parameters for tuning the generator part in the cGAN network. In addition, our method is fast in both training i.e., around 30 seconds per epoch (220 loose frames) and predicting, around 7 images per second. That is 7 to 8 times faster than the segmentation method proposed in Al-antari et al. (2018) and 10 to 15 times faster than the FCN model.From the experimental results, it can be concluded that the proposed breast tumor segmentation method is the most effective to date compared to the currently available state-of-the-art methods. However, our method needs a loose crop around the tumor to obtain a proper segmentation, which can be done by the SSD model. Our segmentation model contains about 13, 607, 043 parameters for tuning the generator part in the cGAN network. In addition, our method is fast in both training i.e., around 30 seconds per epoch (220 loose frames) and predicting, around 7 images per second. That is 7 to 8 times faster than the segmentation method proposed in Al-antari et al. (2018) and 10 to 15 times faster than the FCN model.</p>
        <p>For validating the tumor shape classification performance, we computed the confusion matrix and the overall classification accuracy on the test set of the DDSM dataset. This set contains 292 images divided into 126, 117, 31 and 18 for irregular, lobular, oval and round classes, respectively. However, The DDSM dataset does not have the ground truth binary masks for the breast tumor segmentation. Thus, we applied active contours Akram et al. (2015), which was also used in our previous work Singh et al. (2018b), to generate the ground 2015) also used active contours Lankton and Tannenbaum (2008) to generate the ground truths in a similar fashion. These ground truth masks are verified by expert radiologists of the hospital of Sant Joan de Reus. In addition, for reliable performance results, we used a stratified 5 fold cross validation with 50 epochs per fold.For validating the tumor shape classification performance, we computed the confusion matrix and the overall classification accuracy on the test set of the DDSM dataset. This set contains 292 images divided into 126, 117, 31 and 18 for irregular, lobular, oval and round classes, respectively. However, The DDSM dataset does not have the ground truth binary masks for the breast tumor segmentation. Thus, we applied active contours Akram et al. (2015), which was also used in our previous work Singh et al. (2018b), to generate the ground 2015) also used active contours Lankton and Tannenbaum (2008) to generate the ground truths in a similar fashion. These ground truth masks are verified by expert radiologists of the hospital of Sant Joan de Reus. In addition, for reliable performance results, we used a stratified 5 fold cross validation with 50 epochs per fold.</p>
        <p>In Table 3, the proposed method yielded around 73% of classification accuracy for irregular and lobular classes. This result is logical, since both lobular and irregular shapes have similar irregular boundaries. In turn, our model yielded classification accuracies of 84% and 89% for oval and round shape classes, respectively. since it is the training data accuracy. We provide this result only to show the low degree of overfitting achieved by our network. In turn, the proposed method fed with the masked ROI images provided 70% of overall accuracy. This experiment indicates that gray-level variations inside the segmented area is somehow confusing our shape classification network. In another hand, the multi-task CNN proposed in Kim et al. (2018) based on a pre-trained VGG-16 yielded the worst overall accuracy (66%), probably because the input mammograms are gray-scale images, while the VGG-16 network was trained on color-scale images. In addition, Fig. 11 shows ROC curve illustrating that our model attained AUC about 0.8.In Table 3, the proposed method yielded around 73% of classification accuracy for irregular and lobular classes. This result is logical, since both lobular and irregular shapes have similar irregular boundaries. In turn, our model yielded classification accuracies of 84% and 89% for oval and round shape classes, respectively. since it is the training data accuracy. We provide this result only to show the low degree of overfitting achieved by our network. In turn, the proposed method fed with the masked ROI images provided 70% of overall accuracy. This experiment indicates that gray-level variations inside the segmented area is somehow confusing our shape classification network. In another hand, the multi-task CNN proposed in Kim et al. (2018) based on a pre-trained VGG-16 yielded the worst overall accuracy (66%), probably because the input mammograms are gray-scale images, while the VGG-16 network was trained on color-scale images. In addition, Fig. 11 shows ROC curve illustrating that our model attained AUC about 0.8.</p>
        <p>Furthermore, the proposed shape descriptor contains 767,684 parameters, which can be trained in less than a second per epoch, and predict in about 6 milliseconds per image.Furthermore, the proposed shape descriptor contains 767,684 parameters, which can be trained in less than a second per epoch, and predict in about 6 milliseconds per image.</p>
        <p>Tumor shape could play an important role to predict the breast cancer molecular subtypes Tamaki et al. (2011). Thus, we have computed the correlation between breast cancer molecular subtypes classes of our in-house private dataset with the four shape classes. As shown in Basal-like 5 10 9 13 37 lobular shape classes. In turn, oval and round tumors give indications to the Her-2 and Basal-like samples, (i.e., 23/33 and 22/37 for Her-2 and Basal-like, respectively).Tumor shape could play an important role to predict the breast cancer molecular subtypes Tamaki et al. (2011). Thus, we have computed the correlation between breast cancer molecular subtypes classes of our in-house private dataset with the four shape classes. As shown in Basal-like 5 10 9 13 37 lobular shape classes. In turn, oval and round tumors give indications to the Her-2 and Basal-like samples, (i.e., 23/33 and 22/37 for Her-2 and Basal-like, respectively).</p>
        <p>Moreover, some images related to Basal-like are moderately assigned to the lobular class. Afterwards, from the visual inspection, if the tumor shape is irregular or lobular then radiologist can suspect that it belongs to the Luminal group. In turn, if the tumor shape is round or oval then it is more probable that the tumor is a Her-2 or Basallike Tamaki et al. (2011). Therefore, this study shows the importance of tumor shape, which can be considered as a key feature to distinguish between different malignancies of breast cancer.Moreover, some images related to Basal-like are moderately assigned to the lobular class. Afterwards, from the visual inspection, if the tumor shape is irregular or lobular then radiologist can suspect that it belongs to the Luminal group. In turn, if the tumor shape is round or oval then it is more probable that the tumor is a Her-2 or Basallike Tamaki et al. (2011). Therefore, this study shows the importance of tumor shape, which can be considered as a key feature to distinguish between different malignancies of breast cancer.</p>
        <p>For the segmentation stage, our model has only one limitation. If there are two tumors (i.e., one is with complete shape and the other is incomplete) in the loose framing, the proposed segmentation methods will be able to properly segment the complete tu- To classify the tumor shape, we depend only on the DDSM dataset to train our model, since it is the only public dataset that has the shape classification information.For the segmentation stage, our model has only one limitation. If there are two tumors (i.e., one is with complete shape and the other is incomplete) in the loose framing, the proposed segmentation methods will be able to properly segment the complete tu- To classify the tumor shape, we depend only on the DDSM dataset to train our model, since it is the only public dataset that has the shape classification information.</p>
        <p>Thus, more databases containing more samples are required to improve the classification accuracy of four shape classes.Thus, more databases containing more samples are required to improve the classification accuracy of four shape classes.</p>
        <p>To study the molecular subtypes of breast cancer, Her-2 and Basal-like classes have less samples compared to the other two classes, Luminal-A and Luminal-B. Indeed, we used a weighted loss function to train our shape classification model in order to make a balance between the four classes. However, we anticipate that, by increasing the samples related to the Her-2 and Basal-like classes, we will improve the prediction of molecular subtypes from tumor shape information.To study the molecular subtypes of breast cancer, Her-2 and Basal-like classes have less samples compared to the other two classes, Luminal-A and Luminal-B. Indeed, we used a weighted loss function to train our shape classification model in order to make a balance between the four classes. However, we anticipate that, by increasing the samples related to the Her-2 and Basal-like classes, we will improve the prediction of molecular subtypes from tumor shape information.</p>
        <p>In this paper, we propose a two stage breast tumor segmentation and classification method, which first segments the breast tumor ROI using a cGAN and then classify its binary mask using a CNN based shape descriptor.In this paper, we propose a two stage breast tumor segmentation and classification method, which first segments the breast tumor ROI using a cGAN and then classify its binary mask using a CNN based shape descriptor.</p>
        <p>The segmentation results reveal the importance of the adversarial network in the optimization of the generative network. cGAN-ResNet101 shows an improvement of about 1% to 3% in both Dice and IoU metrics in comparison to the other non-GAN methods. In turn, the proposed method yields an increment of about 10% over the results of cGAN-ResNet101 by training our model from scratch, and replacing the L1norm with the dice loss using loose frame crop on the given datasets. The breast tumor segmentation from full-mammograms yields low segmentation accuracy for all models including the proposed cGAN. For the tight frame crop, the proposed cGAN yields similar or better segmentation accuracy compared to the other methods.The segmentation results reveal the importance of the adversarial network in the optimization of the generative network. cGAN-ResNet101 shows an improvement of about 1% to 3% in both Dice and IoU metrics in comparison to the other non-GAN methods. In turn, the proposed method yields an increment of about 10% over the results of cGAN-ResNet101 by training our model from scratch, and replacing the L1norm with the dice loss using loose frame crop on the given datasets. The breast tumor segmentation from full-mammograms yields low segmentation accuracy for all models including the proposed cGAN. For the tight frame crop, the proposed cGAN yields similar or better segmentation accuracy compared to the other methods.</p>
        <p>The classification results show that our second stage properly infers the tumor shape from the binary mask of the breast tumor, which was obtained from the first stage (cGAN segmentation). Hence, we have empirically shown that our CNN is focusing its learning on the morphological structure of the breast tumor, while the rest of approaches (Kisilev et al. (2015), Kim et al. (2018), Kisilev et al. (2016), Ren et al. (2015)) rely on the original pixel variations of the input mammogram to make the same inference. Moreover, in Al-antari et al. (2018) they used a hybrid strategy in which they include the pixel variability within the mask of breast tumor region to retain the intensity and texture information. However, the higher performance obtained by our method supports our initial idea that the second stage CNN can reliably recognize the tumorThe classification results show that our second stage properly infers the tumor shape from the binary mask of the breast tumor, which was obtained from the first stage (cGAN segmentation). Hence, we have empirically shown that our CNN is focusing its learning on the morphological structure of the breast tumor, while the rest of approaches (Kisilev et al. (2015), Kim et al. (2018), Kisilev et al. (2016), Ren et al. (2015)) rely on the original pixel variations of the input mammogram to make the same inference. Moreover, in Al-antari et al. (2018) they used a hybrid strategy in which they include the pixel variability within the mask of breast tumor region to retain the intensity and texture information. However, the higher performance obtained by our method supports our initial idea that the second stage CNN can reliably recognize the tumor</p>
        <p>and 82/107 for Luminal-A and -B, respectively) are mostly assigned to irregular andand 82/107 for Luminal-A and -B, respectively) are mostly assigned to irregular and</p>
        <p>https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSMhttps://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM</p>
        <p>This work was supported by the Spanish project: DPI2016-77415-R and the Beatriu de Pinós program: 2016-BP-00063.This work was supported by the Spanish project: DPI2016-77415-R and the Beatriu de Pinós program: 2016-BP-00063.</p>
        <p>shape based only on morphological information. Furthermore, this paper provided a study of correlation between the tumor shape and the molecular subtypes of the breast cancer. Most samples of the Luminal-A and -B group are assigned to irregular shapes. In turn, the majority of Her-2 and Basal-like samples are assigned to regular shapes (e.g., oval and round shapes). That gives an indication that the tumor shape can be considered for inferring the molecular subtype of the tumor.shape based only on morphological information. Furthermore, this paper provided a study of correlation between the tumor shape and the molecular subtypes of the breast cancer. Most samples of the Luminal-A and -B group are assigned to irregular shapes. In turn, the majority of Her-2 and Basal-like samples are assigned to regular shapes (e.g., oval and round shapes). That gives an indication that the tumor shape can be considered for inferring the molecular subtype of the tumor.</p>
        <p>Future work aims at refining our multi-stage framework to detect other breast tumor features (i.e., margin type, micro-calcifications), which will be integrated into a more comprehensive diagnostic to compute the degree of malignancy of the breast tumors.Future work aims at refining our multi-stage framework to detect other breast tumor features (i.e., margin type, micro-calcifications), which will be integrated into a more comprehensive diagnostic to compute the degree of malignancy of the breast tumors.</p>
        <p>The authors declare that there is no conflict of interest.The authors declare that there is no conflict of interest.</p>
    </text>
</tei>

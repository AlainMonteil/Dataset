<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:22+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Publisher's PDF, also known as Version of recordPublisher's PDF, also known as Version of record</p>
        <p>• A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website.• A submitted manuscript is the version of the article upon submission and before peer-review. There can be important differences between the submitted version and the official published version of record. People interested in the research are advised to contact the author for the final version of the publication, or visit the DOI to the publisher's website.</p>
        <p>• The final author version and the galley proof are versions of the publication after peer review.• The final author version and the galley proof are versions of the publication after peer review.</p>
        <p>• The final published version features the final layout of the paper including the volume, issue and page numbers.• The final published version features the final layout of the paper including the volume, issue and page numbers.</p>
        <p>Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.</p>
        <p>• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal.• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal.</p>
        <p>Advances in educational technologies and the rise of massive open online courses (MOOCs) have generated increased interest in previously non-feasible approaches to exploring learner behavior data to provide process-oriented feedback (Sedrakyan, 2016). Examining how learners interact within virtual learning environments (i.e., with each other, instructors, the environment) provides opportunities to reveal where things are progressing well and where problems may possibly occur. Using this information, process-oriented feedback can be generated that can help teachers and learners enhance engagement and achievement (Ga sevi c, Dawson, Rogers, &amp; Gasevic, 2016). Such feedback is presented in the form of visualizations in several teacher-and learner-oriented dashboards (Bodily &amp; Verbert, 2017;Dyckhoff, Zielke, Bültmann, Chatti, &amp; Schroeder, 2012;Hu, Lo, &amp; Shih, 2014;Mottus, Graf, &amp; Chen, 2015). Dashboards are instruments intended to improve decision-making by amplifying or directing cognition and capitalizing on human perceptual capabilities (Yigitbasioglu &amp; Velcu, 2012).Advances in educational technologies and the rise of massive open online courses (MOOCs) have generated increased interest in previously non-feasible approaches to exploring learner behavior data to provide process-oriented feedback (Sedrakyan, 2016). Examining how learners interact within virtual learning environments (i.e., with each other, instructors, the environment) provides opportunities to reveal where things are progressing well and where problems may possibly occur. Using this information, process-oriented feedback can be generated that can help teachers and learners enhance engagement and achievement (Ga sevi c, Dawson, Rogers, &amp; Gasevic, 2016). Such feedback is presented in the form of visualizations in several teacher-and learner-oriented dashboards (Bodily &amp; Verbert, 2017;Dyckhoff, Zielke, Bültmann, Chatti, &amp; Schroeder, 2012;Hu, Lo, &amp; Shih, 2014;Mottus, Graf, &amp; Chen, 2015). Dashboards are instruments intended to improve decision-making by amplifying or directing cognition and capitalizing on human perceptual capabilities (Yigitbasioglu &amp; Velcu, 2012).</p>
        <p>Despite their popularity and the proliferation of solution providers, little is known about design aspects, such as the typology of feedback relevant in a learning context (Sedrakyan, J€ arvel€ a, &amp; Kirschner, 2016;Yigitbasioglu &amp; Velcu, 2012). The field lacks knowledge on the type of feedback that works best for different learning goals and learners to provide targeted help.Despite their popularity and the proliferation of solution providers, little is known about design aspects, such as the typology of feedback relevant in a learning context (Sedrakyan, J€ arvel€ a, &amp; Kirschner, 2016;Yigitbasioglu &amp; Velcu, 2012). The field lacks knowledge on the type of feedback that works best for different learning goals and learners to provide targeted help.</p>
        <p>According to Saywer (2014), most research on educational dashboards lacks both theoretical support from recent advancements in the learning sciences and an evidence-informed foundation for choosing the data that can assist in observing and assessing learning processes to identify the feedback needs of learners/ teachers. As a result, instead of being useful, these instruments can be harmful. For example, most current Learning Analytics Dashboards (LADs) are based only on learner performance indicators (e.g., where a learner is doing well/poor, how much content has been completed, how much time was spent, how learners' progress compares to teacher specified and/or peer scores) that do not seem to contribute to learners' motivation and engagement (Blumenfeld, 1992;Elliot &amp; Harackiewicz, 1996). Furthermore, recent research on the effectiveness of learning analytics tools reveals that when using performance-oriented dashboards, learner mastery orientation decreases (Lonn, Aguilar, &amp; Teasley, 2015). This suggests that such goal orientations need to be carefully considered in the design of any intervention, as the resulting approaches and tools can affect students' interpretations of their data and subsequent academic success (Lonn et al., 2015). For instance, regarding learning goals, students can orient themselves toward either mastery-focused or performance-focused goals. Students with mastery goals are typically interested in learning as an end itself (e.g., "One of my goals in class is to learn and understand as much as I can.") while students with performance goals are typically interested in learning as means of demonstrating their ability or competence (e.g., "I want to do better than other students in my class"; Dweck &amp; Leggett, 1988).According to Saywer (2014), most research on educational dashboards lacks both theoretical support from recent advancements in the learning sciences and an evidence-informed foundation for choosing the data that can assist in observing and assessing learning processes to identify the feedback needs of learners/ teachers. As a result, instead of being useful, these instruments can be harmful. For example, most current Learning Analytics Dashboards (LADs) are based only on learner performance indicators (e.g., where a learner is doing well/poor, how much content has been completed, how much time was spent, how learners' progress compares to teacher specified and/or peer scores) that do not seem to contribute to learners' motivation and engagement (Blumenfeld, 1992;Elliot &amp; Harackiewicz, 1996). Furthermore, recent research on the effectiveness of learning analytics tools reveals that when using performance-oriented dashboards, learner mastery orientation decreases (Lonn, Aguilar, &amp; Teasley, 2015). This suggests that such goal orientations need to be carefully considered in the design of any intervention, as the resulting approaches and tools can affect students' interpretations of their data and subsequent academic success (Lonn et al., 2015). For instance, regarding learning goals, students can orient themselves toward either mastery-focused or performance-focused goals. Students with mastery goals are typically interested in learning as an end itself (e.g., "One of my goals in class is to learn and understand as much as I can.") while students with performance goals are typically interested in learning as means of demonstrating their ability or competence (e.g., "I want to do better than other students in my class"; Dweck &amp; Leggett, 1988).</p>
        <p>Regarding the analysis approach, statistical and data-mining approaches prevail in the context of LADs. Existing LAD instruments mostly target performance visualization often in the form of outcome feedback (e.g., "How do I perform?) rather than process-oriented feedback ("How can I do better?"; e.g., by looking for inefficient processes, e.g. sequential aspects of learning, and thus neglecting the procedural aspects of learning). Thus to provide relevant feedback, it is important to know if low performance is affected by a misunderstanding of a problem, task, or concept or rather a procedural aspect of learning (e.g., not sufficient effort put in verifying a solution) thus distinguishing whether there is a need for a cognitive or behavioral type of feedback (Sedrakyan &amp; Snoeck, 2017;Sedrakyan, 2016). Furthermore, most visual representations in the context of LADs are limited to graphs, charts, or other diagrams without providing support mechanisms to facilitate their interpretation (Park &amp; Jo, 2015). Empirical studies show that behavioral change and improved performance were observed when supporting learners in the interpretation of visualizations (Sedrakyan, 2016).Regarding the analysis approach, statistical and data-mining approaches prevail in the context of LADs. Existing LAD instruments mostly target performance visualization often in the form of outcome feedback (e.g., "How do I perform?) rather than process-oriented feedback ("How can I do better?"; e.g., by looking for inefficient processes, e.g. sequential aspects of learning, and thus neglecting the procedural aspects of learning). Thus to provide relevant feedback, it is important to know if low performance is affected by a misunderstanding of a problem, task, or concept or rather a procedural aspect of learning (e.g., not sufficient effort put in verifying a solution) thus distinguishing whether there is a need for a cognitive or behavioral type of feedback (Sedrakyan &amp; Snoeck, 2017;Sedrakyan, 2016). Furthermore, most visual representations in the context of LADs are limited to graphs, charts, or other diagrams without providing support mechanisms to facilitate their interpretation (Park &amp; Jo, 2015). Empirical studies show that behavioral change and improved performance were observed when supporting learners in the interpretation of visualizations (Sedrakyan, 2016).</p>
        <p>Also, as dashboards are tools to be used by the teacher and/or the learner, acceptance of this tool by end-users is yet another factor that can interfere with the achievement of their intended goal (Davis, 1989) and thus affect learner performance. This suggests that most relevant constructs from established technology acceptance models need to be considered in the design of dashboards to allow built-in mechanisms for capturing end-user perceptions (feedback on feedback). User acceptance can be important to ensure the effectiveness and continuous refinements need of dashboard feedback (e.g., the same type of feedback may not be relevant for the same problem as learners' knowledge and expertise level changes during time), and ultimately determine its intended utility.Also, as dashboards are tools to be used by the teacher and/or the learner, acceptance of this tool by end-users is yet another factor that can interfere with the achievement of their intended goal (Davis, 1989) and thus affect learner performance. This suggests that most relevant constructs from established technology acceptance models need to be considered in the design of dashboards to allow built-in mechanisms for capturing end-user perceptions (feedback on feedback). User acceptance can be important to ensure the effectiveness and continuous refinements need of dashboard feedback (e.g., the same type of feedback may not be relevant for the same problem as learners' knowledge and expertise level changes during time), and ultimately determine its intended utility.</p>
        <p>Common to all feedback LADs presented in the literature on dashboards is the lack of theoretical support grounded in the learning sciences (Sedrakyan et al., 2016) and research on feedback and underlying mechanisms of learning processes. Learning science (or the learning sciences) is an interdisciplinary field that works to further scientific understanding of learning as well as to engage in the design and implementation of learning innovations, and the improvement of instructional methodologies (Carr-Chellman, 2004). Research in the learning science traditionally focuses on cognitive-psychological, social-psychological, and culturalpsychological foundations of human learning, as well as on the design of learning environments often following design-based research methods (Carr-Chellman, 2004). Major contributing fields include sociocognitive science (see also the sections on Typology of Feedback and Regulation of Learning), computer science, educational psychology among others. Learning sciences study learning as it happens in real-world situations and how to better facilitate learning in designed environments e in school, online, in the workplace, at home, and in informal environments (Carr-Chellman, 2004).Common to all feedback LADs presented in the literature on dashboards is the lack of theoretical support grounded in the learning sciences (Sedrakyan et al., 2016) and research on feedback and underlying mechanisms of learning processes. Learning science (or the learning sciences) is an interdisciplinary field that works to further scientific understanding of learning as well as to engage in the design and implementation of learning innovations, and the improvement of instructional methodologies (Carr-Chellman, 2004). Research in the learning science traditionally focuses on cognitive-psychological, social-psychological, and culturalpsychological foundations of human learning, as well as on the design of learning environments often following design-based research methods (Carr-Chellman, 2004). Major contributing fields include sociocognitive science (see also the sections on Typology of Feedback and Regulation of Learning), computer science, educational psychology among others. Learning sciences study learning as it happens in real-world situations and how to better facilitate learning in designed environments e in school, online, in the workplace, at home, and in informal environments (Carr-Chellman, 2004).</p>
        <p>In this work, we complement the engineering approach with theories in the learning sciences to design a novel artefact, namely, a process-oriented feedback model in the context of LADs. The term process-oriented feedback refers to early feedback opportunities that can be achieved during a learning process before a formal assessment of its outcome and feedback by a teacher (i.e., teacher intervention) is given (Sedrakyan, 2016). As research on feedback is closely intertwined with the concept of learning regulation, we first review the regulatory mechanisms underlying learning processes. For instance, self-regulated learning (SRL) theory explains the core aspects that can facilitate learning processes such as setting goals, planning, applying strategies, monitoring progress, and reflecting (Zimmerman, 1990). Next, we fine-tune the idea of feedback by distinguishing between learning goals (e.g., teacher specified goals) and goal orientations (e.g., mastery, performance, approach avoidance) that allows the consideration of the personal needs of learners, and efficiency and effectiveness of learning that contributes to determining timeliness aspects. We then review how feedback needs to be implemented in dashboards based on research on feedback typology as defined by cognitive, sociocognitive and behavioral theories (Bransford, Brown, &amp; Cocking, 2000;Tomic, 1993). The design artefact presented in this article uses a conceptual model that visualizes the relationships between dashboard design and the learning science concepts to provide process-oriented feedback that support regulation of learning. It has to be noted that the goal of the work is not to propose a specific feedback design, but rather a conceptual guide for the choice of concepts for designing information systems and, thus, also helping understanding future data needs as basis for educational dashboard feedback. We provide preliminary answers to questions such as:In this work, we complement the engineering approach with theories in the learning sciences to design a novel artefact, namely, a process-oriented feedback model in the context of LADs. The term process-oriented feedback refers to early feedback opportunities that can be achieved during a learning process before a formal assessment of its outcome and feedback by a teacher (i.e., teacher intervention) is given (Sedrakyan, 2016). As research on feedback is closely intertwined with the concept of learning regulation, we first review the regulatory mechanisms underlying learning processes. For instance, self-regulated learning (SRL) theory explains the core aspects that can facilitate learning processes such as setting goals, planning, applying strategies, monitoring progress, and reflecting (Zimmerman, 1990). Next, we fine-tune the idea of feedback by distinguishing between learning goals (e.g., teacher specified goals) and goal orientations (e.g., mastery, performance, approach avoidance) that allows the consideration of the personal needs of learners, and efficiency and effectiveness of learning that contributes to determining timeliness aspects. We then review how feedback needs to be implemented in dashboards based on research on feedback typology as defined by cognitive, sociocognitive and behavioral theories (Bransford, Brown, &amp; Cocking, 2000;Tomic, 1993). The design artefact presented in this article uses a conceptual model that visualizes the relationships between dashboard design and the learning science concepts to provide process-oriented feedback that support regulation of learning. It has to be noted that the goal of the work is not to propose a specific feedback design, but rather a conceptual guide for the choice of concepts for designing information systems and, thus, also helping understanding future data needs as basis for educational dashboard feedback. We provide preliminary answers to questions such as:</p>
        <p>-What are the concepts we need to consider for the design of LAD feedback to allow the observation of learning processes with respect to potential feedback (i.e., regulation) needs for different learning goals? (Section 3) -What artefacts will enable the capture of data that will allow measurement of those concepts during a learning process? (Section 4) -How can such learning-process data be mapped to end-user (learner and/or teacher) feedback to improve the regulation of learning processes? (in particular)-What are the concepts we need to consider for the design of LAD feedback to allow the observation of learning processes with respect to potential feedback (i.e., regulation) needs for different learning goals? (Section 3) -What artefacts will enable the capture of data that will allow measurement of those concepts during a learning process? (Section 4) -How can such learning-process data be mapped to end-user (learner and/or teacher) feedback to improve the regulation of learning processes? (in particular)</p>
        <p>The paper is structured as follows. First a review is given of earlier studies. Then, design implications are given for implementation of those results. Third, a practical case example is given which attempts to implement these ideas by also introducing analytics/visualization techniques based on empirical evidence from earlier research that successfully tested these techniques in various learning contexts. It concludes with a discussion and general scientific contributions of the work, as well as suggestions for possible future work directions.The paper is structured as follows. First a review is given of earlier studies. Then, design implications are given for implementation of those results. Third, a practical case example is given which attempts to implement these ideas by also introducing analytics/visualization techniques based on empirical evidence from earlier research that successfully tested these techniques in various learning contexts. It concludes with a discussion and general scientific contributions of the work, as well as suggestions for possible future work directions.</p>
        <p>In this work, we follow the principles of Design Science in Information Systems (IS) research, which follows an iterative approach for designing/building and evaluating innovative artefacts for problem solving (Hevner, March, Park, &amp; Ram, 2004). According to Hevner et al. (2004), two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. In design science research the initial artefact represents a simplified version for addressing a problem using a subset of theoretical constructs (and variables) of interest (Kerlinger, 1979), serving as a starting point. Progress is made iteratively as the scope of the design problem is refined/ expanded by means of (re)evaluation loops. In the scope of this paper the research objective concerns with the first two principles of Design Science Research 1. Design of an innovative, purposeful artifact (Guideline 1) for a specified problem domain, i.e. its relevance (Guideline 2). The objective of research in information systems is to acquire knowledge and understanding that enable the development and implementation of technology-based solutions to heretofore unsolved and important problem by allowing to (re-) evaluate the designed artifacts (second principle of Design Science in IS). In the context of this work, the problem concerns making learning processes scientifically observable with regard to processoriented feedback needs. We complement the engineering approach with theories in the learning sciences to design our artefact, namely, a process-oriented feedback model in the context of LADs for designing and building information systems as a basis for educational dashboard feedback. The conceptual modeling approach is used to represent the constructs of interest derived from basic concepts and definitions on learning processes and feedback, and visualize the relations among them (Creswell, 1994). The model aims to address the gap between LADs and learning theories serving as both a basis for building information systems and thus also helping to understand future data needs, and an initial platform to guide future research in this domain. The details of implementation mechanisms (e.g. a prototype design), the evaluation and expansion of the design following the second principle of design science research constitutes a further research line.In this work, we follow the principles of Design Science in Information Systems (IS) research, which follows an iterative approach for designing/building and evaluating innovative artefacts for problem solving (Hevner, March, Park, &amp; Ram, 2004). According to Hevner et al. (2004), two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. In design science research the initial artefact represents a simplified version for addressing a problem using a subset of theoretical constructs (and variables) of interest (Kerlinger, 1979), serving as a starting point. Progress is made iteratively as the scope of the design problem is refined/ expanded by means of (re)evaluation loops. In the scope of this paper the research objective concerns with the first two principles of Design Science Research 1. Design of an innovative, purposeful artifact (Guideline 1) for a specified problem domain, i.e. its relevance (Guideline 2). The objective of research in information systems is to acquire knowledge and understanding that enable the development and implementation of technology-based solutions to heretofore unsolved and important problem by allowing to (re-) evaluate the designed artifacts (second principle of Design Science in IS). In the context of this work, the problem concerns making learning processes scientifically observable with regard to processoriented feedback needs. We complement the engineering approach with theories in the learning sciences to design our artefact, namely, a process-oriented feedback model in the context of LADs for designing and building information systems as a basis for educational dashboard feedback. The conceptual modeling approach is used to represent the constructs of interest derived from basic concepts and definitions on learning processes and feedback, and visualize the relations among them (Creswell, 1994). The model aims to address the gap between LADs and learning theories serving as both a basis for building information systems and thus also helping to understand future data needs, and an initial platform to guide future research in this domain. The details of implementation mechanisms (e.g. a prototype design), the evaluation and expansion of the design following the second principle of design science research constitutes a further research line.</p>
        <p>A number of publications discuss the benefits of using dashboards in education for novel feedback opportunities that may enhance learning (e.g. Bodily &amp; Verbert, 2017;Duval et al., 2012;Dyckhoff et al., 2012;Hu et al., 2014;Mottus et al., 2015;Verbert, Duval, Klerkx, Govaerts, &amp; Santos, 2013;Verbert et al., 2014). Dashboards developed for diverse purposes supporting teachers, students or both throughout a synergetic approach combine design principles and technologies (Park &amp; Jo, 2015) to support improved retention or engagement, increased social behavior or recommendations of courses and resources (Bodily &amp; Verbert, 2017) both for individual and group learning purposes (Upton &amp; Kay, 2009).A number of publications discuss the benefits of using dashboards in education for novel feedback opportunities that may enhance learning (e.g. Bodily &amp; Verbert, 2017;Duval et al., 2012;Dyckhoff et al., 2012;Hu et al., 2014;Mottus et al., 2015;Verbert, Duval, Klerkx, Govaerts, &amp; Santos, 2013;Verbert et al., 2014). Dashboards developed for diverse purposes supporting teachers, students or both throughout a synergetic approach combine design principles and technologies (Park &amp; Jo, 2015) to support improved retention or engagement, increased social behavior or recommendations of courses and resources (Bodily &amp; Verbert, 2017) both for individual and group learning purposes (Upton &amp; Kay, 2009).</p>
        <p>In terms of the intended goals of dashboards, most studies limit themselves to student performance outcomes through selfreflection, awareness, and self-assessment (Bodily &amp; Verbert, 2017) positioning learners in comparison with teacher specified and/or peer performance. Several LADs target at delivering cognitive feedback in a limited context (e.g., mathematical problems or formal assessment of writing drafts (Ferguson et al., 2016)).In terms of the intended goals of dashboards, most studies limit themselves to student performance outcomes through selfreflection, awareness, and self-assessment (Bodily &amp; Verbert, 2017) positioning learners in comparison with teacher specified and/or peer performance. Several LADs target at delivering cognitive feedback in a limited context (e.g., mathematical problems or formal assessment of writing drafts (Ferguson et al., 2016)).</p>
        <p>In terms of data collection, most studies are limited to logs that address university settings (Schwendimann et al., 2016). In terms of analysis approach in the context of LADs feedback, data mining approaches prevail targeting performance visualization and outcome feedback ("How do I perform?), rather than process oriented feedback ("How can I do better?" e.g. by looking for inefficient procedural, sequential aspects of learning) (Sedrakyan et al., 2016).In terms of data collection, most studies are limited to logs that address university settings (Schwendimann et al., 2016). In terms of analysis approach in the context of LADs feedback, data mining approaches prevail targeting performance visualization and outcome feedback ("How do I perform?), rather than process oriented feedback ("How can I do better?" e.g. by looking for inefficient procedural, sequential aspects of learning) (Sedrakyan et al., 2016).</p>
        <p>Such representations are in addition limited to graphs, charts or other diagrams without providing support mechanisms to facilitate their interpretation (Bodily &amp; Verbert, 2017;Park &amp; Jo, 2015), though previous studies show that behavior change and improved performance was observed when supporting a student in interpretation of visualizations (Sedrakyan &amp; Snoeck, 2014, 2015, 2016;Sedrakyan, 2016;Sedrakyan, Poelmans, &amp; Snoeck, 2017;Sedrakyan, Snoeck &amp; Poelmans, 2014).Such representations are in addition limited to graphs, charts or other diagrams without providing support mechanisms to facilitate their interpretation (Bodily &amp; Verbert, 2017;Park &amp; Jo, 2015), though previous studies show that behavior change and improved performance was observed when supporting a student in interpretation of visualizations (Sedrakyan &amp; Snoeck, 2014, 2015, 2016;Sedrakyan, 2016;Sedrakyan, Poelmans, &amp; Snoeck, 2017;Sedrakyan, Snoeck &amp; Poelmans, 2014).</p>
        <p>Another issue with current tools is finding evidence for their formal validation, e.g. whether the tools fulfill their intended purpose, such as having a positive impact on learning; encouraging more efficient learning; or more effective learning (Ferguson et al., 2016), which suggests that LADs should consider built-in mechanisms to allow tracking effects from such interventions. Furthermore, recent studies on the effectiveness of existing dashboards suggest that learner goal orientations (Lonn et al., 2015) and differences in achievement levels (Park &amp; Jo, 2015) need to be carefully considered in the design of any intervention, as the resulting approaches and tools can affect students' interpretations of their data and subsequent academic success.Another issue with current tools is finding evidence for their formal validation, e.g. whether the tools fulfill their intended purpose, such as having a positive impact on learning; encouraging more efficient learning; or more effective learning (Ferguson et al., 2016), which suggests that LADs should consider built-in mechanisms to allow tracking effects from such interventions. Furthermore, recent studies on the effectiveness of existing dashboards suggest that learner goal orientations (Lonn et al., 2015) and differences in achievement levels (Park &amp; Jo, 2015) need to be carefully considered in the design of any intervention, as the resulting approaches and tools can affect students' interpretations of their data and subsequent academic success.</p>
        <p>Thus, common to all of these feedback LADs is the lack of theoretical support grounded in the learning sciences (Sedrakyan et al., 2016).Thus, common to all of these feedback LADs is the lack of theoretical support grounded in the learning sciences (Sedrakyan et al., 2016).</p>
        <p>This section provides a preliminary discussion on what concepts we need to support in the design of dashboard feedback to make it possible to observe learning processes with respect to potential feedback (i.e., regulation in learning) needs of different learners for different learning goals.This section provides a preliminary discussion on what concepts we need to support in the design of dashboard feedback to make it possible to observe learning processes with respect to potential feedback (i.e., regulation in learning) needs of different learners for different learning goals.</p>
        <p>The regulation of learning and performance is central to research on feedback (Butler &amp; Winne, 1995). It is a goal-directed intentional and metacognitive activity in which learners take strategic control of their actions (behavior), thinking (cognitive), and beliefs (motivation, emotions) toward the completion of a task (Zimmerman &amp; Schunk, 2011). Research has shown that successful learners use a repertoire of strategies to guide and enhance their learning process e cognitive, behavioral and motivational e toward completing academic tasks (Zimmerman &amp; Schunk, 2011). In practice, self-regulated and strategic learning involves experimenting with, and learning about, effective strategies for regulating aspects of their own, peers', and groups' shared learning processes (Winne, Hadwin, &amp; Perry, 2013), including planning, setting goals, organizing, monitoring, and adapting.The regulation of learning and performance is central to research on feedback (Butler &amp; Winne, 1995). It is a goal-directed intentional and metacognitive activity in which learners take strategic control of their actions (behavior), thinking (cognitive), and beliefs (motivation, emotions) toward the completion of a task (Zimmerman &amp; Schunk, 2011). Research has shown that successful learners use a repertoire of strategies to guide and enhance their learning process e cognitive, behavioral and motivational e toward completing academic tasks (Zimmerman &amp; Schunk, 2011). In practice, self-regulated and strategic learning involves experimenting with, and learning about, effective strategies for regulating aspects of their own, peers', and groups' shared learning processes (Winne, Hadwin, &amp; Perry, 2013), including planning, setting goals, organizing, monitoring, and adapting.</p>
        <p>Although self-regulation concerns individual adaptation, feedback mechanisms that the environment or peers provide can also be considered as a form of co-regulated learning (Isoh€ at€ al€ a, J€ arvenoja, &amp; J€ arvel€ a, 2017). Co-regulated learning (CoRL) occurs when learners' regulatory activities are guided, supported, shaped, or constrained by others, such as peers or teachers, and the social system, including the learning environment (Hadwin, J€ arvel€ a, &amp; Miller, 2017). CoRL can take at least two forms. In the first form, CoRL occurs when learners are prompted to set learning goals. In the second form, CoRL occurs when a social system gradually influences and shapes an individual's SRL (e.g., when learning behavior is affected by comparing one's own behavior with that of one's peers). In summary, tracking regulation patterns during a learning process can be helpful in determining possible intervention needs during a learning process (e.g., "Where is regulation effort needed? Is it sufficient? Is the expected outcome reached? Does the learner's effort need to be redirected, or does he or she need external help?"). To support regulation optimally in the context of LAD feedback, we also need to review the definition and typology of feedback suitable for different goals.Although self-regulation concerns individual adaptation, feedback mechanisms that the environment or peers provide can also be considered as a form of co-regulated learning (Isoh€ at€ al€ a, J€ arvenoja, &amp; J€ arvel€ a, 2017). Co-regulated learning (CoRL) occurs when learners' regulatory activities are guided, supported, shaped, or constrained by others, such as peers or teachers, and the social system, including the learning environment (Hadwin, J€ arvel€ a, &amp; Miller, 2017). CoRL can take at least two forms. In the first form, CoRL occurs when learners are prompted to set learning goals. In the second form, CoRL occurs when a social system gradually influences and shapes an individual's SRL (e.g., when learning behavior is affected by comparing one's own behavior with that of one's peers). In summary, tracking regulation patterns during a learning process can be helpful in determining possible intervention needs during a learning process (e.g., "Where is regulation effort needed? Is it sufficient? Is the expected outcome reached? Does the learner's effort need to be redirected, or does he or she need external help?"). To support regulation optimally in the context of LAD feedback, we also need to review the definition and typology of feedback suitable for different goals.</p>
        <p>According to the general principles of feedback construction (Sadler, 1989; i.e., conditions making it possible for learners to benefit from feedback), feedback should:According to the general principles of feedback construction (Sadler, 1989; i.e., conditions making it possible for learners to benefit from feedback), feedback should:</p>
        <p>1. Clarify what good performance is, 2. Facilitate self-assessment (allow assessment of how current performance relates to good performance), and 3. Provide opportunities to close the identified gap between current and good performance (allowing reflection on how to act).1. Clarify what good performance is, 2. Facilitate self-assessment (allow assessment of how current performance relates to good performance), and 3. Provide opportunities to close the identified gap between current and good performance (allowing reflection on how to act).</p>
        <p>Feedback can be defined as an interactive process in which the output or effect of an action is returned (fed back) to modify the next action toward reaching a goal. To be able to link learners' past and future work and help them create a progressive developmental trajectory, timeliness should be central to any discussion of feedback (Eyers, Jordan, &amp; Hendry, 2016).Feedback can be defined as an interactive process in which the output or effect of an action is returned (fed back) to modify the next action toward reaching a goal. To be able to link learners' past and future work and help them create a progressive developmental trajectory, timeliness should be central to any discussion of feedback (Eyers, Jordan, &amp; Hendry, 2016).</p>
        <p>Research has shown that the sooner students receive feedback on what they have done, the more effective it is for their learning (Irons, 2008). By grounding the idea of LAD feedback on learning regulation, we aim to inform learners/ teachers about regulation needs during a learning process. This should span the phases for planning (e.g., setting goals), actual learning moments (e.g., completion of tasks), monitoring (e.g., checking progress toward expected outcomes), and adaptation effort (e.g., engaging in improving the intermediate outcomes).Research has shown that the sooner students receive feedback on what they have done, the more effective it is for their learning (Irons, 2008). By grounding the idea of LAD feedback on learning regulation, we aim to inform learners/ teachers about regulation needs during a learning process. This should span the phases for planning (e.g., setting goals), actual learning moments (e.g., completion of tasks), monitoring (e.g., checking progress toward expected outcomes), and adaptation effort (e.g., engaging in improving the intermediate outcomes).</p>
        <p>More specifically, the feedback should inform:More specifically, the feedback should inform:</p>
        <p>(2) learners whenever inefficient learning processes are detected, thus stimulating SRL/CoRL (e.g., engaging a learner in another trial for a failed (sub-)task and pointing to relevant resources or approaches that might help to progress in a task) and (3) teachers when failed goals were either not regulated (i.e., the detection of inefficient processes) or repetitive regulation attempts did not lead to success within a meaningful or agreed timeframe, thus pointing to potential issues that might need further targeted feedback.(2) learners whenever inefficient learning processes are detected, thus stimulating SRL/CoRL (e.g., engaging a learner in another trial for a failed (sub-)task and pointing to relevant resources or approaches that might help to progress in a task) and (3) teachers when failed goals were either not regulated (i.e., the detection of inefficient processes) or repetitive regulation attempts did not lead to success within a meaningful or agreed timeframe, thus pointing to potential issues that might need further targeted feedback.</p>
        <p>3.2.2.2. Typology of feedback. Different theories have attempted to explain the process of how people learn. Even though psychologists and educators are not in complete agreement, most agree that learning may be explained by a combination of two basic approaches: cognitive theories (i.e., cognitivism, which views the learning process as a step-by-step knowledge construction process) and behavioral theories (i.e., behaviorism, in which learning is defined as a change of the behavior of a learner by reinforcing some aspect of her behavior; Tomic, 1993). In the context of feedback research, these approaches translate into two major forms: explanations targeted at improving cognitive dimensions of knowledge (e.g., understanding) and guidance to influence a learner's behavior (e.g., engaging in a specific type of activity believed to be related to a successful learning path; Sedrakyan, 2016). As learning is multifaceted, these approaches are often combined. For instance, in sociocognitive learning theory (Bransford et al., 2000), learners are no longer viewed as repositories for information but rather as proactive and active processors of information, acting as constructors of their knowledge by reinforcing themselves with goaldirected behavior. This theory can be exploited in the context of SRL. SRL is defined as a learner's ability to monitor and evaluate his or her progress with respect to self-improvement needs in the process of achieving their learning goals (Zimmerman &amp; Schunk, 2011). This type of process can be referred to as a sequencing of cognitive and behavioral activities, suggesting that in terms of data analysis approaches, we also need to consider the role of process (sequence) analytics (Sedrakyan, 2016;Sedrakyan, Snoeck &amp; De Weerdt, 2014) as opposed to the statistical and data-mining approaches currently widely applied in research on learning analytics.3.2.2.2. Typology of feedback. Different theories have attempted to explain the process of how people learn. Even though psychologists and educators are not in complete agreement, most agree that learning may be explained by a combination of two basic approaches: cognitive theories (i.e., cognitivism, which views the learning process as a step-by-step knowledge construction process) and behavioral theories (i.e., behaviorism, in which learning is defined as a change of the behavior of a learner by reinforcing some aspect of her behavior; Tomic, 1993). In the context of feedback research, these approaches translate into two major forms: explanations targeted at improving cognitive dimensions of knowledge (e.g., understanding) and guidance to influence a learner's behavior (e.g., engaging in a specific type of activity believed to be related to a successful learning path; Sedrakyan, 2016). As learning is multifaceted, these approaches are often combined. For instance, in sociocognitive learning theory (Bransford et al., 2000), learners are no longer viewed as repositories for information but rather as proactive and active processors of information, acting as constructors of their knowledge by reinforcing themselves with goaldirected behavior. This theory can be exploited in the context of SRL. SRL is defined as a learner's ability to monitor and evaluate his or her progress with respect to self-improvement needs in the process of achieving their learning goals (Zimmerman &amp; Schunk, 2011). This type of process can be referred to as a sequencing of cognitive and behavioral activities, suggesting that in terms of data analysis approaches, we also need to consider the role of process (sequence) analytics (Sedrakyan, 2016;Sedrakyan, Snoeck &amp; De Weerdt, 2014) as opposed to the statistical and data-mining approaches currently widely applied in research on learning analytics.</p>
        <p>Cognitive feedback gives information to learners about success or failure concerning the task at hand through prompts, cues, questions, and so on that help learners to reflect on the quality of the problem-solving process (e.g., reasoning, thinking, understanding). This type of feedback aims to improve learners' understanding of intermediate solutions allowing them to engage in selfregulatory learning mechanisms (van Merri€ enboer &amp; Kirschner, 2012).Cognitive feedback gives information to learners about success or failure concerning the task at hand through prompts, cues, questions, and so on that help learners to reflect on the quality of the problem-solving process (e.g., reasoning, thinking, understanding). This type of feedback aims to improve learners' understanding of intermediate solutions allowing them to engage in selfregulatory learning mechanisms (van Merri€ enboer &amp; Kirschner, 2012).</p>
        <p>Previous studies (Alvarez, Espasa, &amp; Guasch, 2012;Guasch, Espasa, Alvarez, &amp; Kirschner, 2011, 2013) identify different types of cognitive feedback, such as corrective, epistemic or suggestive feedback, and their combination. Corrective feedback provides comments to the learner about the adequacy of learners' work (e.g., "This is not correct; the correct answer is …"). Epistemic feedback requests and/or stimulates explanations and/or clarifications in a critical way (e.g., "Do you think what you have written reflects what the author means in her study? Why do you think that XXX is an example of what the author is saying?"). Suggestive feedback (sometimes referred to as directive feedback) includes advice or directions to the learner on how to proceed and/or continue and invites him or her to explore, expand, or improve what he or she has done (e.g., "Giving an instance or an example of your position at the end of your argument would make your point both clearer and stronger"). Of course, it is sometimes possible to combine them (e.g., epistemic and suggestive).Previous studies (Alvarez, Espasa, &amp; Guasch, 2012;Guasch, Espasa, Alvarez, &amp; Kirschner, 2011, 2013) identify different types of cognitive feedback, such as corrective, epistemic or suggestive feedback, and their combination. Corrective feedback provides comments to the learner about the adequacy of learners' work (e.g., "This is not correct; the correct answer is …"). Epistemic feedback requests and/or stimulates explanations and/or clarifications in a critical way (e.g., "Do you think what you have written reflects what the author means in her study? Why do you think that XXX is an example of what the author is saying?"). Suggestive feedback (sometimes referred to as directive feedback) includes advice or directions to the learner on how to proceed and/or continue and invites him or her to explore, expand, or improve what he or she has done (e.g., "Giving an instance or an example of your position at the end of your argument would make your point both clearer and stronger"). Of course, it is sometimes possible to combine them (e.g., epistemic and suggestive).</p>
        <p>As opposed to cognitive feedback that is given in the context of learning tasks such as problem-solving, behavioral feedback targets a change in behavior. This type of feedback relates to learner goals and targets improved awareness of learning progress and potential regulation needs during the learning process. In the context of dashboards, the role of this type of feedback is to inform a learner if he or she is "on track on his or her road map." 3.2.2.3. Learning orientations and behaviors. Goal orientation has been found to affect learning behavior (Stevens &amp; Gist, 1997). Depending on the types of goals learners possess, learning outcomes will target different levels of knowledge, skills, competences, or simply task completion, which also determines how learners engage with the regulated learning process (Winne et al., 2013).As opposed to cognitive feedback that is given in the context of learning tasks such as problem-solving, behavioral feedback targets a change in behavior. This type of feedback relates to learner goals and targets improved awareness of learning progress and potential regulation needs during the learning process. In the context of dashboards, the role of this type of feedback is to inform a learner if he or she is "on track on his or her road map." 3.2.2.3. Learning orientations and behaviors. Goal orientation has been found to affect learning behavior (Stevens &amp; Gist, 1997). Depending on the types of goals learners possess, learning outcomes will target different levels of knowledge, skills, competences, or simply task completion, which also determines how learners engage with the regulated learning process (Winne et al., 2013).</p>
        <p>First, setting goals increases motivation. It has long been known that giving people specific goals to achieve rather than telling them to do their best increases their motivation (Locke &amp; Latham, 2002). Second, setting goals increases achievement (Latham &amp; Locke, 2007). Goal setting is an important phase of planning. As defined by the goal-setting theory, goal setting involves the process of establishing an outcome that serves as the aim of one's actions and a development of an action plan designed to motivate and guide a person or group toward a goal. In educational settings, the ultimate outcome is usually some form of learning as operationalized by the instructor and/or students (Marzano, Pickering, &amp; Pollock, 2001). When instructors set explicit learning goals, students have a clear picture of course expectations, helping them to concentrate their efforts efficiently toward the attainment of those goals (Turkay, 2014). Moreover, when students have clear objectives, they are more likely to seek feedback to close the gap between their current understanding or skills and the desired goal (Hattie &amp; Timperly, 2007). One way to achieve meaningful goal setting is to relate the assignments and topics to students' beliefs and values (i.e., learners' goals; Turkay, 2014). The self-goal setting process can improve students' learning and motivation (Zimmerman, 1990). Thus, instructors can also encourage students to set their goals. Goaldirected behavior that results from self-goal setting is empowering and proactive, as students take responsibility and ownership (Elliot &amp; Fryer, 2008).First, setting goals increases motivation. It has long been known that giving people specific goals to achieve rather than telling them to do their best increases their motivation (Locke &amp; Latham, 2002). Second, setting goals increases achievement (Latham &amp; Locke, 2007). Goal setting is an important phase of planning. As defined by the goal-setting theory, goal setting involves the process of establishing an outcome that serves as the aim of one's actions and a development of an action plan designed to motivate and guide a person or group toward a goal. In educational settings, the ultimate outcome is usually some form of learning as operationalized by the instructor and/or students (Marzano, Pickering, &amp; Pollock, 2001). When instructors set explicit learning goals, students have a clear picture of course expectations, helping them to concentrate their efforts efficiently toward the attainment of those goals (Turkay, 2014). Moreover, when students have clear objectives, they are more likely to seek feedback to close the gap between their current understanding or skills and the desired goal (Hattie &amp; Timperly, 2007). One way to achieve meaningful goal setting is to relate the assignments and topics to students' beliefs and values (i.e., learners' goals; Turkay, 2014). The self-goal setting process can improve students' learning and motivation (Zimmerman, 1990). Thus, instructors can also encourage students to set their goals. Goaldirected behavior that results from self-goal setting is empowering and proactive, as students take responsibility and ownership (Elliot &amp; Fryer, 2008).</p>
        <p>Different approaches can be distinguished in terms of the way people learn (i.e., set their learning goals), which is explained by the concept of goal orientation. If goal orientation is aimed toward obtaining good grades, then this is seen as performance orientation. When goal orientation is aimed toward becoming good or better at something, then this is seen as mastery orientation. Mastery and performance orientation are defined as functions of competence. The expectation of a learning outcome adds another classification of goal orientation, namely, an approach or avoidance orientation (Bernacki, Byrnes, &amp; Cromley, 2012;Elliot &amp; McGregor, 2001;Van Yperen, Elliot, &amp; Anseel, 2009). When a positive, desirable outcome is expected, the learner will have the desire to achieve success (i.e., an approach orientation will be seen). When a negative, undesirable outcome is expected, the learner will have the desire to avoid failure (i.e., an avoidance orientation). In summary, a teacher defines general learning goals, learning environments, however, should be also responsive to learners' orientation. The design of LAD feedback should thus show awareness of the level of knowledge, competence, and expertise that learners target, such as supporting a learner in reaching a level that allows avoidance of failure (e.g., reaching the minimal score) or performing equal to peers (e.g., reaching the average score), achieving skills and competences (e.g., attaining teacher-specified scores for excellence for mastery-oriented goal-specific tasks), and setting preferences for specific topics (e.g., indicating difficulties and thus needs for more detailed support for a specific topic). We posit that based on these goals, LADs should be able to propose personalized learning trajectories (action plans), also taking into consideration the level of preparedness and dependencies between learning (sub-)goals. For instance, a learner might wish to focus more on mastering a concept or topic about which he or she has no prior knowledge and spend less time on concepts or topics on which he or she has background knowledge.Different approaches can be distinguished in terms of the way people learn (i.e., set their learning goals), which is explained by the concept of goal orientation. If goal orientation is aimed toward obtaining good grades, then this is seen as performance orientation. When goal orientation is aimed toward becoming good or better at something, then this is seen as mastery orientation. Mastery and performance orientation are defined as functions of competence. The expectation of a learning outcome adds another classification of goal orientation, namely, an approach or avoidance orientation (Bernacki, Byrnes, &amp; Cromley, 2012;Elliot &amp; McGregor, 2001;Van Yperen, Elliot, &amp; Anseel, 2009). When a positive, desirable outcome is expected, the learner will have the desire to achieve success (i.e., an approach orientation will be seen). When a negative, undesirable outcome is expected, the learner will have the desire to avoid failure (i.e., an avoidance orientation). In summary, a teacher defines general learning goals, learning environments, however, should be also responsive to learners' orientation. The design of LAD feedback should thus show awareness of the level of knowledge, competence, and expertise that learners target, such as supporting a learner in reaching a level that allows avoidance of failure (e.g., reaching the minimal score) or performing equal to peers (e.g., reaching the average score), achieving skills and competences (e.g., attaining teacher-specified scores for excellence for mastery-oriented goal-specific tasks), and setting preferences for specific topics (e.g., indicating difficulties and thus needs for more detailed support for a specific topic). We posit that based on these goals, LADs should be able to propose personalized learning trajectories (action plans), also taking into consideration the level of preparedness and dependencies between learning (sub-)goals. For instance, a learner might wish to focus more on mastering a concept or topic about which he or she has no prior knowledge and spend less time on concepts or topics on which he or she has background knowledge.</p>
        <p>3.2.2.4. Learning progress: efficiency and effectiveness of learning processes. Knowing learning goals and how much (regulation) effort the learner has put into goal achievement is not enough to determine the potential time when feedback would be most relevant to an end user. According to (Frøkjaer, Hertzum, &amp; Hornbaek, 2000), effectiveness is "the accuracy and completeness with which users achieve certain goals" (p. 345). They give a number of indicators of effectiveness, such as the quality of the solution and the number of errors. Regarding learning, it is more effective if the learner learns what he or she is aiming for, either more or better. Efficiency, on the other hand, "is the relation between (1) the accuracy and completeness with which users achieve certain goals and (2) the resources expended in achieving them" (Frøkjaer et al., 2000, p. 345). The authors also give a number of indicators of efficiency, such as the time it takes to complete a task and the time it takes to learn what is aimed for. In summary, by complementing the idea of feedback with the concepts of effectiveness and efficiency, it is possible to assess a learning progress with respect to potential intervention needs and, more specifically, with respect to determining optimal "receive time" of feedback by discovering inefficient or ineffective processes during learning (see more detail in further sections on mapping theoretical concepts into LAD feedback).3.2.2.4. Learning progress: efficiency and effectiveness of learning processes. Knowing learning goals and how much (regulation) effort the learner has put into goal achievement is not enough to determine the potential time when feedback would be most relevant to an end user. According to (Frøkjaer, Hertzum, &amp; Hornbaek, 2000), effectiveness is "the accuracy and completeness with which users achieve certain goals" (p. 345). They give a number of indicators of effectiveness, such as the quality of the solution and the number of errors. Regarding learning, it is more effective if the learner learns what he or she is aiming for, either more or better. Efficiency, on the other hand, "is the relation between (1) the accuracy and completeness with which users achieve certain goals and (2) the resources expended in achieving them" (Frøkjaer et al., 2000, p. 345). The authors also give a number of indicators of efficiency, such as the time it takes to complete a task and the time it takes to learn what is aimed for. In summary, by complementing the idea of feedback with the concepts of effectiveness and efficiency, it is possible to assess a learning progress with respect to potential intervention needs and, more specifically, with respect to determining optimal "receive time" of feedback by discovering inefficient or ineffective processes during learning (see more detail in further sections on mapping theoretical concepts into LAD feedback).</p>
        <p>3.2.2.5. Learner and teacher feedback perspectives. We posit that LADs should allow a learner to keep track of learning progress and support learners' achievements by means of cognitive feedback at the level of goal-specific tasks (e.g. by supporting understanding of a concept and thus improving problem-solving process) and behavioral feedback by assisting in the process of strategic choice of goals, monitoring, adapting, and providing increased awareness on overall progress toward goal achievement and possible needs for regulation (i.e., behavioral change). From a teacher perspective, dashboard feedback should not only allow for observation of individual, collaborative, and group learning processes with respect to feedback (regulation) needs but also make it possible to reflect on instructional design ("How do learners progress with the specific task?", "Does the specific resource support well in completing the task?" and "Are learners progressing well with respect to a learning goal?"). We therefore propose the inclusion of concepts such as task progress with relation to a learning resource to enhance the personalization perspectives for dashboard feedback (e.g., "Does the expected use of a specific learning resource lead to the expected outcome or goal within an agreed or expected timeframe?"; Winne &amp; Hadwin, 1998).3.2.2.5. Learner and teacher feedback perspectives. We posit that LADs should allow a learner to keep track of learning progress and support learners' achievements by means of cognitive feedback at the level of goal-specific tasks (e.g. by supporting understanding of a concept and thus improving problem-solving process) and behavioral feedback by assisting in the process of strategic choice of goals, monitoring, adapting, and providing increased awareness on overall progress toward goal achievement and possible needs for regulation (i.e., behavioral change). From a teacher perspective, dashboard feedback should not only allow for observation of individual, collaborative, and group learning processes with respect to feedback (regulation) needs but also make it possible to reflect on instructional design ("How do learners progress with the specific task?", "Does the specific resource support well in completing the task?" and "Are learners progressing well with respect to a learning goal?"). We therefore propose the inclusion of concepts such as task progress with relation to a learning resource to enhance the personalization perspectives for dashboard feedback (e.g., "Does the expected use of a specific learning resource lead to the expected outcome or goal within an agreed or expected timeframe?"; Winne &amp; Hadwin, 1998).</p>
        <p>In this section, we propose concepts that allow deriving measurable approximations of learners' efforts on the regulation of learning processes. We also define presentation concepts that will allow mapping these approximations into end-user feedback representations. We then summarize our results into a conceptual map that provides a visual representation of the derived LAD feedback concepts and their relationships, which we propose to serve as general a guidance for designing learning analytics dashboard feedback.In this section, we propose concepts that allow deriving measurable approximations of learners' efforts on the regulation of learning processes. We also define presentation concepts that will allow mapping these approximations into end-user feedback representations. We then summarize our results into a conceptual map that provides a visual representation of the derived LAD feedback concepts and their relationships, which we propose to serve as general a guidance for designing learning analytics dashboard feedback.</p>
        <p>This section provides a preliminary discussion on what artefacts will enable the capturing of data during learning processes regarding the concepts of regulation, time, and type of potential feedback.This section provides a preliminary discussion on what artefacts will enable the capturing of data during learning processes regarding the concepts of regulation, time, and type of potential feedback.</p>
        <p>Most learning is an individual mental activity, such as the actual process of thinking, reasoning, reflecting, and so on. Therefore, we need to define effective approximations of learners' efforts based on their behavioral traces within a learning environment. These approximations will allow measurement of learning progress with respect to learning goals (Winne et al., 2006). From a self-regulated learning perspective, goals provide learners with standards against which they can monitor the learning process and progress. Taskspecific sub-goals (TSSG) include steps that are a) specific, b) measurable, c) action-oriented, d) realistic, and e) temporal (Winne &amp; Hadwin, 1998). To that extent, they are visible markers of learners SRL at different points in time. TSSGs can be defined both by learners and instructors. For example, "backward design," which is a well-known instructional design model, uses goal setting as the focal point of lesson design (Wiggins &amp; McTighe, 1998). When using backward design, instructors identify learning goals for the course first, considering what they want students to know and be able to do when they finish the course, and then determine acceptable evidence regarding whether those goals are met and plan learning experiences and instruction to achieve those learning goals (Wiggins &amp; McTighe, 1998). A simple form of a TSSG can include reading course material, engaging in exercises, and the successful completion of tasks, such as online tests. That is, by creating visible artefacts of varying sub-goals related to mastering the new knowledge or skills and/or competence, we can make them observable in the learning process for both the instructor and learner. Thus, both the instructor and learner can define concrete TSSGs that need to be achieved. TSSGs can be defined when, for instance, planning learning processes and achievements. To enable dashboard feedback based on TSSGs, we create the premise that they are initially carefully defined by an instructor in the context of instruction design and in accordance with goal-setting theory. According to goal-setting theory, goals should be short term rather than long term and challenging (Latham &amp; Locke, 2007) and "Specific, Measurable, Achievable, Relevant, Time-framed" (SMART; Drucker, 1954). In this paper, we assume that a teacher is responsible for ensuring the quality of a goal in terms of being SMART. In the context of LAD feedback design, however, we emphasize the need for measurability of a (sub-)goal (i.e., availability of timeframes, threshold levels for mastery such as minimum score) and "default" action plans (e.g., by defining prerequisite TSSGs, relevant tasks, next-level TSSGs, learning resources) that can be used to propose learning trajectories. A learner should be able to set his or her orientation for TSSGs (e.g., "I want to perform equal to my peers," "… reach a mastery level," or "… avoid failure").Most learning is an individual mental activity, such as the actual process of thinking, reasoning, reflecting, and so on. Therefore, we need to define effective approximations of learners' efforts based on their behavioral traces within a learning environment. These approximations will allow measurement of learning progress with respect to learning goals (Winne et al., 2006). From a self-regulated learning perspective, goals provide learners with standards against which they can monitor the learning process and progress. Taskspecific sub-goals (TSSG) include steps that are a) specific, b) measurable, c) action-oriented, d) realistic, and e) temporal (Winne &amp; Hadwin, 1998). To that extent, they are visible markers of learners SRL at different points in time. TSSGs can be defined both by learners and instructors. For example, "backward design," which is a well-known instructional design model, uses goal setting as the focal point of lesson design (Wiggins &amp; McTighe, 1998). When using backward design, instructors identify learning goals for the course first, considering what they want students to know and be able to do when they finish the course, and then determine acceptable evidence regarding whether those goals are met and plan learning experiences and instruction to achieve those learning goals (Wiggins &amp; McTighe, 1998). A simple form of a TSSG can include reading course material, engaging in exercises, and the successful completion of tasks, such as online tests. That is, by creating visible artefacts of varying sub-goals related to mastering the new knowledge or skills and/or competence, we can make them observable in the learning process for both the instructor and learner. Thus, both the instructor and learner can define concrete TSSGs that need to be achieved. TSSGs can be defined when, for instance, planning learning processes and achievements. To enable dashboard feedback based on TSSGs, we create the premise that they are initially carefully defined by an instructor in the context of instruction design and in accordance with goal-setting theory. According to goal-setting theory, goals should be short term rather than long term and challenging (Latham &amp; Locke, 2007) and "Specific, Measurable, Achievable, Relevant, Time-framed" (SMART; Drucker, 1954). In this paper, we assume that a teacher is responsible for ensuring the quality of a goal in terms of being SMART. In the context of LAD feedback design, however, we emphasize the need for measurability of a (sub-)goal (i.e., availability of timeframes, threshold levels for mastery such as minimum score) and "default" action plans (e.g., by defining prerequisite TSSGs, relevant tasks, next-level TSSGs, learning resources) that can be used to propose learning trajectories. A learner should be able to set his or her orientation for TSSGs (e.g., "I want to perform equal to my peers," "… reach a mastery level," or "… avoid failure").</p>
        <p>We posit that tracking learning progress in the context of educational dashboards is possible by exploiting the relationships between four learning process concepts underlying the backward design principles and the indicators of the effectiveness and efficiency of learning: learning resource(s) required per learning (sub-) goal (specific task), the allocation of expected timeframe for achieving a learning (sub-)goal, and the completeness and accuracy of achieved results.We posit that tracking learning progress in the context of educational dashboards is possible by exploiting the relationships between four learning process concepts underlying the backward design principles and the indicators of the effectiveness and efficiency of learning: learning resource(s) required per learning (sub-) goal (specific task), the allocation of expected timeframe for achieving a learning (sub-)goal, and the completeness and accuracy of achieved results.</p>
        <p>Tracking the utility of the learning material can inform a learner/ teacher if a learner effectively makes use of a learning resource defined for a specific goal. For example, if the expected utility of a specific resource within a meaningful (agreed) time does not lead to the achievement of the expected learning goal, either for a specific learner or group of learners (depending on the observation target), it might suggest that the resource for a specific goal contains a certain level of "difficulty" for that cluster, and vice versa. Giving feedback based on the accuracy or completeness of a task using the backward design principle is rather straightforward. This type of feedback is cognitive and aims to inform a learner if his or her outcomes (e.g., a goal-specific task solution) are correct and complete, and/or suggest hints, references for further improvement. The combination of a learner's resource and time utility and (self-, co-) regulation effort, e.g., another attempt to improve his or her solution based on cognitive feedback, can provide information about intervention needs. Absence of another trial might for instance suggest a need for behavioral feedback to engage a learner in a regulatory process, whereas a learner's attempt in combination with outcomes, utilized time and resources (as well as feedback received on earlier attempt) will allow measuring if the selfregulation effort was successful or sufficient. For example, earlier research has shown that increasing effort (repetitive attempts) that does not lead to improved outcomes may suggest that students experience difficulties (Sedrakyan, 2016). It is also important to note that statistical and data mining techniques should be complemented with process analytics approaches (Sedrakyan, 2016) to allow optimal observation of behavioral aspects of learning where sequencing of activities is relevant. In summary, while the outputs of TSSGs and engagement patterns will allow distinguishing the needs for the type of feedback (cognitive or behavioral), these also allow expanding dashboard performance visualizations with textual feedback to learners and teachers. Whereas detailed models for such feedback are beyond the scope of this work and are subject to further extended research, we posit that the detection of inefficient processes can be useful in determining the timeliness (receive time) of the feedback.Tracking the utility of the learning material can inform a learner/ teacher if a learner effectively makes use of a learning resource defined for a specific goal. For example, if the expected utility of a specific resource within a meaningful (agreed) time does not lead to the achievement of the expected learning goal, either for a specific learner or group of learners (depending on the observation target), it might suggest that the resource for a specific goal contains a certain level of "difficulty" for that cluster, and vice versa. Giving feedback based on the accuracy or completeness of a task using the backward design principle is rather straightforward. This type of feedback is cognitive and aims to inform a learner if his or her outcomes (e.g., a goal-specific task solution) are correct and complete, and/or suggest hints, references for further improvement. The combination of a learner's resource and time utility and (self-, co-) regulation effort, e.g., another attempt to improve his or her solution based on cognitive feedback, can provide information about intervention needs. Absence of another trial might for instance suggest a need for behavioral feedback to engage a learner in a regulatory process, whereas a learner's attempt in combination with outcomes, utilized time and resources (as well as feedback received on earlier attempt) will allow measuring if the selfregulation effort was successful or sufficient. For example, earlier research has shown that increasing effort (repetitive attempts) that does not lead to improved outcomes may suggest that students experience difficulties (Sedrakyan, 2016). It is also important to note that statistical and data mining techniques should be complemented with process analytics approaches (Sedrakyan, 2016) to allow optimal observation of behavioral aspects of learning where sequencing of activities is relevant. In summary, while the outputs of TSSGs and engagement patterns will allow distinguishing the needs for the type of feedback (cognitive or behavioral), these also allow expanding dashboard performance visualizations with textual feedback to learners and teachers. Whereas detailed models for such feedback are beyond the scope of this work and are subject to further extended research, we posit that the detection of inefficient processes can be useful in determining the timeliness (receive time) of the feedback.</p>
        <p>This section aims to provide a preliminary discussion on how learning process data can be mapped to end user (learner and/or teacher) feedback to benefit the regulation of learning processes.This section aims to provide a preliminary discussion on how learning process data can be mapped to end user (learner and/or teacher) feedback to benefit the regulation of learning processes.</p>
        <p>The learning environment should allow setting learning goals (i.e., planning). In general, a planning profile includes activities such as a) planning the sub-goals to reach, b) selection of learning strategy (action plan), c) the materials and resources to use, and c) allocation of time (Pintrich, 2000). In the planning phase, using backward design, the task specific sub-goals necessary to obtain a knowledge or skill are linked with mastering the proposed learning material. This means that the actual use of learning resources can to a certain extent be indicative of learning outcomes. Examples of learning resources include instructor's uploads into a learning environment (e.g., lecture slides, URLs for extra reading material, video lectures, exercises, tasks, tests) as a pre-requisite for obtaining specified knowledge or skills, which in turn are a pre-requisite for the next learning level. These resources can serve as artefacts of instructor expectations or planning. Likewise, a learner can choose his or her (sub/super)set of different learning goals (e.g., "By the end of the week, I want to achieve goal A with a mastery level; I must complete task X, Y, and Z for course 1 and tasks P and Q for course 2; I want to spend more time on tasks for mastering the concept M which seems to be complex, and preferably less time for the concept L, on which I already have some prior background knowledge").The learning environment should allow setting learning goals (i.e., planning). In general, a planning profile includes activities such as a) planning the sub-goals to reach, b) selection of learning strategy (action plan), c) the materials and resources to use, and c) allocation of time (Pintrich, 2000). In the planning phase, using backward design, the task specific sub-goals necessary to obtain a knowledge or skill are linked with mastering the proposed learning material. This means that the actual use of learning resources can to a certain extent be indicative of learning outcomes. Examples of learning resources include instructor's uploads into a learning environment (e.g., lecture slides, URLs for extra reading material, video lectures, exercises, tasks, tests) as a pre-requisite for obtaining specified knowledge or skills, which in turn are a pre-requisite for the next learning level. These resources can serve as artefacts of instructor expectations or planning. Likewise, a learner can choose his or her (sub/super)set of different learning goals (e.g., "By the end of the week, I want to achieve goal A with a mastery level; I must complete task X, Y, and Z for course 1 and tasks P and Q for course 2; I want to spend more time on tasks for mastering the concept M which seems to be complex, and preferably less time for the concept L, on which I already have some prior background knowledge").</p>
        <p>These planning profiles will be used both to guide learners and inform teachers about the coherence and alignment between learners' and teacher's specified goals and action plans. The planning profiles can also provide additional information about the overall level of preparedness, preferences and difficulties of learners.These planning profiles will be used both to guide learners and inform teachers about the coherence and alignment between learners' and teacher's specified goals and action plans. The planning profiles can also provide additional information about the overall level of preparedness, preferences and difficulties of learners.</p>
        <p>A (self-)planning profile should allow a learner to 1) "interact with a teacher view" with a goal to "consult the teacher view" regarding defined goals and trajectories for learning, 2) add his or her own (sub-)goals and link them to (sub-)goals, tasks, and resources within and outside the learning environment, and 3) indicate preferences ("I want to perform equally to my peers") and receive self-oriented feedback that helps him or her construct an optimal learning path (action plan) to reflect on the quality and completeness by checking the needs for completion of prerequisite goals, tasks, and so on. Further details are provided in the Case Example section. The (co-)planning profile should allow a learner to view and request teacher and/or peer-oriented feedback either in the form of a comparative overview (e.g., "How do I perform with respect to teacher expectations or my peers' plans?") or engage in shared planning (e.g., within a group task).A (self-)planning profile should allow a learner to 1) "interact with a teacher view" with a goal to "consult the teacher view" regarding defined goals and trajectories for learning, 2) add his or her own (sub-)goals and link them to (sub-)goals, tasks, and resources within and outside the learning environment, and 3) indicate preferences ("I want to perform equally to my peers") and receive self-oriented feedback that helps him or her construct an optimal learning path (action plan) to reflect on the quality and completeness by checking the needs for completion of prerequisite goals, tasks, and so on. Further details are provided in the Case Example section. The (co-)planning profile should allow a learner to view and request teacher and/or peer-oriented feedback either in the form of a comparative overview (e.g., "How do I perform with respect to teacher expectations or my peers' plans?") or engage in shared planning (e.g., within a group task).</p>
        <p>From a regulated learning point of view, metacognitive monitoring enables learners to adjust or change their goals, plans, or strategies for learning. Monitoring profiles should take into consideration the engagement effort with respect to action plans (generated and/or recommended learning paths based on taskspecific goals and their connections, such as prerequisite goals). Monitoring profiles should suggest comparative overviews to provide information about learners' progress with respect to defined goals and effectiveness and efficiency of learning. A selfmonitoring profile can inform a learner about his or her progress with respect to an action plan and self-defined goals. A comonitoring profile will allow the provision of peer-oriented feedback (e.g., "You seem to be efficient at completing this task. Can you give advice to your peer who seems to have difficulty with concept X?"). A co-monitoring profile can also provide information about how well a learner performs with respect to his or her peers if the goal orientation is set to "performance".From a regulated learning point of view, metacognitive monitoring enables learners to adjust or change their goals, plans, or strategies for learning. Monitoring profiles should take into consideration the engagement effort with respect to action plans (generated and/or recommended learning paths based on taskspecific goals and their connections, such as prerequisite goals). Monitoring profiles should suggest comparative overviews to provide information about learners' progress with respect to defined goals and effectiveness and efficiency of learning. A selfmonitoring profile can inform a learner about his or her progress with respect to an action plan and self-defined goals. A comonitoring profile will allow the provision of peer-oriented feedback (e.g., "You seem to be efficient at completing this task. Can you give advice to your peer who seems to have difficulty with concept X?"). A co-monitoring profile can also provide information about how well a learner performs with respect to his or her peers if the goal orientation is set to "performance".</p>
        <p>In general, the role of the monitoring profile in the context of dashboards is to detect and suggest further adaptation needs for learners and inform teachers about difficulties if the expected performance level is not being achieved within an agreed action plan and timeframe (see the case example for further details).In general, the role of the monitoring profile in the context of dashboards is to detect and suggest further adaptation needs for learners and inform teachers about difficulties if the expected performance level is not being achieved within an agreed action plan and timeframe (see the case example for further details).</p>
        <p>Adaptation profiles can inform learners about the level of effort put into learning regulation and the needs for adaptation (e.g., how learners perform with learning challenges, the resources they use, how much time they spend, and whether they need additional feedback). These profiles can use learner behavior following viewing and monitoring activities to show approximations of adaptation effort. For instance, an increased or decreased effort in combination with achievements (e.g., score) can be indicative of whether activities, such as attending classes, use of a learning resource, task completion, or received feedback had an impact on achievement. A self-adaptation profile will inform a learner about how successful he or she was in addressing a challenge and what actions he or she can take to address a detected challenge. A coadaptation profile will utilize the concept of peer-oriented feedback (e.g., "Could you give feedback on your peer task results?") both for individual and group learning plans.Adaptation profiles can inform learners about the level of effort put into learning regulation and the needs for adaptation (e.g., how learners perform with learning challenges, the resources they use, how much time they spend, and whether they need additional feedback). These profiles can use learner behavior following viewing and monitoring activities to show approximations of adaptation effort. For instance, an increased or decreased effort in combination with achievements (e.g., score) can be indicative of whether activities, such as attending classes, use of a learning resource, task completion, or received feedback had an impact on achievement. A self-adaptation profile will inform a learner about how successful he or she was in addressing a challenge and what actions he or she can take to address a detected challenge. A coadaptation profile will utilize the concept of peer-oriented feedback (e.g., "Could you give feedback on your peer task results?") both for individual and group learning plans.</p>
        <p>Despite the intended benefits of dashboards, user acceptance can be yet another important factor affecting the successful use of dashboard feedback. Previous studies have identified important variables dealing with user acceptance for computer-assisted learning environments, such as the Technology Acceptance Model (Davis, 1989), which suggests ease of use and perceived usefulness as important factors contributing to the user acceptance (i.e., actual utility) of LAD feedback. We thus also propose including monitoring mechanisms that take into account individual user assessment (i.e., feedback on feedback) and actual behavior changes when receiving feedback (Park &amp; Jo, 2015) that will allow the improvement of feedback over time based on empirical evidence.Despite the intended benefits of dashboards, user acceptance can be yet another important factor affecting the successful use of dashboard feedback. Previous studies have identified important variables dealing with user acceptance for computer-assisted learning environments, such as the Technology Acceptance Model (Davis, 1989), which suggests ease of use and perceived usefulness as important factors contributing to the user acceptance (i.e., actual utility) of LAD feedback. We thus also propose including monitoring mechanisms that take into account individual user assessment (i.e., feedback on feedback) and actual behavior changes when receiving feedback (Park &amp; Jo, 2015) that will allow the improvement of feedback over time based on empirical evidence.</p>
        <p>During the learning process, monitoring can prove distracting and thereby alter the nature of learning in a negative way. Too much feedback can be as harmful as delayed feedback. We therefore propose the inclusion of the concept of user control to allow both teachers and learners to control the receive time (i.e., the time for receiving specific feedback) and level of detail (e.g., hints, expanded information).During the learning process, monitoring can prove distracting and thereby alter the nature of learning in a negative way. Too much feedback can be as harmful as delayed feedback. We therefore propose the inclusion of the concept of user control to allow both teachers and learners to control the receive time (i.e., the time for receiving specific feedback) and level of detail (e.g., hints, expanded information).</p>
        <p>Technology assessment is viewed as one of the main roots of innovations (Grunwald, 2014). There have been new accentuations, shifts of emphasis and some new aspects in technology assessment research over the past decade. Subsequently, early engagement has received increasing awareness. Based on earlier experiences with new technologies, a strong incentive is to "get things right from the very beginning" (Roco &amp; Bainbridge, 2005). Constructive technology assessment framework (Rip &amp; Robinson, 2013;Rip &amp; Te Kulve, 2008) has been introduced as a particular form of technology assessment, in which challenges and uses of new technologies and innovations are anticipated and the results of the analysis are fed back into the ongoing development and implementation. The assessment process starts in early stages of research and development in order to deal constructively with potential issues as early as possible during the design and development phase. We thus recommend using built-in mechanisms to allow embedding broader and intermediate user assessment during the iteration cycles of LAD feedback development.Technology assessment is viewed as one of the main roots of innovations (Grunwald, 2014). There have been new accentuations, shifts of emphasis and some new aspects in technology assessment research over the past decade. Subsequently, early engagement has received increasing awareness. Based on earlier experiences with new technologies, a strong incentive is to "get things right from the very beginning" (Roco &amp; Bainbridge, 2005). Constructive technology assessment framework (Rip &amp; Robinson, 2013;Rip &amp; Te Kulve, 2008) has been introduced as a particular form of technology assessment, in which challenges and uses of new technologies and innovations are anticipated and the results of the analysis are fed back into the ongoing development and implementation. The assessment process starts in early stages of research and development in order to deal constructively with potential issues as early as possible during the design and development phase. We thus recommend using built-in mechanisms to allow embedding broader and intermediate user assessment during the iteration cycles of LAD feedback development.</p>
        <p>A number of personality models have been proposed in the literature to understand the individuals' behaviors and characteristics (Tlili, Essalmi, Jemni, Kinshuk, &amp; Chen, 2016). Each one of these models is based on a different personality theory and presents different personality traits. Bayne (2004) claimed that the differences of the learners' personalities result in different ways of learners' involvement in the learning progress regardless of their personal interests or the degree of cognitive development. Previous studies provide overview of different personality models for different learning approaches as well as the way user interfaces aspects (e.g. color, fonts, saturation, brightness, size, label and texture, shape, spatial attribute e.g. 2D …) might impact depending on the personality types (Tlili et al., 2016).A number of personality models have been proposed in the literature to understand the individuals' behaviors and characteristics (Tlili, Essalmi, Jemni, Kinshuk, &amp; Chen, 2016). Each one of these models is based on a different personality theory and presents different personality traits. Bayne (2004) claimed that the differences of the learners' personalities result in different ways of learners' involvement in the learning progress regardless of their personal interests or the degree of cognitive development. Previous studies provide overview of different personality models for different learning approaches as well as the way user interfaces aspects (e.g. color, fonts, saturation, brightness, size, label and texture, shape, spatial attribute e.g. 2D …) might impact depending on the personality types (Tlili et al., 2016).</p>
        <p>While these characteristics and the way these models are implemented in a concrete feedback solution are beyond the scope of this work, we nevertheless include a reference to personal characteristics in our conceptual model that might be helpful when considering design of learning analytics 
            <rs type="software">dahsboards</rs>, e.g. defining the type of data artefacts that will help collecting such variables from user interactions within as well as outside learning environments.
        </p>
        <p>Based on the conceptual review, we derive a conceptual model that depicts the design implications for LAD feedback (see Fig. 1). The model provides a visual representation of the learning, measurement, and presentation concepts and their relationships, as discussed earlier in this paper. This model can serve as a general framework for designing and building information systems as a basis for educational dashboard feedback. The model allows for the generation of process-oriented feedback to different users (e.g., teacher and learners) related to different learning goals. To facilitate the reading of the model, a summary of the model is provided as follows.Based on the conceptual review, we derive a conceptual model that depicts the design implications for LAD feedback (see Fig. 1). The model provides a visual representation of the learning, measurement, and presentation concepts and their relationships, as discussed earlier in this paper. This model can serve as a general framework for designing and building information systems as a basis for educational dashboard feedback. The model allows for the generation of process-oriented feedback to different users (e.g., teacher and learners) related to different learning goals. To facilitate the reading of the model, a summary of the model is provided as follows.</p>
        <p>In this model learning goals are distinguished per knowledge, skills, and competences and measured by performance on goalspecific tasks. Tasks are related to assessment criteria (e.g., teacher-specified score, timeframe). Each goal can have prerequisite and next-level goals that allow the generation of personalized action plans. Goal orientation is attributed to learning goals and allows the input of learner preferences (e.g., performance, mastery) that can define the level of knowledge, skills or competences. Cognitive feedback aims to improve outcomes of goalspecific tasks and includes epistemic, corrective, and suggestive types of feedback that are not mutually exclusive (i.e., can be combined). Behavioral feedback aims at improving process awareness during learning processes. This type of feedback is linked to the regulation mechanisms of learning (sequential/procedural aspects) and is supportive of regulation phases such as planning, monitoring, and adapting. Self-oriented feedback aims to support SRL, whereas peer-and group-oriented feedback are supportive of CoRL. Learning progress is tracked with the help of the concepts of effectiveness and efficiency of learning. Measuring effectiveness and efficiency is supported by the concepts of utility of resources per learning goal and agreed vs. Actual time allocation per goal-specific task. Measuring learning progress allows determination of time of feedback. Both behavioral and cognitive feedback to end users are provided through profiles. The profiles are linked to regulation phases that, at a more granular level, are supported by the information on goal-specific task performance and subsequent behavior upon task-level feedback.In this model learning goals are distinguished per knowledge, skills, and competences and measured by performance on goalspecific tasks. Tasks are related to assessment criteria (e.g., teacher-specified score, timeframe). Each goal can have prerequisite and next-level goals that allow the generation of personalized action plans. Goal orientation is attributed to learning goals and allows the input of learner preferences (e.g., performance, mastery) that can define the level of knowledge, skills or competences. Cognitive feedback aims to improve outcomes of goalspecific tasks and includes epistemic, corrective, and suggestive types of feedback that are not mutually exclusive (i.e., can be combined). Behavioral feedback aims at improving process awareness during learning processes. This type of feedback is linked to the regulation mechanisms of learning (sequential/procedural aspects) and is supportive of regulation phases such as planning, monitoring, and adapting. Self-oriented feedback aims to support SRL, whereas peer-and group-oriented feedback are supportive of CoRL. Learning progress is tracked with the help of the concepts of effectiveness and efficiency of learning. Measuring effectiveness and efficiency is supported by the concepts of utility of resources per learning goal and agreed vs. Actual time allocation per goal-specific task. Measuring learning progress allows determination of time of feedback. Both behavioral and cognitive feedback to end users are provided through profiles. The profiles are linked to regulation phases that, at a more granular level, are supported by the information on goal-specific task performance and subsequent behavior upon task-level feedback.</p>
        <p>The profiles also allow storage of user feedback on feedback (e.g., acceptance in terms of usability, usefulness, ease) and include user control (e.g., preferences for the time and detail of feedback) as well as other UI variables linked to personality models. The examples aim at facilitating understanding of how the learning, measurement, and presentation concepts and their relations discussed in this paper can be exploited to provide end-user feedback. The list of possible implementation scenarios for different feedback types is therefore not exhaustive. Neither have we targeted the technical implementation detail for each feedback type or examined which visualization techniques work better for supporting the concepts discussed in this paper, which is rather subject to further research.The profiles also allow storage of user feedback on feedback (e.g., acceptance in terms of usability, usefulness, ease) and include user control (e.g., preferences for the time and detail of feedback) as well as other UI variables linked to personality models. The examples aim at facilitating understanding of how the learning, measurement, and presentation concepts and their relations discussed in this paper can be exploited to provide end-user feedback. The list of possible implementation scenarios for different feedback types is therefore not exhaustive. Neither have we targeted the technical implementation detail for each feedback type or examined which visualization techniques work better for supporting the concepts discussed in this paper, which is rather subject to further research.</p>
        <p>Fig. 2 shows an action-plan construction example in which a learner adds a learning goal (a specific skill in this case) to a planning list, and a possible action plan is generated in his or her planning profile. In this specific case, the action plan shows the dependencies on other subgoals (e.g., pre-requisites for each subgoal) required to achieve the chosen goal (see the discussed theoretical concept in section 0, and the measurements and presentation layer concepts in). The trajectory scenario also shows the expected level of performance, coverage of what has already been achieved (topic D in this case), and what the learner still needs to achieve (topics B and C) towards her specified learning goal.Fig. 2 shows an action-plan construction example in which a learner adds a learning goal (a specific skill in this case) to a planning list, and a possible action plan is generated in his or her planning profile. In this specific case, the action plan shows the dependencies on other subgoals (e.g., pre-requisites for each subgoal) required to achieve the chosen goal (see the discussed theoretical concept in section 0, and the measurements and presentation layer concepts in). The trajectory scenario also shows the expected level of performance, coverage of what has already been achieved (topic D in this case), and what the learner still needs to achieve (topics B and C) towards her specified learning goal.</p>
        <p>The trajectory also includes information on learning resources as defined by a teacher (in this case, video lecture X) and the goalspecific tasks (online tests in this scenario) that a learner needs to accomplish for a specific goal. The profile utilizes the concept of self-oriented behavioral feedback discussed in earlier sections to guide the learner regarding the actions he or she needs to complete to reach a specific learning goal.The trajectory also includes information on learning resources as defined by a teacher (in this case, video lecture X) and the goalspecific tasks (online tests in this scenario) that a learner needs to accomplish for a specific goal. The profile utilizes the concept of self-oriented behavioral feedback discussed in earlier sections to guide the learner regarding the actions he or she needs to complete to reach a specific learning goal.</p>
        <p>Fig. 3 shows a sample system-generated trajectory in a learner's planning profile based on a performance-orientation preference (e.g., "I want to pass the course"). The action plan includes a specific order of subgoals, goal-specific tasks per goal, and teacher-specified performance levels, i.e. achievement levels needed for each task (see the discussed theoretical concept in section 0, and the measurment and presentation layer concepts in). For example, topic C requires reaching a performance of 60% on a goal-specific task, which includes learning resources of four web lectures and successful completion of two homework assignments with an expected effective time allocation of no more than 5 h. In this example, a combination of area-and bar-charts visualization techniques was used. Bar-charts are visualization techniques to show a relationship between a part to a whole or compare categories, thus allowing to compare the planned and actual achievements during a learning process (in this specific case online tests to measure learning goal outcomes). Area-charts can emphasize the magnitude of change over time and draw attention to trends (in this specific case intermediate results, such as homework and lectures). The effectiveness of these techniques in quantifying achievement levels and supporting progress awareness has been shown in different learning contexts (Charleer, Klerkx, Duval, Laet, &amp; Verbert, 2017).Fig. 3 shows a sample system-generated trajectory in a learner's planning profile based on a performance-orientation preference (e.g., "I want to pass the course"). The action plan includes a specific order of subgoals, goal-specific tasks per goal, and teacher-specified performance levels, i.e. achievement levels needed for each task (see the discussed theoretical concept in section 0, and the measurment and presentation layer concepts in). For example, topic C requires reaching a performance of 60% on a goal-specific task, which includes learning resources of four web lectures and successful completion of two homework assignments with an expected effective time allocation of no more than 5 h. In this example, a combination of area-and bar-charts visualization techniques was used. Bar-charts are visualization techniques to show a relationship between a part to a whole or compare categories, thus allowing to compare the planned and actual achievements during a learning process (in this specific case online tests to measure learning goal outcomes). Area-charts can emphasize the magnitude of change over time and draw attention to trends (in this specific case intermediate results, such as homework and lectures). The effectiveness of these techniques in quantifying achievement levels and supporting progress awareness has been shown in different learning contexts (Charleer, Klerkx, Duval, Laet, &amp; Verbert, 2017).</p>
        <p>Fig. 4 shows an example of monitoring feedback (Sedrakyan &amp; Snoeck, 2012). The profile informs a learner that her learning outcomes do not match the expected or planned performance to accomplish a goal-specific task (online test for topic A). The profile (see the discussed theoretical concept in and the measurment and presentation layer concepts in) also includes cognitive suggestive or corrective feedback for improving (for two failed questions in this case) the effectiveness of which has been empirically tested in a learning context (Sedrakyan, &amp; Snoeck, 2012). Similarly, a bar-chart visualization technique has been used to support progress awareness by comparing the actual and required achievement levels.Fig. 4 shows an example of monitoring feedback (Sedrakyan &amp; Snoeck, 2012). The profile informs a learner that her learning outcomes do not match the expected or planned performance to accomplish a goal-specific task (online test for topic A). The profile (see the discussed theoretical concept in and the measurment and presentation layer concepts in) also includes cognitive suggestive or corrective feedback for improving (for two failed questions in this case) the effectiveness of which has been empirically tested in a learning context (Sedrakyan, &amp; Snoeck, 2012). Similarly, a bar-chart visualization technique has been used to support progress awareness by comparing the actual and required achievement levels.</p>
        <p>Fig. 5 shows an example of adaptation profile feedback (see the discussed theoretical concept in and the measurment and presentation layer concepts in) using the observations on learner engagement in goal-specific task completion and subsequent corrective trials (adaptations). The example shows that a learner successfully regulated his or her learning process by engaging in multiple trials of the same goal-specific task (behavioral selforiented feedback), and using intermediate cognitive suggestive feedback from each trial (reference to specific learning resource in this case) to increase performance. From the example, we can also identify that the last trials were not linked with the utility of planned resources but rather an intensive query in a search engine. This might suggest a difficulty in identifying information in a learning resource and in combination with the task progress (ineffective time utility), suggesting potential intervention needs. This type of profile exploits a process-analytics based feedback approach that has been empirically tested in a learning context (Sedrakyan et al., 2016;Sedrakyan, Snoeck &amp; De Weerdt, 2014). The approach allowed exploring learning process patterns that associate themselves with better/worse learning outcomes, subsequently being used as guidelines to improve teaching (Sedrakyan, 2016;Sedrakyan et al., 2016). In this specific example, we make use of a bar-chart visualization technique to show a sequence of trials for completing a chosen learning goal, each bar showing an achievement level for each trial and arrows showing resources and time spent in between the trials.Fig. 5 shows an example of adaptation profile feedback (see the discussed theoretical concept in and the measurment and presentation layer concepts in) using the observations on learner engagement in goal-specific task completion and subsequent corrective trials (adaptations). The example shows that a learner successfully regulated his or her learning process by engaging in multiple trials of the same goal-specific task (behavioral selforiented feedback), and using intermediate cognitive suggestive feedback from each trial (reference to specific learning resource in this case) to increase performance. From the example, we can also identify that the last trials were not linked with the utility of planned resources but rather an intensive query in a search engine. This might suggest a difficulty in identifying information in a learning resource and in combination with the task progress (ineffective time utility), suggesting potential intervention needs. This type of profile exploits a process-analytics based feedback approach that has been empirically tested in a learning context (Sedrakyan et al., 2016;Sedrakyan, Snoeck &amp; De Weerdt, 2014). The approach allowed exploring learning process patterns that associate themselves with better/worse learning outcomes, subsequently being used as guidelines to improve teaching (Sedrakyan, 2016;Sedrakyan et al., 2016). In this specific example, we make use of a bar-chart visualization technique to show a sequence of trials for completing a chosen learning goal, each bar showing an achievement level for each trial and arrows showing resources and time spent in between the trials.</p>
        <p>Several researchers argue that showing an aggregated analysis with respect to peer performance is a good approach (Lonn et al., 2015). In this work, we motivate the use of an aggregated view approach by the fact that social influence can play an important role in students' motivations (Gruzd, Staves, &amp; Wilk, 2012). Fig. 6 shows a sample peer-oriented feedback based on learning resource utility, which suggests that for a given timeframe, a learner performs lower than his or her peers for the same goal-specific tasks (see the discussed theoretical concept in and the measurment and presentation layer concepts in).Several researchers argue that showing an aggregated analysis with respect to peer performance is a good approach (Lonn et al., 2015). In this work, we motivate the use of an aggregated view approach by the fact that social influence can play an important role in students' motivations (Gruzd, Staves, &amp; Wilk, 2012). Fig. 6 shows a sample peer-oriented feedback based on learning resource utility, which suggests that for a given timeframe, a learner performs lower than his or her peers for the same goal-specific tasks (see the discussed theoretical concept in and the measurment and presentation layer concepts in).</p>
        <p>Although such performance-oriented feedback may not affect learners with high learning achievement, this type of feedback may be useful to regulate learning processes for learners who lack sufficient motivation for goal setting and/or are highly performance oriented (the desire to outperform others) or performanceavoidance oriented (the desire to avoid performing poorer than others do).Although such performance-oriented feedback may not affect learners with high learning achievement, this type of feedback may be useful to regulate learning processes for learners who lack sufficient motivation for goal setting and/or are highly performance oriented (the desire to outperform others) or performanceavoidance oriented (the desire to avoid performing poorer than others do).</p>
        <p>In this specific example, line-charts visualization are used to support awareness of progress during specified periods of time. The line-chart is known to convey changes for multiple datasets over time by connecting data along an interval scale, which will show how data changes at equal intervals of time. Typically, the y-axis represents the dependent variable (test results in this example) and the x-axis represents the independent variable (time interval in this example). For the comparative analysis of the actual achievements with either own planning or peer performance, we make use of area-charts which help to highlight the coverage, i.e. part-to-whole relationship, along with the time trends. To avoid overlaps in comparative views, especially for multiple datasets, transparent coloring can be recommended.In this specific example, line-charts visualization are used to support awareness of progress during specified periods of time. The line-chart is known to convey changes for multiple datasets over time by connecting data along an interval scale, which will show how data changes at equal intervals of time. Typically, the y-axis represents the dependent variable (test results in this example) and the x-axis represents the independent variable (time interval in this example). For the comparative analysis of the actual achievements with either own planning or peer performance, we make use of area-charts which help to highlight the coverage, i.e. part-to-whole relationship, along with the time trends. To avoid overlaps in comparative views, especially for multiple datasets, transparent coloring can be recommended.</p>
        <p>Group standards can be used to provide group-oriented monitoring feedback (see the discussed theoretical concept in and the measurment and presentation layer concepts in). One such example of dashboard feedback can include an (anonymous) evaluation of each actual performance against the expected performance for shared tasks and/or shared standards for collaboration, as shown in Fig. 7.Group standards can be used to provide group-oriented monitoring feedback (see the discussed theoretical concept in and the measurment and presentation layer concepts in). One such example of dashboard feedback can include an (anonymous) evaluation of each actual performance against the expected performance for shared tasks and/or shared standards for collaboration, as shown in Fig. 7.</p>
        <p>In the example, participants 3 and 4 might detect that certain indicators of their performance need improvement with respect to group expectations (e.g., a communication score of 5 out of 20, initiative and attendance rated on average 10 out of 20 by the group members). The group dynamics and effects of feedback can become measurable by allowing flexible access to progress information during a learning process (e.g., filtering mechanisms for observable time and evaluation dimensions) that will allow observation of whether feedback was effective in terms of further adaptation of learners' behavior. Earlier research has demonstrated evidence of the effectiveness of such peer-oriented feedback mechanisms, and radar chart visualization in particular, revealing more convergence between self and peer assessments, and reporting higher social group performance compared with groups that do not use such an approach (Charleer et al., 2017;Gutierrez et al., 2018;Leony, Sedrakyan, Munoz-Merino, Delgado Kloos, &amp; Verbert, 2017;Phielix, Prins, &amp; Kirschner, 2010;Phielix, Prins, Kirschner, Erkens, &amp; Jaspers, 2011;Sedrakyan, Leony, Munoz-Merino, Delgado Kloos, &amp; Verbert, 2017).In the example, participants 3 and 4 might detect that certain indicators of their performance need improvement with respect to group expectations (e.g., a communication score of 5 out of 20, initiative and attendance rated on average 10 out of 20 by the group members). The group dynamics and effects of feedback can become measurable by allowing flexible access to progress information during a learning process (e.g., filtering mechanisms for observable time and evaluation dimensions) that will allow observation of whether feedback was effective in terms of further adaptation of learners' behavior. Earlier research has demonstrated evidence of the effectiveness of such peer-oriented feedback mechanisms, and radar chart visualization in particular, revealing more convergence between self and peer assessments, and reporting higher social group performance compared with groups that do not use such an approach (Charleer et al., 2017;Gutierrez et al., 2018;Leony, Sedrakyan, Munoz-Merino, Delgado Kloos, &amp; Verbert, 2017;Phielix, Prins, &amp; Kirschner, 2010;Phielix, Prins, Kirschner, Erkens, &amp; Jaspers, 2011;Sedrakyan, Leony, Munoz-Merino, Delgado Kloos, &amp; Verbert, 2017).</p>
        <p>The next example makes use of process-mining techniques and process-discovery maps to visualize group interactivity for collaborative task-completion processes. The motivation for this choice is that group dynamics and interactivity are to a large extent predictive for the collaborative nature of learning processes. As mentioned in earlier sections, it is important to complement statistical and data mining techniques with process analytics approaches (Sedrakyan, 2016) to allow optimal observation of behavioral aspects of learning where sequencing of activities is relevant. In this specific example, although the use of statistics can reveal a participation to group work, process/sequence analytics techniques can help revealing issues not visible by quantification of interactions only. For instance, visual inspection can easily reveal if the interactions are balanced within a group or concentrated among particular members. Furthermore, the use of process mining has been successfully tested in various learning contexts for exploring the interactivity and learning regulation (Schoor &amp; Bannert, 2012;Sedrakyan, 2016;Sedrakyan, Snoeck, &amp; De Weerdt, 2014;Sedrakyan et al., 2016a,b). The feedback idea presented in this example was piloted by the analysis of data from the authors' experiment conducted at 
            <rs type="creator">LeaForum</rs> (
            <rs type="url">http://www.oulu.fi/ leaf-eng</rs>/). Fig. 8 shows interaction patterns based on user activities within the 
            <rs type="software">WeSpot</rs> environment (Mikroyannidis et al., 2013;Specht et al., 2013). 
            <rs type="software">WeSpot</rs> is a cloud-based approach for collaborative inquiry learning that allows learners to perform sciencerelated investigations (Mikrodyannis et al., 2013). It also gives instructors a flexible tool to arrange and script collaborative inquiry learning.
        </p>
        <p>The students' collaborative task was to design a healthy breakfast basket. The 
            <rs type="software">WeSpot</rs> learning environment included the task instructions, along with informative descriptions of what a healthy breakfast should include. In addition to the task instructions and information, the learning environment used a script that included subgoals required to complete the task. The subgoals were as follows: 1) activate the prior knowledge and plan your collaborative work, 2) set the criteria for the task's completion, 3) search for information, 4) discuss and complement the findings in the learning environment, and 5) communicate the results. The students' collaborative task outcome was a detailed list including a description of nutrients that a healthy breakfast should include. The data thus include the interactions of learners abstracted from learner activity logs (e.g., "ask," "answer," "comment," "rate") within the 
            <rs type="software">WeSpot</rs> learning environment (Mikroyannidis et al., 2013;et al., 2013). The visualizations of activities (sequences and frequencies) can suggest to a teacher if a satisfactory level of group dynamics has been reached. For instance, if a teacher specified a balanced interactivity for a task in which a group member should contribute to active discussion on the analysis results, it is obvious from Fig. 8 that in groups 1e5, there are some problems with collaboration because either not all participants are involved or some participants' activities are limited. This already can suggest to a teacher that regulation might be needed. According to the goals, Group 6 shows a desired (e.g., balanced) interaction. However, to understand the quality of interaction and collaboration, additional mechanisms can be used (e.g., by taking into consideration different roles, such as an active role for a learner with more knowledge or experience and a passive role for a learner responsible, for example, a programming part of a task).
        </p>
        <p>This article discusses design implications for LAD feedback preliminary answers on how such feedback can be grounded in the learning sciences. Based on a conceptual analysis, a conceptual model that can be used to design information systems as a basis for process-oriented feedback in the context of LADs was derived. The work contributes to the learning sciences with respect to the lack of methodologies for designing and building (1) LADs, (2) feedback automation (which, to our knowledge, is nonexistent), and (3) data and information sciences with respect to the type of data concepts needed to store and track learning processes regarding feedback. In summary, from a learning sciences perspective, the learning process can be positively influenced by dashboard feedback if it takes into consideration the regulatory mechanisms underlying learning processes (Zimmerman, 1990). Such feedback can be constructed based on learner profiles of regulation process phases spanning the planning, monitoring, and adapting activities thus allowing detecting inefficient processes in learning. Cognitive feedback can support learning regulation at a level of task-specific goals aiming to improve intermediate learning outcomes, whereas behavioral feedback should provide information about needs for behavioral change by improving awareness of learning progress. Furthermore, virtual-learning environments should consider learning goals to expand the scope of LAD feedback to support mastery orientation in addition to performance orientation, which is the main focus of existing solutions. By complementing the feedback with the concepts of effectiveness and efficiency of learning processes, it is also possible to track learning progress and refine detection mechanisms for potential intervention time by allowing detecting ineffective or inefficient processes during learning. Also, dashboard design should consider concepts of user acceptance and control both for learners and teachers to support its intended utility (e.g., perceived usefulness, ease of use, satisfaction, preferences) and continuous improvement based on empirical evidence. However, the detailed mechanisms for user intervention in feedback still remain challenging.This article discusses design implications for LAD feedback preliminary answers on how such feedback can be grounded in the learning sciences. Based on a conceptual analysis, a conceptual model that can be used to design information systems as a basis for process-oriented feedback in the context of LADs was derived. The work contributes to the learning sciences with respect to the lack of methodologies for designing and building (1) LADs, (2) feedback automation (which, to our knowledge, is nonexistent), and (3) data and information sciences with respect to the type of data concepts needed to store and track learning processes regarding feedback. In summary, from a learning sciences perspective, the learning process can be positively influenced by dashboard feedback if it takes into consideration the regulatory mechanisms underlying learning processes (Zimmerman, 1990). Such feedback can be constructed based on learner profiles of regulation process phases spanning the planning, monitoring, and adapting activities thus allowing detecting inefficient processes in learning. Cognitive feedback can support learning regulation at a level of task-specific goals aiming to improve intermediate learning outcomes, whereas behavioral feedback should provide information about needs for behavioral change by improving awareness of learning progress. Furthermore, virtual-learning environments should consider learning goals to expand the scope of LAD feedback to support mastery orientation in addition to performance orientation, which is the main focus of existing solutions. By complementing the feedback with the concepts of effectiveness and efficiency of learning processes, it is also possible to track learning progress and refine detection mechanisms for potential intervention time by allowing detecting ineffective or inefficient processes during learning. Also, dashboard design should consider concepts of user acceptance and control both for learners and teachers to support its intended utility (e.g., perceived usefulness, ease of use, satisfaction, preferences) and continuous improvement based on empirical evidence. However, the detailed mechanisms for user intervention in feedback still remain challenging.</p>
        <p>To enable LAD feedback, data collection needs to be based on a definition of measurable approximations of learner effort put into individual and collaborative learning progress (i.e., engaging in goal setting, actual learning moments, monitoring one's own progress, and adaptation). To obtain visible artefacts that make the phases of learning processes and underlying regulatory mechanisms observable, we should make a strong connection between dashboard design and the design principles of the learning environment and, more specifically, with instructional design. Further, "backward instructional design" provides richer opportunities for tracking learning processes due to the use of goal setting as the focal point of lesson design and TSSGs, which among others, combine steps that are measurable, action-oriented, and temporal. From a data-analytics perspective, we suggest, as opposed to traditional statistical and data-mining approaches, that such feedback should be based on a process analytics view (Sedrakyan, 2016) that will allow detection of the procedural and sequential aspects of learning specific to regulatory mechanisms of learning processes.To enable LAD feedback, data collection needs to be based on a definition of measurable approximations of learner effort put into individual and collaborative learning progress (i.e., engaging in goal setting, actual learning moments, monitoring one's own progress, and adaptation). To obtain visible artefacts that make the phases of learning processes and underlying regulatory mechanisms observable, we should make a strong connection between dashboard design and the design principles of the learning environment and, more specifically, with instructional design. Further, "backward instructional design" provides richer opportunities for tracking learning processes due to the use of goal setting as the focal point of lesson design and TSSGs, which among others, combine steps that are measurable, action-oriented, and temporal. From a data-analytics perspective, we suggest, as opposed to traditional statistical and data-mining approaches, that such feedback should be based on a process analytics view (Sedrakyan, 2016) that will allow detection of the procedural and sequential aspects of learning specific to regulatory mechanisms of learning processes.</p>
        <p>Another thing to keep in mind is that LAD feedback should have a bidirectional purpose by allowing a learner to observe and improve his or her learning progress with respect to regulation needs and by allowing a teacher to not only observe individual and group learners' potential needs for targeted feedback but also to reflect on instructional design. Ultimately, the proposed design model allows extending dashboard feedback with textual explanations in the form of cognitive and behavioral feedback which makes learning-process data visualizations easier to interpret for an end user.Another thing to keep in mind is that LAD feedback should have a bidirectional purpose by allowing a learner to observe and improve his or her learning progress with respect to regulation needs and by allowing a teacher to not only observe individual and group learners' potential needs for targeted feedback but also to reflect on instructional design. Ultimately, the proposed design model allows extending dashboard feedback with textual explanations in the form of cognitive and behavioral feedback which makes learning-process data visualizations easier to interpret for an end user.</p>
        <p>Because learning is not limited to the learning environment, exploring approaches to integrate activities in learning-process analysis and assessment outside of traditional learning environments in the context of dashboard feedback is a possible future research direction. While LADs as a form of recommender systems can enhance experiences by providing such targeted information outside the learning platforms, the entry barriers in terms of data acquisition can be very high, often limiting recommender solutions to closed systems of isolated data and user/context models. The future research direction thus includes exploring how multi-modal data that can originate from a multitude of sources and formats can be harvested, curated and fused into semantically enhanced data (Sedrakyan et al., 2018) to enhance the scope of feedback model described in this work. With the introduction of MOOCs the big data (analytics) related dimensions became relevant in the literature on learning dashboards. In addition, recent research shows increased interest in exploring biofeedback opportunities based on multi-modal data collected from various wearable sensors, audio/ video streams. Thus scalability is yet another requirement and future work direction for learning analytics dashboards that will use large volumes of (live) learner data (Rojas Mel endez, Sedrakyan, Colpaert, Vander Sande, &amp; Verborgh, 2018) such as data from wearable sensors collected from various sources in a variety of data formats.Because learning is not limited to the learning environment, exploring approaches to integrate activities in learning-process analysis and assessment outside of traditional learning environments in the context of dashboard feedback is a possible future research direction. While LADs as a form of recommender systems can enhance experiences by providing such targeted information outside the learning platforms, the entry barriers in terms of data acquisition can be very high, often limiting recommender solutions to closed systems of isolated data and user/context models. The future research direction thus includes exploring how multi-modal data that can originate from a multitude of sources and formats can be harvested, curated and fused into semantically enhanced data (Sedrakyan et al., 2018) to enhance the scope of feedback model described in this work. With the introduction of MOOCs the big data (analytics) related dimensions became relevant in the literature on learning dashboards. In addition, recent research shows increased interest in exploring biofeedback opportunities based on multi-modal data collected from various wearable sensors, audio/ video streams. Thus scalability is yet another requirement and future work direction for learning analytics dashboards that will use large volumes of (live) learner data (Rojas Mel endez, Sedrakyan, Colpaert, Vander Sande, &amp; Verborgh, 2018) such as data from wearable sensors collected from various sources in a variety of data formats.</p>
        <p>A few other challenges for future research in this domain include:A few other challenges for future research in this domain include:</p>
        <p>1. Developing analysis models that will allow capturing the dynamics of learning, as, for example, the same feedback for the same task may not be relevant the next time as learners become "more experienced";1. Developing analysis models that will allow capturing the dynamics of learning, as, for example, the same feedback for the same task may not be relevant the next time as learners become "more experienced";</p>
        <p>2. Exploring analysis mechanisms that take into account personal characteristics of learners (e.g., level of preparedness and previous knowledge, preferences, cultural, demographic characteristics such as age, gender, etc.) as well as personality models (e.g. how different types of personalities engage/interact during a learning process, etc.) in the feedback model and subsequent design; 3. Exploring biofeedback possibilities by integrating the socioemotional context of learning based on multimodal data that can be collected, for example, from wearable sensors, audio/ video stream analysis, and so on; 4. Identifying mechanisms that will allow filtering data relevant to learning processes when extending observations to external sources of activity data outside of the learning environment and the synchronization of such data (e.g., time management between data collected from different platforms, devices); and 5. As the case example makes use of visualization techniques based on empirical evidence from earlier research, investigating other visualization techniques and testing which techniques can work better in supporting the feedback ideas presented in this paper can yet be another possible future research direction.2. Exploring analysis mechanisms that take into account personal characteristics of learners (e.g., level of preparedness and previous knowledge, preferences, cultural, demographic characteristics such as age, gender, etc.) as well as personality models (e.g. how different types of personalities engage/interact during a learning process, etc.) in the feedback model and subsequent design; 3. Exploring biofeedback possibilities by integrating the socioemotional context of learning based on multimodal data that can be collected, for example, from wearable sensors, audio/ video stream analysis, and so on; 4. Identifying mechanisms that will allow filtering data relevant to learning processes when extending observations to external sources of activity data outside of the learning environment and the synchronization of such data (e.g., time management between data collected from different platforms, devices); and 5. As the case example makes use of visualization techniques based on empirical evidence from earlier research, investigating other visualization techniques and testing which techniques can work better in supporting the feedback ideas presented in this paper can yet be another possible future research direction.</p>
        <p>We invite further research for empirical and experimental evaluation and further refinements of the LAD feedback design model presented in this work.We invite further research for empirical and experimental evaluation and further refinements of the LAD feedback design model presented in this work.</p>
        <p>This work was supported by a Belgian Scientific Organization (FWO) grant (V4.533.15N) for international research collaboration, Internal Funds of KU Leuven -PROFEELEARN PDM/16/044, JUMO mobility grant, and was carried out in collaboration with the SLAM project, funded by the Academy of Finland. The research was conducted at KU Leuven, Oulu University, and Open University of The Netherlands.This work was supported by a Belgian Scientific Organization (FWO) grant (V4.533.15N) for international research collaboration, Internal Funds of KU Leuven -PROFEELEARN PDM/16/044, JUMO mobility grant, and was carried out in collaboration with the SLAM project, funded by the Academy of Finland. The research was conducted at KU Leuven, Oulu University, and Open University of The Netherlands.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-13T15:19+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>or visit the DOI to the publisher's website.or visit the DOI to the publisher's website.</p>
        <p>• The final author version and the galley proof are versions of the publication after peer review.• The final author version and the galley proof are versions of the publication after peer review.</p>
        <p>• The final published version features the final layout of the paper including the volume, issue and page numbers.• The final published version features the final layout of the paper including the volume, issue and page numbers.</p>
        <p>Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.</p>
        <p>• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal.• Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal.</p>
        <p>If the publication is distributed under the terms of Article 25fa of the Dutch Copyright Act, indicated by the "Taverne" license above, please follow below link for the End UserIf the publication is distributed under the terms of Article 25fa of the Dutch Copyright Act, indicated by the "Taverne" license above, please follow below link for the End User</p>
        <p>Breakthroughs in artificial intelligence (AI) hold enormous potential as it can automate complex tasks and go even beyond human performance. In their study, McKinney et al. 1 showed the high potential of AI for breast cancer screening. However, the lack of details of the methods and algorithm code undermines its scientific value. Here, we identify obstacles that hinder transparent and reproducible AI research as faced by McKinney et al. 1 , and provide solutions to these obstacles with implications for the broader field.Breakthroughs in artificial intelligence (AI) hold enormous potential as it can automate complex tasks and go even beyond human performance. In their study, McKinney et al. 1 showed the high potential of AI for breast cancer screening. However, the lack of details of the methods and algorithm code undermines its scientific value. Here, we identify obstacles that hinder transparent and reproducible AI research as faced by McKinney et al. 1 , and provide solutions to these obstacles with implications for the broader field.</p>
        <p>The work by McKinney et al. 1 demonstrates the potential of AI in medical imaging, while highlighting the challenges of making such work reproducible. The authors assert that their system improves the speed and robustness of breast cancer screening, generalizes to populations beyond those used for training, and outperforms radiologists in specific settings. Upon successful prospective clinical validation and approval by regulatory bodies, this new system holds great potential for streamlining clinical workflows, reducing false positives, and improving patient outcomes. However, the absence of sufficiently documented methods and computer code underlying the study effectively undermines its scientific value. This shortcoming limits the evidence required for others to prospectively validate and clinically implement such technologies. By identifying obstacles hindering transparent and reproducible AI research as faced by McKinney et al. 1 , we provide potential solutions with implications for the broader field.The work by McKinney et al. 1 demonstrates the potential of AI in medical imaging, while highlighting the challenges of making such work reproducible. The authors assert that their system improves the speed and robustness of breast cancer screening, generalizes to populations beyond those used for training, and outperforms radiologists in specific settings. Upon successful prospective clinical validation and approval by regulatory bodies, this new system holds great potential for streamlining clinical workflows, reducing false positives, and improving patient outcomes. However, the absence of sufficiently documented methods and computer code underlying the study effectively undermines its scientific value. This shortcoming limits the evidence required for others to prospectively validate and clinically implement such technologies. By identifying obstacles hindering transparent and reproducible AI research as faced by McKinney et al. 1 , we provide potential solutions with implications for the broader field.</p>
        <p>Scientific progress depends on the ability of independent researchers to scrutinize the results of a research study, to reproduce the study's main results using its materials, and to build on them in future studies (https://www.nature.com/nature-research/editorial-policies/ reporting-standards). Publication of insufficiently documented research does not meet the core requirements underlying scientific discovery 2,3 . Merely textual descriptions of deep-learning models can hide their high level of complexity. Nuances in the computer code may have marked effects on the training and evaluation of results 4 , potentially leading to unintended consequences 5 . Therefore, transparency in the form of the actual computer code used to train a model and arrive at its final set of parameters is essential for research reproducibility. McKinney et al. 1 stated that the code used for training the models has "a large number of dependencies on internal tooling, infrastructure and hardware", and claimed that the release of the code was therefore not possible. Computational reproducibility is indispensable for high-quality AI applications 6,7 ; more complex methods demand greater transparency 8 . In the absence of code, reproducibility falls back on replicating methods from textual description. Although, McKinney and colleagues 1 claim that all experiments and implementation details were described in sufficient detail in the supplementary methods section of their Article 1 to "support replication with non-proprietary libraries", key details about their analysis are lacking. Even with extensive description, reproducing complex computational pipelines based purely on text is a subjective and challenging task 9 .Scientific progress depends on the ability of independent researchers to scrutinize the results of a research study, to reproduce the study's main results using its materials, and to build on them in future studies (https://www.nature.com/nature-research/editorial-policies/ reporting-standards). Publication of insufficiently documented research does not meet the core requirements underlying scientific discovery 2,3 . Merely textual descriptions of deep-learning models can hide their high level of complexity. Nuances in the computer code may have marked effects on the training and evaluation of results 4 , potentially leading to unintended consequences 5 . Therefore, transparency in the form of the actual computer code used to train a model and arrive at its final set of parameters is essential for research reproducibility. McKinney et al. 1 stated that the code used for training the models has "a large number of dependencies on internal tooling, infrastructure and hardware", and claimed that the release of the code was therefore not possible. Computational reproducibility is indispensable for high-quality AI applications 6,7 ; more complex methods demand greater transparency 8 . In the absence of code, reproducibility falls back on replicating methods from textual description. Although, McKinney and colleagues 1 claim that all experiments and implementation details were described in sufficient detail in the supplementary methods section of their Article 1 to "support replication with non-proprietary libraries", key details about their analysis are lacking. Even with extensive description, reproducing complex computational pipelines based purely on text is a subjective and challenging task 9 .</p>
        <p>In addition to the reproducibility challenges inherent to purely textual descriptions of methods, the description by McKinney et al. 1 of the model development as well as data processing and training pipelines lacks crucial details. The definitions of several hyperparameters for the model's architecture (composed of three networks referred to as the breast, lesion and case models) are missing (Table 1). In their Nature | Vol 586 | 15 October 2020 | E15 publication, McKinney et al. 1 did not disclose the settings for the augmentation pipeline; the transformations used are stochastic and can considerably affect model performance 10 . Details of the training pipeline were also missing. Without this key information, independent reproduction of the training pipeline is not possible.In addition to the reproducibility challenges inherent to purely textual descriptions of methods, the description by McKinney et al. 1 of the model development as well as data processing and training pipelines lacks crucial details. The definitions of several hyperparameters for the model's architecture (composed of three networks referred to as the breast, lesion and case models) are missing (Table 1). In their Nature | Vol 586 | 15 October 2020 | E15 publication, McKinney et al. 1 did not disclose the settings for the augmentation pipeline; the transformations used are stochastic and can considerably affect model performance 10 . Details of the training pipeline were also missing. Without this key information, independent reproduction of the training pipeline is not possible.</p>
        <p>Numerous frameworks and platforms exist to make artificial intelligence research more transparent and reproducible (Table 2). For the sharing of code, these include Bitbucket, 
            <rs type="software">GitHub</rs> and 
            <rs type="software">GitLab</rs>, among others. The many software dependencies of large-scale machine learning applications require appropriate control of the software environment, which can be achieved through package managers including 
            <rs type="software">Conda</rs>, as well as container and virtualization systems, including Code Ocean, Gigantum, Colaboratory and Docker. If virtualization of the McKinney et al. 1 internal tooling proved to be difficult, they could have released the computer code and documentation. The authors could have also created small artificial examples or used small public datasets 11 to show how new data must be processed to train the model and generate predictions. Sharing the fitted model (architecture along with learned parameters) should be simple aside from privacy concerns that the model may reveal sensitive information about the set of patients used to train it. Nevertheless, techniques for achieving differential privacy exist to alleviate such concerns. Many platforms allow sharing of deep learning models, including 
            <rs type="software">TensorFlow Hub</rs>, 
            <rs type="software">ModelHub.ai</rs>, 
            <rs type="software">ModelDepot</rs> and 
            <rs type="software">Model Zoo</rs> with support for several frameworks such as 
            <rs type="software">PyTorch</rs> and 
            <rs type="software">Caffe</rs>, as well as the 
            <rs type="software">TensorFlow</rs> library used by the authors. In addition to improving accessibility and transparency, such resources can considerably accelerate model development, validation and transition into production and clinical implementation.
        </p>
        <p>Another crucial aspect of ensuring reproducibility lies in access to the data the models were derived from. In their study, McKinney et al. 1 used two large datasets under license, properly disclosing this limitation in their publication. The sharing of patient health information is highly regulated owing to privacy concerns. Despite these challenges, the sharing of raw data has become more common in biomedical literature, increasing from under 1% in the early 2000s to 20% today 12 . However, if the data cannot be shared, the model predictions and data labels themselves should be released, allowing further statistical analyses. Above all, concerns about data privacy should not be used as a way to distract from the requirement to release code.Another crucial aspect of ensuring reproducibility lies in access to the data the models were derived from. In their study, McKinney et al. 1 used two large datasets under license, properly disclosing this limitation in their publication. The sharing of patient health information is highly regulated owing to privacy concerns. Despite these challenges, the sharing of raw data has become more common in biomedical literature, increasing from under 1% in the early 2000s to 20% today 12 . However, if the data cannot be shared, the model predictions and data labels themselves should be released, allowing further statistical analyses. Above all, concerns about data privacy should not be used as a way to distract from the requirement to release code.</p>
        <p>Although sharing of code and data are widely seen as a crucial part of scientific research, the adoption varies across fields. In fields such as genomics, complex computational pipelines and sensitive datasets have been shared for decades 13 . Guidelines related to genomic data are clear, detailed and, most importantly, enforced. It is generally accepted that all code and data are released alongside a publication. In other fields of medicine and science as a whole, this is much less common, and data and code are rarely made available. For scientific efforts in which a clinical application is envisioned and human lives would be at stake, we argue that the bar of transparency should be set even higher. If a dataset cannot be shared with the entire scientific community, because of licensing or other insurmountable issues, at a minimum a mechanism should be set so that some highlytrained, independent investigators can access the data and verify the analyses.Although sharing of code and data are widely seen as a crucial part of scientific research, the adoption varies across fields. In fields such as genomics, complex computational pipelines and sensitive datasets have been shared for decades 13 . Guidelines related to genomic data are clear, detailed and, most importantly, enforced. It is generally accepted that all code and data are released alongside a publication. In other fields of medicine and science as a whole, this is much less common, and data and code are rarely made available. For scientific efforts in which a clinical application is envisioned and human lives would be at stake, we argue that the bar of transparency should be set even higher. If a dataset cannot be shared with the entire scientific community, because of licensing or other insurmountable issues, at a minimum a mechanism should be set so that some highlytrained, independent investigators can access the data and verify the analyses.</p>
        <p>The lack of access to code and data in prominent scientific publications may lead to unwarranted and even potentially harmful clinical trials 14 . These unfortunate lessons have not been lost on journal editors and their readers. Journals have an obligation to hold authors to the standards of reproducibility that benefit not only other researchers, but also the authors themselves. Making one's methods reproducible may surface biases or shortcomings to authors before publication 5 . Preventing external validation of a model will likely reduce its impact, as it also prevents other researchers from using and building upon it in future studies. The failure of McKinney et al. to share key materials and information transforms their work from a scientific publication open to verification and adoption by the scientific community into a promotion of a closed technology.The lack of access to code and data in prominent scientific publications may lead to unwarranted and even potentially harmful clinical trials 14 . These unfortunate lessons have not been lost on journal editors and their readers. Journals have an obligation to hold authors to the standards of reproducibility that benefit not only other researchers, but also the authors themselves. Making one's methods reproducible may surface biases or shortcomings to authors before publication 5 . Preventing external validation of a model will likely reduce its impact, as it also prevents other researchers from using and building upon it in future studies. The failure of McKinney et al. to share key materials and information transforms their work from a scientific publication open to verification and adoption by the scientific community into a promotion of a closed technology.</p>
        <p>We have high hopes for the utility of AI methods in medicine. Ensuring that these methods meet their potential, however, requires that these studies be scientifically reproducible. The recent advances in computational virtualization and AI frameworks are greatly facilitating the implementations of complex deep neural networks in a more structured, transparent, and reproducible way. Adoption of these technologies will increase the impact of published deep-learning algorithms and accelerate the translation of these methods into clinical settings. !"#$ % &amp; ' ( )" $#"$ *+ *+"#$ % &amp; ' ( , $ !-#.." + /"$ # , " 0%1 %$ $ . 2$ % #0 * 3 $ +4 4$ %1 5$ %"$1 1#*3 %67% 4 . 2 $ #0$ # 4 0 $ 0+"$ " " 0+ $ !684 # $ % 4 ."$ /"$ # , " 0%3 0 9 #: $ "3;3 0 "$ %: $ "3;3 0+%053 $ 6 -$ "$ $ 0 8"3 3 $ "$ $ 0"3""3 + 904 .$ %"$$ %4 3 3 1 ! $ ." $ $ %4 !# 3 !9$ "*3 3 !9." $ &lt;$ 9 =$ % 0$ 6 &gt; " 4 .We have high hopes for the utility of AI methods in medicine. Ensuring that these methods meet their potential, however, requires that these studies be scientifically reproducible. The recent advances in computational virtualization and AI frameworks are greatly facilitating the implementations of complex deep neural networks in a more structured, transparent, and reproducible way. Adoption of these technologies will increase the impact of published deep-learning algorithms and accelerate the translation of these methods into clinical settings. !"#$ % &amp; ' ( )" $#"$ *+ *+"#$ % &amp; ' ( , $ !-#.." + /"$ # , " 0%1 %$ $ . 2$ % #0 * 3 $ +4 4$ %1 5$ %"$1 1#*3 %67% 4 . 2 $ #0$ # 4 0 $ 0+"$ " " 0+ $ !684 # $ % 4 ."$ /"$ # , " 0%3 0 9 #: $ "3;3 0 "$ %: $ "3;3 0+%053 $ 6 -$ "$ $ 0 8"3 3 $ "$ $ 0"3""3 + 904 .$ %"$$ %4 3 3 1 ! $ ." $ $ %4 !# 3 !9$ "*3 3 !9." $ &lt;$ 9 =$ % 0$ 6 &gt; " 4 .</p>
        <p>7%&lt;"0$ ".3 ? &amp; @'4 "0%&lt; .$ "3! #&gt; 0 $ 9! 2" "" 0 $ #.*"# $4 4." # .$ A $ "$ .$ 1%$ %." # .$ 1 $ "54 . $ 0$ ".3 1%$ %$ % ". ".3 1"." # "$ 3 + 7% $ "$ $ 0"3$ $ &amp; '# A/B1%$ %$ %+" C $ 1C7%&lt;"0$ ".3 ? &amp; @'4 "0%&lt; .$ "3! #&gt; 0 $ 9! 2" "" 0 $ #.*"# $4 4." # .$ A $ "$ .$ 1%$ %." # .$ 1 $ "54 . $ 0$ ".3 1%$ %$ % ". ".3 1"." # "$ 3 + 7% $ "$ $ 0"3$ $ &amp; '# A/B1%$ %$ %+" C $ 1C</p>
        <p>D@E FG HIJJI@G K LM K M G M NIOE PG QLG PLM HR S QLPG M IE LE FG QFG @TJLU G PLM HR S QLG JIR LG HIJVE LWG K LHN@S XOLM G S @G K NLG YLK NIPM G M LHK S I@Z A 0 $ 4 4"3 302" "$ $ $ A 0 $ 4 4"+" #.$ 0 0$ 9 #0%" "$ $ 4 4 ."3 $ +""[ # $ .$4 .#3 $ 3 0." A4 #3 3 0 $ 4 4$ % $ "$ $ 0"3" ".$ 03 # !0$ "3$ 0+&amp; 6 !6." ' $ %*" 0 $ ."$ &amp; 6 !6 ! 04 4 0 $ ' A/B2" "$ &amp; 6 !6 $ "" 2 "$ ' " 0 "$ $ ."$ 4 4#0 $ " $ +&amp; 6 !604 0 $ 2"3 'D@E FG HIJJI@G K LM K M G M NIOE PG QLG PLM HR S QLPG M IE LE FG QFG @TJLU G PLM HR S QLG JIR LG HIJVE LWG K LHN@S XOLM G S @G K NLG YLK NIPM G M LHK S I@Z A 0 $ 4 4"3 302" "$ $ $ A 0 $ 4 4"+" #.$ 0 0$ 9 #0%" "$ $ 4 4 ."3 $ +""[ # $ .$4 .#3 $ 3 0." A4 #3 3 0 $ 4 4$ % $ "$ $ 0"3" ".$ 03 # !0$ "3$ 0+&amp; 6 !6." ' $ %*" 0 $ ."$ &amp; 6 !6 ! 04 4 0 $ ' A/B2" "$ &amp; 6 !6 $ "" 2 "$ ' " 0 "$ $ ."$ 4 4#0 $ " $ +&amp; 6 !604 0 $ 2"3 '</p>
        <p>8a"+ """3 + 9 4 ."$ $ %0% 04 4 "=" 520%" =$ " 3 $ $ ! 8% " 0% 0"3"0.3 &lt; ! 9 $ 4 0"$ 4 4$ %" "$ 3 234 $ $ "4 #3 3 $ !4 4#$ 0. : $ ."$ 4 44 4 0$ ? &amp; 6 !6%b P9;" b R ' 9 ' 9 0"$ !%1$ %+1 0"3 0#3 "$ DOR G `LQG HIE E LHK S I@G I@G M K TK S M K S HM G c IR G QS IE IdS M K M G HI@K TS @M G TR K S HE LM G I@G JT@FG Ic G K NLG VIS @K M G TQI_LZ -4 $ 1" "0 ;3 0+ 4 ."$ "*#$"2" 3 "* 3 $ +4 40.#$ 0 B"$ "03 3 0$ B"$ """3 +8a"+ """3 + 9 4 ."$ $ %0% 04 4 "=" 520%" =$ " 3 $ $ ! 8% " 0% 0"3"0.3 &lt; ! 9 $ 4 0"$ 4 4$ %" "$ 3 234 $ $ "4 #3 3 $ !4 4#$ 0. : $ ."$ 4 44 4 0$ ? &amp; 6 !6%b P9;" b R ' 9 ' 9 0"$ !%1$ %+1 0"3 0#3 "$ DOR G `LQG HIE E LHK S I@G I@G M K TK S M K S HM G c IR G QS IE IdS M K M G HI@K TS @M G TR K S HE LM G I@G JT@FG Ic G K NLG VIS @K M G TQI_LZ -4 $ 1" "0 ;3 0+ 4 ."$ "*#$"2" 3 "* 3 $ +4 40.#$ 0 B"$ "03 3 0$ B"$ """3 +</p>
        <p>B"$ " ;3 0+ 4 ."$ "*#$"2" 3 "* 3 $ +4 4"$ " A3 3."# 0 $ .# $ 03 #""$ ""2" 3 "* 3 $ + $ "$ .$7% $ "$ .$ %#3 2 $ %4 3 3 1 ! 4 ."$ 91% "3 0"*3 ( CA00 0 9# i# $ 4 9 1*3 54 #*3 03 +"2" 3 "*3 "$ " $ CA3 $4 44 !# $ %"$%"2" 0 "$ "1"$ " CA 0 $ 4 4"+ $ 0$ "$ ""2" 3 "* 3 $ + 8 3 C 0 4 0 $ ! kIOQE Ll QE S @PG VLLR G R L_S L`G M OQJS M M S I@M m G `R S K LG kn]oG T@PG FIOR G JT@OM HR S VK G @OJQLR G NLR LG S @M K LTPG Ic G TOK NIR G @TJLM Z ppppl YYl kk q q q q q q q q q q / /"$ "%"2*! "$ " "" $4 4$ % ."# 0 $ 6 / /0.#$ 0%"*! "$ " "" $4 4$ % ."# 0 $ 6 / /"$ "%"2*! "$ " "" $4 4$ % . ) 4 0 0 $ #+ ! A3 3 $ # .# $ 03 $ % $ 21%$ % 03 # !"$ 26 -".3 ? B"$ "&lt;03 # ,3 0"$ ,". ? "$ a3 ! , $ !4 0 4 0."$ "3 9 + $ .".$ % e e i# 4 ."$ 4 ."#$ % "*#$ .$ +4 4."$ "3 9&lt; .$ "3 + $ .".$ %# ."+ $ # 6g 9 0"$ 1%$ %"0%."$ "3 9 + $ . .$ %3 $ 3 2"$$ $ +# $ #+6j 4 j 4+#" $ # 4 4"3 $ $ ."3 $ $ +# " 0%9 "$ %" "$ 0$ *4 3 0$ !" 6B"$ " ;3 0+ 4 ."$ "*#$"2" 3 "* 3 $ +4 4"$ " A3 3."# 0 $ .# $ 03 #""$ ""2" 3 "* 3 $ + $ "$ .$7% $ "$ .$ %#3 2 $ %4 3 3 1 ! 4 ."$ 91% "3 0"*3 ( CA00 0 9# i# $ 4 9 1*3 54 #*3 03 +"2" 3 "*3 "$ " $ CA3 $4 44 !# $ %"$%"2" 0 "$ "1"$ " CA 0 $ 4 4"+ $ 0$ "$ ""2" 3 "* 3 $ + 8 3 C 0 4 0 $ ! kIOQE Ll QE S @PG VLLR G R L_S L`G M OQJS M M S I@M m G `R S K LG kn]oG T@PG FIOR G JT@OM HR S VK G @OJQLR G NLR LG S @M K LTPG Ic G TOK NIR G @TJLM Z ppppl YYl kk q q q q q q q q q q / /"$ "%"2*! "$ " "" $4 4$ % ."# 0 $ 6 / /0.#$ 0%"*! "$ " "" $4 4$ % ."# 0 $ 6 / /"$ "%"2*! "$ " "" $4 4$ % . ) 4 0 0 $ #+ ! A3 3 $ # .# $ 03 $ % $ 21%$ % 03 # !"$ 26 -".3 ? B"$ "&lt;03 # ,3 0"$ ,". ? "$ a3 ! , $ !4 0 4 0."$ "3 9 + $ .".$ % e e i# 4 ."$ 4 ."#$ % "*#$ .$ +4 4."$ "3 9&lt; .$ "3 + $ .".$ %# ."+ $ # 6g 9 0"$ 1%$ %"0%."$ "3 9 + $ . .$ %3 $ 3 2"$$ $ +# $ #+6j 4 j 4+#" $ # 4 4"3 $ $ ."3 $ $ +# " 0%9 "$ %" "$ 0$ *4 3 0$ !" 6</p>
        <p>="$ "3 h&lt; .$ "3 + $ .="$ "3 h&lt; .$ "3 + $ .</p>
        <p>&gt; " j 23 2 $ % $ #+ A$ * :#5" +$ 003 33 ;"3 "$ 3 !+"" 0%"3 !+ A ."3 "$ % !" . g#." " 0%" $ 0 "$ 3 0"3"$ " B#"3# " 0%4 400 =$ % &gt; " j 23 2 $ % $ #+ %j ;C i 83 10+$ .$ + =,j C *" # ."! ! &gt; " &gt; " &gt; " &gt; " &gt; " q q q q q q q q q q&gt; " j 23 2 $ % $ #+ A$ * :#5" +$ 003 33 ;"3 "$ 3 !+"" 0%"3 !+ A ."3 "$ % !" . g#." " 0%" $ 0 "$ 3 0"3"$ " B#"3# " 0%4 400 =$ % &gt; " j 23 2 $ % $ #+ %j ;C i 83 10+$ .$ + =,j C *" # ."! ! &gt; " &gt; " &gt; " &gt; " &gt; " q q q q q q q q q q</p>
        <p>Acknowledgements We thank A. Dai and E. Gabrilovich for comments.Acknowledgements We thank A. Dai and E. Gabrilovich for comments.</p>
        <p>This work was supported in part by the National Cancer Institute (R01 CA237170).This work was supported in part by the National Cancer Institute (R01 CA237170).</p>
        <p>Gigantum https
            <rs type="url">://gigantum.com</rs> Colaboratory https
            <rs type="url">://colab.research.google.com</rs> Deep-learning models 
            <rs type="software">TensorFlow Hub</rs>
            <rs type="url">https://www.tensorflow.org/hub ModelHub</rs> http
            <rs type="url">://modelhub.ai ModelDepot</rs> https
            <rs type="url">://modeldepot.io</rs> Model Zoo https://modelzoo.co Deep-learning frameworks 
            <rs type="software">TensorFlow</rs>
            <rs type="url">https://www.tensorflow.org/ Caffe</rs>
            <rs type="url">https://caffe.berkeleyvision.org/</rs>
            <rs type="url">PyTorch</rs>
            <rs type="url">https://pytorch.org/</rs> communication regarding the materials and methods of their study.
        </p>
        <p>No data have been generated as part of this manuscript. Code BitBucket 
            <rs type="url">https://bitbucket.org</rs> GitHub https
            <rs type="url">://github.com</rs>
            <rs type="software">GitLab</rs>
            <rs type="url">https://about.gitlab.com</rs> Software dependencies Conda https
            <rs type="url">://conda.io</rs> Code Ocean 
            <rs type="url">https://codeocean.com</rs>
        </p>
        <p>Further information on research design is available in the Nature Research Reporting Summary linked to this paper. We thank the authors of the accompanying Comment 1 for their interest in our work 2 and their thoughtful contribution. We agree that transparency and reproducibility are paramount for scientific progress. In keeping with this principle, the largest data source used in our publication is available to the academic community. Any researcher can apply for access to the OPTIMAM database (https://medphys.royalsurrey.nhs. uk/omidb/getting-access), which our institution helped fund. The broad accessibility of the database was part of the reason we pursued this collaboration. In fact, since our article came out, another group has already published results on this very dataset 3 .Further information on research design is available in the Nature Research Reporting Summary linked to this paper. We thank the authors of the accompanying Comment 1 for their interest in our work 2 and their thoughtful contribution. We agree that transparency and reproducibility are paramount for scientific progress. In keeping with this principle, the largest data source used in our publication is available to the academic community. Any researcher can apply for access to the OPTIMAM database (https://medphys.royalsurrey.nhs. uk/omidb/getting-access), which our institution helped fund. The broad accessibility of the database was part of the reason we pursued this collaboration. In fact, since our article came out, another group has already published results on this very dataset 3 .</p>
        <p>The other dataset, from the United States, was shared with our research team after approval from the hospital system's Institutional Review Board (IRB). The IRB judged that the potential benefits of the research outweighed the minimal privacy risks associated with sharing de-identified data with a trusted party capable of and committed to safeguarding these data. As the authors understand, we are not at liberty to share data that we do not own. More generally, widely releasing data considerably alters the risk-benefit calculus for patients, so institutions must be thoughtful about how and when they do this. Because of these considerations, large medical image datasets with associated breast cancer outcomes are rarely made openly available [3][4][5] . However, as our support for the OPTIMAM database demonstrates, we endorse such efforts where practical. Although there are some small, publicly available mammography datasets 6 , restricting published research to such datasets would provide an extremely limited picture of an algorithm's clinical applicability.The other dataset, from the United States, was shared with our research team after approval from the hospital system's Institutional Review Board (IRB). The IRB judged that the potential benefits of the research outweighed the minimal privacy risks associated with sharing de-identified data with a trusted party capable of and committed to safeguarding these data. As the authors understand, we are not at liberty to share data that we do not own. More generally, widely releasing data considerably alters the risk-benefit calculus for patients, so institutions must be thoughtful about how and when they do this. Because of these considerations, large medical image datasets with associated breast cancer outcomes are rarely made openly available [3][4][5] . However, as our support for the OPTIMAM database demonstrates, we endorse such efforts where practical. Although there are some small, publicly available mammography datasets 6 , restricting published research to such datasets would provide an extremely limited picture of an algorithm's clinical applicability.</p>
        <p>The commenters 1 asked for more information concerning the training of our deep learning models. We strove to document all relevant machine learning methods while keeping the paper accessible to a clinical and general scientific audience. We thank the authors for highlighting the omission of some hyperparameters. We have supplied the requested methodological details and further elaborated on our data augmentation strategies in an Addendum 7 to our original Article 2 .The commenters 1 asked for more information concerning the training of our deep learning models. We strove to document all relevant machine learning methods while keeping the paper accessible to a clinical and general scientific audience. We thank the authors for highlighting the omission of some hyperparameters. We have supplied the requested methodological details and further elaborated on our data augmentation strategies in an Addendum 7 to our original Article 2 .</p>
        <p>The authors of the Comment 1 suggest open-sourcing all the code associated with this project. Most of our work builds on open-source implementations, such as 
            <rs type="software">ResNet</rs> (
            <rs type="url">https://github.com/tensorflow/ models/blob/master/research/slim/nets/resnet_v1.py</rs>), 
            <rs type="software">MobileNet</rs> (
            <rs type="url">https://github.com/tensorflow/models/blob/master/research/slim/ nets/mobilenet/mobilenet_v2.py</rs>), multidimensional image augmentation (
            <rs type="url">https://github.com/deepmind/multidim-image-augmentation)</rs>, and the 
            <rs type="software">Tensorflow Object Detection API</rs> (
            <rs type="url">https://github.com/tensorflow/models/tree/master/research/object_detection)</rs>, all of which were released by our institution. Much of the remaining code concerns data input-output and the orchestration of the training process across internal compute clusters, both of which are of scant scientific value and limited utility to researchers outside our organization. Given the extensive textual description in the supplementary information of our Article 2 , we believe that investigators proficient in deep learning should be able to learn from and expand upon our approach.
        </p>
        <p>The authors 1 further suggest releasing a containerized version of our model for others to apply to new images. It is important to note that regulators commonly classify technologies such as the one proposed here as 'medical device software' or 'software as a medical device'. Unfortunately, the release of any medical device without appropriate regulatory oversight could lead to its misuse. As such, doing so would overlook material ethical concerns. Because liability issues surrounding artificial intelligence in healthcare remain unresolved 8 , providing unrestricted access to such technologies may place patients, providers, and developers at risk. In addition, the development of impactful medical technologies must remain a sustainable venture to promote a vibrant ecosystem that supports future innovation. Parallels to hardware medical devices and pharmaceuticals may be useful to consider in this regard. Finally, increasing evidence suggests that a model's learned parameters may inadvertently expose properties of its training set to attack; how to safeguard potentially susceptible models is the subject of active research 9 . As our training data are private or under restricted access, sharing the model openly seems premature and may introduce risks that are not well characterized. On the basis of these concerns, we deliberately approach sharing artefacts derived from patient data (even if de-identified) with an abundance of caution.The authors 1 further suggest releasing a containerized version of our model for others to apply to new images. It is important to note that regulators commonly classify technologies such as the one proposed here as 'medical device software' or 'software as a medical device'. Unfortunately, the release of any medical device without appropriate regulatory oversight could lead to its misuse. As such, doing so would overlook material ethical concerns. Because liability issues surrounding artificial intelligence in healthcare remain unresolved 8 , providing unrestricted access to such technologies may place patients, providers, and developers at risk. In addition, the development of impactful medical technologies must remain a sustainable venture to promote a vibrant ecosystem that supports future innovation. Parallels to hardware medical devices and pharmaceuticals may be useful to consider in this regard. Finally, increasing evidence suggests that a model's learned parameters may inadvertently expose properties of its training set to attack; how to safeguard potentially susceptible models is the subject of active research 9 . As our training data are private or under restricted access, sharing the model openly seems premature and may introduce risks that are not well characterized. On the basis of these concerns, we deliberately approach sharing artefacts derived from patient data (even if de-identified) with an abundance of caution.</p>
        <p>No doubt the commenters 1 are motivated by protecting future patients as much as scientific principle. We share that sentiment. This work serves as an initial proof of concept, and is by no means the end of the story. We intend to subject our software to extensive testing before its use in a clinical environment, working alongside patients, providers and regulators to ensure efficacy and safety.No doubt the commenters 1 are motivated by protecting future patients as much as scientific principle. We share that sentiment. This work serves as an initial proof of concept, and is by no means the end of the story. We intend to subject our software to extensive testing before its use in a clinical environment, working alongside patients, providers and regulators to ensure efficacy and safety.</p>
    </text>
</tei>

<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T06:54+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>We provide an exact deterministic reformulation for data-driven chance constrained programs over Wasserstein balls. For individual chance constraints as well as joint chance constraints with right-hand side uncertainty, our reformulation amounts to a mixed-integer conic program. In the special case of a Wasserstein ball with the 1-norm or the ∞-norm, the cone is the nonnegative orthant, and the chance constrained program can be reformulated as a mixed-integer linear program. Our reformulation compares favourably to several state-of-the-art data-driven optimization schemes in our numerical experiments.We provide an exact deterministic reformulation for data-driven chance constrained programs over Wasserstein balls. For individual chance constraints as well as joint chance constraints with right-hand side uncertainty, our reformulation amounts to a mixed-integer conic program. In the special case of a Wasserstein ball with the 1-norm or the ∞-norm, the cone is the nonnegative orthant, and the chance constrained program can be reformulated as a mixed-integer linear program. Our reformulation compares favourably to several state-of-the-art data-driven optimization schemes in our numerical experiments.</p>
        <p>Distributionally robust optimization is a powerful modeling paradigm for optimization under uncertainty, where the distribution of the uncertain problem parameters is itself uncertain, and where the performance of a decision is assessed in view of the worst-case distribution from a prescribed ambiguity set. The earlier literature on distributionally robust optimization has focused on moment ambiguity sets which contain all distributions that obey certain (standard or generalized) moment conditions; see, e.g., Delage and Ye (2010), Goh and Sim (2010) and Wiesemann et al. (2014). Pflug and Wozabal (2007) were the first to propose an ambiguity set of the form of a ball in the space of distributions with respect to the celebrated Wasserstein, Kanthorovich or optimal transport distance. The type-1 Wasserstein distance d W (P 1 , P 2 ) between two distributions P 1 and P 2 on R K , equipped with a general norm ∥ • ∥, is defined as the minimal transportation cost of moving P 1 to P 2 under the premise that the cost of moving a Dirac point mass from ξ 1 to ξ 2 amounts to ∥ξ 1 -ξ 2 ∥. Mathematically, this implies that d W (P 1 , P 2 ) = infDistributionally robust optimization is a powerful modeling paradigm for optimization under uncertainty, where the distribution of the uncertain problem parameters is itself uncertain, and where the performance of a decision is assessed in view of the worst-case distribution from a prescribed ambiguity set. The earlier literature on distributionally robust optimization has focused on moment ambiguity sets which contain all distributions that obey certain (standard or generalized) moment conditions; see, e.g., Delage and Ye (2010), Goh and Sim (2010) and Wiesemann et al. (2014). Pflug and Wozabal (2007) were the first to propose an ambiguity set of the form of a ball in the space of distributions with respect to the celebrated Wasserstein, Kanthorovich or optimal transport distance. The type-1 Wasserstein distance d W (P 1 , P 2 ) between two distributions P 1 and P 2 on R K , equipped with a general norm ∥ • ∥, is defined as the minimal transportation cost of moving P 1 to P 2 under the premise that the cost of moving a Dirac point mass from ξ 1 to ξ 2 amounts to ∥ξ 1 -ξ 2 ∥. Mathematically, this implies that d W (P 1 , P 2 ) = inf</p>
        <p>where ξ1 ∼ P 1 , ξ2 ∼ P 2 , and P(P 1 , P 2 ) represents the set of all distributions on R K × R K with marginals P 1 and P 2 . The Wasserstein ambiguity set F(θ) is then defined as a ball of radius θ ≥ 0 with respect to the Wasserstein distance, centered at a prescribed reference distribution P:where ξ1 ∼ P 1 , ξ2 ∼ P 2 , and P(P 1 , P 2 ) represents the set of all distributions on R K × R K with marginals P 1 and P 2 . The Wasserstein ambiguity set F(θ) is then defined as a ball of radius θ ≥ 0 with respect to the Wasserstein distance, centered at a prescribed reference distribution P:</p>
        <p>One can think of the Wasserstein radius θ as a budget on the transportation cost. Indeed, any member distribution in F(θ) can be obtained by rearranging the reference distribution P at a transportation cost of at most θ. If only a finite training dataset { ξi } i∈[N ] is available, a natural choice for P is the empirical distribution P = 1 N N i=1 δ ξi , which represents the uniform distribution on the training samples. Throughout the paper, we will assume that P is the empirical distribution.One can think of the Wasserstein radius θ as a budget on the transportation cost. Indeed, any member distribution in F(θ) can be obtained by rearranging the reference distribution P at a transportation cost of at most θ. If only a finite training dataset { ξi } i∈[N ] is available, a natural choice for P is the empirical distribution P = 1 N N i=1 δ ξi , which represents the uniform distribution on the training samples. Throughout the paper, we will assume that P is the empirical distribution.</p>
        <p>While it has been recognized early on that Wasserstein ambiguity sets offer many conceptual advantages (e.g., their member distributions do not need to be absolutely continuous with respect to P and, if properly calibrated, they constitute confidence regions for the unknown true datagenerating distribution), it was believed that they almost invariably lead to hard global optimization problems. Recently, Mohajerin Esfahani and Kuhn (2018) and Zhao and Guan (2018) discovered that many interesting distributionally robust optimization problems over Wasserstein ambiguity sets can actually be reformulated as tractable convex programs-provided that P is discrete and that the problem's objective function satisfies certain convexity properties. These reformulations have subsequently been generalized to Polish spaces and non-discrete reference distributions by Blanchet and Murthy (2019) and Gao and Kleywegt (2016). Since then, distributionally robust optimization models over Wasserstein ambiguity sets have been proposed for many applications, including transportation (Carlsson et al. 2018) and machine learning (Blanchet et al. 2019, Gao et al. 2017, Shafieezadeh-Abadeh et al. 2019and Sinha et al. 2017).While it has been recognized early on that Wasserstein ambiguity sets offer many conceptual advantages (e.g., their member distributions do not need to be absolutely continuous with respect to P and, if properly calibrated, they constitute confidence regions for the unknown true datagenerating distribution), it was believed that they almost invariably lead to hard global optimization problems. Recently, Mohajerin Esfahani and Kuhn (2018) and Zhao and Guan (2018) discovered that many interesting distributionally robust optimization problems over Wasserstein ambiguity sets can actually be reformulated as tractable convex programs-provided that P is discrete and that the problem's objective function satisfies certain convexity properties. These reformulations have subsequently been generalized to Polish spaces and non-discrete reference distributions by Blanchet and Murthy (2019) and Gao and Kleywegt (2016). Since then, distributionally robust optimization models over Wasserstein ambiguity sets have been proposed for many applications, including transportation (Carlsson et al. 2018) and machine learning (Blanchet et al. 2019, Gao et al. 2017, Shafieezadeh-Abadeh et al. 2019and Sinha et al. 2017).</p>
        <p>In this paper we study distributionally robust chance constrained programs of the formIn this paper we study distributionally robust chance constrained programs of the form</p>
        <p>where the goal is to find a decision x from within a compact polyhedron X ⊆ R L that minimizes a linear cost function c ⊤ x and ensures that the exogenous random vector ξ falls within a decisiondependent safety set S(x) ⊆ R K with high probability 1 -ε under every distribution P ∈ F(θ).where the goal is to find a decision x from within a compact polyhedron X ⊆ R L that minimizes a linear cost function c ⊤ x and ensures that the exogenous random vector ξ falls within a decisiondependent safety set S(x) ⊆ R K with high probability 1 -ε under every distribution P ∈ F(θ).</p>
        <p>Since the reference distribution P in (2) is the empirical distribution over the training dataset { ξi } i∈[N ] , we refer to (2) as a data-driven chance constrained program.Since the reference distribution P in (2) is the empirical distribution over the training dataset { ξi } i∈[N ] , we refer to (2) as a data-driven chance constrained program.</p>
        <p>To date, the literature on data-driven chance constraints has focused primarily on variants of problem (2) where the Wasserstein ambiguity set F(θ) is replaced with an ambiguity set G(θ) that contains all distributions close to the empirical distribution P with respect to a ϕ-divergence (such as the Kullback-Leibler divergence or the χ 2 -distance):To date, the literature on data-driven chance constraints has focused primarily on variants of problem (2) where the Wasserstein ambiguity set F(θ) is replaced with an ambiguity set G(θ) that contains all distributions close to the empirical distribution P with respect to a ϕ-divergence (such as the Kullback-Leibler divergence or the χ 2 -distance):</p>
        <p>where ϕ : R + → R is the divergence function. Hu and Hong (2013) show that a distributionally robust chance constrained program over a Kullback-Leibler ambiguity set reduces to a classical chance constrained progam over the reference distribution P and an adjusted risk threshold ε ′ &lt; ε.where ϕ : R + → R is the divergence function. Hu and Hong (2013) show that a distributionally robust chance constrained program over a Kullback-Leibler ambiguity set reduces to a classical chance constrained progam over the reference distribution P and an adjusted risk threshold ε ′ &lt; ε.</p>
        <p>While this result holds for any reference distribution, ϕ-divergence ambiguity sets only contain distributions that are absolutely continuous with respect to P, that is, any distribution in G(θ)While this result holds for any reference distribution, ϕ-divergence ambiguity sets only contain distributions that are absolutely continuous with respect to P, that is, any distribution in G(θ)</p>
        <p>only assigns positive probability to those measurable subsets A ⊆ R K for which P[ ξ ∈ A] &gt; 0. This is undesirable for problems with a large dimension K and/or few training data, where it is unlikely that every possible value of ξ has been observed inonly assigns positive probability to those measurable subsets A ⊆ R K for which P[ ξ ∈ A] &gt; 0. This is undesirable for problems with a large dimension K and/or few training data, where it is unlikely that every possible value of ξ has been observed in</p>
        <p>. This shortcoming is addressed by Jiang andGuan (2016, 2018), who replace the reference distribution with a Kernel density estimator.. This shortcoming is addressed by Jiang andGuan (2016, 2018), who replace the reference distribution with a Kernel density estimator.</p>
        <p>Despite their tremendous success and widespread adoption in recent years, the use of ϕdivergences can lead to undesirable side effects in some applications: they compare distributions on a "scenario-by-scenario" basis and thus do not consider the possibility of noisy measurements (Gao and Kleywegt 2016), and they generically fail to be probability metrics as they typically violate symmetry as well as the triangle inequality. Moreover, as we show next, ϕ-divergence ambiguity sets may be overly optimistic when only few training samples are available.Despite their tremendous success and widespread adoption in recent years, the use of ϕdivergences can lead to undesirable side effects in some applications: they compare distributions on a "scenario-by-scenario" basis and thus do not consider the possibility of noisy measurements (Gao and Kleywegt 2016), and they generically fail to be probability metrics as they typically violate symmetry as well as the triangle inequality. Moreover, as we show next, ϕ-divergence ambiguity sets may be overly optimistic when only few training samples are available.</p>
        <p>Motivating Example. Consider the arguably simplest instance of the data-driven optimization problem (2), which estimates the worst-case value-at-risk sup P∈F (θ) P-VaR ε ( ξ) of a scalar random variable ξ at level ε from a limited set of i.i.d. training samples { ξi } N i=1 of ξ under the unknown data-generating distribution P 0 that are summarized by the empirical distribution P = 1 N N i=1 δ ξi at the centre of the Wasserstein ball F(θ). To avoid technicalities, we assume that P 0 is atomless.Motivating Example. Consider the arguably simplest instance of the data-driven optimization problem (2), which estimates the worst-case value-at-risk sup P∈F (θ) P-VaR ε ( ξ) of a scalar random variable ξ at level ε from a limited set of i.i.d. training samples { ξi } N i=1 of ξ under the unknown data-generating distribution P 0 that are summarized by the empirical distribution P = 1 N N i=1 δ ξi at the centre of the Wasserstein ball F(θ). To avoid technicalities, we assume that P 0 is atomless.</p>
        <p>to be used subsequently. Here, ξ(j) denotes the j-th order statistic of the training samples { ξi } N i=1 .to be used subsequently. Here, ξ(j) denotes the j-th order statistic of the training samples { ξi } N i=1 .</p>
        <p>Reliability bounds for the Wasserstein (worst-case) and ϕ-divergence (best-case) ambiguity sets whenReliability bounds for the Wasserstein (worst-case) and ϕ-divergence (best-case) ambiguity sets when</p>
        <p>approximating the VaR at level ε = 0.1 (left), ε = 0.05 (middle) and ε = 0.01 (right). We choose the radius θ = 1/ √ N for the Wassestein ball (see, e.g., Mohajerin Esfahani and Kuhn 2018).approximating the VaR at level ε = 0.1 (left), ε = 0.05 (middle) and ε = 0.01 (right). We choose the radius θ = 1/ √ N for the Wassestein ball (see, e.g., Mohajerin Esfahani and Kuhn 2018).</p>
        <p>The reliability of the aforementioned worst-case value-at-risk, that is, the probability that it weakly exceeds the unknown true value-at-risk P 0 -VaR ε ( ξ), can be bounded from below byThe reliability of the aforementioned worst-case value-at-risk, that is, the probability that it weakly exceeds the unknown true value-at-risk P 0 -VaR ε ( ξ), can be bounded from below by</p>
        <p>where P N 0 is the N -fold product of P 0 that generates { ξi } N i=1 . The first inequality holds since P † is contained in F(θ). The first equality holds since P † -VaR ε ( ξ) = P-VaR ε ( ξ) + θ/ε by construction of P † , and the last inequality is due to a standard concentration inequality for empirical quantiles (see, e.g., Theorem 2.3.2 of Serfling 2009).where P N 0 is the N -fold product of P 0 that generates { ξi } N i=1 . The first inequality holds since P † is contained in F(θ). The first equality holds since P † -VaR ε ( ξ) = P-VaR ε ( ξ) + θ/ε by construction of P † , and the last inequality is due to a standard concentration inequality for empirical quantiles (see, e.g., Theorem 2.3.2 of Serfling 2009).</p>
        <p>If we replace the Wasserstein ambiguity set F(θ) with the ambiguity G(θ) of any ϕ-divergence, on the other hand, then we can bound the reliability from above byIf we replace the Wasserstein ambiguity set F(θ) with the ambiguity G(θ) of any ϕ-divergence, on the other hand, then we can bound the reliability from above by</p>
        <p>Here, the first inequality holds since all distributions in G(θ) share a common support with P, and the second inequality follows from the definition of P 0 -VaR ε ( ξ). We highlight that this probability bound holds for every radius θ of the ϕ-divergence ball G(θ).Here, the first inequality holds since all distributions in G(θ) share a common support with P, and the second inequality follows from the definition of P 0 -VaR ε ( ξ). We highlight that this probability bound holds for every radius θ of the ϕ-divergence ball G(θ).</p>
        <p>Figure 1 compares the worst-case reliability offered by the Wasserstein ambiguity set with the best-case reliability of the ϕ-divergence ambiguity set for a uniform distribution over the interval [0, 1]. We observe that in low-sample regimes, ϕ-divergence ambiguity sets may underestimate the true VaR with high probability. ♣Figure 1 compares the worst-case reliability offered by the Wasserstein ambiguity set with the best-case reliability of the ϕ-divergence ambiguity set for a uniform distribution over the interval [0, 1]. We observe that in low-sample regimes, ϕ-divergence ambiguity sets may underestimate the true VaR with high probability. ♣</p>
        <p>To our best knowledge, the paper of Xie and Ahmed (2020) is the only previous work on data-driven chance constraints over Wasserstein ambiguity sets. The authors study the special class of covering problems, where the feasible region X satisfies ηX ⊆ X for every η ≥ 1. This problem class encompasses, among others, portfolio optimization problems without budgetary restrictions and lot-sizing problems in the absence of setup costs. The authors prove that the resulting individual chance constrained program is NP-hard. They also demonstrate that two popular approximation schemes, the CVaR approximation as well as the scenario approximation, can perform arbitrarily poorly for classical individual chance constraints, that is, when the Wasserstein radius is θ = 0. Based on this insight, the authors propose a bicriteria approximation scheme for covering problems with classical as well as distributionally robust individual chance constraints over moment and Wasserstein ambiguity sets. This bicriteria approximation scheme determines solutions that trade off a higher risk threshold ε ′ &gt; ε in the chance constraint with a smaller optimality gap ε ′ /(ε ′ -ε). This is achieved by solving a tractable convex relaxation of the chance constrained problem (using, e.g., a Markovian or Bernstein generator) and subsequently scaling the solution to this relaxation so that it becomes feasible for the chance constraint with the higher risk threshold ε ′ . By design, the performance guarantee of the bicriteria approximation scheme becomes weaker (and eventually trivial) as the selected risk threshold ε ′ approaches the risk threshold ε of the original problem formulation.To our best knowledge, the paper of Xie and Ahmed (2020) is the only previous work on data-driven chance constraints over Wasserstein ambiguity sets. The authors study the special class of covering problems, where the feasible region X satisfies ηX ⊆ X for every η ≥ 1. This problem class encompasses, among others, portfolio optimization problems without budgetary restrictions and lot-sizing problems in the absence of setup costs. The authors prove that the resulting individual chance constrained program is NP-hard. They also demonstrate that two popular approximation schemes, the CVaR approximation as well as the scenario approximation, can perform arbitrarily poorly for classical individual chance constraints, that is, when the Wasserstein radius is θ = 0. Based on this insight, the authors propose a bicriteria approximation scheme for covering problems with classical as well as distributionally robust individual chance constraints over moment and Wasserstein ambiguity sets. This bicriteria approximation scheme determines solutions that trade off a higher risk threshold ε ′ &gt; ε in the chance constraint with a smaller optimality gap ε ′ /(ε ′ -ε). This is achieved by solving a tractable convex relaxation of the chance constrained problem (using, e.g., a Markovian or Bernstein generator) and subsequently scaling the solution to this relaxation so that it becomes feasible for the chance constraint with the higher risk threshold ε ′ . By design, the performance guarantee of the bicriteria approximation scheme becomes weaker (and eventually trivial) as the selected risk threshold ε ′ approaches the risk threshold ε of the original problem formulation.</p>
        <p>In this paper, we study distributionally robust chance constrained programs over the Wasserstein ambiguity set (1). We derive deterministic reformulations for individual chance constrained programs, where S(x) = {ξ ∈ R K | a(ξ) ⊤ x &lt; b(ξ)} for affine functions a(•) : R K → R L and b(•) :In this paper, we study distributionally robust chance constrained programs over the Wasserstein ambiguity set (1). We derive deterministic reformulations for individual chance constrained programs, where S(x) = {ξ ∈ R K | a(ξ) ⊤ x &lt; b(ξ)} for affine functions a(•) : R K → R L and b(•) :</p>
        <p>R K → R, as well as for joint chance constrained programs with right-hand side uncertainty, whereR K → R, as well as for joint chance constrained programs with right-hand side uncertainty, where</p>
        <p>and an affine function b : R K → R M . Our reformulations are mixed-integer conic programs that reduce to mixed-integer linear programs when the norm ∥•∥ on R K is the 1-norm or the ∞-norm.and an affine function b : R K → R M . Our reformulations are mixed-integer conic programs that reduce to mixed-integer linear programs when the norm ∥•∥ on R K is the 1-norm or the ∞-norm.</p>
        <p>While preparing this paper for publication, we became aware of the independent work by Xie (2019), which derives similar reformulations for distributionally individual and joint chance constraints over Wasserstein ambiguity sets. In contrast to our work, however, Xie (2019) assumes that each safety condition a ⊤ m x &lt; b m (ξ), m ∈ [M ], in the joint chance constraint depends on a subvector of ξ, and that these subvectors are pairwise disjoint for different safety conditions. In other words, different safety conditions of the joint chance constraints studied by Xie (2019) must depend on different random variables. Furthermore, the reformulations of Xie (2019) are derived via duality theory, whereas our reformulations directly leverage the structural insights into the worst-case distributions. This enables us to keep our reformulations largely independent of the selected ground metric for the Wasserstein ball, which opens up possibilities to incorporate other cost functions in our definition of the Wasserstein distance. Since the initial submission of this paper, our exact reformulation for data-driven chance constrained program over Wasserstein balls has been further studied and tightened; see, for instance, Ho-Nguyen et al. (2020, 2021), Shen and Jiang (2021) and Zhang and Dong (2021). Along with these theoretical extensions, our reformulation has also been applied in several domains, including risk sharing in finance (Chen and Xie 2021), network design for humanitarian operations (Jiang et al. 2021) and optimal power flows in energy systems (Arrigo et al. 2022).While preparing this paper for publication, we became aware of the independent work by Xie (2019), which derives similar reformulations for distributionally individual and joint chance constraints over Wasserstein ambiguity sets. In contrast to our work, however, Xie (2019) assumes that each safety condition a ⊤ m x &lt; b m (ξ), m ∈ [M ], in the joint chance constraint depends on a subvector of ξ, and that these subvectors are pairwise disjoint for different safety conditions. In other words, different safety conditions of the joint chance constraints studied by Xie (2019) must depend on different random variables. Furthermore, the reformulations of Xie (2019) are derived via duality theory, whereas our reformulations directly leverage the structural insights into the worst-case distributions. This enables us to keep our reformulations largely independent of the selected ground metric for the Wasserstein ball, which opens up possibilities to incorporate other cost functions in our definition of the Wasserstein distance. Since the initial submission of this paper, our exact reformulation for data-driven chance constrained program over Wasserstein balls has been further studied and tightened; see, for instance, Ho-Nguyen et al. (2020, 2021), Shen and Jiang (2021) and Zhang and Dong (2021). Along with these theoretical extensions, our reformulation has also been applied in several domains, including risk sharing in finance (Chen and Xie 2021), network design for humanitarian operations (Jiang et al. 2021) and optimal power flows in energy systems (Arrigo et al. 2022).</p>
        <p>Notation. We use boldface uppercase and lowercase letters to denote matrices and vectors, respectively. Special vectors of appropriate dimensions include 0 and e, which respectively correspond to the zero vector and the vector of all ones. We denote by ∥ • ∥ * the dual norm of a general norm ∥ • ∥.Notation. We use boldface uppercase and lowercase letters to denote matrices and vectors, respectively. Special vectors of appropriate dimensions include 0 and e, which respectively correspond to the zero vector and the vector of all ones. We denote by ∥ • ∥ * the dual norm of a general norm ∥ • ∥.</p>
        <p>We use the shorthand [N ] = {1, 2, . . . , N } to represent the set of all integers up to N . Given a (possibly fractional) real number ℓ ∈ [0, N ], we define the partial sum of the ℓ first values inWe use the shorthand [N ] = {1, 2, . . . , N } to represent the set of all integers up to N . Given a (possibly fractional) real number ℓ ∈ [0, N ], we define the partial sum of the ℓ first values in</p>
        <p>Random vectors are denoted by tilde signs (e.g., ξ), while their realizations are denoted by the same symbols without tildes (e.g., ξ). Given a random vector ξ governed by a distribution P, a measurable loss function ℓ(ξ) and a risk threshold ε ∈ (0, 1), the value-at-risk (VaR) of ℓ(ξ) at level ε is defined asRandom vectors are denoted by tilde signs (e.g., ξ), while their realizations are denoted by the same symbols without tildes (e.g., ξ). Given a random vector ξ governed by a distribution P, a measurable loss function ℓ(ξ) and a risk threshold ε ∈ (0, 1), the value-at-risk (VaR) of ℓ(ξ) at level ε is defined as</p>
        <p>Section 2.1 reviews a previously established result on the quantification of uncertainty over Wasserstein balls. We use this result to derive an exact reformulation of generic data-driven chance constrained programs in Section 2.2. We finally specialize this generic reformulation to the subclasses of data-driven individual chance constrained programs as well as data-driven joint chance constrained programs with right-hand side uncertainty in Sections 2.3 and 2.4, respectively.Section 2.1 reviews a previously established result on the quantification of uncertainty over Wasserstein balls. We use this result to derive an exact reformulation of generic data-driven chance constrained programs in Section 2.2. We finally specialize this generic reformulation to the subclasses of data-driven individual chance constrained programs as well as data-driven joint chance constrained programs with right-hand side uncertainty in Sections 2.3 and 2.4, respectively.</p>
        <p>Consider an open safety set S ⊆ R K , and denote by S = R K \ S its closed complement. The uncertainty quantification problem supConsider an open safety set S ⊆ R K , and denote by S = R K \ S its closed complement. The uncertainty quantification problem sup</p>
        <p>computes the worst (largest) probability of the system under consideration being unsafe, which is the case whenever the random vector ξ attains a value in the unsafe set S. Throughout the rest of the paper, we exclude trivial special cases and assume that θ &gt; 0 and ε ∈ (0, 1).computes the worst (largest) probability of the system under consideration being unsafe, which is the case whenever the random vector ξ attains a value in the unsafe set S. Throughout the rest of the paper, we exclude trivial special cases and assume that θ &gt; 0 and ε ∈ (0, 1).</p>
        <p>To solve the uncertainty quantification problem (3), denote by dist( ξi , S) the distance of the i th data point ξi ∈ R K of the empirical distribution P to the unsafe set S. This distance is based on a norm ∥•∥, which we keep generic at this stage. Without loss of generality, we assume that the data points { ξi } i∈[N ] are ordered in increasing distance to S, that is, dist( ξi , S) ≤ dist( ξj , S) for all 1 ≤ i ≤ j ≤ N . We also assume that dist( ξi , S) = 0 (that is, the data point ξi is unsafe) if andTo solve the uncertainty quantification problem (3), denote by dist( ξi , S) the distance of the i th data point ξi ∈ R K of the empirical distribution P to the unsafe set S. This distance is based on a norm ∥•∥, which we keep generic at this stage. Without loss of generality, we assume that the data points { ξi } i∈[N ] are ordered in increasing distance to S, that is, dist( ξi , S) ≤ dist( ξj , S) for all 1 ≤ i ≤ j ≤ N . We also assume that dist( ξi , S) = 0 (that is, the data point ξi is unsafe) if and</p>
        <p>. Finally, we denote by ξ ⋆ i ∈ S an unsafe point that is closest to the data point ξi , i ∈ [N ], in terms of the distance dist( ξi , S). Blanchet and Murthy (2019) as well as Gao and Kleywegt (2016) have characterized the solution to the uncertainty quantification problem (3) in closed form. To keep our paper self-contained, we reproduce their findings without proof in Theorem 1 below.. Finally, we denote by ξ ⋆ i ∈ S an unsafe point that is closest to the data point ξi , i ∈ [N ], in terms of the distance dist( ξi , S). Blanchet and Murthy (2019) as well as Gao and Kleywegt (2016) have characterized the solution to the uncertainty quantification problem (3) in closed form. To keep our paper self-contained, we reproduce their findings without proof in Theorem 1 below.</p>
        <p>where p ⋆ = (θN -where p ⋆ = (θN -</p>
        <p>Intuitively speaking, the worst-case distribution P ⋆ in Theorem 1 transports the training datasetIntuitively speaking, the worst-case distribution P ⋆ in Theorem 1 transports the training dataset</p>
        <p>to the unsafe set S in a greedy fashion, see Figure 2. The data points ξ1 , . . . , ξI are already unsafe and hence do not need to be transported. The subsequent data points ξI+1 , . . . , ξj ⋆ +1 are closest to the unsafe set and are thus transported from S to S. Due to the limited transportation budget θ, the data point ξj ⋆ +1 is only partially transported. The safe samples ξj ⋆ +2 , . . . ξN , finally, are too far away from the unsafe set S and are thus left unchanged. Note that the distribution characterized in Theorem 1 may not be the only distribution that solves problem (3).to the unsafe set S in a greedy fashion, see Figure 2. The data points ξ1 , . . . , ξI are already unsafe and hence do not need to be transported. The subsequent data points ξI+1 , . . . , ξj ⋆ +1 are closest to the unsafe set and are thus transported from S to S. Due to the limited transportation budget θ, the data point ξj ⋆ +1 is only partially transported. The safe samples ξj ⋆ +2 , . . . ξN , finally, are too far away from the unsafe set S and are thus left unchanged. Note that the distribution characterized in Theorem 1 may not be the only distribution that solves problem (3).</p>
        <p>We now develop deterministic reformulations for the distributionally robust chance constrained program (2). To this end, we focus on the ambiguous chance constraint supWe now develop deterministic reformulations for the distributionally robust chance constrained program (2). To this end, we focus on the ambiguous chance constraint sup</p>
        <p>For any fixed decision x ∈ X , we let S(x) be an arbitrary open safety set, and we denote by S(x) its closed complement, which comprises all unsafe scenarios. Every fixed training dataset then induces a (decision-dependent) permutation π(x) of [N ] that orders the training samples in increasing distance to the unsafe set, that is,For any fixed decision x ∈ X , we let S(x) be an arbitrary open safety set, and we denote by S(x) its closed complement, which comprises all unsafe scenarios. Every fixed training dataset then induces a (decision-dependent) permutation π(x) of [N ] that orders the training samples in increasing distance to the unsafe set, that is,</p>
        <p>We first show that a fixed decision x satisfies the ambiguous chance constraint (4) over the Wasserstein ambiguity set (1) if and only if the partial sum of the εN smallest transportation distances to the unsafe set multiplied by the mass 1/N of a training sample exceeds θ.We first show that a fixed decision x satisfies the ambiguous chance constraint (4) over the Wasserstein ambiguity set (1) if and only if the partial sum of the εN smallest transportation distances to the unsafe set multiplied by the mass 1/N of a training sample exceeds θ.</p>
        <p>Theorem 2. For any fixed decision x ∈ X , the ambiguous chance constraint (4) over the Wasserstein ambiguity set (1) is equivalent to the deterministic inequalityTheorem 2. For any fixed decision x ∈ X , the ambiguous chance constraint (4) over the Wasserstein ambiguity set (1) is equivalent to the deterministic inequality</p>
        <p>The left-hand side of ( 5) can be interpreted as the minimum cost of moving a fraction ε of the training samples to the unsafe set. If this cost exceeds the prescribed transportation budget θ, then no distribution in the Wasserstein ambiguity set can assign the unsafe set a probability of more than ε, which means that the distributionally robust chance constraint (4) is satisfied.The left-hand side of ( 5) can be interpreted as the minimum cost of moving a fraction ε of the training samples to the unsafe set. If this cost exceeds the prescribed transportation budget θ, then no distribution in the Wasserstein ambiguity set can assign the unsafe set a probability of more than ε, which means that the distributionally robust chance constraint (4) is satisfied.</p>
        <p>Proof of Theorem 2. From Theorem 1 we know that the worst-case distribution P ⋆ is an optimal solution (not necessarily unique) to the maximization problem embedded in the left-hand side of the ambiguous chance constraint (4). We thus conclude that the constraint (4) is satisfied if and only if P ⋆ [ ξ / ∈ S(x)] ≤ ε for P ⋆ defined in the statement of that theorem.Proof of Theorem 2. From Theorem 1 we know that the worst-case distribution P ⋆ is an optimal solution (not necessarily unique) to the maximization problem embedded in the left-hand side of the ambiguous chance constraint (4). We thus conclude that the constraint (4) is satisfied if and only if P ⋆ [ ξ / ∈ S(x)] ≤ ε for P ⋆ defined in the statement of that theorem.</p>
        <p>In case (i ) of Theorem 1, the ambiguous chance constraint (4) is violated since P ⋆ [ ξ / ∈ S(x)] = 1 while ε &lt; 1 by assumption. At the same time, since j ⋆ = N , we have 1In case (i ) of Theorem 1, the ambiguous chance constraint (4) is violated since P ⋆ [ ξ / ∈ S(x)] = 1 while ε &lt; 1 by assumption. At the same time, since j ⋆ = N , we have 1</p>
        <p>If the inequality is satisfied as an equality, on the other hand, we know that dist( ξπ N (x) , S(x)) &gt; 0 since θ &gt; 0 by assumption and dist( ξπ i (x) , S(x)) ≤ dist( ξπ j (x) , S(x)) for all i ≤ j by construction of the re-ordering π(x). Thus, since ε &lt; 1 by assumption, we haveIf the inequality is satisfied as an equality, on the other hand, we know that dist( ξπ N (x) , S(x)) &gt; 0 since θ &gt; 0 by assumption and dist( ξπ i (x) , S(x)) ≤ dist( ξπ j (x) , S(x)) for all i ≤ j by construction of the re-ordering π(x). Thus, since ε &lt; 1 by assumption, we have</p>
        <p>, S(x)) = θ and equation ( 5) is violated as desired. In case (ii ) of Theorem 1, we have x) , S(x)). We claim that j ⋆ + p ⋆ is the optimal value of the bivariate mixed-integer optimization problem max, S(x)) = θ and equation ( 5) is violated as desired. In case (ii ) of Theorem 1, we have x) , S(x)). We claim that j ⋆ + p ⋆ is the optimal value of the bivariate mixed-integer optimization problem max</p>
        <p>Indeed, the solution (j, p) = (j ⋆ , p ⋆ ) is feasible in (6) by definition of j ⋆ and p ⋆ . Moreover, we have j + p &lt; j ⋆ + p ⋆ for any other feasible solution (j, p) that satisfies j = j ⋆ and p ̸ = p ⋆ . Assume now that the optimal solution (j, p) to (6) would satisfy j &gt; j ⋆ . Any such solution would violate the first constraint since j i=1 dist( ξπ i (x) , S(x)) &gt; θN by definition of j ⋆ while p ≥ 0. Similarly, any solution (j, p) with j &lt; j ⋆ cannot be optimal in (6) since j ≤ j ⋆ -1 while p &lt; p ⋆ + 1.Indeed, the solution (j, p) = (j ⋆ , p ⋆ ) is feasible in (6) by definition of j ⋆ and p ⋆ . Moreover, we have j + p &lt; j ⋆ + p ⋆ for any other feasible solution (j, p) that satisfies j = j ⋆ and p ̸ = p ⋆ . Assume now that the optimal solution (j, p) to (6) would satisfy j &gt; j ⋆ . Any such solution would violate the first constraint since j i=1 dist( ξπ i (x) , S(x)) &gt; θN by definition of j ⋆ while p ≥ 0. Similarly, any solution (j, p) with j &lt; j ⋆ cannot be optimal in (6) since j ≤ j ⋆ -1 while p &lt; p ⋆ + 1.</p>
        <p>We can re-express problem (6) as the univariate discrete optimization problemWe can re-express problem (6) as the univariate discrete optimization problem</p>
        <p>Using our definition of partial sums, we observe that this problem is equivalent toUsing our definition of partial sums, we observe that this problem is equivalent to</p>
        <p>By construction, the mapping ϑ(j) = j i=1 dist( ξπ i (x) , S(x)), j ∈ [0, N ], is continuous and monotonically nondecreasing. It therefore affords the right inverse ϑ 3 visualizes the relationship between ϑ and ϑ -1 . We thus conclude that the ambiguous chance constraint (4) is satisfied if and only if where the last equivalence follows from ϑ • ϑ -1 (θN ) = θN , which holds because θN ≤ ϑ(N ) for j ⋆ &lt; N , as well as the fact that ϑ is monotonically nondecreasing. By definition, the right-hand side of the last equivalence holds if and only if (5) in the statement of the theorem is satisfied. □ Remark 1. We emphasize that the inequality (5) fails to be equivalent to the ambiguous chance constraint (4) when θ = 0, in which case the Wasserstein ball collapses to the singleton set F(0) = { P}. To see this, suppose that ξπ i (x) ∈ S(x) for all i = 1, . . . , I and ξπ i (x) ∈ S(x) for all i = I + 1, . . . , N , where I ≥ 1. If ε &lt; I/N , then the chance constraint (4) is violated becauseBy construction, the mapping ϑ(j) = j i=1 dist( ξπ i (x) , S(x)), j ∈ [0, N ], is continuous and monotonically nondecreasing. It therefore affords the right inverse ϑ 3 visualizes the relationship between ϑ and ϑ -1 . We thus conclude that the ambiguous chance constraint (4) is satisfied if and only if where the last equivalence follows from ϑ • ϑ -1 (θN ) = θN , which holds because θN ≤ ϑ(N ) for j ⋆ &lt; N , as well as the fact that ϑ is monotonically nondecreasing. By definition, the right-hand side of the last equivalence holds if and only if (5) in the statement of the theorem is satisfied. □ Remark 1. We emphasize that the inequality (5) fails to be equivalent to the ambiguous chance constraint (4) when θ = 0, in which case the Wasserstein ball collapses to the singleton set F(0) = { P}. To see this, suppose that ξπ i (x) ∈ S(x) for all i = 1, . . . , I and ξπ i (x) ∈ S(x) for all i = I + 1, . . . , N , where I ≥ 1. If ε &lt; I/N , then the chance constraint (4) is violated because</p>
        <p>while the inequality (5) holds trivially because εN i=1 dist( ξπ i (x) , S(x)) ≥ 0. Theorem 2 establishes that a decision x ∈ X satisfies the ambiguous chance constraint (4) if and only if the sum of the εN smallest distances of the training samples to the unsafe set S(x) weakly exceeds θN . This result is of computational interest because the sum of the εN smallest out of N real numbers is concave in those real numbers (while being convex in ε). This reveals that the constraint (5) is convex in the decision-dependent distances {dist( ξi , S(x))} i∈ [N ] . In the remainder we develop an efficient reformulation of this convex constraint that does not require an enumeration of all possible sums of εN different distances between the training samples and the unsafe set. This reformulation is based on the following auxiliary lemma.while the inequality (5) holds trivially because εN i=1 dist( ξπ i (x) , S(x)) ≥ 0. Theorem 2 establishes that a decision x ∈ X satisfies the ambiguous chance constraint (4) if and only if the sum of the εN smallest distances of the training samples to the unsafe set S(x) weakly exceeds θN . This result is of computational interest because the sum of the εN smallest out of N real numbers is concave in those real numbers (while being convex in ε). This reveals that the constraint (5) is convex in the decision-dependent distances {dist( ξi , S(x))} i∈ [N ] . In the remainder we develop an efficient reformulation of this convex constraint that does not require an enumeration of all possible sums of εN different distances between the training samples and the unsafe set. This reformulation is based on the following auxiliary lemma.</p>
        <p>Lemma 1. For any ε ∈ (0, 1), the sum of the εN smallest out of N real numbers k 1 , . . . , k N coincides with the optimal value of the linear programLemma 1. For any ε ∈ (0, 1), the sum of the εN smallest out of N real numbers k 1 , . . . , k N coincides with the optimal value of the linear program</p>
        <p>Proof of Lemma 1. By definition, the sum of the εN smallest elements of the set {k 1 , . . . , k N } corresponds to the optimal value of the (manifestly feasible) linear programProof of Lemma 1. By definition, the sum of the εN smallest elements of the set {k 1 , . . . , k N } corresponds to the optimal value of the (manifestly feasible) linear program</p>
        <p>The claim now follows from strong linear programming duality. □The claim now follows from strong linear programming duality. □</p>
        <p>Armed with Theorem 2 and Lemma 1, we are now ready to reformulate the chance constrained program (2) as a deterministic optimization problem.Armed with Theorem 2 and Lemma 1, we are now ready to reformulate the chance constrained program (2) as a deterministic optimization problem.</p>
        <p>Proof of Theorem 3. The claim follows immediately by using Theorem 2 to reformulate the chance constraint (4) as the inequality (5), using Lemma 1 to express the left-hand side of (5) as a linear maximization problem and substituting the resulting constraint back into (2). □Proof of Theorem 3. The claim follows immediately by using Theorem 2 to reformulate the chance constraint (4) as the inequality (5), using Lemma 1 to express the left-hand side of (5) as a linear maximization problem and substituting the resulting constraint back into (2). □</p>
        <p>We emphasize that the reformulation offered by Theorem 3 is independent of the selected ground metric dist(•, •). In the remainder, we assume that the ground metric is based on a norm ∥•∥.We emphasize that the reformulation offered by Theorem 3 is independent of the selected ground metric dist(•, •). In the remainder, we assume that the ground metric is based on a norm ∥•∥.</p>
        <p>Assume now that problem (2) accommodates an individual chance constraint defined through theAssume now that problem (2) accommodates an individual chance constraint defined through the</p>
        <p>Individual chance constrained programs have been studied, among others, in network design (Wang 2007), vehicle routing (Gounaris et al. 2013, Ghosal andWiesemann 2020) and portfolio optimization (Rujeerapaiboon et al. 2016, Dert andOldenkamp 2000). By Lemma 2 in the appendix, we haveIndividual chance constrained programs have been studied, among others, in network design (Wang 2007), vehicle routing (Gounaris et al. 2013, Ghosal andWiesemann 2020) and portfolio optimization (Rujeerapaiboon et al. 2016, Dert andOldenkamp 2000). By Lemma 2 in the appendix, we have</p>
        <p>where we adopt the convention that 0/0 = 0, and thus Theorem 3 allows us to reformulate problem (7) as the deterministic optimization problem minwhere we adopt the convention that 0/0 = 0, and thus Theorem 3 allows us to reformulate problem (7) as the deterministic optimization problem min</p>
        <p>Unfortunately, problem (8) fails to be convex as its constraints involve fractions of convex functions.Unfortunately, problem (8) fails to be convex as its constraints involve fractions of convex functions.</p>
        <p>Below we show, however, that problem (8) can be reformulated as a mixed integer conic program.Below we show, however, that problem (8) can be reformulated as a mixed integer conic program.</p>
        <p>Proposition 1. Assume that A ⊤ x ̸ = b for all x ∈ X . For the safety setProposition 1. Assume that A ⊤ x ̸ = b for all x ∈ X . For the safety set</p>
        <p>where M is a suitably large (but finite) positive constant.where M is a suitably large (but finite) positive constant.</p>
        <p>Proof of Proposition 1. We already know that the chance constrained program (2) is equivalent to the non-convex optimization problem (8). A complicating feature of this problem is the appearance of the maximum operator in the second constraint group, which evaluates the positive part ofProof of Proposition 1. We already know that the chance constrained program (2) is equivalent to the non-convex optimization problem (8). A complicating feature of this problem is the appearance of the maximum operator in the second constraint group, which evaluates the positive part of</p>
        <p>To eliminate this maximum operator, for each i ∈ [N ] we introduce a binary variable q i ∈ {0, 1}, and we re-express the i th member of the second constraint group via the two auxiliary constraintsTo eliminate this maximum operator, for each i ∈ [N ] we introduce a binary variable q i ∈ {0, 1}, and we re-express the i th member of the second constraint group via the two auxiliary constraints</p>
        <p>Note that at optimality we haveNote that at optimality we have</p>
        <p>x is negative and q i = 0 otherwise.x is negative and q i = 0 otherwise.</p>
        <p>Intuitively, q i thus activates the less restrictive one of the two auxiliary constraints in (10). Next, we apply the variable substitutions t ← t/∥b -A ⊤ x∥ * and s ← s/∥b -A ⊤ x∥ * , which is admissible because A ⊤ x ̸ = b for all x ∈ X . This change of variables yields the postulated reformulation (9).Intuitively, q i thus activates the less restrictive one of the two auxiliary constraints in (10). Next, we apply the variable substitutions t ← t/∥b -A ⊤ x∥ * and s ← s/∥b -A ⊤ x∥ * , which is admissible because A ⊤ x ̸ = b for all x ∈ X . This change of variables yields the postulated reformulation (9).</p>
        <p>To see that a finite value of M is sufficient for our reformulation to be exact, we show that the expression ((b -A ⊤ x) ⊤ ξi + ba ⊤ x)/∥b -A ⊤ x∥ * as well as the values of t and s i , i ∈ [N ], in (10) can all be bounded without affecting the optimal value of problem (9). This is clear for the fraction as X is compact and the denominator is non-zero for all x ∈ X . Moreover, t is nonnegative as otherwise the first constraint in (9) would be violated. For any fixed values of x and t, an optimal value of s i , i ∈ [N ], is given by sTo see that a finite value of M is sufficient for our reformulation to be exact, we show that the expression ((b -A ⊤ x) ⊤ ξi + ba ⊤ x)/∥b -A ⊤ x∥ * as well as the values of t and s i , i ∈ [N ], in (10) can all be bounded without affecting the optimal value of problem (9). This is clear for the fraction as X is compact and the denominator is non-zero for all x ∈ X . Moreover, t is nonnegative as otherwise the first constraint in (9) would be violated. For any fixed values of x and t, an optimal value of s i , i ∈ [N ], is given by s</p>
        <p>bounded, it thus remains to show that t can be bounded from above. Indeed, for sufficiently large (but finite) t, the slope of εN te ⊤ s ⋆ (x, t) on the left-hand side of the first constraint in ( 9) is -(1 -ε)N . Since ε &lt; 1, we thus conclude that this constraint is violated for large values of t. □ Remark 2. The condition that A ⊤ x ̸ = b for all x ∈ X does not restrict the generality of our formulation. Indeed, if an optimal solution (q ⋆ , s ⋆ , t ⋆ , x ⋆ ) to problem (9) satisfies A ⊤ x ⋆ ̸ = b, then x ⋆ solves problem (2) since our argument in the proof of Proposition 1 applies to x ⋆ even if A ⊤ x = b for some x ∈ X . Assume now that an optimal solution (q ⋆ , s ⋆ , t ⋆ , x ⋆ ) to problem (9) satisfiesbounded, it thus remains to show that t can be bounded from above. Indeed, for sufficiently large (but finite) t, the slope of εN te ⊤ s ⋆ (x, t) on the left-hand side of the first constraint in ( 9) is -(1 -ε)N . Since ε &lt; 1, we thus conclude that this constraint is violated for large values of t. □ Remark 2. The condition that A ⊤ x ̸ = b for all x ∈ X does not restrict the generality of our formulation. Indeed, if an optimal solution (q ⋆ , s ⋆ , t ⋆ , x ⋆ ) to problem (9) satisfies A ⊤ x ⋆ ̸ = b, then x ⋆ solves problem (2) since our argument in the proof of Proposition 1 applies to x ⋆ even if A ⊤ x = b for some x ∈ X . Assume now that an optimal solution (q ⋆ , s ⋆ , t ⋆ , x ⋆ ) to problem (9) satisfies</p>
        <p>In that case, the ambiguous chance constraint in problem (2) requires that a ⊤ x ⋆ &lt; b. If that is the case for x ⋆ , it is optimal in problem (2). If, finally, an optimal solution (q ⋆ , s ⋆ , t ⋆ , x ⋆ ) to problem (9) satisfies A ⊤ x ⋆ = b and a ⊤ x ⋆ ≥ b, then one would ideally like to solve a variant of problem ( 9) that includes the additional constraintIn that case, the ambiguous chance constraint in problem (2) requires that a ⊤ x ⋆ &lt; b. If that is the case for x ⋆ , it is optimal in problem (2). If, finally, an optimal solution (q ⋆ , s ⋆ , t ⋆ , x ⋆ ) to problem (9) satisfies A ⊤ x ⋆ = b and a ⊤ x ⋆ ≥ b, then one would ideally like to solve a variant of problem ( 9) that includes the additional constraint</p>
        <p>This variant of problem ( 9) can be solved by solving 2K + 1 versions of problem ( 9), where each version includes exactly one of the constraintsThis variant of problem ( 9) can be solved by solving 2K + 1 versions of problem ( 9), where each version includes exactly one of the constraints</p>
        <p>One readily verifies that the solution that attains the least objective value amongst these 2K + 1 versions of problem ( 9) is an optimal solution to problem (9) with the added constraint (11).One readily verifies that the solution that attains the least objective value amongst these 2K + 1 versions of problem ( 9) is an optimal solution to problem (9) with the added constraint (11).</p>
        <p>Remark 3. The mixed-integer conic program (9) simplifies to a mixed-integer linear program whenever ∥ • ∥ represents the 1-norm or the ∞-norm, and it can be reformulated as a mixedinteger second-order cone program whenever ∥ • ∥ represents a p-norm for some p ∈ Q, p &gt; 1, see Section 2.3.1 in Ben-Tal and Nemirovski (2001).Remark 3. The mixed-integer conic program (9) simplifies to a mixed-integer linear program whenever ∥ • ∥ represents the 1-norm or the ∞-norm, and it can be reformulated as a mixedinteger second-order cone program whenever ∥ • ∥ represents a p-norm for some p ∈ Q, p &gt; 1, see Section 2.3.1 in Ben-Tal and Nemirovski (2001).</p>
        <p>Remark 4. The deterministic reformulation ( 9) is remarkably parsimonious. For an Ldimensional feasible region X ⊆ R L and an empirical distribution P with N data points, our reformulation (9) has N binary variables, L + N + 1 continuous decisions as well as 2N + 1 constraints (excluding those that describe X ). In comparison, a classical chance constrained formulation, which is tantamount to setting the Wasserstein radius to θ = 0 in problem (2), has N binary variables, L continuous decisions as well as N + 1 constraints. Thus, adding distributional robustness only requires an additional N + 1 continuous decisions as well as N further constraints.Remark 4. The deterministic reformulation ( 9) is remarkably parsimonious. For an Ldimensional feasible region X ⊆ R L and an empirical distribution P with N data points, our reformulation (9) has N binary variables, L + N + 1 continuous decisions as well as 2N + 1 constraints (excluding those that describe X ). In comparison, a classical chance constrained formulation, which is tantamount to setting the Wasserstein radius to θ = 0 in problem (2), has N binary variables, L continuous decisions as well as N + 1 constraints. Thus, adding distributional robustness only requires an additional N + 1 continuous decisions as well as N further constraints.</p>
        <p>Remark 5. The deterministic reformulation (9) requires the specification of a sufficiently large constant M, which can typically be determined by an investigation of the structure of problem (9).Remark 5. The deterministic reformulation (9) requires the specification of a sufficiently large constant M, which can typically be determined by an investigation of the structure of problem (9).</p>
        <p>Alternatively, many commercial solver packages allow to directly specify the following reformulation of problem ( 9) via the use of piecewise linear constraints:Alternatively, many commercial solver packages allow to directly specify the following reformulation of problem ( 9) via the use of piecewise linear constraints:</p>
        <p>This formulation has the advantage that it does not require the specification of the constant M.This formulation has the advantage that it does not require the specification of the constant M.</p>
        <p>Assume next that problem (2) accommodates a joint chance constraint defined through the safetyAssume next that problem (2) accommodates a joint chance constraint defined through the safety</p>
        <p>}, in which the uncertainty affects only the righthand sides of the safety conditions. Without loss of generality, we may assume that b m ̸ = 0 for all m ∈ [M ]. Indeed, if b m = 0, then the m th safety condition in the chance constraint becomes independent of the uncertainty and can thus be absorbed in X . Joint chance constrained programs with right-hand side uncertainty have been proposed, among others, for problems in transportation (Luedtke et al. 2010), lot-sizing (Beraldi andRuszczyński 2002, Küçükyavuz 2012), unit commitment (Yanagisawa and Osogami 2013) and project management (Wiesemann et al. 2012).}, in which the uncertainty affects only the righthand sides of the safety conditions. Without loss of generality, we may assume that b m ̸ = 0 for all m ∈ [M ]. Indeed, if b m = 0, then the m th safety condition in the chance constraint becomes independent of the uncertainty and can thus be absorbed in X . Joint chance constrained programs with right-hand side uncertainty have been proposed, among others, for problems in transportation (Luedtke et al. 2010), lot-sizing (Beraldi andRuszczyński 2002, Küçükyavuz 2012), unit commitment (Yanagisawa and Osogami 2013) and project management (Wiesemann et al. 2012).</p>
        <p>Observe that the complement of the safety set is now representable as SObserve that the complement of the safety set is now representable as S</p>
        <p>. By Lemma 2 in the appendix we have. By Lemma 2 in the appendix we have</p>
        <p>With this closed-form expression for the distance to the unsafe set, we can reformulate problem (2) as a mixed integer conic program.With this closed-form expression for the distance to the unsafe set, we can reformulate problem (2) as a mixed integer conic program.</p>
        <p>Proposition 2. For the safety setProposition 2. For the safety set</p>
        <p>where M is a suitably large (but finite) positive constant.where M is a suitably large (but finite) positive constant.</p>
        <p>Proof of Proposition 2. By Theorem 3, the chance constrained program (2) is equivalent to (7).Proof of Proposition 2. By Theorem 3, the chance constrained program (2) is equivalent to (7).</p>
        <p>Using ( 12), the i th member of the second constraint group in (7) can be reformulated asUsing ( 12), the i th member of the second constraint group in (7) can be reformulated as</p>
        <p>To eliminate the maximum operator, we introduce a binary variable q i ∈ {0, 1} to re-express the above constraint asTo eliminate the maximum operator, we introduce a binary variable q i ∈ {0, 1} to re-express the above constraint as</p>
        <p>A similar argument as in the proof of Proposition 1 shows that a finite value of M is sufficient for our reformulation to be exact. □A similar argument as in the proof of Proposition 1 shows that a finite value of M is sufficient for our reformulation to be exact. □</p>
        <p>Similar to Remark 5 in the previous section, many commercial solvers allow to directly specify a reformulation of problem ( 13) that replaces the constant M with piecewise linear constraints.Similar to Remark 5 in the previous section, many commercial solvers allow to directly specify a reformulation of problem ( 13) that replaces the constant M with piecewise linear constraints.</p>
        <p>Remark 6. The deterministic reformulation (13) has N binary variables, L + N + 1 continuous decisions as well as (M + 1)N + 1 constraints (excluding those that describe X ). In comparison, the corresponding classical chance constrained formulation has N binary variables, L continuous decisions as well as M N + 1 constraints. Thus, adding distributional robustness requires an additional N + 1 continuous decisions as well as N further (linear) constraints.Remark 6. The deterministic reformulation (13) has N binary variables, L + N + 1 continuous decisions as well as (M + 1)N + 1 constraints (excluding those that describe X ). In comparison, the corresponding classical chance constrained formulation has N binary variables, L continuous decisions as well as M N + 1 constraints. Thus, adding distributional robustness requires an additional N + 1 continuous decisions as well as N further (linear) constraints.</p>
        <p>We compare our exact reformulation of the ambiguous chance constrained program (2) with the bicriteria approximation scheme of Xie and Ahmed (2020) on a portfolio optimization problem in Section 3.1 as well as with a classical (non-ambiguous) chance constrained formulation and a Kernel density estimator based version of the ambiguous chance constrained program over a ϕdivergence ambiguity set on a transportation problem in Section 3.2. Our goal is to investigate the computational scalability of our reformulation as well as its out-of-sample performance in a datadriven setting. All results were produced on an Intel Xeon 2.66GHz processor with 8GB memory in single-core mode using 
            <rs type="software">CPLEX</rs>
            <rs type="version">12.8</rs>. Following Remark 5, we avoid the specification of the constant M in our ambiguous chance constrained program through the use of piecewise linear constraints.
        </p>
        <p>We consider a portfolio optimization problem studied by Xie and Ahmed (2020). The problem asks for the minimum-cost portfolio investment x into K assets with random returns ξ1 , . . . , ξK that exceeds a pre-specified target return w with high probability 1 -ε. The problem can be cast as the following instance of the ambiguous chance constrained program (2): Objective and runtime ratios of the bicriteria approximation scheme for different values of ε and θ. For each parameter setting, we report the 5%, 50% and 95% quantiles over 50 randomly generated instances.We consider a portfolio optimization problem studied by Xie and Ahmed (2020). The problem asks for the minimum-cost portfolio investment x into K assets with random returns ξ1 , . . . , ξK that exceeds a pre-specified target return w with high probability 1 -ε. The problem can be cast as the following instance of the ambiguous chance constrained program (2): Objective and runtime ratios of the bicriteria approximation scheme for different values of ε and θ. For each parameter setting, we report the 5%, 50% and 95% quantiles over 50 randomly generated instances.</p>
        <p>We compare our exact reformulation of problem ( 14) with the (σ, γ)-bicriteria approximation scheme of Xie and Ahmed (2020), which produces solutions that satisfy the ambiguous chance constraint in ( 14) with probability 1 -σε, σ &gt; 1, and whose costs are guaranteed to exceed the optimal costs in ( 14) by a factor of at most γ = σ/(σ -1). Since the bicriteria approximation scheme can readily utilize support information for the random vector ξ, we replace the ambiguity set F(θ) with F(θ) = F(θ) ∩ {P | P[ ξ ∈ R K + ] = 1} in their approach. Contrary to the experiments conducted by Xie and Ahmed (2020), we set σ = 1. This is to the disadvantage of their approach, as it does not provide any approximation guarantees in that case, but it allows us to compare the resulting portfolios as they provide the same return guarantees. For the performance of the bicriteria approximation scheme with σ &gt; 1, we refer to Section 6.2 of Xie and Ahmed (2020).We compare our exact reformulation of problem ( 14) with the (σ, γ)-bicriteria approximation scheme of Xie and Ahmed (2020), which produces solutions that satisfy the ambiguous chance constraint in ( 14) with probability 1 -σε, σ &gt; 1, and whose costs are guaranteed to exceed the optimal costs in ( 14) by a factor of at most γ = σ/(σ -1). Since the bicriteria approximation scheme can readily utilize support information for the random vector ξ, we replace the ambiguity set F(θ) with F(θ) = F(θ) ∩ {P | P[ ξ ∈ R K + ] = 1} in their approach. Contrary to the experiments conducted by Xie and Ahmed (2020), we set σ = 1. This is to the disadvantage of their approach, as it does not provide any approximation guarantees in that case, but it allows us to compare the resulting portfolios as they provide the same return guarantees. For the performance of the bicriteria approximation scheme with σ &gt; 1, we refer to Section 6.2 of Xie and Ahmed (2020).</p>
        <p>In our numerical experiments, we consider a similar setting as Xie and Ahmed (2020). We set K = 50, w = 1 and choose the cost coefficients c 1 , . . . , c 50 uniformly at random from {1, . . . , 100}.In our numerical experiments, we consider a similar setting as Xie and Ahmed (2020). We set K = 50, w = 1 and choose the cost coefficients c 1 , . . . , c 50 uniformly at random from {1, . . . , 100}.</p>
        <p>Each asset return ξi is governed by a uniform distribution on [0.8, 1.5], and we assume that N = 100 training samples ξ1 , . . . , ξ100 are available. We use the 2-norm Wasserstein ambiguity set, which implies that our exact reformulation of problem ( 14) is a mixed-integer second-order cone program, and set the Wasserstein radius to θ ∈ {0.05, 0.1, 0.2}. The risk threshold is set to ε ∈ {0.05, 0.1}.Each asset return ξi is governed by a uniform distribution on [0.8, 1.5], and we assume that N = 100 training samples ξ1 , . . . , ξ100 are available. We use the 2-norm Wasserstein ambiguity set, which implies that our exact reformulation of problem ( 14) is a mixed-integer second-order cone program, and set the Wasserstein radius to θ ∈ {0.05, 0.1, 0.2}. The risk threshold is set to ε ∈ {0.05, 0.1}.</p>
        <p>Table 1 compares the objective values and runtimes of our exact reformulation and the bicriteria approximation scheme for various combinations of the risk threshold ε and Wasserstein radius θ.Table 1 compares the objective values and runtimes of our exact reformulation and the bicriteria approximation scheme for various combinations of the risk threshold ε and Wasserstein radius θ.</p>
        <p>The table shows that despite incorporating additional support information, the bicriteria approximation scheme determines solutions whose costs significantly exceed those of the solutions found by our exact reformulation. Perhaps more surprisingly, the bicriteria approximation scheme is also computationally more expensive. As Figure 4 shows, however, this is an artifact of the small sample Runtimes (left) and reciprocal runtime ratios (right) of our exact reformulation and the bicriteria approximation scheme for (ε, θ) = (0.10, 0.05) and different sample sizes N . The shaded regions cover the 5% to 95% quantiles of 50 randomly generated instances, whereas the solid lines describe the median statistics.The table shows that despite incorporating additional support information, the bicriteria approximation scheme determines solutions whose costs significantly exceed those of the solutions found by our exact reformulation. Perhaps more surprisingly, the bicriteria approximation scheme is also computationally more expensive. As Figure 4 shows, however, this is an artifact of the small sample Runtimes (left) and reciprocal runtime ratios (right) of our exact reformulation and the bicriteria approximation scheme for (ε, θ) = (0.10, 0.05) and different sample sizes N . The shaded regions cover the 5% to 95% quantiles of 50 randomly generated instances, whereas the solid lines describe the median statistics.</p>
        <p>size N employed in the experiments of Xie and Ahmed (2020), and the bicriteria approximation scheme is faster than our exact reformulation for larger samples sizes.size N employed in the experiments of Xie and Ahmed (2020), and the bicriteria approximation scheme is faster than our exact reformulation for larger samples sizes.</p>
        <p>We consider a probabilistic transportation problem studied by Luedtke et al. (2010) and Yanagisawa and Osogami (2013). The problem asks for the cost-optimal distribution of a single good from a setWe consider a probabilistic transportation problem studied by Luedtke et al. (2010) and Yanagisawa and Osogami (2013). The problem asks for the cost-optimal distribution of a single good from a set</p>
        <p>x ≥ 0.x ≥ 0.</p>
        <p>where M is a sufficiently large positive constant. The results show that for the smallest Wasserstein radius θ 1 = 0.001, the ambiguous chance constrained program ( 15) is-as expected-more difficult to solve than the corresponding classical chance constrained program (16). Interestingly, the ambiguous chance constrained program becomes considerably easier to solve than the classical chance constrained program for the larger Wasserstein radii θ 2 , . . . , θ 10 . This surprising result is explained in Figure 6, which shows that the feasible region of the ambiguous chance constrained program tends to convexify as the Wasserstein radius θ increases. In fact, one can show that the set of vectors q ∈ {0, 1} N that are feasible in the deterministic reformulation of problem (15) shrinks monotonically with θ. Since it is the presence of these binary vectors that causes the non-convexity of problem (15), one can expect the problem to become better behaved as θ increases.where M is a sufficiently large positive constant. The results show that for the smallest Wasserstein radius θ 1 = 0.001, the ambiguous chance constrained program ( 15) is-as expected-more difficult to solve than the corresponding classical chance constrained program (16). Interestingly, the ambiguous chance constrained program becomes considerably easier to solve than the classical chance constrained program for the larger Wasserstein radii θ 2 , . . . , θ 10 . This surprising result is explained in Figure 6, which shows that the feasible region of the ambiguous chance constrained program tends to convexify as the Wasserstein radius θ increases. In fact, one can show that the set of vectors q ∈ {0, 1} N that are feasible in the deterministic reformulation of problem (15) shrinks monotonically with θ. Since it is the presence of these binary vectors that causes the non-convexity of problem (15), one can expect the problem to become better behaved as θ increases.</p>
        <p>We next compare the out-of-sample performance of our ambiguous chance constrained program (15), where the risk threshold ε ∈ {0. (i) the classical chance constrained program ( 16), where the risk threshold is fixed to ε = 0.1 ('SAA'), (ii) a variant of the classical chance constrained program ( 16), where the risk threshold ε ∈ {1E -i : i = 1, 2, . . . , 5} ∪ {0.05} is selected using a 7-fold cross-validation on the training dataset 9). In all cases, the demands are truncated to the non-negative real line. Our results indicate that the classical chance constrained program (16) generates solutions that significantly violate the chance constraint, even if we select the risk threshold ε out-of-sample. The two ambiguous chance constrained formulations, on the other hand, achieve the desired risk threshold, often at a modest increase in transportation costs. While our approach and the ϕ-divergence ambiguity set perform similarly, our formulation appears to result in lower transportation costs, especially when data is scarce.We next compare the out-of-sample performance of our ambiguous chance constrained program (15), where the risk threshold ε ∈ {0. (i) the classical chance constrained program ( 16), where the risk threshold is fixed to ε = 0.1 ('SAA'), (ii) a variant of the classical chance constrained program ( 16), where the risk threshold ε ∈ {1E -i : i = 1, 2, . . . , 5} ∪ {0.05} is selected using a 7-fold cross-validation on the training dataset 9). In all cases, the demands are truncated to the non-negative real line. Our results indicate that the classical chance constrained program (16) generates solutions that significantly violate the chance constraint, even if we select the risk threshold ε out-of-sample. The two ambiguous chance constrained formulations, on the other hand, achieve the desired risk threshold, often at a modest increase in transportation costs. While our approach and the ϕ-divergence ambiguity set perform similarly, our formulation appears to result in lower transportation costs, especially when data is scarce.</p>
        <p>The distance of a point ξ ∈ R K to a closed set C ⊆ R K with respect to a norm ∥ • ∥ is defined as dist( ξ, C) = min{∥ξ -ξ∥ | ξ ∈ C}.The distance of a point ξ ∈ R K to a closed set C ⊆ R K with respect to a norm ∥ • ∥ is defined as dist( ξ, C) = min{∥ξ -ξ∥ | ξ ∈ C}.</p>
        <p>Note that the minimum is always attained. In the following, we derive a closed-form expression for the distance of a point to the union of finitely many closed halfspaces.Note that the minimum is always attained. In the following, we derive a closed-form expression for the distance of a point to the union of finitely many closed halfspaces.</p>
        <p>1, 0.05, 0.01} and the Wasserstein radius θ ∈ {1E -i : i = 2, 3, . . . , 6} are selected using a 7-fold cross-validation on the training dataset ('DRO'), with1, 0.05, 0.01} and the Wasserstein radius θ ∈ {1E -i : i = 2, 3, . . . , 6} are selected using a 7-fold cross-validation on the training dataset ('DRO'), with</p>
        <p>The authors are grateful to the review team for constructive comments that led to substantial improvements of the paper. The authors gratefully acknowledge financial support from the ECS grant 9048191, the SNSF grant BSCGI0 157733 and the EPSRC grant EP/N020030/1.The authors are grateful to the review team for constructive comments that led to substantial improvements of the paper. The authors gratefully acknowledge financial support from the ECS grant 9048191, the SNSF grant BSCGI0 157733 and the EPSRC grant EP/N020030/1.</p>
    </text>
</tei>

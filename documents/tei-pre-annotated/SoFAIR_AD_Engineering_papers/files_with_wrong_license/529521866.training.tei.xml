<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:57+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial-Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. In this work, we propose a novel Spatial-Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self--attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving backbone results. Compared with methods that use the same input data, the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints' coordinates as input, and results on-par with state-of-the-art when adding bones information.</p>
        <p>Human Action Recognition is achieving increasing interest in recent years for the progress achieved in deep learning and computer vision and for the interest of its applications in human-computer interaction, eldercare and healthcare assistance, as well as video surveillance. Recent advances in 3D depth cameras such as Microsoft Kinect (Zhang (2012)) and Intel RealSense (Keselman et al. (2017)) sensors, and advanced human pose estimation algorithms (Cao et al. (2019)) made it possible to estimate 3D skeleton coordinates quickly and accurately with cheap devices. Nevertheless, several aspects of skeleton-based action recognition still remain open (Zhang et al. (2019); Aggarwal and Ryoo (2011); Ren et al. (2020)). The most widespread method to perform skeletonbased action recognition has nowadays become Graph Neural Networks (GNNs), and in particular, Graph Convolutional Networks (GCNs) since, being an efficient representation of non-Euclidean data, they are able to effectively capture spatial (intra-frame) and temporal (inter-frame) information. Models making use of GCN were first introduced in skeleton-based action recognition by Yan et al. (2018) and they are usually referred to as Spatial-Temporal Graph Convolutional Networks (ST-GCNs). These models process spatial information by operating on skeleton bone-connections along space, and temporal information by considering additional time-connections between each skeleton joint along time. Despite being proven to perform very well on skeleton data, ST-GCN models have some structural limitations, some of them already addressed by Shi et al. (2019a); Simonyan and Zisserman (2014); Cheng et al. (2020); Liu et al. (2020).</p>
        <p>First of all, the topology of the graph representing the human body is fixed for all layers and all the actions; this may prevent the extraction of rich representations for the skeleton movements during time, especially if graph links are directed and information can only flow along a predefined path. Secondly, both Spatial and Temporal Convolution are implemented starting from a standard 2D convolution. As such, they are lim-</p>
        <p>(2)</p>
        <p>(3) (4)</p>
        <p>Fig. 1. Self-attention on skeleton joints. (1) For each body joint, a query q, a key k and a value vector v are calculated.</p>
        <p>(2) Then, the dot product ( ) between the query of the joint and the key of all the other nodes is performed, representing the connection strength between each pair of nodes.</p>
        <p>(3) Finally, each node is scaled by its correlation w.r.t. the current node, (4) whose new features are obtained summing the weighted nodes together.</p>
        <p>ited to operate in a local neighborhood, somehow restricted by the convolution kernel size. And finally, as a consequence of the previous, correlations between body joints not linked in the human skeleton, e.g., the left and right hands, are underestimated even if relevant in actions such as "clapping".</p>
        <p>In this paper, we face all these limitations by employing a modified Transformer self-attention operator, as depicted in Figure 1. Despite being originally designed for Natural Language Processing (NLP) tasks, Transformer self-attention has shown remarkable results on a broad range of computer vision tasks, spanning from classical classification and detection (Dosovitskiy et al. (2020); Bello et al. (2019a); Wang et al. (2018); Carion et al. (2020)), to more complex tasks such as those involving point clouds (Zhao et al. (2020)), generative modeling (Oord et al. (2016); Parmar et al. (2018)) and captioning (He et al. (2020)). In our setting, the sequentiality and hierarchical structure of human skeleton sequences, as well as the flexibility of Transformer self-attention (Vaswani et al. (2017)) in modeling long-range dependencies, make the Transformer a perfect solution to tackle ST-GCN weaknesses. In our work, we aim to apply Transformer to spatial-temporal skeleton-based architectures, and in particular to joints representing the human skeleton, with the goal of modeling long-range interactions within human actions both in space, through a Spatial Self-Attention (SSA) module, and time, through a Temporal Self-Attention (TSA) module. Main contributions of this paper are summarized as follows:</p>
        <p>• We propose a novel two-stream Transformer-based model for skeleton activity recognition tasks, employing selfattention on both the spatial and the temporal dimensions</p>
        <p>• We design a Spatial Self-Attention (SSA) module to dynamically build links between skeleton joints, representing the relationships between human body parts, conditionally on the action and independently from the natural human body structure. On the temporal dimension, we introduce a Temporal Self-Attention (TSA) module to study the dynamics of a joint along time. We made both layers publicly available for experiments replication and further use 1</p>
        <p>• Our model outperforms ST-GCN (Yan et al. (2018)) and 1 Code at 
            <rs type="url">https://github.com/Chiaraplizz/ST-TR</rs> A-GCN (Shi et al. (2019b)) consistently improving backbone results on all datasets and achieving state-of-the-art performance when using joint information, and results onpar with state-of-the-art when bones information is used.
        </p>
        <p>Most of the early studies in skeleton-based action recognition relied on handcrafted features (Hu et al. (2015); Vemulapalli et al. ( 2014 2020)), make use of both spatial and temporal data by exploiting information contained in the natural topological graph structure of the human skeleton. These latter methods have demonstrated to be the most expressive among the three, and among these, the first model capturing the balance between spatial and temporal dependencies has been the Spatio-Temporal Graph Convolutional Network (ST-GCN) (Yan et al. (2018)).</p>
        <p>In this work, we used ST-GCN as the baseline model; its functioning is presented in details in Section 3.2. Specifically, we propose to substitute regular graph convolutions on both space and time with the transformer self-attention operator. Cho et al. (2020) also proposed a Self-Attention Network (SAN) in which embeddings are extracted by segmenting the action sequence in temporal clips, and self-attention is applied among them in order to model long-term semantic information. However, since it applies self-attention on course-grained embeddings rather than skeleton joints, it hardly captures low-level joints' correlations within and between frames. Instead, by applying self-attention directly on nodes in the graph, both intraand inter-frame, we efficiently model both spatial and temporal dependencies of the skeleton sequence.</p>
        <p>Geometric deep learning (Bronstein et al. (2017)) refers to all emerging techniques attempting to generalize deep learning models to non-Euclidean domains such as graphs. The notion of Graph Neural Network (GNN) was initially outlined by Gori et al. (2005) and further elaborated by Scarselli et al. (2008).</p>
        <p>The intuitive idea underlying GNNs is that nodes in a graph represent objects or concepts while edges represent their relationships. Due to the success of Convolutional Neural Networks, the concept of convolution has later been generalized from grid to graph data. GNNs iteratively process the graph, each time representing nodes as the result of applying a transformation to nodes' and their neighbors' features. The first formulation of CNNs on graphs is due to Bruna et al. (2014), who generalized convolution to signals using a spectral construction. This approach had computational drawbacks that have been subsequently addressed by Henaff et al. (2015) and Defferrard et al. (2016). The latter has been further simplified and extended by Kipf and Welling (2017). A complementary approach is the spatial one, where graph convolution is defined as information aggregation (Micheli (2009); Niepert et al. (2016); Such et al. (2017)). In this work we make use of the spectral construction proposed by Kipf and Welling (2017), whose formulation is provided in Section 3.2.</p>
        <p>The Transformer is the leading neural model for Natural Language Processing (NLP), proposed by Vaswani et al. (2017) as an alternative to recurrent networks. It has been designed to face two key problems: (i) the processing of very long sequences, which are often intractable both for LSTMs and RNNs, and (ii) the limitations in parallelizing sentence processing, which is usually performed sequentially, word by word, in standard RNNs architectures. The Transformer follows a usual encoder-decoder structure, but it relies solely on multihead self-attention (Vaswani et al. (2017)). Recently, Transformer self-attention has been applied in many popular computer vision tasks. Wang et al. (2018) proposed a differentiable non-local operator based on self-attention, which allows to capture long-range dependencies both in space and time for a more accurate video classification. After the first attempt of Bello et al. (2019a) to use self-attention as an alternative to convolutional operators, Dosovitskiy et al. (2020) proposed a Vision Transformer (ViT), which shows how Transformers can effectively replace standard convolutions on images. He et al. (2020) proposed a novel image transformer architecture for the image captioning task. Carion et al. (2020) made the first attempt to use a Transformer model to tackle detection problems, namely the Detection Transformer (DeTR). Zhao et al. (2020) proposed Point Transformer, a model which uses transformer self-attention to encode relations between point clouds, exploiting their permutation invariant nature. Other applications of Transformers in segmentation tasks (Huang et al. (2019)), multi-modal tasks (Lee et al. (2020)) and generative modeling (Oord et al. (2016); Parmar et al. (2018)) have been recently developed, showing the potential of Transformer models on a broad range of tasks.</p>
        <p>In this section, Spatial-Temporal Graph Convolutional Networks (ST-GCN) by Yan et al. (2018) and the original Transformer self-attention by Vaswani et al. (2017) are summarized, being the basic blocks of the model we propose in this paper.</p>
        <p>Given a sequence of skeletons, we define V as the number of joints representing each skeleton and T as the total number of skeletons composing the sequence, also named frames in the following. In order to represent the sequence, a spatial temporal graph is built, i.e., G = (N, E), where N = {v ti |t = 1, ..., T, i = 1, ..., V} represents the set of all the nodes v ti of the graph, i.e., the body joints of the skeleton along all the time sequence, and E represents the set of all the connections between nodes. E consists of two subsets; the first subset E S = {(v ti , v t j ) | i, j = 1, . . . , V, t = 1, . . . , T } is composed by the intra-skeleton connections at each time interval t, for any pair of joints (i, j) connected by a bone in the human skeleton.</p>
        <p>The subset E S of intra-skeleton connections is commonly further divided into K disjoint partitions, based on some criterion (Yan et al. (2018)) (e.g., distance from the center of gravity), and encoded using a set of adjacency matrices Ãk ∈ {0, 1} V×V . The second subset</p>
        <p>consists of all the inter-frame connections between joints along consecutive time frames. The result is a graph extending on both the spatial and the temporal dimension.</p>
        <p>Spatial Temporal Graph Convolutional Networks (ST-GCN) have been introduced by Yan et al. (2018). A ST-GCN is structured as a hierarchy of stacked spatial-temporal blocks, which are internally composed of a spatial convolution (GCN) followed by a temporal convolution (TCN).</p>
        <p>The spatial sub-module uses the Graph Convolution formulation proposed by Kipf and Welling (2017), which can be summarized as it follows:</p>
        <p>where K s is the kernel size on the spatial dimension, Ãk is the adjacency matrix of the undirected graph representing intrabody connections, I is the identity matrix and W k is a trainable weight matrix. The temporal convolution sub-module (TCN) is implemented as a 1 × K t 2D convolution operating on (V, T ) dimensions of the (C in , V, T ) input volume, where K t is the number of frames considered within the kernel receptive field.</p>
        <p>As shown in Equation 1, the graph structure is predefined, being the adjacency matrix fixed. In order to make it adapative, Shi et al. (2019b) introduced the Adaptive Graph Convolutional Network (A-GCN), where the GCN formulation in Equation 1 is replaced by the following:</p>
        <p>where A k is the same as the one in Equation 1, B k is learned during training, and C k determines whether two vertices are connected or not through a similarity function.</p>
        <p>(a) Spatial Self-Attention (b) Temporal Self-Attention Fig. 2. Spatial Self-Attention (SSA) and Temporal Self-Attention (TSA). Self-attention operates on each pair of nodes, by computing a weight for each of them which represents the strength of their correlation. Those weights are then used to score the contribution of each body joint v ti , proportionally to how relevant the node is w.r.t. to all the others. Please notice that on SSA (a), the procedure is illustrated only of a group of five nodes for simplicity, while in practice it operates on all the nodes.</p>
        <p>The original Transformer model of Vaswani et al. (2017) employs self-attention, i.e., a non-local operator originally designed to operate on words in NLP tasks with the goal of enriching the embedding of each word based on the surrounding context. In the Transformer, new word embeddings are computed by comparing pairs of words an then mixing their embeddings together based on how much a word is relevant w.r.t. the others. By gathering clues from the surrounding context, selfattention enables to extract a better meaning from each word, dynamically building relations within and between phrases.</p>
        <p>In particular, for each word embedding w i ∈ W = {w 1 , ..., w n }, a query q ∈ R d q , a key k ∈ R d k and a value vector v ∈ R d v are computed through trainable linear transformations, independently. Then, a score for each word embedding is obtained by taking the dot product α i j = q i • k T j ∀i, j = 1, ..., n, where n is the total number of nodes being considered. This score represents how much the word j is relevant for word i.</p>
        <p>To compute the final embedding for word i, a weighted sum is computed by first multiplying the value vector of each other word v j by the corresponding score α i j , scaled through the softmax function, and then summing these vectors together. This process, also called scaled dot-product attention, can be written in matrix form as it follows:</p>
        <p>where Q, K, and V are matrices containing the predicted query, key and value vectors, respectively, packed together and d k is the channel dimension of the key vectors. The division by √ d k is performed in order to increase gradients stability during training. In order to obtain better performance, a mechanism called multi-headed attention is usually applied, which consists in applying attention, i.e., a head, multiple times with different learnable parameters and then finally combining the results.</p>
        <p>We propose the Spatial Temporal Transformer (ST-TR) network, an architecture which uses Transformer self-attention to operate on both space and time. We propose to achieve this goal using two modules, the Spatial Self-Attention (SSA) and the Temporal Self-Attention (TSA) modules, each one focusing on extracting correlations on one of the two dimensions.</p>
        <p>The idea behind the original Transformer self-attention is to allow the encoding of both short-and long-range correlations between words in the sentence. Our intuition is that the same approach can be applied to skeleton-based action recognition as well, as correlations between nodes are crucial both on the spatial and on the temporal dimension. We consider the joints comprising the skeleton as a bag-of-words and make use of the Transformer self-attention to extract node embeddings encoding the relation between surrounding joints, just like words in a phrase in NLP. Contrary to a standard graph convolution, where only the adjacent nodes are compared, we discard any predefined skeleton structure and instead let the Transformer selfattention automatically discover joint relations which are relevant for predicting the current action. The resulting operation acts similarly to a graph convolution, but in which the kernel values are dynamically predicted based on the discovered joint relations. The same idea is also applied at the sequence level, by analyzing how each joint changes during the action and building long-range relations that span different frames, similarly to how relations between phrases are built in NLP. The resulting operator is capable of obtaining a dynamical representation extending both on the spatial and the temporal dimension.</p>
        <p>The Spatial Self-Attention module applies self-attention inside each frame to extract low-level features embedding the relations between body parts. This is achieved by computing correlations between each pair of joints in every single frame independently, as depicted in Figure 2a. Given the frame at time t, for each node v ti of the skeleton, a query vector q t i ∈ R dq , a key vector k t i ∈ R dk and a value vector v t i ∈ R dv are first computed by applying trainable linear transformations to the node features body nodes (v ti , v t j ), a query-key dot product is applied to obtain a weight α t i j = q t i • k t j T ∈ R, ∀t ∈ T representing the correlation strength between the two nodes. The resulting score α t i j is used to weight each joint value v t j , and a weighted sum is computed to obtain a new embedding z t i for node v ti , as in the following:</p>
        <p>where z t i ∈ R C out (with C out the number of output channels) constitutes the new embedding of node v ti .</p>
        <p>Multi-head attention is applied by repeating this embedding extraction process N h times, each time with a different set of learnable parameters. The set (z t i 1 , ..., z t i H ) of node embeddings thus obtained, all referring to the same node v ti , is then combined with a learnable transformation, i.e., concat(z t i 1 , ..., z t i H ) • W o , and constitutes the output features of SSA.</p>
        <p>As shown in Figure 2a, the relations between nodes (i.e., the α t i j scores) are dynamically predicted in SSA; the correlation structure in the skeleton is then not fixed for all the actions, but it changes adaptively for each sample. SSA operates similar to a graph convolution on a fully connected graph where, however, the kernel values (i.e., the α t i j scores) are predicted dynamically based on the skeleton pose.</p>
        <p>With the Temporal Self-Attention (TSA) module, the dynamics of each joint is studied separately along all the frames, i.e., each single joint is considered as independent and correlations between frames are computed by comparing the change in the embeddings of the same body joint along the temporal dimension (see Figure 2b). The formulation is symmetrical to the one reported in Equation (5) for SSA:</p>
        <p>where v ti , v ui indicate the same joint v in two different instants t, u, α i tu ∈ R is the correlation score, q i t ∈ R dq is the query</p>
        <p>Fig. 4. Illustration of a SSA module (the implementation of TSA is the same, with the only difference that the dimension V corresponds to T and viceversa). The input f in is reshaped by moving T in the batch dimension, such that self-attention operates on each time frame separately. SSA is implemented as a matrix multiplication, where Q, K and V are the query, key and value matrix respectively, and ⊗ denotes the matrix multiplication.</p>
        <p>associated to v ti , k i u ∈ R dk and v i u ∈ R dv are the key and value associated to joint v ui (all computed using trainable linear transformations as in SSA), and z i t ∈ R C out is the resulting node embedding. Note that the notation used in this section is opposite w.r.t. the one used in Section 4.2; subscripts indicate time while superscripts indicate the joint. Multi-head attention is applied in TSA as in SSA. An example of TSA is depicted in Figure 2b.</p>
        <p>The TSA module, by extracting inter-frame relations between nodes in time, can learn how to correlate frames apart from each other (e.g., nodes in the first frame with those in the last one), capturing discriminant features that are not otherwise possible to capture with a standard ST-GCN convolution, being this limited by the kernel size.</p>
        <p>To combine the SSA and TSA modules, a two-stream architecture named ST-TR is used, as similarly proposed by Shi et al. (2019b) and Shi et al. (2019a). In our formulation, the two streams differentiate on the way the proposed self-attention mechanisms are applied: SSA operates on the spatial stream (named S-TR), while TSA on the temporal one (named T-TR). On both streams, node features are first extracted by a threelayers residual network, where each layer processes the input on the spatial dimension through graph convolution (GCN), and on the temporal dimension through a standard 2D convolution (TCN), as done by Yan et al. (2018) foot_0 . SSA and TSA are then applied on the S-TR and on the T-TR streams in the subsequent layers in substitution to the GCN and TCN feature extraction modules respectively (Figure 3). The S-TR stream and T-TR stream are end-to-end trained separately along with their corresponding feature extraction layers. The sub-networks outputs are eventually fused together by summing up their softmax output scores to obtain the final prediction, as proposed by Shi et al. (2019b) and Shi et al. (2019a).</p>
        <p>In the spatial stream, self-attention is applied at the skeleton level through a SSA module, which focuses on spatial relations between joints. The output of the SSA module is passed to a 2D convolutional module with kernel K t on the temporal dimension (TCN), as done by Yan et al. (2018), in order to extract temporally relevant features, as shown in Figure 3 and expressed in the following:</p>
        <p>Following the original Transformer, the input is passes through a Batch Normalization layer (Ioffe and Szegedy (2015); Nguyen and Salazar ( 2019)), and skip connections are used to sum the input to the output of the SSA module (see Figure 4).</p>
        <p>The temporal stream, instead, focuses on discovering inter-frame temporal relations.</p>
        <p>Similarly to the S-TR stream, inside each T-TR layer, a standard graph convolution sub-module (Yan et al. (2018)) is followed by the proposed Temporal Self-Attention module:</p>
        <p>TSA operates on graphs linking the same joint along all the time dimension (e.g., all left feet, or all right hands).</p>
        <p>The matrix implementation of SSA (and of TSA) is based on the implementation of Transformer on pixels by Bello et al. (2019b). As shown in Figure 4, given an input tensor of shape (C in , T, V), where C in is the number of input features, T is the number of frames and V is the number of nodes, a matrix X V ∈ R T ×C in ×V is obtained by rearranging the input. Here the T dimension is moved inside the batch dimension, effectively implementing parameter sharing along the temporal dimension and applying the transformation separately on each frame:</p>
        <p>where the product with</p>
        <p>, being N h the number of heads, and W o a learnable linear transformation combining the heads outputs. The output of the Spatial Transformer is then rearranged back into R C out ×T ×V . The TSA matrix implementation has the same expression as Equation ( 9), differing only in the way the input X is processed. Indeed, in order to be processed by each TSA module, the input is reshaped into a matrix X T ∈ R V×C in ×T , where the V dimension has been moved in the first position and aggregated to the batch dimension, not reported here explicitly, in order to operate separately on each joint along the time dimension. The formulation is analogous to Equation ( 9), differing only in the shape of matrices, which become</p>
        <p>To understand the impact of both the Spatial and Temporal Transformer streams, we analyze their performance separately and in different configurations through extensive experiments on NTU-RGB+D 60 (Shahroudy et al. ( 2016)) (see Table 1-3). Then, for a comparison with the state-of-the-art, we test the resulting best configurations on the Kinetics dataset (Kay et al. (2017)) and on the NTU-RGB+D 120 dataset (Liu et al. (2019)), which represents to date one of the most complex skeleton-based action recognition benchmarks (see Table 45.</p>
        <p>NTU RGB+D 60 and NTU RGB+D 120. The NTU RGB+D 60 (NTU-60) dataset is a large-scale benchmark for 3D human action recognition collected using 
            <rs type="creator">Microsoft</rs>
            <rs type="software">Kinect</rs> v
            <rs type="version">2</rs> by Shahroudy et al. (2016). Skeleton information consists of 3D coordinates of 25 body joints and a total of 60 different action classes. The NTU-60 dataset follows two different criteria for evaluation. The first one, called Cross-View Evaluation (X-View), uses 37, 920 training and 18, 960 test samples, split according to the camera views from which the action is taken. The second one, called Cross-Subject Evaluation (X-Sub), is composed instead of 40, 320 training and 26, 560 test samples. Data collection has been performed with 40 different subjects performing actions and divided into two groups, one for training and the other for testing. NTU RGB+D 120 (Liu et al. ( 2019)) (NTU-120) is an extension of NTU-60, which adds 57, 367 new skeleton sequences representing 60 new actions. To perform the evaluation, the extended dataset follows two criteria: the first one is the Cross-Subject Evaluation (X-Sub), the same used for NTU-60, while the second one is called Cross-Setup Evaluation (X-Set), which substitutes Cross-View by splitting training and testing samples based on the parity of the camera setup IDs.
        </p>
        <p>Kinetics. The Kinetics skeleton dataset (Yan et al. (2018)) is obtained by extracting skeleton annotations from videos composing the Kinetics 400 dataset (Kay et al. (2017)), by using the 
            <rs type="software">OpenPose toolbox</rs> (Cao et al. (2019)). It consists of 240, 436 training and 19, 796 testing samples, representing a total of 400 action classes. Each skeleton is composed by 18 joints, each one provided with the 2D coordinates and a confidence score. For each frame, a maximum of 2 people are selected based on the highest confidence scores.
        </p>
        <p>We perform an analysis on the complexity of the different self-attention modules we designed, and compare them to ST-GCN modules (Yan et al. (2018)), based on standard convolution, and to 1s-AGCN (Shi et al. (2019b)) modules, based on adaptive graph convolution. First, we compare in Figure 5a, singularly, a layer of standard convolution with our transformer mechanism, setting C in = C out channels. This results in the same number of parameters for both TSA and SSA, since the convolutions performed internally have the same kernel dimensions and both the query-key dot product and the logit-value product are parameter free. It can be seen that SSA introduces less parameters than GC, especially when dealing with a large number of channels, where the maximum ∆ GC-S S A , i.e., the decrease in terms of parameters, is 1.1 × 10 5 . When dealing with adaptive modules (AGC), an additional number of parameters has to be considered, resulting in a difference with respect to SSA of ∆ AGC-S S A = 5 × 10 5 . On the temporal dimension ∆ TC-T S A reaches a value of 16.8 × 10 5 . Temporal convolution in Yan et al. (2018) is implemented as a 2D convolution with filter 1 × F, where F is the number of frames considered along the time dimension, and it is usually set to 9, striding along T = 300 frames. Thus, substituting it with a self-attention mechanism results in a great complexity reduction, in addition to better performance, as reported in the next sections. Finally, in Figure 5b we also compare the entire stream architectures, i.e., ST-GCN (Yan et al. (2018)) and 1s-AGCN (Shi et al. (2019b)) with the proposed S-TR and T-TR streams in terms of parameters. As expected from the considerations above, the biggest improvement in parameters reduction is achieved by substituting temporal convolution with TSA, i.e., in T-TR, with a ∆ S T -GCN--T -T R = 16.7 × 10 5 . On the spatial dimension the difference in terms of parameters is not as pronounced as in temporal dimension, but it is still significant, with a ∆ S T -GCN--S -T R = 1.07×10 5 and ∆ 1s-AGCN--S -T R = 5.0×10 5 .</p>
        <p>Using 
            <rs type="software">PyTorch</rs> (Paszke et al. (2019)) framework, we trained our models for a total of 120 epochs with batch size 32 and SGD as optimizer on NTU-60 and NTU-120, while on Kinetics we trained our models for a total of 65 epochs, with batch size 128. The learning rate is set to 0.1 at the beginning and then reduced by a factor of 10 at the epochs {60, 90} and {45, 55} for NTU and Kinetics respectively. These schedulings have been selected as they have been shown to provide good results on ST-GCN networks used by Shi et al. (2019a). When using adaptive AGCN modules, we performed a linear warmup (2019b) and Shi et al. (2019a). In order to avoid overfitting, we also used DropAttention, a particular dropout technique introduced by Zehui et al. (2019) for regularizing attention weights in Transformer networks, that consists in randomly dropping columns of the attention logits matrix. In all of these experiments, the number of heads for multi-head attention is set to 8, and d q , d k , d v embedding dimensions to 0.25×C out in each layer, as done in Bello et al. (2019b). We did not perform grid search on these parameters. As far as it concerns the model architecture, each stream is composed by 9 layers, of channel dimension 64, 64, 64, 128, 128, 128, 256, 256 and 256. Batch normalization is applied to input coordinates, a global average pooling layer is applied before the softmax classifier and each stream is trained using the standard cross-entropy loss.
        </p>
        <p>To verify in a fair way the effectiveness of our SSA and TSA modules, we compare the S-TR and T-TR streams individually against the ST-GCN (Yan et al. (2018)) baseline (whose results are reported using our learning rate scheduling) and other models that modify its basic GCN module (see Table 1): (i) ST-GCN (fc): we implemented a version of ST-GCN whose adjacency matrix is composed of all ones (referred as A f c ), to simulate the fully-connected skeleton structure underlying our SSA module and verify the superiority of self-attention over graph convolution on the spatial dimension; (ii) 1s-AGCN: Adaptive Graph Convolutional Network (AGCN) (Shi et al. (2019b)) (see Section 3.2), as it demonstrated in the literature to be more robust than standard ST-GCN, in order to remark the robustness of our SSA module over more recent methods; (iii) 1s-AGCN w/o A: 1s-AGCN without the static adjacency matrix, to verify the effectiveness of our SSA over graph convolution in a similar setting where all the links between joints are exclusively learnt. All these methods use the same implementation of convolution on the temporal dimension (TCN). We make a comparison both in terms of model accuracy and number of parameters.</p>
        <p>Regarding SSA, the performance of S-TR is superior to all methods mentioned above, demonstrating that self-attention can be used in place of graph convolution, increasing the network performance while also decreasing the number of parameters. In fact, as it can be seen from Table 1, S-TR introduces 0.3 × 10 5 parameters less then ST-GCN and 4 × 10 5 less than 1s-AGCN, with a performance increment w.r.t. all GCN configu- rations. Similarly, regarding TSA, what emerges from the comparison between T-TR and the ST-GCN baseline adopting standard convolution, is that by using self-attention on the temporal dimension the model is significantly lighter (13.4 × 10 5 less parameters), and achieves an increment in accuracy of 0.9%.</p>
        <p>In Table 2 we first analyze the performance of the S-TR stream, T-TR stream and their combination by using input data consisting of joint information only. As it can be seen from Table 2a, on NTU-60 the S-TR stream achieves slightly better performance (+0.4%) than the T-TR stream, on both X-View and X-Sub. This can be motivated by the fact that SSA in S-TR operates on 25 joints only, while on temporal dimension the number of correlations is proportional to the huge number of frames. Again, as shown in Table 1a, applying self-attention instead of convolution clearly benefits the model on both spatial and temporal dimensions. The combination of the two streams achieves 88.7% of accuracy on X-Sub and 95.6% of accuracy on X-View, outperforming the baseline ST-GCN and surpassing other two-stream architectures (see Table 3).</p>
        <p>As adding the differential of spatial coordinates (bones information) demonstrated to lead to better results in previous works (Shi et al. (2019a); Simonyan and Zisserman (2014)), we also studied our Transformer modules on combined joint and bones information. For each node v 1 = (x 1 , y 1 , z 1 ) and v 2 = (x 2 , y 2 , z 2 ), the bone connecting the two is calculated as b v 1 ,v 2 = (x 2x 1 , y 2y 1 , z 2z 1 ). Both joint and bone information are concatenated along the channel dimension, and then fed to the network. At each layer, the dimension of the input and output channels are doubled as done by Shi et al. (2019a) and Simonyan and Zisserman (2014). Results are shown again in Table 2a, where all previous configurations improve when bones information is added as input. This highlights the flexibility of our method, which is capable of adapting to different input types and network configurations.</p>
        <p>To further test its flexibility, we also perform experiments in which the GCN module is substituted by the AGCN adaptive module on the temporal stream. As it can be seen from Table 2a, these configurations (T-TR-agcn) achieve better results than the one using standard GCN on both X-Sub and X-View.</p>
        <p>We designed our streams to operate starting from high-level features, rather than directly from coordinates, extracted using a sequence of residual GCN and TCN modules as reported in Section 4.4. This set of experiments validates our design choice. In these experiments SSA (TSA) substitutes GCN (TCN) on the S-TR (T-TR) stream, from the very first layer. The configurations reported in Table 2b (named S-TR-alllayers), performs worse than the corresponding ones in Table 2a, while still outperforming the baseline ST-GCN (Shi et al. (2019a)) (see Table 3). Indeed, self-attention has demonstrated being more efficient when incorporated in later stages of the network (Carion et al. (2020); Huang et al. (2019); Wang et al. (2018)). Notice that on T-TR, in order to deal with the great number of frames in the very first layers (T = 300), we divided them into blocks within which SSA is applied, and then gradually reduce the number of blocks (d block = 10 where C out = 64, d block = 10 where C out = 128, and a single block of d block = T l on layers l with C out = 256).</p>
        <p>The standard protocol used in recent works (Cheng et al. (2020); Shi et al. (2019b)) that propose alternative modules for ST-GCN based networks is to keep the original ST-GCN backbone architecture fixed in terms of layers composition. Following these works, we kept the three original feature extraction layers for a fair comparison. We further conduct some targeted experiments in which we vary their number k (Table 2b). As it can be seen, performance is not sensitive to variations of k, confirming the effectiveness of the proposed approach.</p>
        <p>Motivated by the results in Bello et al. (2019b), we studied the effect of applying the proposed Transformer mechanism as an augmentation procedure to the original ST-GCN modules. In this configuration, 0.75 × C out features result from GCN (TCN) and they are concatenated to the remaining 0.25 × C out features from SSA (TSA), a setup that has proven to be effective in Bello et al. (2019b). To compensate the reduction of attention channels, wide attention is used, i.e., half of the attention channels are assigned to each head, then recombined together while merging heads. The results are reported in Table 2b (referred as ST-TR-augmented). Graph convolution is the one that benefits the most from SSA attention (S-TR-augmented, 94.5%), to be compared with S-TR's 94% in Table 2a. Nevertheless, the lower number of output features assigned to self-attention prevent temporal convolution improving on T-TR stream.</p>
        <p>We tested the efficiency of the model when SSA and TSA are combined in a single stream architecture (see Table 2b, referred as S-TR-1s). In this configuration, feature extraction is still performed by the original GCN and TCN modules, while from the 4th layer on, each layer is composed by SSA followed by TSA, i.e., ST-TR-1s(x) = TSA(SSA(x)).</p>
        <p>We also tested this configuration on NTU-60, obtaining an accuracy of 93.3%, slightly lower than the 95.6% accuracy of 2s-ST-TR (see Table 1a, ST-TR). However, it should be noted strong baseline in skeleton-based action recognition. Considering that MS-G3D features a multi-path design with multi-scale graph convolutions, the performance obtained by ST-TR is remarkable given that the latter is based on a simpler backbone. Finally, on Kinetics (Table 5), our model using only joints outperforms the ST-GCN baseline by 5% and all previous methods using only joint information. When bones information is added, it outperforms both 2s-AGCN and DGNN, and achieves results on-par with the very recent state-of-the-art method MS-G3D.</p>
        <p>In Figure 6, we report some actions and the corresponding Spatial Self-Attention maps. On the top we draw the skeleton of the subjects, where the radius of the circles in correspondence to each joint is proportional its relevance predicted by the self-attention. The heatmaps on the bottom represent the attention scores of the last layer; these are 25×25 matrices, where each row and each column represents a body joint. An element in position (i, j) represents the predicted correlation between joint i and joint j in the same frame. As it can be observed, depending on the action, different parts of the body are activated. In Figure 7 are shown the same heatmaps at each layer. In the first layers, self-attention captures low-level correlations between body joints, as highlighted by the sparsity of the activations. While going deeper through the network, the global importance of each node emerges instead, as highlighted by the vertical lines corresponding to the most relevant joints.</p>
        <p>In this paper we propose a novel approach that introduces Transformer self-attention in skeleton activity recognition as an alternative to graph convolution. Through extensive experiments on NTU-60, NTU-120 and Kinetics, we demonstrated that our Spatial Self-Attention module (SSA) can replace graph convolution, enabling more flexible and dynamic representations. Similarly, Temporal Self-Attention module (TSA) overcomes the strict locality of standard convolution, enabling the extraction of long-range dependencies across the action. Moreover, our final Spatial-Temporal Transformer network (ST-TR) achieves state-of-the-art performance on all dataset w.r.t. methods using same input joint information and stream setup, and results on-par with state-of-the-art methods when bones information is added. As configurations only involving self-attention modules revealed to be sub-optimal, a possible future work is to search for a unified Transformer architecture able to replace graph convolution in a variety of tasks.</p>
        <p>In principle other features, e.g., visual features, could be added here but we want in this paper to focus on pure skeleton base action recognition and we leave this option for future investigations.</p>
    </text>
</tei>

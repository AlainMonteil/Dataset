<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-24T12:55+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>A comprehensive review of non-probabilistic machine learning for battery SOH estimation is presented.</p>
        <p>• For every algorithm, the principle derivation process is provided followed by flow charts with a unified form.</p>
        <p>• The challenges and unresolved issues of battery SOH estimation using machine learning technology are discussed.</p>
        <p>• The estimation performance, the publication trend, and the training mode of each method are compared.</p>
        <p>• The outlook of the research on future machine learning-based battery SOH estimation methods is given.</p>
        <p>In order to reduce carbon emissions and cope with the associated climate change and energy shortages, the worldwide energy system is changing [1]. With the rapid development of renewable energy including wind power, solar energy, and hydroelectric power, etc. the use of fossil fuel is gradually reduced. Due to the high power and energy density (up to 200 Wh/kg), the high energy efficiency (more than 95%), and also the relatively long cycle life (3000 cycles at deep discharge of 80%), lithium-ion (li-ion) batteries are used in a wide range of applications including energy storage systems, electric transportations, and portable electronic devices [2]. As the energy storage unit or the main source of power for these devices, the safe and reliable operation as well as the economic viability of batteries is important [3]. However, similar to any energy storage device, their performance is subject to degradation (i.e., capacity fade and power decrease) during long-term operation [4]. Hence, it becomes necessary to know the state of health (SOH) of the batteries at any point during their operation [5].</p>
        <p>The aging modes of the battery can be summarized as the loss of liion inventory and loss of anode/cathode active materials [6][7][8]. Those degradation modes are caused by complicated and coupled physical or chemical side reactions (i.e., aging mechanisms) inside of the battery, such as graphite exfoliation, loss of electrolyte, solid electrolyte interface (SEI) film formation, and continuous thickening, lithium plating, etc. [6]. As a result, at a macroscopic scale, the aging of the battery will be observed in capacity fade and power fade (resistance increase) [9]. The capacity fade affects the range of an electric vehicle and power fade, which is the increase in the internal resistance or impedance of the cell, can limit the power capability of the system and decrease the efficiency of the electric vehicle. Therefore, the capacity and resistance are the main parameters, which describe the battery performance behavior during their entire life. Depending on the type (i.e., requirements) of the application, the SOH of the battery is usually related to one of these parameters or both. For example, in energy applications (e.g., electric vehicles), the capacity is more important and thus the SOH can/should be related to the battery capacity. On the other hand, in power applications (e.g., grid support applications), the power is the dominant performance and thus the SOH can/should be related to the battery resistance.</p>
        <p>In order to obtain the evolution of the SOH-related parameters, batteries are aged under different laboratory conditions and measurements such as the capacity test, the DC pulse test, the electrochemical impedance spectroscopy (EIS) test, etc. are performed [10]. According to the recorded voltage (V), current (I), temperature (T), and time (t), the SOH can be estimated by the following four types of methods as presented in Fig. 1. One way is to use the measurements directly. For example, the capacity can be obtained by measuring the charge transferred through the battery during charging or discharging, and the resistance can be obtained by calculating the instantaneous voltage drop during the pulse test, etc. [13][14][15]. The indirect methods, such as incremental capacity analysis (ICA) and differential voltage analysis (DVA), extract the related SOH features by processing the original measurements. It is more convenient and efficient than the direct methods because the features can be obtained from partial charging/discharging curves [17]. However, these methods are less feasible in real applications. Firstly, the methods need high-precision current measurement sensors. Secondly, in order to perform the specific battery tests (e.g., capacity test, EIS test, etc.), the battery has to stop the normal operation. Finally, certain measurements are limited in real-life systems (e.g., DC pulses with high currents are not allowed by the BMS as they are seen as not normal operating conditions). Additionally, the state-space model is an effective way of representing a dynamic system and the observer can explain the aging mechanism between two adjacent cycles. Therefore, the observer-based SOH estimation methods have been proposed [18][19][20][21], in which the internal state variables can be observed through an iterative mechanism. All the aging data (i.e. the voltage response curves under different aging conditions) will be stored in a table, and the SOH value of the battery can be obtained by looking up the table. Based on electrochemical models or equivalent-circuit models, state observers such as multi-scale extended Kalman filter [18,19], multi-scale nonlinear predictive filter [20], and the particle filter (PF) [21] have been designed for battery SOH estimation.</p>
        <p>However, establishing an accurate battery model is difficult due to the complex internal principles and uncertain working conditions of the battery. Machine learning (ML) technologies are emerging due to their flexibility and being battery model-free. The ML estimates the SOH by learning the relationship between features of the measured battery data</p>
        <p>The (i.e., V, I, T, t) and the SOH (i.e., capacity, internal resistance). The same aging experimental test is necessary for data collection. These methods include amongst others, support vector machine (SVM), relevance vector machine, artificial neural network (ANN), Gaussian process regression, etc. Moreover, with the rapid development of big data technology, cloud storage provides a convenient platform for processing real-time monitoring parameters such as V, I, and T. This reduces the requirements of the microprocessor while also improves the SOH estimation accuracy [22]. At present, there are mainly four publicly available datasets and the private datasets that are used to validate the algorithms in the published paper. The four public datasets include the dataset from NASA Ames Prognostics Center of Excellence [23][24], Oxford battery degradation dataset from the Howey research group, the dataset from the Center for Advanced Life Cycle Engineering (CALCE) at the University of Maryland [25], and research &amp; development data repository from Sandia National Labs [26].</p>
        <p>The ML algorithms used for battery state estimation can be grouped into two categories: non-probabilistic-based methods and probabilisticbased methods. The typical probabilistic-based methods such as the Gaussian process regression and Bayesian network are suitable for longterm battery remaining useful lifetime prediction. For battery SOH estimation, researchers are mainly focusing on the non-probabilistic methods, such as SVM, ANN, and random forest (RF) as these types of algorithms can be fully qualified for the task of battery SOH estimation. Consequently, only the non-probabilistic algorithms are reviewed in this paper.</p>
        <p>In recent years, some review articles have been published presenting the status of various ML-based methods for SOH estimation. The main contents of several of these reviews are summarized in Table 1. These review articles mainly summarized the general methods for SOH estimation, and classify them into experimental methods, model-based, data-driven, and hybrid methods. The status, advantages, and drawbacks of various methods are discussed. Since these review articles involve the introduction of the broad categories of SOH estimation methods, the discussion on the ML-based SOH estimation method is not in-depth enough. In particular, the principle of the ML algorithms and the derivation of important formulas are rarely mentioned. Due to the development of ML technology, its good performance and potential in health monitoring have attracted the attention of researchers. Understanding the core ideas of various algorithms is essential for the improvement of algorithms so that they can be better applied to battery SOH estimation. In order to address this research gap, this paper reviews 144 papers that use non-probabilistic ML algorithms (i.e., Linear regression, SVM, k-nearest neighbor regression, ANN, and ensemble learning) for battery SOH estimation.</p>
        <p>The main contributions of this paper are as follows:</p>
        <p>• The basic principles of each algorithm are derived in a unified form and the flowchart of each algorithm is given. Hence, the difference among these algorithms can be clearly compared from the perspective of applications and the principles. Suggestions for future direction on algorithm improvement are therefore provided. • The algorithms and their variants used in the existing papers, the corresponding features, estimation errors, and other details are summarized in a table (see Section 2. F) for easy comparison. • The algorithms are compared according to five performance evaluation metrics including the estimation accuracy, the implementation easiness, the computational complexity, the training data size requirement, and the ability to deal with overfitting. Thus, the suggestion for algorithm selection is provided. • Three training and estimation modes for the ML-based SOH estimation method are proposed. These algorithms are classified according to the modes for which they are suitable.</p>
        <p>The rest of this paper is organized as follows. The principles of each ML algorithms as well as their applications for SOH estimation are introduced in Section 2. Section 3 presents the comparison between ML algorithms from three aspects: the pros and cons of each algorithm, the publication trend, and the training modes. Then the challenges and issues for SOH estimation are presented in Section 4. Finally, Section 5 gives the conclusion of this work by providing some selective proposals.</p>
        <p>As shown in Fig. 2, SOH estimation based on ML technologies consists of two parts, the training process, and the estimation process. The training process is usually performed offline while the estimation process can be realized either offline or online. During the model training, the aging data (e.g. the V, I, T, and t) should be firstly collected. Secondly, based on the collected raw data, the features, which contain sufficient aging information, will be extracted. The features, together with the real SOH value, constitute the training data set. Thirdly, the ML algorithms learn and update the weights and biases to fit the training data. Thus, the nonlinear relationship between the input (i.e., the SOH features) and output (usually the SOH or the capacity) can be obtained using the established ML model. In this section, five commonly used ML algorithms are introduced; they are the linear regression (LR), SVM, k-nearest neighbor regression (k-NN), ANN, and the ensemble learning (EL) method. The basic principle of each algorithm is introduced and the core idea is revealed in a schematic diagram. Following each algorithm, examples of their application for battery SOH estimation are presented.</p>
        <p>The purpose of the regression problem is to find a linear function f(x) which minimizes the squared distance between the observed data and the function. The structure of LR for battery SOH estimation can be seen in Fig. 3. For the battery SOH estimation, let X={x 1 , x 2 , …, x N } and Y= {y 1 , y 2 , …, y N } be the N input feature vectors and the SOH, respectively. Let D N ={(x i , y i ), i = 1, 2,…, N} denote the training data containing N data points, and assume that each data point contains d features,</p>
        <p>where, w j is the weight of j-th feature of x i , b denotes the bias, and d is the number of features. The goal is to minimize the sum of squared errors between the model and the output as follows:</p>
        <p>where y i and ŷi are the real SOH and the predicted SOH values, respectively. When minimizing (2), the weights are optimized by solving the following equations:</p>
        <p>where w j is the estimated value of j-th weight. If the model is linear, these equations can be solved explicitly as:</p>
        <p>where ŵ is a vector of the parameters, X is a matrix where each column contains the features of data-point i, and Y is the vector of the output. Alternatively, the gradient descent (GD) method can be used to update the parameters iteratively using the direction of the gradients, seen in ( 3), as follows:</p>
        <p>where w j is initialized randomly, and α is called the learning rate. ( 5) is repeatedly updated until w j convergences. When using a one-dimensional feature to estimate SOH, the LR model is simplified and only one weight w 1 and the bias w 0 need to be optimized. Thus, the SOH estimation can be quickly obtained based on the established linear relationship between the feature and SOH [34,35]. However, due to the non-linear relationship between battery aging and the selected features, LR has poor estimation accuracy and generalization properties. It is also not able to track the capacity regeneration of the Li-ion batteries. Through the accelerated aging tests, the degradation behavior of the battery under different stress conditions was studied in [10]. Then, the authors proposed a three-step LR method to parameterize a performance-degradation lifetime model, which can predict the capacity and the power capability decrease. Some features and SOH show an approximately linear relationship which can be captured by LR. Huang et al. find that both SOC and SOH are related to the instantaneous discharging voltage [36]. By introducing the modification factor as a function of the SOC, the linear relationship between the instantaneous discharging voltages with the SOH is established. Similarly, the differential voltage (DV) curve is used to extract the feature (i.e., the location interval between two inflection points) in [37]. When the inconsistency of the battery voltage is taken into account, the peak point feature on the incremental capacity (IC) curve from the narrowed voltage operation range is still available. To obtain smooth IC or DV curves with obvious features (e.g., peak values, peak areas, and peak shifts), the capacity and voltage have to be pre-processed [38][39][40]. Using the 
            <rs type="software">Matlab</rs> curve fitting toolbox, Li et al. establish the LR function between battery SOH and the three positions of features on the IC curves [41]. Because the position features are shown in the partial area of the charging voltage curve, the accurate SOH estimation can be obtained without the full voltage curve. Therefore, the testing time can be reduced. The same method is applied for SOH estimation of a battery pack with the voltage imbalance [42]. The sample entropy (SE) is an effective SOH feature that can be extracted from a short-term voltage profile. Hu et al. utilized the SE of voltage sequence under hybrid pulse power characterization (HPPC) test as the input of LR function. The capacity loss was estimated at multiple temperatures [11]. Furthermore, Sui et al. studied the effect of dataset selection on the SE-based estimator, and they found that when the battery SOC enters into the polarization zone, it helps to improve the accuracy of the entropy-based SOH estimation method [12].
        </p>
        <p>SVM was first proposed by Vapnik in [43] and has been successfully applied to regression problems including grid load forecasting [44], fault diagnosis [45], and image processing [46]. SVM shows great performance in high-dimensional function approximation problems due to the use of the kernel technique, which maps feature vectors to a higherdimensional space. It is one of the most popular and versatile models in ML, suitable for both classification and regression of complex small datasets. Hence, many researchers use SVM to estimate the SOH of batteries. The architecture of the SVM method for regression is shown in Fig. 4. In general, the SVM model [47] is defined as Fig. 3. The illustration of linear regression (LR).</p>
        <p>where ψ(•) is a mapping that makes the input data linear in a new feature space with dimension d. Different from the general linear regression models, the SVM model uses the ε-insensitive loss function. This states that any error larger than ε is deemed unacceptable. That is, the objective of the basic SVM is to find the optimal coefficients w and b such that the function, f, does not contain errors larger than ε. This is, therefore, also called the hard-margin SVM. The hard-margin SVM leads to the following constrained optimization problem.</p>
        <p>However, it is not always feasible to find a minimum under these constraints. Therefore, the following loss function is introduced:</p>
        <p>Based on (8), the samples with the predicted error less than ε are deemed acceptable, while the samples outside of the ε band will increase the regression error. Slack variables ξ i and ξ * i are introduced to create a soft-margin and allowing for measurement errors, making the optimization feasible with otherwise infeasible constraints. The primal SVM optimization problem has the following form:</p>
        <p>where C is a positive constant regulating the penalty, it determines the trade-off between the flatness of the regression function and the amount to which deviations larger than ε are tolerated. Flatness in the case of (9) means a small ‖w‖. In order to solve this problem, the Lagrange multipliers α i , α i , β i , β * i ⩾0 are introduced, and the Lagrangian can be expressed as follows:</p>
        <p>The min-max problem can be transferred into its dual max-min problem which satisfies the Karushe-Kuhne-Tucker (KKT) conditions [48]. The first KKT condition states that the gradients of the primal variables are equal to zero i.e., ∇ w L = 0,</p>
        <p>The second KKT condition called the complementary conditions states that multiplying the constraint by its Lagrange multiplier has to equal zero in the optimum. That is, either the constraint is active, or the Lagrange multiplier is zero. As a consequence of the second KKT condition, the Lagrange multiplier α i and α * i for the samples inside the ε-tube will vanish; while when</p>
        <p>⃒ ⩾ε, the multipliers α i and α * i are nonzero. Therefore, only the samples x i with non-vanishing coefficients are enough to describe w, and these samples are commonly called the support vectors (SVs). The primal SVM optimization problem is converted into the following dual SVM optimization problem</p>
        <p>After optimizing (11) w.r.t. the Lagrange multipliers α i , and α i *, the coefficients w and b can be computed from the α's using ( 12) and ( 13), respectively.</p>
        <p>Finally, the regression function can be described as:</p>
        <p>where K(x i , x) = 〈ψ(x i ), ψ(x)〉 is the kernel function. The kernel function implicitly maps the input to the high-dimensional feature space. This method has higher computational efficiency than if the features were first mapped using ψ(•), thereby, overcoming the curse of dimensionality. Common kernel functions K(x ii , x jj ) used in SVM are:</p>
        <p>(1) Polynomial kernel:</p>
        <p>(2) Gaussian radial basis function:</p>
        <p>(3) Hyperbolic tangent kernel:</p>
        <p>The Hyperbolic tangent kernel often used as an activation function for artificial neurons, expressed as</p>
        <p>where M, σ, κ, and c are adjustable parameters of the above kernel functions.</p>
        <p>There are mainly four aspects where the researchers start to improve the estimation performance of the SVM method. Firstly, some novel features are proposed [49][50][51][52][53][54][55][56][57][58][59][60][61][62][63][64][65][66]. SVM was initially used to learn the battery aging behavior under different conditions, and estimate the model parameters, such as the terminal voltage [49] and internal resistance [50,51]. These parameters are found to be approximately linearly related to the battery capacity. In recent years, some effective features are extracted directly from the partial constant current (CC) charging or discharging voltage curves. For example, the IC peak values and IC positions [52][53][54][55], DV [52], differential temperature [53], the energy signal [56], the knee point in the pulse voltage response [57], and the time interval of an equal voltage difference [58] have been used as inputs/features for the SVM model to track the battery degradation. Furthermore, based on the similarity of the partial voltage curves, the SOH can be easily estimated. Feng et al. [55] establish an SVM model that can capture the characteristics of the battery charging curve at different SOH. Then, according to a customized similarity function, the SOH can be calculated. Based on the similarity, the SE and fuzzy entropy (FE) give an accurate definition of the complexity of a signal. The nonlinear relationship between the entropy feature and SOH can be established by the SVM model [59][60][61][62]. The entropy feature is easily extracted from the battery signal (e.g., V and T) and show strong robustness to data noise and temperature variation [62]. Furthermore, according to the empirical mode decomposition (EMD) method to filter the noise of the original voltage data, the improved EF feature shows better estimation accuracy [63]. Regarding the improvement of the feature, multiple features fusion helps increase the estimation accuracy and robustness of the SVM model [52,[64][65][66]. Cai et al. [64] find the optimal combination of features based on the hybrid encoding technology. In [66], the short-term historical information is captured by the multiple-view feature fusion method and the established SVM model can reflect the capacity regeneration phenomena accurately. More details about the outcome of this modeling approach are summarized in Table 3.</p>
        <p>Secondly, the kernel function used in SVM is modified to improve the performance of the model. Because the kernel function largely determines the characteristics of the output curve, Feng et al. [67] use the double deviation parameter in the Gaussian kernel. The improved kernel function can adapt to the curve shape with different curvatures, thus avoid overfitting and under-fitting. As a result, the overall error of the SOH estimation using IC as the feature is less than 1%. Liu et al. [68] separate the kernel function into two terms: one is used to represent the overall degradation trend of the battery SOH, the other is used to simulate the small fluctuations. Therefore, since the proposed model can reflect the battery capacity regeneration, the SOH estimation accuracy is improved.</p>
        <p>Thirdly, SVM is used as an auxiliary method to update the parameters of the observer-based SOH estimator [69]. In [70], a robust and realtime SOC and SOH estimation for Li-ion batteries is developed. The offline established SVM can estimate the battery SOH, which is not only used as the initial capacity of the Kalman filter but also can update the current capacity value. Therefore, the accuracy of SOC estimation is improved. Similarly, Michel et al. model the battery capacity degradation behavior by SVM, and the SVM function, as a bias, is added to the state equation of the capacity [71]. As a result, the state noise is reduced and the Kalman filter is more robust. In [21], SVM was used to build the state-space function to represent battery aging dynamics, and the PF was established to determine the SOH in real-time. In [72], SVM was used to rebuild a posterior distribution thus to avoid the loss of particle diversity of PF.</p>
        <p>Finally, as an important variant of the SVM algorithm, the leastsquares support vector machine (LS-SVM) has been used for battery SOH estimation [73][74][75][76][77][78][79][80][81]. The optimization problem with constraints in the primal space of LS-SVM can be described by</p>
        <p>where e i is the error variable.</p>
        <p>Compared with the optimization problem of the standard SVM, given Fig. 4. The illustration of the support vector machine (SVM) for regression.</p>
        <p>X. Sui et al. in (9), LS-SVM has a less computational burden and faster solving speed because it solves linear equations instead of quadratic programming problems. Deng et al. [76] use temperature as a feature to train the LS-SVM model and combine the genetic algorithm to optimize the parameters. Since the influence of the temperature is considered, the proposed method can achieve accurate capacity estimation at various temperatures during the life of the battery. Liu et al. [77] extracted ten features from the cycling aging data to train an LS-SVM model. Before the model training, the kernel principal components analysis (PCA) algorithm was introduced to fuse these features. As a result, the obtained self-adaptive feature shows higher relevance to the battery capacity than most of the single features. Also, the LS-SVM model is optimized by the particle swarm optimization (PSO) algorithm in [78]. As the global parameters can be obtained by the PSO, the improved model enhances the SOH estimation accuracy and robustness. However, the LS-SVM suffers from the problem of non-sparseness.</p>
        <p>Because LS-SVM replaces the ε-insensitive loss function used in standard SVM with a quadratic loss function dependent on the entire training set, LS-SVM allows every data point in the training set to become an SV [79,80]. In this case, the objective function of LS-SVM should be able to fully fuse the characteristics of the training set. As a result, the model will be complicated and poor in the generalization ability. To increase the sparsity and improve the generalization of the LS-SVM model, a fixed size LS-SVM was proposed [81][82][83]. Chen et al.</p>
        <p>propose an entropy maximization-based algorithm to select the SVs [81]; they first use a very small part of the training dataset to obtain the SVs. Then they randomly select a data point in the training dataset to replace one of the SVs. According to the iterative replacement process, the SVs are updated until the entropy of the SVs reaches the maximum value. It was found that only fixed size of SVs are required for accurate SOH estimation.</p>
        <p>K-NN is efficient for classification purposes in pattern recognition. As a kind of lazy learning, k-NN uses the k closest neighbors in the feature space to classify a new point. When used for regression, as presented in Fig. 5, k-NN first finds the k closest points x 1 , x 2 , …, x k of a new point x new based on a distance measure, and calculates the weighted average of their response to predict the response of x new [84]. For a given training dataset with N points X={x 1 , x 2 , …, x N }, where each point possesses d features, the response of a new data x new can be estimated by k-NN as follows.</p>
        <p>First, in order to describe how close each training points x i is to the testing points x new , the weighted Euclidean distance between them is calculated, which can be expressed as</p>
        <p>wherex new,j and x i,j are the jth feature of the new point x new and the training points x i , respectively. Besides, w j is the weight of jth feature, with the weights being subjected to the constraint ∑ d j=1 w j = 1. The weight w j reflects the importance of the feature and can be found using an optimization algorithm, such as PSO [84], or differential evolution (DE) algorithm [85]. According to the distance d, the k training points x (1) , x (2) , …, x (k) ordered from the nearest to furthest are obtained. These are called the k nearest neighbors of x new . A kernel function is then used to assign weights to each neighbor (the kernel is usually dependent on the calculated distance), and the prediction for new sample x new can be obtained by:</p>
        <p>where k represents the number of nearest neighbors which controls the flexibility of the model (the higher k is the smoother the model is going to be). y (i) represents the known response of x (i) , ŷnew the predicted response of x new , and K(x new , x (i) ) denotes the kernel function, as given in ( 15)- (17).</p>
        <p>The principle of k-NN regression is simple and is easy to be implemented. In [84], Hu et al. extracted five characteristic features (i.e., the initial charge voltage, the CC charge capacity, the CV charge capacity, the final charge voltage, and the final charge current) of the constant charge curves and used them as the inputs to the k-NN regression model. The PSO algorithm obtained the optimal combination of the feature weight. It not only shows the relative importance of each feature but also ensures accurate capacity estimation. Even though the k-NN regression model is simple and an accurate SOH estimation is easily obtained, the algorithm has a clear disadvantage: the entire range of the battery degradation has to be known, as the k-NN model cannot predict values outside of the observed range.</p>
        <p>The artificial neural network (ANN) [86] was designed to mathematically mimic the genetic activity of the human brain, and it is one of the most popular algorithms for various applications, such as pattern recognition, optimization, and prediction. Generally, an ANN consists of an input layer, multiple hidden layers, and an output layer [87]. The input layer receives the data and transfers the information directly to the hidden layer. Then, each neuron in the hidden layer performs a weighted linear combination computation and propagates the information to the next hidden layer through the activation function. This process continues until the output layer is reached predicting the target output of the model. Based on the network structure, the ANN algorithms are usually divided into traditional neural networks and deep learning algorithms, as illustrated in Fig. 6. The traditional neural networks such as the feed-forward neural network (FFNN) contain only one hidden layer, while deep learning (DL) adopts the adjective "deep" to describe the use of multiple hidden layers. Typical DL algorithms include the recurrent neural network (RNN), where the context unit is used to consider the historical aging information; the deep neural network (DNN), whose hidden layers are fully connected; the convolutional neural network (CNN), where the convolutional layers and the pooling layers are added before the hidden layers, to reduce the dimensionality of the input.</p>
        <p>An FFNN, as shown in Fig. 7, feeds d-dimensional features as inputs into the input layer. The input of the hidden layer will be the sum of the inner-product and the bias, as expressed in ( 21)</p>
        <p>where x In i,p is the pth feature of the ith sample data in the input layer, ω H pq is the weight connecting the pth input neuron and the qth hidden neuron, b q is the bias of the qth hidden neuron, l is the number of the neurons in the hidden layer, and g(• •) is the activation function. Five popular choices of activation function are the linear, sigmoid, hyperbolic tangent (tanh), rectified linear unit (ReLU), and the leaky ReLU functions, summarized in Table 2. Subsequently, the output is calculated as follows:</p>
        <p>where h q is the output, ω O q is the output weight of the qth hidden neuron. There are several approaches for training the weights of an FFNN, such as backpropagation, genetic algorithm, PSO, and DE. By far, the most used and well-known method is backpropagation. During the training process, the backpropagation algorithm can be divided into two parts: the forward phase, and the backward phase. In the forward-phase, the input is fed and propagated forward through the network. This updates the values of every hidden neuron, h q , both before and after activation. Given the output of the network, the loss function is computed (i.e. the error between the predicted and measured output). The backward phase computes the gradient of the loss function w.r.t. each of the weights and biases in the network (the method gets its name as the gradient of the weights in layer k, depending on the gradient of the weights in layer k + ▪ Does not create sparse solutions, when compared to ReLU.</p>
        <p>X. Sui et al.</p>
        <p>1). Given the updated gradient, the GD algorithm can be used as:</p>
        <p>where E w is the loss function. A typical choice of the loss function is the mean square error</p>
        <p>The structure shown in Fig. 7 is used to estimate the battery SOH in . The effectiveness of the FFNN for battery SOH estimation has been verified by both one-year real-time data collected from BMS [88] and calendar aging data at various degradation conditions [89]. Since the complete battery charging and discharging curve are not always available under practical use, extracting SOH features from the partial curve is important [91]. As an effective SOH feature, IC peak from partial IC curves smoothed by Gaussian filter was used for FFNN training in [92]. The authors then selected the most important feature values according to the correction analysis, therefore the proposed FFNN framework is simple but with good accuracy and generalization performance. Considering that the voltage differentiation operation is needed to generate an IC curve, voltage smoothing is usually necessary before the model training step. For this reason, an FFNN is developed in [95] to model the battery voltage charging characteristics. As a result, the smooth IC curve can be derived directly. What's more, since the node parameters of the FFNN have certain physical meanings and are related to the capacities corresponding to different phase transformation reactions, the capacity can be easily derived from the node. To simplify the constructed network and reduce the dimension of the input feature vector, Wu et al. use the importance sampling to select the input for the FFNN [96]. Because the voltage varies obviously at the end of the charging process, the sampling frequency is increased to get more data points. At the same time, fewer samples are picked from the voltage platform. The proposed FFNN with important sampling helps to reduce the computation burden and the estimation error. To capture the local capacity fluctuations, Cao et al. proposed a method for interval prediction of the battery SOH [97]. The proposed method used the sample entropy of the discharge voltage as the input of an FFNN and output the lower and upper SOH estimation. Based on the lower and upper bound, the loss function of FFNN is constructed. The proposed method can successfully predict the local fluctuations of the battery and the overall degradation trend. Furthermore, in order to improve the generalization ability of the model, the monotonicity of the features, as the prior knowledge, is transformed into constraints to optimize the traditional FFNN [99]. With the prior knowledge, the improved FFNN has a better performance than the traditional one for specific tasks. In addition, the equivalent circuit model of the battery is combined with FFNN to realize the joint estimation of SOC and SOH. In this method, the voltage variation [102][103][104], the SOC variation [102,105], and the battery model parameters such as impedance and resistance [106][107][108][109] can be used as the SOH feature to train the FFNN, and the estimated SOH can be used to update the SOC value.</p>
        <p>However, the GD-based methods are generally slow or the parameters easily get trapped into the local optimum. For solving this problem, random vector functional link neural networks were introduced by Pao et al. [110] and a simplified variant called extreme learning machine (ELM) was later proposed by Huang et al. [111]. ELM does not need the iterative network parameter optimization, as after the input weights and the hidden layer biases are chosen randomly, only the output weights are estimated. Estimating the weights of the output-layer is equivalent to that of a linear model and can, therefore, be easily determined based on a generalized Moore-Penrose inverse operation. It follows that estimating the weights of an ELM is much faster than the traditional learning methods and has less amount of computation. However, as it contains less trainable weights the ELM approach will always have smaller accuracy than an FFNN where the weights have been trained by BP, if the BP has found the global optimum. As shown in Fig. 8, the output nodes are chosen the output of ELM can be presented as</p>
        <p>Then, ( 24) can be written compactly as</p>
        <p>where Y is the output vector given input matrix X , W is the randomized input weight matrix, b is the bias of hidden nodes, g(•) is the activation function, and β is the output weight vector. The optimal β can be analytically computed as</p>
        <p>where H = g ( W⋅X T + b ) and H + is the Moore-Penrose generalized inverse of the hidden layer output matrix H.</p>
        <p>Considering that the feature has a big influence on the SOH estimation results, the data for model training and estimation sometimes need to be measured under the same operating conditions. Pan et al. Because the recursive least square can accurately identify the model parameters without being affected by different loading profiles, the online identification of the feature, i.e., the internal resistance, was achieved [112]. The ELM is trained offline based on the collected dataset, and then the established parameters can be sent to the BMS. Besides, the ELM has a faster learning speed than the traditional FFNN, and the estimation time of these two methods is 0.01 s and 0.3 s, respectively. Therefore, the proposed SOH estimation method is suitable for online implementation. In order to enable online SOH estimation at different discharge rates, Liu et al. [113] developed an energy-based feature, which contains both voltage sequence and discharge rates.</p>
        <p>The data is collected online in many practical applications, so it takes a certain amount of time to obtain the required sample data. In this case, an online sequential ELM is proposed by Zhu et al. in [114] to effectively use historical and new data. The output weight of ELM is first obtained from a small part of the samples, and then the new sample is used to update the weight. The proposed method has the advantages of fast learning, good generalization performance, and high accuracy. In order to further improve the accuracy of ELM, some combined methods were proposed. Ma et al. proposed a broad ELM approach with reconstructed nodes where the broad learning was used to handle the input data and the mapped features were further enhanced by activation function [115]. The proposed method was considered to be an alternative to the DL method because it effectively reconstructed the system in an incremental form, where the nodes were broadened laterally. In [116], transfer learning was used to transfer the knowledge gained from the known data to the unknown data. As a result, the proposed method showed good estimation accuracy with limited data.</p>
        <p>Due to the advantage of automatically extracting features from raw data, DL has been increasingly considered for battery SOH prediction. DL is a branch of ML algorithms based on ANN. The word "deep" comes from the use of multiple hidden layers in the network [117]. Three DL algorithms (i.e., DNNs, CNNs, and RNNs) with very different architectures are used for battery SOH prediction. DNNs are direct extensions of the FFNN. As shown in Fig. 9, they contain multiple hidden layers, and the information is passed through the hidden layers activated by one, or more, of the activation functions seen in Table 1.</p>
        <p>DNN was constructed and compared with LR, SVM, k-NN, and ANN in [118]. The SOH estimation results on the NASA dataset show that DNN outperforms other methods in terms of accuracy. Furthermore, the deep architecture enables SOH estimation using the V, I, T, t time-series data which are the sensor easily accessible, eliminating the need for input feature extraction. To track the different characteristics in various phases of the CC charging curve, Park et al. used the SOC to correct the kernel function in DNN [119]. Hence, the proposed method can accurately estimate the SOH of the battery with various initial SOC and Crate current, making the method not restricted by the complicated and changeable practical working conditions. In [120], the DNN algorithm was also implemented using 
            <rs type="software">Tensorflow</rs> to utilize GPU calculation. As for simplifying the calculation, the SOH was divided into five intervals evenly from 0.8 to 1, and the SOH estimation error is&lt;1.5%. It should be noted that the higher accuracy of using DNN also comes with the drawback of higher computational time and the need for more computing resources.
        </p>
        <p>To solve this problem, on the one hand, the network can be simplified by reducing the connections between neurons, and on the other hand, the input can be simplified by dimensionality reduction methods. In the DNN constructed in [121], the polynomial function was chosen to transfer the information between hidden layers, and the output of each neuron in the hidden layer can be estimated by two variables from the previously hidden layer. Four differential geometry features were extracted from the CC charging voltage curve, and the output SOH can be estimated by a series of partial quadratic polynomials. The proposed structure with a self-organized network simplifies the calculation while ensuring the depth of the network. Song et al. successfully utilized the real-time data collected from the electric vehicles big data platform and realized the online SOH estimation of battery pack [122]. In the proposed intelligent SOH estimation framework, the distribution of C-rate and temperature, the depth of charging/discharging, as well as the mileage of vehicles were considered as SOH feature, ensuring the good generalization performance of the DNN under the dynamic situation. Moreover, the PCA method was used to compress the dataset before model training, thereby reducing the computational burden of the iterative process.</p>
        <p>Another DL algorithm applied to battery SOH estimation is the CNN [123][124][125][126][127]. As shown in Fig. 10(a), the CNN with 2D input contains one or more stacks of convolutional layers and pooling layers, fullyconnected layers, and the output layer. Different from fully connected, each output in the convolutional layer is connected to a part of the inputs. The sparse connectivity is achieved by sliding a filter (i.e., the weights matrix) over the input space. The calculation process between the filter (i.e., the weights matrix) and the subset of input is called "convolution", as shown in Fig. 10(b). In the convolutional product, the stride parameter needs to be considered, which is the step when sliding the filter. Slide = 1 is used in the following formula derivation. Let element are defined manually. The convolution operation is expressed as</p>
        <p>where the element o cov p, q constitutes the feature maps (i.e., the output matrix of the convolutions). The convolutional layer maps the input space to the feature space, while the pooling layer further reduce the size of the feature map by summarizing features presenting in a given window on the feature map. The pooling output can be calculated as</p>
        <p>where l and pool (⋅) denotes the size of the window and the pooling function, respectively. Average pooling and max pooling are two commonly used methods. As the name suggests, the average and maximum values of the elements in the window are used for output respectively. The next step is introducing the non-linearity to the output of the pooling layer; usually, the ReLU activation function is used in CNN as shown in Table 2.</p>
        <p>Due to the existence of the convolutional layer and of the pooling layer, the CNN method can extract and select the features from the raw data automatically. Based on the V, I, and charging capacity measurement during a partial charging cycle, Shen et al. used CNN for SOH estimation for the first time in [123]. The high estimation accuracy, reduced test time, and less memory make the CNN a feasible method for the online SOH estimation. Utilizing the 
            <rs type="software">STM32Cube.AI</rs> software tool [124], the pre-trained model was embedded into microcontrollers, and the model showed unchanged performance on both PC and microcontrollers. The CNN and RNN were combined in [124][125][126] in order to make full use of the advantages of both methods, i.e. the feature extraction capabilities of CNN and the time-series prediction capacities of the RNN. The details about the RNNs are presented in the next subsection, and simply to say, the RNNs are used to handle the time series. In RNN, the structure of hidden states acts as the memory of the network, and the current state is estimated by the current input and the previous state. When processing the temporal data, causal convolutions will be used instead of standard convolutions. However, using the causal convolution model will make the network deeper and, thus, reduce the training speed and increase the memory occupation. To solve these problems, dilated convolution, which skips over the input when convolving, was used in [127]. The authors in [127] combined the CNN with the EMD algorithm, which was used to separate the global capacity degradation and local capacity regeneration. Compared to the RNN method, the proposed method can capture the local capacity regeneration phenomenon and has higher SOH estimation accuracy and robustness. The effectiveness of the proposed method has been validated on two public datasets from the NASA Prognostics Data Repository [23] and CALCE at the 
            <rs type="creator">University of Maryland</rs> [25].
        </p>
        <p>The architecture of RNNs is derived from FFNNs. Unlike the FFNN, the RNN has an additional context unit that is used to store the historical information. Thus, the outputs of some hidden layers are fed back into the input of the previous layer. This addition makes RNNs powerful models that are uniquely capable of dealing with sequential data. RNNs are useful in the state forecasting of dynamic systems and, therefore, also the long-term prediction of battery degradation. Elman NNs are basic RNN structures, as shown in Fig. 11.</p>
        <p>The context unit receives the output values of the hidden layer and stores these values directly. At each time step k, the input x(k) together with the previous values h(k-1) stored in the context units, determine the present output of the hidden layer.</p>
        <p>where k is the current time-step, x In i,p is the pth input feature of ith sample point, w H pq is the weight connecting the input neuron and the hidden neuron, υ p is the weight value connecting the context unit and the hidden neuron, b q is the bias, and g(• •) is the activation function. These weights are optimized by GD using an approach similar to backpropagation. In [128], Chen et al. designed an Elman NN to estimate the SOH, and use the EMD method to process the raw aging data. As a result, the phenomenon of the capacity regeneration is eliminated and two types of sequences are generated; one is the intrinsic mode function, which shows high fluctuation and can be estimated by the autoregressive moving average model. Another one is the final residue, which is a smooth monotonous curve, and it can be estimated by pre-trained Elman NN. The fusion algorithm can reflect the local changes of the capacity fade and also estimate the SOH accurately. In [129], Elman NN is employed to estimate the SOH, which is further used to correct the battery SOC in real-time.</p>
        <p>When modeling time-dependent behavior, RNNs have a clear advantage over FFNNs, as the dependence on previous states is directly included in the network. Thus, the behavior of the battery up until the point of training is taken into account when predicting the future SOH of the battery. However, this can be seen in some applications as a downside, as the previous behavior of the battery has to be known in order to predict the future, unlike the FFNNs, SVM, and k-NN regression. Additionally, for long-term time sequences and complex hidden layers, the long-term dependency problem will cause the gradient vanishing and exploding during the backward propagation of RNNs. To address these questions, the echo state network (ESN) and the long short-term memory (LSTM) were proposed.</p>
        <p>In ESN, reservoir computing is used as an alternative to GD methods for training RNNs [130,131]. On the one hand, ESN has a good memory because it adopts the dynamic reserve that contains a large number of sparsely connected neurons. On the other hand, ESN has a simple training process because only the output weights need to be trained, just like for the ELM. In [130], the time interval of equal discharging voltage difference was extracted from the CC voltage curve as the SOH feature, and then multiple ESNs were established based on the subset obtained after bagging. Furthermore, according to the probability distribution, models were integrated to improve stability and estimation accuracy.</p>
        <p>In LSTM, the gated recurrent unit is used to control gradients information propagation. Unlike the basic Elman NN, the LSTM layers elaborately design the context unit using three gates, as shown in Fig. 12: (1) the input gate, (2) the forget gate, and (3) the output gate [137]. Gates are combinations of activation functions and decide which information should pass through the context unit. Based on the current input x(t) and the previous output of the hidden layer h(t-1), the new internal state vector is:</p>
        <p>When updating the new internal state, the input gate i(t) decides what new information is going to be stored in the internal state and the forget gate f(t) can discard some previous, now redundant, information, which does not influence the prediction result of next time-step. They are calculated as follows:</p>
        <p>The current internal state is updated by combining ( 30)- (32):</p>
        <p>Finally, the output gate computes the output and the new hidden layer. This is implemented as</p>
        <p>Researchers have made contributions to the improvement of LSTM in terms of SOH features optimization [134][135][136][137][138][139][140] and the network enhancement [124][125][126][141][142][143]. According to the published works, the feature extraction and selection methods have been extensively studied, including the use of correlation analysis to select optimal features [134], the use of PF filter to smooth the feature curve [135], the use of multi-channel measurements (i.e. the V, I, T signals) to increase the diversity of input data [138,139], and the use of sparse sampling to improve the model dynamic performance [140]. In [134], the authors first optimize the features according to the grey relational analysis (GRA) and entropy weight methods. Then, the mapping relationship between the enhanced features and the SOH is established by the designed LSTM where the parameters are optimized by the algorithm. You et al. utilized the k-means clustering method to segment the V/I points and obtained 30 subregions [137]. Based on the density distribution of these V/I subregions which can reflect the battery aging, the LSTM only needs V/I instantiations during a short period to estimate the SOH. In addition to good flexibility, the proposed method can achieve strong noise robustness and high accuracy with an average error lower than 2.46%. Choi et al. found that compared to using voltage only, utilizing a multi-channel technique based on V, I, T profiles helps reduce the estimation error of LSTM up to 25% [138]. Similarly, according to the effective fusion of multi-cell degradation information by LSTM, the accuracy and stability of the SOH estimation of the battery pack can be improved. The results were verified in battery packs with different series, parallel and series-parallel structures [139]. Apart from increasing the diversity of the input signal, sparse sampling is also an effective way to improve model performance [140]. Compared with the densely sampled aging data, the sparsely sampled data represent the battery characteristics in a compressed form. Through optimization, the sparsity of a signal can be exploited to recover it from fewer data without any loss in the information.</p>
        <p>In order to make the LSTM algorithm easier to implement online, the network structure is optimized, e.g.by adding the convolutional layer to LSTM. The optimized network structure can fully use the advantages of automatic feature extraction of the CNN [124][125][126], and due to the utilization of shared weights in the convolutional layers, the computational burden of the model is also reduced. Li et al. changed the connected way between the gates in LSTM [141]. In the improved LSTM, the old information in the forget gate and the new information in the input gate can be determined simultaneously. In addition, the historical cell states were fed back to input and output, thereby remaining more beneficial information and shielding the unwanted error signals. The proposed method can achieve a 2.16% average root mean square error (RMSE) on the NASA dataset. In [142], a deep LSTM was proposed and it is promising for online implementation; in this work, the parameters of the model can be properly adjusted in real-time by using the historical data. Furthermore, the proposed method allows variable input dimensions, which provides a better understanding of the historical data and ensures good generalization ability. Compared with traditional FFNN, DNN, RNN, and LSTM with fixed network structures, the proposed method with variable input dimension obtains noticeably smaller estimation errors.</p>
        <p>Ensemble in EL means combing the results of multiple base learners. This method typically achieves higher accurate and more robust outcomes with respect to the ones produced by a single base learner. EL methods can be divided into the model-level ensemble and data-level ensemble.</p>
        <p>In the model-level ensemble, the final output is obtained by a weighted average of multiple base learners [144][145][146][147][148]. Yu utilized the EMD method to decompose the original capacity signal, then LR and Gaussian process regression models were established based on the residuals and a series of intrinsic mode functions. The mean combination method is used to determine the final outputs of multiple base learners to provide the SOH estimation [144]. Cheng et al. selected four algorithms, namely, ANN, SVM, grey model, and the autoregressive integrated moving average model (ARIMA) as the base learners, and combined them through the time-varying weight assignment approach to suit practical applications [145]. Furthermore, the base learners can be enhanced by probabilistic integration, i.e., the final output is determined according to the probability distribution of base learners' output [130,146]. Shen et al. pre-trained eight CNN models based on the aging data of eight battery cells [147]. Transfer learning was then introduced to enhance the models and the EL method was used to combine them. The proposed method can achieve higher accuracy and robustness compared with directly integrating these CNNs. To promote the variety of base learners, the model parameters can randomly be initialized for achieving different parameter settings. In addition, there is no need to precisely optimize the parameters of each base learner. Although this will lead to low estimation accuracy of a single base learner, EL can compensate for the performance defects of the base learner. The estimation results based on fusion CNNs [147], SVMs [148], and RNNs [149] shows that in addition to improving the accuracy, the EL framework overcomes the challenge in parameters determination for the base learner [152].</p>
        <p>2) Data-level ensemble Except for training multiple different base learners first, and then compensating for the prediction error of a single model through the model-level ensemble, we can also use a single base learning algorithm to set up the EL method. These homogeneous base learners can be trained in different ways such as bagging and boosting [150,151]. The first step in bagging is generating subsamples from the original dataset using the bootstrap sampling method. Based on these subsamples, multiple models are established by using the same algorithm. Unlike the bagging method where the models are trained independently, boosting built the model iteratively. In each step, a base learner will be built, and then aggregated to the ensemble model. At the same time, the dataset will be updated to correct the error of the previous base learner.</p>
        <p>The RF is a representative EL algorithm using bagging for SOH estimation, in which the base learner is the classification and regression tree (CART). The CART was firstly introduced by Breiman et al., in 1984, for solving classification or regression predictive modeling problems [153]. A CART model contains a root node, child nodes, and leaf nodes. The root node contains all the N training sample D N , and the leaf nodes correspond to the predicted SOH. The CART algorithm creates a decision tree, which partitions the input space into a set of disjoint regions. CART is a binary tree, which means that during each split a region is portioned into two disjoint regions.</p>
        <p>The partitioning procedure from the root node starts by searching through all features to find the split variable x j and the split point, s, which provides the best partition of the node into two child nodes. The (j, s)-pair should be chosen such that the decision tree sees the largest possible improvement. For regression, the best (j, s) pair can be obtained by minimizing the sum squared error</p>
        <p>where c 1 and c 2 are the predicted values of each region, usually the average of the output values (i.e. the SOH) corresponding to all samples in the region.</p>
        <p>The above steps are repeated for the obtained child nodes until a termination condition is reached. Common termination conditions include: when the maximum depth is reached, when a node contains less than a predefined number of samples, or when it reaches a pre-defined depth. Finally, the prediction function f (X, D N ) is constructed over D N .</p>
        <p>Random forest (RF) integrates multiple CART models using the bootstrap aggregation technique, as illustrated in Fig. 13. In bootstrapping, B subsets of D b N , b = 1, 2, …, B are generated by randomly sampling with replacement from the original training data, D N . The CART algorithm is then applied to each subset and B decision trees are constructed. By averaging the outputs of the B trees, the predicted value of a new observation, x new , is</p>
        <p>It should be noticed that the CART algorithm used in RF is modified, as the best split is chosen from a randomly selected subspace of the features, instead of the feature yielding the largest improvement. The bootstrapping procedure decreases the variance associated with prediction and helps to avoid overfitting the model, thereby, improving the prediction performance of the model on new unseen data. It also reduces the correlation of different trees, making the RF model more robust to noise.</p>
        <p>RF has the following good performance:</p>
        <p>• Reduced computation complexity: RF selects a set of random input features from the subspace to provide the best split. The regression criteria are simple so that RF is comparatively faster than other algorithms. • Easy in real implementation: The feature used for RF can be the raw data which is sensor easily extractable, making the RF suitable for real application. • Strong generalization ability: RFs are ensembles of tree-type classifiers, where each classifier is trained with a different subset of the training set ("bagging"), thereby improving the generalization ability of the classifier. • Strong robustness: RF aggregates the outcomes of all individual trees, therefore the overfitting problem can be avoided.</p>
        <p>Moreover, when determining the best split, RF can discard the irrelevant features that do not contribute to improving the estimation accuracy. It means that the RF algorithm can determine the importance degree of features automatically, and at the same time eliminate the requirement for manual feature extraction. The collected raw data can be directly fed into the trained model without any pre-processing, leading to a low computational cost. While for other algorithms, such as LR and SVM, the only way to obtain the importance of the SOH feature is to perform different training sessions with one or more missing feature values and then perform comparisons.</p>
        <p>Various data are used to train the RF model such as the accumulated capacities within different charging voltage range [154,155] and the time required for each stage of the CC-CV charging or CC discharging process [156,157]. The effectiveness of RF has been verified on different datasets. Li et al. [154] performed the cyclic aging test under different conditions on two types of commercial NMC batteries. Using the charging capacity corresponding to the equal voltage interval in a predefined voltage region as the feature, an RF with 500 trees was constructed for battery capacity estimation. The trained RF model provides good estimation results as the maximum RMSE is 1.3%. In [156], four time-related variables are used for training the RF. The results show that the time required for reaching the charging cut-off voltage has less importance, while the time required for reaching the maximum charging temperature is of relatively high importance. Alexander et al. [158] analyzed the battery pack's behavior during the active charge balancing process, and extract the corresponding parameters, such as the voltage difference, the receiver/sender step discrepancy, etc. Finally, an accurate SOH estimation of the battery pack is achieved.</p>
        <p>In Section 2, we introduced the principles and applications of each algorithm in detail. In this section, the performance comparison among various ML algorithms will be given from three aspects: the estimation performance, the publication trend, and the training modes. First, in order to facilitate the reference of researchers or engineers, and enable them to quickly understand the research status of ML methods, a detailed synopsis of each of the ML algorithms is provided in Table 3. The SOH estimation error indicated in publications is summarized, along with the tested battery chemistry, used data profiles, feature extraction modes, input features, and the required training data size.</p>
        <p>In this part, the pros and cons of various algorithms and their variants are compared, and the results are summarized in Table 4.</p>
        <p>LR is simple to implement and the coefficients of the algorithm can be easily derived. However, the battery, as a complex electrochemical system, undergoes a nonlinear degradation process which does not satisfy LR's assumption of linear correlation between dependents and independent variables. Also, the outliers have a big effect on the estimation results, making LR less suitable for SOH estimation of batteries operating under different conditions.</p>
        <p>K-NN is easy to implement because the model is established based on the distance between the points. Moreover, the training process is not needed. However, K-NN is memory intensive and costly for estimating when the dataset is large or the data has a high dimensional feature, as it has to track all data and find the neighbor nodes.</p>
        <p>SVM has the regularization parameter C, so it has good generalization capabilities and can prevent the over-fitting problem. Due to the utilization of kernel function, SVM can efficiently handle non-linear data even with high dimension. But the appropriate selection of the kernel function is not easy, and the Gaussian kernel function is often used to generate the SVM-based SOH estimation model. Besides, SVM is not suitable for large datasets because the support vectors should be stored in the memory, which will increase the training time and computation requirements.</p>
        <p>data files that provide the cycle number, the authors choose a part of the cycles for model training and the rest for validation. In the column of "estimation", multiple denotes authors use the dataset from different batteries to train the ML model and estimate the SOH. Single-cell denotes the training data and the estimation data come from different parts of the dataset of the same battery. The root mean square error (RMSE), the mean absolute error (MAE), and the maximum error (MaxE) are used to evaluate the performance of the proposed method. ANNs have the capacity to learn the degradation behavior of the battery because they use the activation function to introduce the nonlinear properties to the network, making thus ANNs universal function approximators. DL is good at modeling nonlinear data with a large number of inputs through the cooperation between neurons in multiple layered networks. However, DL is subject to the drawbacks of a large computational burden and the need for large datasets. When a large amount of battery aging data is available, DL can achieve high estimation accuracy that other methods cannot match. Among the variants of DL, DNN is usually used for extracting global features from raw data, which will be suitable for online estimation of the battery's SOH. The data collected by the sensors can directly input the network without manually feature extraction. Similarly, CNN uses the filter to extract the relevant feature from the input data automatically. RNN has memory, so it can remember the historical information and can process time-series inputs of any length. Therefore, RNN is not only capable of SOH estimation under dynamic working conditions (such as the randomized load profiles [24]), but also can track local capacity regeneration. Because RNNs share the same parameters across different time steps, the computational cost is reduced. However, this will also lead to vanishing and exploding gradient during the BP process for a long time series. LSTM solves this problem by using the gated recurrent unit which can control the propagation of the gradient information.</p>
        <p>EL can achieve more accurate and stable SOH estimation results than single base learners. By setting different parameters for each base learner, the diversity of these base learning algorithms is employed in the final model. Therefore, different aging conditions can be utilized by EL, and EL has good generalization performance. Also, it is less critical to optimize the parameters because the base learners with poor estimation performance will contribute less to the final ensemble model. However, it is time-consuming to train multiple base learners. Relatively, RF adopts simple regression criteria, thereby, reducing the computational complexity. By aggregating the outcomes of all individual trees, the overfitting can be avoided.</p>
        <p>Based on the above analysis, we use the radar chart presented in Fig. 14 to summarize the comparison of the ML algorithms across five performance metrics i.e., the estimation accuracy, the implementation easiness, the computational complexity, the training data size requirement, and the ability to deal with overfitting. Among them, accuracy is used to measure the training error of the known data and the estimation error of the new data. The easiness of algorithm implementation considers both the occupation of computing resources and the dependence on manual feature extraction. The computational complexity is used to evaluate the memory required by an algorithm, generally expressed using the big O notation [159]. The data requirement refers to the size of the training dataset, regardless of the feature dimension. Overfitting means that the established model is too closely fit a limited training data, while the estimation results of the new data are poor.</p>
        <p>It should be noted that various ANN variants and EL composed of different base learners show great differences in these performances. In order to simplify the analysis, the RNN and RF are selected as the representative method of ANN and EL respectively, and they are compared with other methods. Furthermore, the number on each axes indicates the ranking of the algorithms on each performance index, and the larger the number, the better the performance of the corresponding metric. For algorithm selection, we can assign different weights to these Fig. 14. Comparison of the five ML algorithms across different metrics (it should be noted that for the "accuracy", "implementation easiness", and "dealing with overfitting", a higher score denotes higher performance while, for the "computation complexity" and "dataset requirement", a lower score indicates better performance. For better visualization, the coordinate scales of computational complexity and dataset requirement are shown in reverse order). metrics according to the actual demands, and evaluate the overall performance of the algorithms.</p>
        <p>We rigorously reviewed the publication of such methods in the past ten years by searching the Elsevier Scopus international databases, and the annual number of publications is shown in Fig. 15. Only journals and conference papers cited at least twice were counted. It can be seen that among these methods, the ANN was first applied for battery health management in 2005. With the development of DL, more and more researchers are interested in using CNN and RNN for battery SOH estimation, making ANN a research hotspot in the past five years. Similarly, mature SVM technology is the second most used method. On the contrary, using k-NN alone is no longer a research focus of SOH estimation, as only one paper using this method was published in the past 5 years. In addition, EL has been adopted by researchers since 2018, and their good generalization characteristics and robustness have attracted increased attention.</p>
        <p>Using ML methods to estimate SOH mainly includes data preparation (including data collection and noise reduction), feature extraction, feature selection, and parameter optimization of the ML model. Depending on the chosen algorithm, three different training modes can be followed for obtaining the battery SOH estimation, as presented in Fig. 16. Each procedure is divided into four steps, i.e., data preparation, feature extraction, feature selection, and ML model training. The data preparation stage is similar for all three modes. During the battery aging process, V, I, T, t will be measured and stored for subsequent data analysis. Nevertheless, major differences between these modes can be seen in the feature dimensionality reduction stages (i.e., feature extraction and selection), which occurs between data processing and model input. In general, the feature can be obtained manually or automatically by the ML-based SOH estimation method.</p>
        <p>For mode A, battery aging features are defined and extracted from previously performed experimental tests. Many effective features are used and they can be summarized into three types, namely differential features, geometric features, and statistical features [160,161]. Firstly, ICA is a widely used technique for li-ion battery SOH evaluation [16,17]. By differentiating the battery charging capacity against the battery voltage, the IC curve can be obtained, and the IC peaks and valleys could accurately reflect the characteristics of the underlying battery chemistry. Similarly, the differential voltage dV/dt and the differential temperature dT/dt can also be used for SOH estimation. Secondly, the geometrical feature such as the knee points on the pulse voltage response curve [121], the curvature of the voltage curve, and the time related to a fixed voltage interval contain plentiful information on battery aging. They are used as features for parameterizing SOH estimation algorithms. Thirdly, as a powerful statistic for measuring the complexity of a signal, sample entropy, and its improved form, fuzzy entropy, have been used for SOH estimation [62]. Mode A is often used in the training and estimation process of LR and k-NN methods because the highdimensional input (i.e., the measured V, I, T, t) will increase the computation cost. Of course, the SVM and the ANN with simple structures are also suitable for this mode. However, manual feature extraction brings problems accordingly. For example, it is difficult to create correct and robust features artificially. Then the extraction process is time-consuming and requires a lot of manual calculation. Additionally, the extracted features are sometimes only suitable for the investigated experimental conditions, and it is difficult for the battery to operate under the same conditions in practical applications.</p>
        <p>In mode A, the feature extraction is always followed by a feature selection step, because some features have poor correlations with SOH, which will reduce the estimation accuracy and lead to overfitting.</p>
        <p>Generally, there are three feature selection methods including the filterbased, wrapper-based, and fusion-based methods [162]. Among the filter-based method, the GRA [16,80], Pearson correlation coefficient analysis (PCC) [163], and Spearman correlation coefficient analysis (SCC) [163] are used. For example, in [128], GRA was used to select the sub-sequences and residuals, which were obtained through EMD processing on the capacity degradation data. The final residual was determined to be the SOH feature. The residual is a smooth monotone curve, which can effectively represent the SOH variation trend. Based on the GRA results, Wu et al. further used the entropy weight method to assign corresponding weights to various features [134]. Similarly, an enhanced SOH feature was proposed by linearly combining the selected features [68]. Zhou et al. used the Box-Cox method to improve the linearity between the mean voltage falloff and the capacity and then combined PCC and SCC to find the optimal parameters. As a result, the SOH features that have a nearly linear relationship with capacity were obtained [163]. The wrapper-based method evaluates different feature subsets repeatedly. It always uses a cross-validation strategy but brings a large computational burden. Therefore, the fusion method which combines the filter and wrapper is used to improve the estimation accuracy and calculation efficiency [162]. Some flexible solutions through designing the interpretable ML framework are respectively proposed in [164] and [165]. Among them, the feature extraction methods based on Gaussian process regression and RF can not only accurately predict the battery electrode performance, but also effectively quantify feature importance and correlations, which well benefit the battery smarter manufacturing and related battery applications.</p>
        <p>Mode B automatically extracts features based on dimensionality reduction methods (such as the PCA [128,166,167] and linear discriminant analysis, LDA [168,169]), avoiding the feature predefinition step. Based on the full-connected layers of ANN or the memory unit of RNN, the important information is retained and learned by the network. In other words, ANN and RNN select the features automatically.</p>
        <p>Mode C further simplifies the feature setting process. Firstly, SVM can deal with high-dimensional feature computation by using the kernel trick. Secondly, DNN is suitable to extract global features from the raw data. CNN realizes automatic feature extraction by adopting the convolutional techniques so that CNN can better handle multi-dimensional data. Thirdly, RF builds multiple random trees by bootstrapping sampling, so RF can handle large data sets with high dimensions. The most significant variables can be automatically identified through the aforementioned three methods, so the measured V, I, T, t can be directly inputted into the ML model. Thus, the feature extraction and selection, as well as the SOH-feature model, are realized automatically by the ML algorithm.</p>
        <p>Currently, a lot of comprehensive work has been carried out in the development of ML technology for battery SOH estimation. There are still some challenges facing in this field mainly from three aspects: the feature extraction, the algorithm development, and the on-board implementation.</p>
        <p>First, the battery is a complex electrochemical system, its degradation involves many parasitic side reactions in the anode, cathode, electrolyte, and electrode-electrolyte interfaces. Therefore, batteries' aging behavior will be significantly influenced by the operating conditions. In this case, the SOH feature extracted from the lab data will be invalid as the battery may exhibit different degradation behavior in real applications. Hence, the SOH features with strong robustness should be developed to deal with different working conditions. In addition, SOH features should preferably be extracted from short-term measurement data for facilitating practical applications.</p>
        <p>Second, the offline trained ML model will be used directly for online estimation and the model parameters will remain unchanged. That is because the traditional ML method assumes that the training data and test data follow the identical statistical distribution. However, the offline trained model cannot take all the working conditions into consideration without the support of big data. As a result, the estimation error will gradually increase as the battery ages. To cope with this issue, ML algorithms with self-learning or online model update should be developed. Such methods can reduce the required training data size while improving the estimation accuracy.</p>
        <p>Third, due to the occupation of computing resources, the onboard implementation of ML-based SOH estimation algorithms is not yet mature. Additionally, because SOH changes slowly and is always estimated at the macro scale, few papers show the SOH estimate results in real-time. Nowadays, GPUs will become local and common devices that are specifically designed to make the inference of ML algorithms approachable. Therefore, the development of algorithms in future research should focus more on the difficulty of on-board implementation.</p>
        <p>According to the above comparison and analysis from the perspective of theory and application, it can be concluded that ML technologies possess immense potentialities in battery SOH estimation. Considering the advancement of the algorithms and engineering application, we can explore the opportunities of using the great advantages of ML from the following four aspects:</p>
        <p>Firstly, SVMs will still be an important and effective ML algorithm at present and even in the future, especially in the current situation where the amount of data is small. For the unique degradation behavior of the battery, especially the local capacity regeneration phenomenon, the estimation performance of SVM can be improved by correcting the kernel function or adopting the dual-kernel function. Besides, the aging mechanism of the battery can be transformed into constraints and added to the SVM. However, there is still a lack of rigorous theoretical proofs of the convergence of the SVM model using the improved kernel. In addition, the SVM can be combined with model-based adaptive filters to realize the online SOH estimation.</p>
        <p>Secondly, in terms of how easy it is to get the model input, the research trend is to develop ML algorithms that can automatically extract features from raw data, such as CNN, DNN, and RF. Because the raw data is sensor easily extractable, the online SOH estimation will be more easily achieved in real applications.</p>
        <p>Thirdly, the EL algorithm offers a data-efficient and accurate alternative for SOH estimation when the base learner chooses a simple algorithm that requires a small amount of data and is easy to implement, such as LR, k-NN, SVM, and ELM. By combining these base learners, the robustness and accuracy of the model can be improved, while the computation complexity can be reduced.</p>
        <p>Finally, with the development of big data and cloud computation platform, DL is a very promising candidate for SOH estimation. When a large amount of data is available, DL has obvious advantages over other algorithms. Specifically, DNN could be used to process very large-scale data, CNN is better at handling multi-dimensional data, and LSTM is good at dealing with time-series data. Although it is computationally expensive and time-consuming to train DL with traditional CPUs, with the popularity of GPUs and cloud computing servers, the computational burden should not be a major concern.</p>
        <p>Battery degradation is a complex electrochemical process. To ensure the reliable operation of the batteries and extend their service lifetime, it is of great importance to get accurate knowledge of the SOH for the battery management systems. The interest in ML methods has increased because of their capability in modeling complex nonlinear systems. This paper gives a timely and systematic review of battery SOH estimation methods based on ML algorithms.</p>
        <p>Five non-probability algorithms including LR, k-NN, SVM, ANN, and EL are introduced, and the derivation of these methods is presented, and flow charts with a unified form are provided for each method to help readers to clearly understand the connections and differences between them. The advantages and drawbacks of these ML methods are compared based on the theoretical analysis and thorough discussion.</p>
        <p>General review of methods and associated issues for SOC, State of Energy, SOH, State of Power, State of Temperature, and State of Safety estimation Rezvanizaniani et al., 2014 [29] SOC and SOH estimation Review of physical-models, datadriven models, and fusion model used for battery SOC and SOH estimation Ng et al., 2020 [30] SOC and SOH estimation Review of battery equivalent circuit, physics-based models, and ML methods for state estimation, and suggestions on methods selection measurement, indirect analysis, adaptive filtering, and data-driven methods, and suggestions on methods selection Li et al., 2019 [33] SOH estimation and RUL prediction Review of data-driven methods including differential analysis techniques for online estimation, semi-empirical and empirical model for data fitting, and ML methods</p>
        <p>Leaky ReLU g(u) =</p>
        <p>{ a⋅u, for u⩽0 (0 &lt; a &lt; 1) u, otherwise ▪ An improvement of ReLU without dying ReLU problem.</p>
        <p>X.</p>
        <p>Sui et al.</p>
        <p>The details, i.e. the data used, the input features, and the estimation accuracy of the algorithms proposed in some of the papers are summarized in a table for easy reference. Surveying 144 relevant journal papers published in the recent ten years, shows that SVM and ANN algorithms are research hotspots. With the development of big data and cloud computation platforms, DL has great potential in estimating battery SOH under complex aging conditions. Nevertheless, DL suffers from a large amount of calculation, and EL of simple base learners (e.g. LR, SVM, etc.) represents an emerging alternative while trading off between data size and accuracy.Furthermore, three training modes are summarized according to the way the algorithms process the input data. Finally, future research opportunities of ML in battery SOH estimation are presented. We hope to inspire the interested researchers to continuously improve the application of ML in SOH estimation.</p>
        <p>This work was partially supported by EUDP Denmark under the "CloudBMS: The New Generation of Intelligent Battery Management Systems" project (grant number: 64017-05167).</p>
        <p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
    </text>
</tei>

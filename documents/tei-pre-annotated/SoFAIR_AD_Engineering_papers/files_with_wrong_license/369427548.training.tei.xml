<?xml version="1.0" encoding="UTF-8"?>
<tei xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc xml:id="_1"/>
        <encodingDesc>
            <appInfo>
                <application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-06-14T07:02+0000">
                    <ref target="https://github.com/kermitt2/grobid">A machine learning software for extracting information from scholarly documents</ref>
                </application>
            </appInfo>
        </encodingDesc>
    </teiHeader>
    <text xml:lang="en">
        <p>Online battery capacity estimation is a critical task for battery management system to maintain the battery performance and cycling life in electric vehicles and grid energy storage applications.Online battery capacity estimation is a critical task for battery management system to maintain the battery performance and cycling life in electric vehicles and grid energy storage applications.</p>
        <p>Convolutional Neural Networks, which have shown great potentials in battery capacity estimation, have thousands of parameters to be optimized and demand a substantial number of battery aging data for training. However, these parameters require massive memory storage while collecting a large volume of aging data is time-consuming and costly in real-world applications.Convolutional Neural Networks, which have shown great potentials in battery capacity estimation, have thousands of parameters to be optimized and demand a substantial number of battery aging data for training. However, these parameters require massive memory storage while collecting a large volume of aging data is time-consuming and costly in real-world applications.</p>
        <p>To tackle these challenges, this paper proposes a novel framework incorporating the concepts of transfer learning and network pruning to build compact Convolutional Neural Network models on a relatively small dataset with improved estimation performance. First, through the trans-To tackle these challenges, this paper proposes a novel framework incorporating the concepts of transfer learning and network pruning to build compact Convolutional Neural Network models on a relatively small dataset with improved estimation performance. First, through the trans-</p>
        <p>fer learning technique, the Convolutional Neural Network model pre-trained on a large battery dataset is transferred to a small dataset of the targeted battery to improve the estimation accuracy. Then a contribution-based neuron selection method is proposed to prune the transferred model using a fast recursive algorithm, which reduces the size and computational complexity of the model while maintaining its performance. The proposed model is capable of achieving fast online capacity estimation at any time, and its effectiveness is verified on a target dataset collected from four Lithium iron phosphate battery cells, and the performance is compared with other Convolutional Neural Network models. The test results confirm that the proposed model outperforms other models in terms of accuracy and computational efficiency, achieving up to 68.34% model size reduction and 80.97% computation savings.fer learning technique, the Convolutional Neural Network model pre-trained on a large battery dataset is transferred to a small dataset of the targeted battery to improve the estimation accuracy. Then a contribution-based neuron selection method is proposed to prune the transferred model using a fast recursive algorithm, which reduces the size and computational complexity of the model while maintaining its performance. The proposed model is capable of achieving fast online capacity estimation at any time, and its effectiveness is verified on a target dataset collected from four Lithium iron phosphate battery cells, and the performance is compared with other Convolutional Neural Network models. The test results confirm that the proposed model outperforms other models in terms of accuracy and computational efficiency, achieving up to 68.34% model size reduction and 80.97% computation savings.</p>
        <p>The battery energy storage systems, which capture energy and store it using electrochemical solutions, are playing vital roles in mass roll-out of electric vehicles (EVs) [1] and acceptance of significant penetration of renewable power worldwide [2], and can provide clear socio-economic benefits in the transition to low carbon economies [3]. Particularly, the lithium-ion battery has been widely used in EVs and grid energy storage systems due to a number of desirable features [1], and it is crucial to maintain its functional performance, cycling life, and safe and reliable operation. The inevitable battery degradation during its utilization, which may lead to potential system failures and safety issues, is often described as the battery state of health (SOH). The capacity and internal resistance are two most commonly used indicators that quantitatively characterize the SOH and assess the battery aging degree [1]. Among the two SOH indicators, the capacity is more intuitive to reflect the driving range that users care about [4]. Thus, a variety of approaches haven been reported to provide accurate capacity estimation and they can be roughly grouped into two categories, namely model-based and machine learning-based approaches.The battery energy storage systems, which capture energy and store it using electrochemical solutions, are playing vital roles in mass roll-out of electric vehicles (EVs) [1] and acceptance of significant penetration of renewable power worldwide [2], and can provide clear socio-economic benefits in the transition to low carbon economies [3]. Particularly, the lithium-ion battery has been widely used in EVs and grid energy storage systems due to a number of desirable features [1], and it is crucial to maintain its functional performance, cycling life, and safe and reliable operation. The inevitable battery degradation during its utilization, which may lead to potential system failures and safety issues, is often described as the battery state of health (SOH). The capacity and internal resistance are two most commonly used indicators that quantitatively characterize the SOH and assess the battery aging degree [1]. Among the two SOH indicators, the capacity is more intuitive to reflect the driving range that users care about [4]. Thus, a variety of approaches haven been reported to provide accurate capacity estimation and they can be roughly grouped into two categories, namely model-based and machine learning-based approaches.</p>
        <p>In model-based battery capacity estimation approaches, physical models [5], empirical models [6], thermal models [7] and fusion models [8] are often used in conjunction with observers or adaptive filtering algorithms to achieve online capacity estimation. For example, Yu et al. [9] used the least squares algorithm to estimate the battery capacity based on the equivalent circuit model. Zheng et al. [10] estimated the capacity using proportional-integral observers based on an electrochemical model. Xiong et al. [11] extracted five parameters that are strongly correlated with battery aging from the electrochemical model to estimate the capacity. Zheng et al. [12] applied sequential extended Kalman filters to estimate capacity based on the empirical model.In model-based battery capacity estimation approaches, physical models [5], empirical models [6], thermal models [7] and fusion models [8] are often used in conjunction with observers or adaptive filtering algorithms to achieve online capacity estimation. For example, Yu et al. [9] used the least squares algorithm to estimate the battery capacity based on the equivalent circuit model. Zheng et al. [10] estimated the capacity using proportional-integral observers based on an electrochemical model. Xiong et al. [11] extracted five parameters that are strongly correlated with battery aging from the electrochemical model to estimate the capacity. Zheng et al. [12] applied sequential extended Kalman filters to estimate capacity based on the empirical model.</p>
        <p>Different types of battery models were compared and analysed in [13]. And three widely used filtering algorithms for estimating the capacity, i.e. extended Kalman filter, particle filter and recursive least squares, were investigated in [14], and their performances are compared and analysed in terms of accuracy and convergence speed. However, the performance of these methods is highly dependent on the model quality, while to build an accurate battery model is difficult to achieve as it requires a large amount of priori knowledge and experimental data, which are not always available. Therefore, although model-based methods have merits of offering clear physical insights, their dependence on precise battery model limits their broad applications.Different types of battery models were compared and analysed in [13]. And three widely used filtering algorithms for estimating the capacity, i.e. extended Kalman filter, particle filter and recursive least squares, were investigated in [14], and their performances are compared and analysed in terms of accuracy and convergence speed. However, the performance of these methods is highly dependent on the model quality, while to build an accurate battery model is difficult to achieve as it requires a large amount of priori knowledge and experimental data, which are not always available. Therefore, although model-based methods have merits of offering clear physical insights, their dependence on precise battery model limits their broad applications.</p>
        <p>As an alternative approach, machine learning-based battery capacity estimation methods have attracted substantial interests in recent years. These methods do not require priori knowledge on the complex physical principles of the battery and are easy to implement. Methods such as Gaussian process regression (GPR) [15], kernel ridge regression [16], support vector machine (SVM) [17], and support vector regression [18], just to name a few, have been successfully applied in battery capacity estimation. Among these methods, Liu et al. [19] employed a GPR-based framework to simultaneously predict capacity and quantify the uncertainty of predicted values. Li et al. [20] proposed a GPR-based method to estimate the battery SOH using the health features extracted from partial incremental capacity curves. The accuracy, robustness and effectiveness of this approach are verified on four batteries from NASA battery degradation dataset. Feng et al.As an alternative approach, machine learning-based battery capacity estimation methods have attracted substantial interests in recent years. These methods do not require priori knowledge on the complex physical principles of the battery and are easy to implement. Methods such as Gaussian process regression (GPR) [15], kernel ridge regression [16], support vector machine (SVM) [17], and support vector regression [18], just to name a few, have been successfully applied in battery capacity estimation. Among these methods, Liu et al. [19] employed a GPR-based framework to simultaneously predict capacity and quantify the uncertainty of predicted values. Li et al. [20] proposed a GPR-based method to estimate the battery SOH using the health features extracted from partial incremental capacity curves. The accuracy, robustness and effectiveness of this approach are verified on four batteries from NASA battery degradation dataset. Feng et al.</p>
        <p>[21] developed a SVM-based online SOH estimation method. The SVM model is first established offline using features extracted from the battery charging curves of cells at different SOHs. Then the model is employed for online SOH estimation by comparing the features of the measured charging voltage segment and the stored models. Guo et al. [22] extracted health features from charging measurements, and eight features which are most relevant to the capacity degradation are selected as the inputs to a relevance vector machine (RVM) model. It is worth noting that the performance of these methods heavily relies on the manually extracted features, which requires significant manual and computational efforts, and the resultant model is often application specific and may not be generalized.[21] developed a SVM-based online SOH estimation method. The SVM model is first established offline using features extracted from the battery charging curves of cells at different SOHs. Then the model is employed for online SOH estimation by comparing the features of the measured charging voltage segment and the stored models. Guo et al. [22] extracted health features from charging measurements, and eight features which are most relevant to the capacity degradation are selected as the inputs to a relevance vector machine (RVM) model. It is worth noting that the performance of these methods heavily relies on the manually extracted features, which requires significant manual and computational efforts, and the resultant model is often application specific and may not be generalized.</p>
        <p>To address this bottleneck, some other machine learning-based techniques such as Elman neural network [23], long short-term memory (LSTM) [24], random forest [25] and Convolutional Neural Network (CNN) [26] have been applied to automatically extract features from the measurements.To address this bottleneck, some other machine learning-based techniques such as Elman neural network [23], long short-term memory (LSTM) [24], random forest [25] and Convolutional Neural Network (CNN) [26] have been applied to automatically extract features from the measurements.</p>
        <p>The Elman neural network was applied in [23] to estimate the battery capacities of the following cycles until its end-of-life in real-time, only using the charged capacities in the past cycles as inputs of the network. A LSTM network was designed in [24] for capacity estimation using direct measurable data, i.e. current and voltage, and the model's robustness and flexibility in dynamic environments has been extensively verified with data collected from more than seventy Lithium-ion batteries cycled with more than ten driving profiles. In [25], the charge capacity recorded in a specific voltage region was used as the input to a random forest regression model, the trained model can directly use online recorded data for capacity estimation. And [26] directly used the voltage, current and charge capacity of each cycle as inputs to train a CNN model for capacity estimation. The performance of three different neural network models for capacity estimation, i.e. feed-forward neural network, LSTM and CNN, were compared in [27], and the test results revealed the difficulty of the resultant models in dealing well with limited available battery data. It is clear that these machine learning-based methods have shown great potentials in battery capacity estimation, yet their performance is heavily dependent on the size of the training dataset. Only models trained with sufficient data are capable of achieving satisfactory accuracy.The Elman neural network was applied in [23] to estimate the battery capacities of the following cycles until its end-of-life in real-time, only using the charged capacities in the past cycles as inputs of the network. A LSTM network was designed in [24] for capacity estimation using direct measurable data, i.e. current and voltage, and the model's robustness and flexibility in dynamic environments has been extensively verified with data collected from more than seventy Lithium-ion batteries cycled with more than ten driving profiles. In [25], the charge capacity recorded in a specific voltage region was used as the input to a random forest regression model, the trained model can directly use online recorded data for capacity estimation. And [26] directly used the voltage, current and charge capacity of each cycle as inputs to train a CNN model for capacity estimation. The performance of three different neural network models for capacity estimation, i.e. feed-forward neural network, LSTM and CNN, were compared in [27], and the test results revealed the difficulty of the resultant models in dealing well with limited available battery data. It is clear that these machine learning-based methods have shown great potentials in battery capacity estimation, yet their performance is heavily dependent on the size of the training dataset. Only models trained with sufficient data are capable of achieving satisfactory accuracy.</p>
        <p>However, to collect a large battery degradation dataset requires a substantial number of cycling tests, which is extremely time-consuming and costly. To this end, transfer learning techniques can be incorporated into these methods to improve the estimation performance on small dataset.However, to collect a large battery degradation dataset requires a substantial number of cycling tests, which is extremely time-consuming and costly. To this end, transfer learning techniques can be incorporated into these methods to improve the estimation performance on small dataset.</p>
        <p>In [28], the battery health was estimated by combining the kernel ridge regression and transfer learning to improve the prediction accuracy. In [29], transfer learning was applied to achieve accurate, quick and steady prediction based on a LSTM model. A CNN model combining the concepts of transfer learning and ensemble learning was used for capacity estimation in [30] with voltage, current and charge capacity as the inputs of the network. Although the inputs to the model were extracted from a partial charge cycle, which can start from any partially discharged state, it must end up being fully charged. However, it does not always have the opportunity to fully charge/discharge an EV in practical applications [21]. Similar work has been reported in [31].In [28], the battery health was estimated by combining the kernel ridge regression and transfer learning to improve the prediction accuracy. In [29], transfer learning was applied to achieve accurate, quick and steady prediction based on a LSTM model. A CNN model combining the concepts of transfer learning and ensemble learning was used for capacity estimation in [30] with voltage, current and charge capacity as the inputs of the network. Although the inputs to the model were extracted from a partial charge cycle, which can start from any partially discharged state, it must end up being fully charged. However, it does not always have the opportunity to fully charge/discharge an EV in practical applications [21]. Similar work has been reported in [31].</p>
        <p>In view of the limited computational capability of the current battery management systems (BMSs) and the difficulty in collecting long-term battery aging data, though the CNN has shown to be a powerful tool to automatically extract features from direct measurements and estimate the capacity with satisfactory accuracy [32], its large model size and significant computational cost together with its limitations in handling small amounts of data hinder its practical applications. This paper aims to address two research questions in order to tackle the aforementioned challenges, 1) how to leverage the knowledge embedded in the existing often huge amount of battery data to predict the capacity degradation of the same type of batteries in new applications where available data are however quite limited; 2) how to significantly reduce the CNN model complexity such that it can be implemented in embedded systems like battery management systems (BMS) used in electric vehicles where memory storage is however quite limited. To answer these research questions, a pruned CNN (PCNN) in combination with transfer learning is developed for battery capacity estimation in this paper, which can greatly compress the model size and reduce the computational complexity, while improve the estimation accuracy on small aging dataset. In this proposed method, measurements from flexible partial charging curves with a fixed length of 225 data points and a flexible starting point are directly used as the model inputs and no additional feature extraction procedure is required. The main contributions of this paper are elaborated as follows:In view of the limited computational capability of the current battery management systems (BMSs) and the difficulty in collecting long-term battery aging data, though the CNN has shown to be a powerful tool to automatically extract features from direct measurements and estimate the capacity with satisfactory accuracy [32], its large model size and significant computational cost together with its limitations in handling small amounts of data hinder its practical applications. This paper aims to address two research questions in order to tackle the aforementioned challenges, 1) how to leverage the knowledge embedded in the existing often huge amount of battery data to predict the capacity degradation of the same type of batteries in new applications where available data are however quite limited; 2) how to significantly reduce the CNN model complexity such that it can be implemented in embedded systems like battery management systems (BMS) used in electric vehicles where memory storage is however quite limited. To answer these research questions, a pruned CNN (PCNN) in combination with transfer learning is developed for battery capacity estimation in this paper, which can greatly compress the model size and reduce the computational complexity, while improve the estimation accuracy on small aging dataset. In this proposed method, measurements from flexible partial charging curves with a fixed length of 225 data points and a flexible starting point are directly used as the model inputs and no additional feature extraction procedure is required. The main contributions of this paper are elaborated as follows:</p>
        <p>• Firstly, a new CNN-based battery capacity estimation framework incorporating the concepts of transfer learning and network pruning is proposed. The transfer learning techniques are applied to improve the model performance on a small battery degradation dataset by leveraging the knowledge learned from a large but different dataset. While the network pruning is used to reduce the model size and computational cost, which makes it possible to implement the CNN model in on-board BMSs. The performance improvements of the proposed method in terms of estimation accuracy, model size and computation cost are evaluated on a small battery degradation dataset.• Firstly, a new CNN-based battery capacity estimation framework incorporating the concepts of transfer learning and network pruning is proposed. The transfer learning techniques are applied to improve the model performance on a small battery degradation dataset by leveraging the knowledge learned from a large but different dataset. While the network pruning is used to reduce the model size and computational cost, which makes it possible to implement the CNN model in on-board BMSs. The performance improvements of the proposed method in terms of estimation accuracy, model size and computation cost are evaluated on a small battery degradation dataset.</p>
        <p>• Secondly, a novel fast recursive algorithm (FRA)-based network pruning approach is proposed to select the neurons one by one that contribute most to the model output and re-assign the weight for each selected neuron, while redundant neurons are removed. This lead to a compact CNN with much fewer neurons and parameters, while the accuracy loss due to nodes pruning is negligible, and the pruned CNN model even performs better than the unpruned one.• Secondly, a novel fast recursive algorithm (FRA)-based network pruning approach is proposed to select the neurons one by one that contribute most to the model output and re-assign the weight for each selected neuron, while redundant neurons are removed. This lead to a compact CNN with much fewer neurons and parameters, while the accuracy loss due to nodes pruning is negligible, and the pruned CNN model even performs better than the unpruned one.</p>
        <p>• Finally, the CNN model inputs are generated by a signal-to-image transformation and data segmentation method proposed in this paper, which is an enabling stage to fit the CNN model for time series signals and to estimate the battery capacity using only partial charging segment.• Finally, the CNN model inputs are generated by a signal-to-image transformation and data segmentation method proposed in this paper, which is an enabling stage to fit the CNN model for time series signals and to estimate the battery capacity using only partial charging segment.</p>
        <p>With these features, once the model parameters are updated offline using the small historical dataset collected from the target battery, the resultant model can then be used to estimate the unknown capacity of a battery using online measured variables during a partial charging cycle.With these features, once the model parameters are updated offline using the small historical dataset collected from the target battery, the resultant model can then be used to estimate the unknown capacity of a battery using online measured variables during a partial charging cycle.</p>
        <p>The remainder of this paper is organized as follows. In Section 2, a brief introduction of the transfer learning, network pruning and FRA algorithm are presented. Section 3 details the proposed battery capacity estimation method, including the signal-to-image transformation method enabling the application of CNNs for time series signals, and construction of the CNN model with knowledge transfer and redundant neurons pruning. Section 4 introduces the experimental data and implementation details. Section 5 presents and discusses the experimental results. Finally, Section 6 concludes the paper.The remainder of this paper is organized as follows. In Section 2, a brief introduction of the transfer learning, network pruning and FRA algorithm are presented. Section 3 details the proposed battery capacity estimation method, including the signal-to-image transformation method enabling the application of CNNs for time series signals, and construction of the CNN model with knowledge transfer and redundant neurons pruning. Section 4 introduces the experimental data and implementation details. Section 5 presents and discusses the experimental results. Finally, Section 6 concludes the paper.</p>
        <p>To improve the readability, this section presents the concepts of transfer learning, network pruning and fast recursive algorithm, which are the key elements of the proposed battery capacity estimation method.To improve the readability, this section presents the concepts of transfer learning, network pruning and fast recursive algorithm, which are the key elements of the proposed battery capacity estimation method.</p>
        <p>Most machine learning technologies are developed under the assumption that the data used for training and testing should be drawn from the same feature space and have the same distribution [33]. In other words, the statistical model trained on a dataset can not be directly applied to another dataset with different distribution, and the model has to be reconstructed and retrained from scratch on the new dataset. However, collecting enough data to retrain a new model is often time-consuming and costly in real-world applications, and on the other hand, the training process often takes a long time. Transfer learning was proposed to handle such cases, aiming to reduce the need and effort for data recollection and model re-training while leveraging the knowledge learned from a source task to a different but related task though the data of these two tasks may have different distributions [34]. Based on the transfer learning, the latter task requires much less data to retrain the model.Most machine learning technologies are developed under the assumption that the data used for training and testing should be drawn from the same feature space and have the same distribution [33]. In other words, the statistical model trained on a dataset can not be directly applied to another dataset with different distribution, and the model has to be reconstructed and retrained from scratch on the new dataset. However, collecting enough data to retrain a new model is often time-consuming and costly in real-world applications, and on the other hand, the training process often takes a long time. Transfer learning was proposed to handle such cases, aiming to reduce the need and effort for data recollection and model re-training while leveraging the knowledge learned from a source task to a different but related task though the data of these two tasks may have different distributions [34]. Based on the transfer learning, the latter task requires much less data to retrain the model.</p>
        <p>Generally speaking, the transfer learning can be achieved in two ways for neural network models:Generally speaking, the transfer learning can be achieved in two ways for neural network models:</p>
        <p>one is to utilize the original pre-trained network except for its last fully-connected (FC) layer as a fixed feature extractor for the new task, and the other approach is to fine-tune the parameters of the pre-trained network using data of the new task. Choosing the suitable transfer learning technique mainly depends on the size and similarity of the target dataset to the original one used in pre-training [34]. It is widely accepted that the first several layers of a network are used to extract low-level features while the following few layers are used to extract high-level features [35], the FC layers are used to learn non-linear combinations of these features. Therefore, the first transfer learning technique is suitable for cases when the target dataset is quite similar to the source dataset. On the other hand, fine-tuning the pre-trained network is more suitable for a target dataset which has different distribution statistics from the original dataset.one is to utilize the original pre-trained network except for its last fully-connected (FC) layer as a fixed feature extractor for the new task, and the other approach is to fine-tune the parameters of the pre-trained network using data of the new task. Choosing the suitable transfer learning technique mainly depends on the size and similarity of the target dataset to the original one used in pre-training [34]. It is widely accepted that the first several layers of a network are used to extract low-level features while the following few layers are used to extract high-level features [35], the FC layers are used to learn non-linear combinations of these features. Therefore, the first transfer learning technique is suitable for cases when the target dataset is quite similar to the source dataset. On the other hand, fine-tuning the pre-trained network is more suitable for a target dataset which has different distribution statistics from the original dataset.</p>
        <p>When the CNN is applied for battery capacity estimation, it is obvious that the model trained on one battery degradation dataset can not be directly applied to another type of batteries with different specifications and charging/discharging policies. When the battery specification changes or the operation condition varies significantly, the CNN model needs to be retrained or even rebuilt from scratch on the new battery degradation data. To this end, this paper combines the CNN with transfer learning, aiming to eliminate the need to recollect a complete battery cycling data and build a completely new CNN model.When the CNN is applied for battery capacity estimation, it is obvious that the model trained on one battery degradation dataset can not be directly applied to another type of batteries with different specifications and charging/discharging policies. When the battery specification changes or the operation condition varies significantly, the CNN model needs to be retrained or even rebuilt from scratch on the new battery degradation data. To this end, this paper combines the CNN with transfer learning, aiming to eliminate the need to recollect a complete battery cycling data and build a completely new CNN model.</p>
        <p>When CNNs are used for battery capacity estimation, the deep and complex network structure and a large number of parameters to tune demand huge memory storage and high computational cost, which hinder its implementation in embedded devices [36]. To acquire a smaller and much more compact model, network pruning, which can remove redundant network connections based on a predefined criteria, has proven to be an effective approach. After pruning, the model will have much fewer parameters with little or no impact on performance, while the computational complexity and storage space can be significantly reduced. According to the granularity level, the pruning can be applied at four levels, namely weight-level, filter-level, channel-level and layer-level [37]. Though a number of methods have been proposed so far for network pruning at different granularity level, they are all based on the similar framework, that is, to evaluate the importance of each weight/ kernel/ channel/ layer and remove those insignificant ones. The core of these methods is to choose a suitable importance indicator that determines which element should be removed. Different criteria have been proposed in the literature, for example, the absolute values of weights were directly used as the importance indicator in [38], the second order derivatives of a layer-wise loss with respect to the corresponding weights were used to identified the importance of each weight in [39], while [40] calculated the percentage of zero activation of a neuron after the ReLU mapping to determine which neuron should be removed, based on the assumptions that neurons with higher percentage values provide less power to the results.When CNNs are used for battery capacity estimation, the deep and complex network structure and a large number of parameters to tune demand huge memory storage and high computational cost, which hinder its implementation in embedded devices [36]. To acquire a smaller and much more compact model, network pruning, which can remove redundant network connections based on a predefined criteria, has proven to be an effective approach. After pruning, the model will have much fewer parameters with little or no impact on performance, while the computational complexity and storage space can be significantly reduced. According to the granularity level, the pruning can be applied at four levels, namely weight-level, filter-level, channel-level and layer-level [37]. Though a number of methods have been proposed so far for network pruning at different granularity level, they are all based on the similar framework, that is, to evaluate the importance of each weight/ kernel/ channel/ layer and remove those insignificant ones. The core of these methods is to choose a suitable importance indicator that determines which element should be removed. Different criteria have been proposed in the literature, for example, the absolute values of weights were directly used as the importance indicator in [38], the second order derivatives of a layer-wise loss with respect to the corresponding weights were used to identified the importance of each weight in [39], while [40] calculated the percentage of zero activation of a neuron after the ReLU mapping to determine which neuron should be removed, based on the assumptions that neurons with higher percentage values provide less power to the results.</p>
        <p>Considering that the CNN structure used in capacity estimation often has less number of filters and layers, weight-level and channel-level pruning are more suitable. Further, neuron pruning simplifies the network structure and also reduces the number of weights, therefore it is often more effective than the weight pruning approach only [41]. As a consequence, a channel-level pruning method is proposed in this paper to remove unimportant neurons, where each neuron is viewed as one channel. Considering that the FC layers are less sensitive to pruning than the convolutional layers [38], and the FC layers have the most number of parameters, we choose the FC layers as the pruning object. In this way, more parameters can be pruned with minimal impact on the output. The final compressed model will use much less memory and require less calculations, which makes is possible to implement the resultant model in the on-board BMS.Considering that the CNN structure used in capacity estimation often has less number of filters and layers, weight-level and channel-level pruning are more suitable. Further, neuron pruning simplifies the network structure and also reduces the number of weights, therefore it is often more effective than the weight pruning approach only [41]. As a consequence, a channel-level pruning method is proposed in this paper to remove unimportant neurons, where each neuron is viewed as one channel. Considering that the FC layers are less sensitive to pruning than the convolutional layers [38], and the FC layers have the most number of parameters, we choose the FC layers as the pruning object. In this way, more parameters can be pruned with minimal impact on the output. The final compressed model will use much less memory and require less calculations, which makes is possible to implement the resultant model in the on-board BMS.</p>
        <p>The detailed pruning method will be elaborated in the following sections.The detailed pruning method will be elaborated in the following sections.</p>
        <p>In the field of system identification, the linear-in-the-parameter model is a popular model structure for approximating a large class of nonlinear systems. These models linearly combine a set of model terms that are nonlinear functions of the system variables. Such models often have an excessive number of candidate terms which may cause overfitting and high computational complexity, therefore, model selection algorithms have been proposed to generate parsimonious models with a much smaller number of terms. In particular, a fast recursive algorithm (FRA) [42] was proposed to simultaneously select most significant model terms and estimate model parameters.In the field of system identification, the linear-in-the-parameter model is a popular model structure for approximating a large class of nonlinear systems. These models linearly combine a set of model terms that are nonlinear functions of the system variables. Such models often have an excessive number of candidate terms which may cause overfitting and high computational complexity, therefore, model selection algorithms have been proposed to generate parsimonious models with a much smaller number of terms. In particular, a fast recursive algorithm (FRA) [42] was proposed to simultaneously select most significant model terms and estimate model parameters.</p>
        <p>Consider a nonlinear discrete-time dynamic system represented by a linear-in-the-parameters model, which is identified by N data samples {x(i),Consider a nonlinear discrete-time dynamic system represented by a linear-in-the-parameters model, which is identified by N data samples {x(i),</p>
        <p>wherewhere</p>
        <p>is the regression matrix that contains all candidate model terms, each termis the regression matrix that contains all candidate model terms, each term</p>
        <p>where Ψ k ∈ ℜ N ×k contains the first k columns of the full regression matrix Ψ, additionally, k = 1, ..., S, and R 0 = I. The distinguished properties of R k are given in Appendix A.where Ψ k ∈ ℜ N ×k contains the first k columns of the full regression matrix Ψ, additionally, k = 1, ..., S, and R 0 = I. The distinguished properties of R k are given in Appendix A.</p>
        <p>Thus, when the first k columns in Ψ are select, the estimation of parameters that minimizes the cost function and the associated minimal cost function can be formulated asThus, when the first k columns in Ψ are select, the estimation of parameters that minimizes the cost function and the associated minimal cost function can be formulated as</p>
        <p>To simplify the formulas and decrease the computational complexity, three quantities are consequently defined asTo simplify the formulas and decrease the computational complexity, three quantities are consequently defined as</p>
        <p>where j = 1, ..., S, and k = 0, 1, ..., S. According to the properties of R k , the net contribution of a new model term ϕ k+1 to the cost function can be explicitly calculated aswhere j = 1, ..., S, and k = 0, 1, ..., S. According to the properties of R k , the net contribution of a new model term ϕ k+1 to the cost function can be explicitly calculated as</p>
        <p>By calculating the net contribution of each term, the model terms with maximum contributions will be selected one by one. Finally, after all important model terms have been selected, the parameter for each selected term is calculated asBy calculating the net contribution of each term, the model terms with maximum contributions will be selected one by one. Finally, after all important model terms have been selected, the parameter for each selected term is calculated as</p>
        <p>Note: the summation term in Equation ( 8) is zero for j = k, i.e. k i=k+1 θi a k,i = 0. Equations ( 7) and ( 8) constitute the main steps of the FRA, which selects model terms one by one based on (7) and calculates the model parameters for the resultant model based on (8).Note: the summation term in Equation ( 8) is zero for j = k, i.e. k i=k+1 θi a k,i = 0. Equations ( 7) and ( 8) constitute the main steps of the FRA, which selects model terms one by one based on (7) and calculates the model parameters for the resultant model based on (8).</p>
        <p>In this section, the proposed battery capacity estimation method is described in detail. First, the input generation method including data normalization, segmentation and time series-to-image conversion is introduced, and the target output of the model is clarified. Then the construction of the proposed CNN model is described step by step, which incorporates both the transfer learning and network pruning.In this section, the proposed battery capacity estimation method is described in detail. First, the input generation method including data normalization, segmentation and time series-to-image conversion is introduced, and the target output of the model is clarified. Then the construction of the proposed CNN model is described step by step, which incorporates both the transfer learning and network pruning.</p>
        <p>The input variables used for battery capacity estimation in this paper are current, terminal voltage and charge capacity, where the charge capacity is calculated by integrating the current with respect to time for a partial charging segment. These variables are highly correlated, embedding lots of information. To make the most of the embedded information in these measurements and to automatically extract features, a CNN model is built and a time series-to-image conversion method is proposed to convert the measured time series signals to 3 dimensional images, as the CNN usually takes images as the input. Given that there is little chance to fully charge/discharge a battery from a fixed initial state in practical applications [21], partial charging curves based capacity estimation methods are more practical. Therefore, data segmentation is performed before the conversion stage to generate partial charging curves with flexible start/end point.The input variables used for battery capacity estimation in this paper are current, terminal voltage and charge capacity, where the charge capacity is calculated by integrating the current with respect to time for a partial charging segment. These variables are highly correlated, embedding lots of information. To make the most of the embedded information in these measurements and to automatically extract features, a CNN model is built and a time series-to-image conversion method is proposed to convert the measured time series signals to 3 dimensional images, as the CNN usually takes images as the input. Given that there is little chance to fully charge/discharge a battery from a fixed initial state in practical applications [21], partial charging curves based capacity estimation methods are more practical. Therefore, data segmentation is performed before the conversion stage to generate partial charging curves with flexible start/end point.</p>
        <p>Figure 1 illustrates the steps of the proposed input generation method. Note that the raw data, i.e. current, voltage and charge capacity, have different scales, therefore directly converting these variables to 3D inputs will slow the training process and significantly degrade the model performance. Thus, data normalization is applied first. Then M data chunks are segmented from one charge cycle, each data chunk is a n × 3 matrix that represents a partial charging segment, where 3 is the number of involved variables, and n denotes n continuous data points for each variable.Figure 1 illustrates the steps of the proposed input generation method. Note that the raw data, i.e. current, voltage and charge capacity, have different scales, therefore directly converting these variables to 3D inputs will slow the training process and significantly degrade the model performance. Thus, data normalization is applied first. Then M data chunks are segmented from one charge cycle, each data chunk is a n × 3 matrix that represents a partial charging segment, where 3 is the number of involved variables, and n denotes n continuous data points for each variable.</p>
        <p>Two adjacent data chunks have c overlapping data points. Finally, each partial charging segment is converted into a 3 dimensional image with the size ofTwo adjacent data chunks have c overlapping data points. Finally, each partial charging segment is converted into a 3 dimensional image with the size of</p>
        <p>This input generation method can increase the number of samples used for CNN training, and allow fast online capacity estimation only using flexible partial charging curves, thus it is an enabling block in order to apply the CNNs for time series signals. Each input sample is associated with an output sample, which is the full discharge capacity that is calculated by integrating the discharge current over time for the entire full discharge cycle which immediately follows the charge cycle that generated the input sample.This input generation method can increase the number of samples used for CNN training, and allow fast online capacity estimation only using flexible partial charging curves, thus it is an enabling block in order to apply the CNNs for time series signals. Each input sample is associated with an output sample, which is the full discharge capacity that is calculated by integrating the discharge current over time for the entire full discharge cycle which immediately follows the charge cycle that generated the input sample.</p>
        <p>The designed CNN architecture is illustrated in Figure 3 and Table 1, and it has 4 convolutional layers, 2 max pooling layers, a flatten layer and 2 fully-connected layers. As aforementioned, the inputs are For each convolutional layer, N @k × m × d denotes the filter design, which implies that N filtersThe designed CNN architecture is illustrated in Figure 3 and Table 1, and it has 4 convolutional layers, 2 max pooling layers, a flatten layer and 2 fully-connected layers. As aforementioned, the inputs are For each convolutional layer, N @k × m × d denotes the filter design, which implies that N filters</p>
        <p>The source dataset was split into three sets: training, validation and testing. The CNN model is trained from scratch using the training and validation sets, and then tested on the testing set.The source dataset was split into three sets: training, validation and testing. The CNN model is trained from scratch using the training and validation sets, and then tested on the testing set.</p>
        <p>All the 12693 parameters are trained and then used in the following steps.All the 12693 parameters are trained and then used in the following steps.</p>
        <p>In real-life applications, it is not a trivial task to collect and annotate battery long-term cycling data. While the CNN architectures used for battery capacity estimation usually contain tens of thousands of free parameters to train, requiring a large number of labeled training data and long training time. To circumvent this problem for practical applications, transfer learning is adopted to transfer the knowledge learned from the source dataset to a new task with a much smaller dataset. In this paper, transfer learning is achieved through the following two steps (Fig. 2): first, the structure and parameters of the CNN model pre-trained on the source dataset are transferred to the target dataset, then specific layers are fine-tuned using the target dataset. The resultant CNN model after the transfer learning is denoted as CNN(S)-TL in the remainder of the paper.In real-life applications, it is not a trivial task to collect and annotate battery long-term cycling data. While the CNN architectures used for battery capacity estimation usually contain tens of thousands of free parameters to train, requiring a large number of labeled training data and long training time. To circumvent this problem for practical applications, transfer learning is adopted to transfer the knowledge learned from the source dataset to a new task with a much smaller dataset. In this paper, transfer learning is achieved through the following two steps (Fig. 2): first, the structure and parameters of the CNN model pre-trained on the source dataset are transferred to the target dataset, then specific layers are fine-tuned using the target dataset. The resultant CNN model after the transfer learning is denoted as CNN(S)-TL in the remainder of the paper.</p>
        <p>As aforementioned, the CNN(S)-TL model used for capacity estimation fine-tuned using the target dataset has 4 convolutional layers, 2 max pooling layers, a flatten layer and 2 fullyconnected layers with a total of 12693 parameters. In real-world applications, such a CNN model will require large computation and memory resources, which limits its implementation in onboard BMS. An intuitive way to solve this problem is to reduce the model complexity and the number of parameters. Inspired by the work presented in [43] that selects the best hidden neurons and avoids redundant structure, a FRA-based network pruning method is proposed in this paper to remove all redundant neurons based on measuring the contributions of all neurons to the outputs. Compared with other pruning methods, the proposed method identifies and preserves the most important neurons of networks and re-assign weights to the retained neurons. In other words, the important features are retained, which may help to improve the accuracy of capacity estimation for the CNN model refined on a rather small task dataset. Further, the ability of the proposed method to simultaneously update weights and select neurons implies that there is no need to fine-tune the network after pruning. As shown in Table 1, more than half of the total parameters of the CNN model are from the last two fully-connected layers, the proposed method is therefore deployed to these two layers. The redundant neurons and all the incoming and outgoing connections associated with these neurons are removed, leading to significantly reduced memory usage and computational complexity for online capacity estimation.As aforementioned, the CNN(S)-TL model used for capacity estimation fine-tuned using the target dataset has 4 convolutional layers, 2 max pooling layers, a flatten layer and 2 fullyconnected layers with a total of 12693 parameters. In real-world applications, such a CNN model will require large computation and memory resources, which limits its implementation in onboard BMS. An intuitive way to solve this problem is to reduce the model complexity and the number of parameters. Inspired by the work presented in [43] that selects the best hidden neurons and avoids redundant structure, a FRA-based network pruning method is proposed in this paper to remove all redundant neurons based on measuring the contributions of all neurons to the outputs. Compared with other pruning methods, the proposed method identifies and preserves the most important neurons of networks and re-assign weights to the retained neurons. In other words, the important features are retained, which may help to improve the accuracy of capacity estimation for the CNN model refined on a rather small task dataset. Further, the ability of the proposed method to simultaneously update weights and select neurons implies that there is no need to fine-tune the network after pruning. As shown in Table 1, more than half of the total parameters of the CNN model are from the last two fully-connected layers, the proposed method is therefore deployed to these two layers. The redundant neurons and all the incoming and outgoing connections associated with these neurons are removed, leading to significantly reduced memory usage and computational complexity for online capacity estimation.</p>
        <p>Supposing X ∈ ℜ N ×(L+1) is the input matrix to the first fully-connected (FC1) layer, where N is the number of samples and L is the length of the vector formed in the flatten layer and inputted to the FC1 layer, and the input matrix X includes a bias vector with all elements being set to 1. Y ∈ ℜ N ×1 is the output of the second fully-connected (FC2) layer, i.e. the output of the CNN(S)-TL model, which can be written asSupposing X ∈ ℜ N ×(L+1) is the input matrix to the first fully-connected (FC1) layer, where N is the number of samples and L is the length of the vector formed in the flatten layer and inputted to the FC1 layer, and the input matrix X includes a bias vector with all elements being set to 1. Y ∈ ℜ N ×1 is the output of the second fully-connected (FC2) layer, i.e. the output of the CNN(S)-TL model, which can be written as</p>
        <p>where Θ ∈ ℜ (L+1)×S1 represents the parameter matrix of the FC1 layer, which consists of the L × S 1 weight matrix and 1 × S 1 bias vector. S 1 is the number of neurons inputted to the FC2 layer, namely, the number of latent features obtained by the CNN-TL model to estimate capacity. W denotes the parameter matrix of weights and bias of the FC2 layer, and Ξ is the residual vector.where Θ ∈ ℜ (L+1)×S1 represents the parameter matrix of the FC1 layer, which consists of the L × S 1 weight matrix and 1 × S 1 bias vector. S 1 is the number of neurons inputted to the FC2 layer, namely, the number of latent features obtained by the CNN-TL model to estimate capacity. W denotes the parameter matrix of weights and bias of the FC2 layer, and Ξ is the residual vector.</p>
        <p>To remove redundant neurons in FC1 and FC2 layers without sacrificing the overall performance of the network, the detailed procedure is illustrated in Figure 4, and the neuron selection process is elaborated as follows.To remove redundant neurons in FC1 and FC2 layers without sacrificing the overall performance of the network, the detailed procedure is illustrated in Figure 4, and the neuron selection process is elaborated as follows.</p>
        <p>Take the FC1 as an example, the pruning procedure consists of 3 steps:Take the FC1 as an example, the pruning procedure consists of 3 steps:</p>
        <p>Step 1 -Calculate the contribution of each neuron in the FC1 layer. LetStep 1 -Calculate the contribution of each neuron in the FC1 layer. Let</p>
        <p>is the output of the FC1 layer. According to Equation ( 7), the net contribution of each neuron in this layer can be calculated asis the output of the FC1 layer. According to Equation ( 7), the net contribution of each neuron in this layer can be calculated as</p>
        <p>Step 2 -Rank the neurons based on their contributions, and then select the highest ranked neuron. With the selected neurons, a new output of the FC1 layer is obtained, denoted by Ŷf .Step 2 -Rank the neurons based on their contributions, and then select the highest ranked neuron. With the selected neurons, a new output of the FC1 layer is obtained, denoted by Ŷf .</p>
        <p>The root mean square error between Y f and Ŷf over all the training samples is given asThe root mean square error between Y f and Ŷf over all the training samples is given as</p>
        <p>where • F is the Euclidean norm.where • F is the Euclidean norm.</p>
        <p>Step 3 -Repeat Step 2 until the maximum number of neurons in the FC1 layer is reached or the rmse f is smaller than the predefined error bound. The parameters for the selected terms are calculated by θj = xStep 3 -Repeat Step 2 until the maximum number of neurons in the FC1 layer is reached or the rmse f is smaller than the predefined error bound. The parameters for the selected terms are calculated by θj = x</p>
        <p>where j = k, k -1, ..., 1, and k is the total number of the selected terms.where j = k, k -1, ..., 1, and k is the total number of the selected terms.</p>
        <p>The neuron pruning process for FC2 is the same as for the FC1 layer. Repeat step 1-3, and until the equivalent or even better result with fewer neurons is achieved.The neuron pruning process for FC2 is the same as for the FC1 layer. Repeat step 1-3, and until the equivalent or even better result with fewer neurons is achieved.</p>
        <p>The proposed method using FRA for neuron pruning can simultaneously reduce the model size and re-tune the weights, which eliminates the need to fine-tune the network after neuron pruning, and reduces the number of computing operations without sacrificing the accuracy. After pruning, the new network is more compact than the original CNN(S)-TL model in terms of the model size, and hence the computational complexity is significantly reduced. The resultant model is denoted as pruned CNN(S)-TL (PCNN(S)-TL) in the rest of the paper.The proposed method using FRA for neuron pruning can simultaneously reduce the model size and re-tune the weights, which eliminates the need to fine-tune the network after neuron pruning, and reduces the number of computing operations without sacrificing the accuracy. After pruning, the new network is more compact than the original CNN(S)-TL model in terms of the model size, and hence the computational complexity is significantly reduced. The resultant model is denoted as pruned CNN(S)-TL (PCNN(S)-TL) in the rest of the paper.</p>
        <p>In the following, the source dataset used to pre-train the CNN model and the target dataset The detailed information of the source and target battery degradation datasets are summarized in Table 2. These include one large-scale test dataset for 16 cells and one small-scale dataset for 4 cells, and each cell in the first dataset has roughly gone through 1000 charging/discharging cycles, while cells in the small dataset were only tested for 30 reference cycles. Therefore, the large-scale dataset is employed as the source dataset to pre-train the CNN model, and then the learned knowledge will be transferred to the small-scale dataset. Note that these two sets of cells are all the same type, but have different specifications and are tested under different cycling scenarios.In the following, the source dataset used to pre-train the CNN model and the target dataset The detailed information of the source and target battery degradation datasets are summarized in Table 2. These include one large-scale test dataset for 16 cells and one small-scale dataset for 4 cells, and each cell in the first dataset has roughly gone through 1000 charging/discharging cycles, while cells in the small dataset were only tested for 30 reference cycles. Therefore, the large-scale dataset is employed as the source dataset to pre-train the CNN model, and then the learned knowledge will be transferred to the small-scale dataset. Note that these two sets of cells are all the same type, but have different specifications and are tested under different cycling scenarios.</p>
        <p>The source dataset is collected from 124 commercial lithium iron phosphate (LFP)/graphite batteries that are manufactured by A123 System (APR18650M1A), with a nominal voltage of 3.3 V and nominal capacity of 1.1 Ah [44]. They are cycled to failure under fast-charging policy at a constant temperature of 30 • C. Under the fast-charging policy, denoted as "C1(Q1)-C2", Note that the charge capacity is calculated by integrating the current with respect to time for a partial charging segment.The source dataset is collected from 124 commercial lithium iron phosphate (LFP)/graphite batteries that are manufactured by A123 System (APR18650M1A), with a nominal voltage of 3.3 V and nominal capacity of 1.1 Ah [44]. They are cycled to failure under fast-charging policy at a constant temperature of 30 • C. Under the fast-charging policy, denoted as "C1(Q1)-C2", Note that the charge capacity is calculated by integrating the current with respect to time for a partial charging segment.</p>
        <p>Important hyperparameters are pre-defined before training and fine-tuning the model to maximize the performance of the model. When the CNN model is trained from scratch, all the kernels are initialized based on Xavier uniform initializer and the bias are initialized to zero [45].Important hyperparameters are pre-defined before training and fine-tuning the model to maximize the performance of the model. When the CNN model is trained from scratch, all the kernels are initialized based on Xavier uniform initializer and the bias are initialized to zero [45].</p>
        <p>The maximum training epochs are set to 80 with the mini-batch size of 128 samples. To avoid overfitting problem, early stopping method with patience set to 5 is applied to stop training once the model performance has not improved for 5 consecutive epochs on the validation dataset. The Adam algorithm with learning rate of 0.001 is used to update the parameters.The maximum training epochs are set to 80 with the mini-batch size of 128 samples. To avoid overfitting problem, early stopping method with patience set to 5 is applied to stop training once the model performance has not improved for 5 consecutive epochs on the validation dataset. The Adam algorithm with learning rate of 0.001 is used to update the parameters.</p>
        <p>The CNN(S)-TL model is fine-tuned from the pre-trained CNN(S) model on the target dataset.The CNN(S)-TL model is fine-tuned from the pre-trained CNN(S) model on the target dataset.</p>
        <p>First, all layers of the pre-trained CNN(S) are copied to the CNN(S)-TL model. Considering that the target dataset is much smaller than the source dataset, and the features extracted from the first few layers are universal, the parameters of layers prior to the third convolutional layers are retained unchanged. Then the subsequent layers are fine-tuned to learn the specific features on target dataset with a learning rate of 0.0001, which is ten times smaller than the one used in the scratch training.First, all layers of the pre-trained CNN(S) are copied to the CNN(S)-TL model. Considering that the target dataset is much smaller than the source dataset, and the features extracted from the first few layers are universal, the parameters of layers prior to the third convolutional layers are retained unchanged. Then the subsequent layers are fine-tuned to learn the specific features on target dataset with a learning rate of 0.0001, which is ten times smaller than the one used in the scratch training.</p>
        <p>In the pruning stage, all the layers copied from the CNN(S)-TL model are fixed except for the last two fully-connected layers. When the desired performance of the model is achieved with the minimum number of neurons using the proposed FRA-based network pruning method, it eliminates the necessity of fine-tuning the network after removing redundant neurons, due to that new weights have already been assigned to remaining neurons using the proposed algorithm.In the pruning stage, all the layers copied from the CNN(S)-TL model are fixed except for the last two fully-connected layers. When the desired performance of the model is achieved with the minimum number of neurons using the proposed FRA-based network pruning method, it eliminates the necessity of fine-tuning the network after removing redundant neurons, due to that new weights have already been assigned to remaining neurons using the proposed algorithm.</p>
        <p>In the experiment, the total mean absolute error (MAE), root mean-square error (RMSE)In the experiment, the total mean absolute error (MAE), root mean-square error (RMSE)</p>
        <p>and normalized estimation error (NEE) are used to assess the model performance. They are effective measures to assess the deviations in distances between the estimated capacities Ŷ and the reference values Y. The formulas are defined as follows:and normalized estimation error (NEE) are used to assess the model performance. They are effective measures to assess the deviations in distances between the estimated capacities Ŷ and the reference values Y. The formulas are defined as follows:</p>
        <p>where N is the sample size of test data, Q is the nominal capacity. T ∈ ℜ N ×1 , y i is the reference capacity for the i th sample, while ŷi is the corresponding estimated value.where N is the sample size of test data, Q is the nominal capacity. T ∈ ℜ N ×1 , y i is the reference capacity for the i th sample, while ŷi is the corresponding estimated value.</p>
        <p>In addition, the floating point operations (FLOPs) are considered to measure the computational cost in this study. As the pruning algorithm is applied to the fully-connected layers in this study, only the number of FLOPs for a fully-connected layer is considered, which is calculated asIn addition, the floating point operations (FLOPs) are considered to measure the computational cost in this study. As the pruning algorithm is applied to the fully-connected layers in this study, only the number of FLOPs for a fully-connected layer is considered, which is calculated as</p>
        <p>, where I and O are the number of input and output neurons, respectively., where I and O are the number of input and output neurons, respectively.</p>
        <p>In this section, the datasets introduced in section 4 are used to validate the performance of the proposed method. The capacity estimation results of the proposed method on different cells are given. The performance of the proposed model is compared with the CNN model, CNN with network pruning, and CNN with transfer learning. The performance improvements of the proposed framework in terms of accuracy and model size are analysed. Moreover, the effect of the number of layers fine-tuned during the transfer learning stage is investigated to explain the reasons why we choose to fine-tune 4 layers.In this section, the datasets introduced in section 4 are used to validate the performance of the proposed method. The capacity estimation results of the proposed method on different cells are given. The performance of the proposed model is compared with the CNN model, CNN with network pruning, and CNN with transfer learning. The performance improvements of the proposed framework in terms of accuracy and model size are analysed. Moreover, the effect of the number of layers fine-tuned during the transfer learning stage is investigated to explain the reasons why we choose to fine-tune 4 layers.</p>
        <p>The capacity estimation results of the proposed PCNN(S)-TL model on target dataset are shown in Figure 8 and Figure 9. It can be observed from Figure 8 that the proposed model can produce accurate estimation on the test cells, and the estimated capacity can well track the degradation trend. Here the four-fold cross validation approach is used, when one cell is used for testing, data of the other three cells are used to generate the training samples according to the input generation method introduced in Section 3.1 to fine-tune and prune the pre-trained CNN model. The experimental results have revealed that the parameters of the developed model can be updated using the data from one cell, and then the resultant model can be directly applied to other cells who have the same specifications and similar charging policies. Figure 9 illustrates that updating the proposed model parameters using the data of the first several cycles, the degradation trend for the following cycles can be well captured. Here data of the first 2/3 cycles of all cells in the target dataset are used to fine-tune and prune the model pre-trained on the source dataset, and the remaining 1/3 cycles are used for testing. Both figures demonstrate the performance of the PCNN(S)-TL model for battery capacity estimation.The capacity estimation results of the proposed PCNN(S)-TL model on target dataset are shown in Figure 8 and Figure 9. It can be observed from Figure 8 that the proposed model can produce accurate estimation on the test cells, and the estimated capacity can well track the degradation trend. Here the four-fold cross validation approach is used, when one cell is used for testing, data of the other three cells are used to generate the training samples according to the input generation method introduced in Section 3.1 to fine-tune and prune the pre-trained CNN model. The experimental results have revealed that the parameters of the developed model can be updated using the data from one cell, and then the resultant model can be directly applied to other cells who have the same specifications and similar charging policies. Figure 9 illustrates that updating the proposed model parameters using the data of the first several cycles, the degradation trend for the following cycles can be well captured. Here data of the first 2/3 cycles of all cells in the target dataset are used to fine-tune and prune the model pre-trained on the source dataset, and the remaining 1/3 cycles are used for testing. Both figures demonstrate the performance of the PCNN(S)-TL model for battery capacity estimation.</p>
        <p>The proposed battery capacity estimation method has been further verified in this section.The proposed battery capacity estimation method has been further verified in this section.</p>
        <p>The CNN model directly trained using the target dataset is denoted as CNN(T), while CNN(S) refers to the model trained using the source dataset. The models acquired using network pruning, transfer learning and both of them are denoted as PCNN(T), CNN(S)-TL and PCNN(S)-TL, 5. The results presented in these tables confirm the effectiveness of the proposed capacity estimation methods. Since the CNN(S)-TL model was trained on a larger dataset, the specific features extracted in the fully-connected layers for the capacity estimation are expected to be much richer than that of the CNN(T) model. That is to say, the CNN(S)-TL model requires to retain more neurons to capture these features during the network pruning process. Consequently, the PCNN(S)-TL model will be slightly bigger than the PCNN(T) model.The CNN model directly trained using the target dataset is denoted as CNN(T), while CNN(S) refers to the model trained using the source dataset. The models acquired using network pruning, transfer learning and both of them are denoted as PCNN(T), CNN(S)-TL and PCNN(S)-TL, 5. The results presented in these tables confirm the effectiveness of the proposed capacity estimation methods. Since the CNN(S)-TL model was trained on a larger dataset, the specific features extracted in the fully-connected layers for the capacity estimation are expected to be much richer than that of the CNN(T) model. That is to say, the CNN(S)-TL model requires to retain more neurons to capture these features during the network pruning process. Consequently, the PCNN(S)-TL model will be slightly bigger than the PCNN(T) model.</p>
        <p>In summary, the CNN model with transfer learning and network pruning techniques can achieve more accurate estimation results with much less neurons than a model trained on the target dataset from scratch.In summary, the CNN model with transfer learning and network pruning techniques can achieve more accurate estimation results with much less neurons than a model trained on the target dataset from scratch.</p>
        <p>To investigate the influence of the number of layers fine-tuned during the transfer learning stage, the identical training and testing datasets were utilized for all tests. In each test, the entire structure and parameters of the CNN(S) model were copied first, and the last n layers were finetuned on the target sets while the first 6n layers were fixed. It should be noted that, the whole CNN has 9 layers in total, the two max pooling layers added after the first two convolutional layers and the flatten layer after the last convolutional layer do not have parameters, therefore, maximally six layers can be fine-tuned. Thus, n refers to the number of tunable layers, and was selected from 1 to 6, where 1 implies that all layers except for the last FC layer are fixed, and 6 implies that the entire network needs to be fine-tuned. A complete fine-tuning and testing procedure was executed 100 times for each selected n, and the averaged max error (MaxE), MAE, RMSE, NEE of 100 runs are summarized in Table 6, where FC stands for the fully-connected layer, 2FC refers to the last two fully-connected layers, Conv represents the convolutional layer, and the number prefixing to Conv stands for the number of convolutional layers involved in fine-tuning from back to front.To investigate the influence of the number of layers fine-tuned during the transfer learning stage, the identical training and testing datasets were utilized for all tests. In each test, the entire structure and parameters of the CNN(S) model were copied first, and the last n layers were finetuned on the target sets while the first 6n layers were fixed. It should be noted that, the whole CNN has 9 layers in total, the two max pooling layers added after the first two convolutional layers and the flatten layer after the last convolutional layer do not have parameters, therefore, maximally six layers can be fine-tuned. Thus, n refers to the number of tunable layers, and was selected from 1 to 6, where 1 implies that all layers except for the last FC layer are fixed, and 6 implies that the entire network needs to be fine-tuned. A complete fine-tuning and testing procedure was executed 100 times for each selected n, and the averaged max error (MaxE), MAE, RMSE, NEE of 100 runs are summarized in Table 6, where FC stands for the fully-connected layer, 2FC refers to the last two fully-connected layers, Conv represents the convolutional layer, and the number prefixing to Conv stands for the number of convolutional layers involved in fine-tuning from back to front.</p>
        <p>As shown in Table 6, the estimation error decreases as the number of fine-tuned layers increases.As shown in Table 6, the estimation error decreases as the number of fine-tuned layers increases.</p>
        <p>When only fully-connected layers are fine-tuned, the NEEs are all greater than 1.3%, while if one or more convolutional layers are fine-tuned, the NEE are decreased by at least 30.88% (decreasing from1.36% to 0.94%). This improvement implies that for batteries with different specifications and subject to different charge/discharge policies, they have different high-level specific features projected at the last few convolution layers. The specific high-level features learned from the source dataset can not precisely describe the target dataset, thus they need to be learned using the target dataset.When only fully-connected layers are fine-tuned, the NEEs are all greater than 1.3%, while if one or more convolutional layers are fine-tuned, the NEE are decreased by at least 30.88% (decreasing from1.36% to 0.94%). This improvement implies that for batteries with different specifications and subject to different charge/discharge policies, they have different high-level specific features projected at the last few convolution layers. The specific high-level features learned from the source dataset can not precisely describe the target dataset, thus they need to be learned using the target dataset.</p>
        <p>Further, Figure 10 shows the number of neurons selected in FC1 and FC2 layer of the PCNN(S)-TL model during the network pruning process for different number of fine-tuned layers n. Two observations can be concluded from this bar chart. Firstly, the number of neurons selected in FC2 layer almost does not change with n. Secondly, for the FC1 layer, as the number of fine-tuned layers increases, the number of selected neurons first increases until it reaches the maximum value at n = 3, and then gradually converges. Only relatively fewer neurons are selected in FC1 layer when only fine-tune the last one or two layers (n = 1, 2), while the maximum number of neurons is selected when the last convolutional layer is fine-tuned together with the fully-connected layers (n = 3). Then the number of selected neurons decreases as the number of fine-tuned convolutional layers increases. This again implies that the last few convolutional layers can extract more specific features. When n = 1, 2, the number of useful specific features for interpreting the target data is insufficient, as a consequence, only a limited neurons in FC1 are needed which can contribute to the final specific features captured in FC2, and the estimation performance can not be maintained (Table 6). When n increases from 1 to 3, the increase of the number of selected neurons and the decrease of the estimation error reveals that as the number of fine-tuned layers increases, the number of useful extracted features increases and the number of redundant neurons decreases. However, when n is further increased from 3 to 6, the features extracted from the data have already been fully exploited through the fine-tuning of the convolutional layers proceeding to FC1 and FC2, therefore the number of selected neurons in FC1 and FC2 can be reduced. This is observable from Fig. 6 that as n increases from 3 to 6, the number of selected neurons in FC1 and FC2 is slightly decreased. The aforementioned analysis reveals that there is a trade-off in the transfer learning between the number of layers required to be fine-tuned, the model complexity, and associated computational effort.Further, Figure 10 shows the number of neurons selected in FC1 and FC2 layer of the PCNN(S)-TL model during the network pruning process for different number of fine-tuned layers n. Two observations can be concluded from this bar chart. Firstly, the number of neurons selected in FC2 layer almost does not change with n. Secondly, for the FC1 layer, as the number of fine-tuned layers increases, the number of selected neurons first increases until it reaches the maximum value at n = 3, and then gradually converges. Only relatively fewer neurons are selected in FC1 layer when only fine-tune the last one or two layers (n = 1, 2), while the maximum number of neurons is selected when the last convolutional layer is fine-tuned together with the fully-connected layers (n = 3). Then the number of selected neurons decreases as the number of fine-tuned convolutional layers increases. This again implies that the last few convolutional layers can extract more specific features. When n = 1, 2, the number of useful specific features for interpreting the target data is insufficient, as a consequence, only a limited neurons in FC1 are needed which can contribute to the final specific features captured in FC2, and the estimation performance can not be maintained (Table 6). When n increases from 1 to 3, the increase of the number of selected neurons and the decrease of the estimation error reveals that as the number of fine-tuned layers increases, the number of useful extracted features increases and the number of redundant neurons decreases. However, when n is further increased from 3 to 6, the features extracted from the data have already been fully exploited through the fine-tuning of the convolutional layers proceeding to FC1 and FC2, therefore the number of selected neurons in FC1 and FC2 can be reduced. This is observable from Fig. 6 that as n increases from 3 to 6, the number of selected neurons in FC1 and FC2 is slightly decreased. The aforementioned analysis reveals that there is a trade-off in the transfer learning between the number of layers required to be fine-tuned, the model complexity, and associated computational effort.</p>
        <p>In summary, according to Table 6 and Figure 10, fine-tuning 4 layers (2 fully-connected layers and 2 convolutional layers) are the best trade-off among estimation performance, model complexity, fine-tuning effort and computational cost. Finally, in this paper, we first build the CNN model offline using both source data and target data, and then use the developed model for online capacity estimation. A potential limitation for this method is that the parameters may drift as the battery cycling life increases, and the estimation error may increase and eventually exceed tolerable limit. This problem however can be overcome by updating the model parameters regularly using cycling test data which could be collected when the battery systems are under maintenance, or in other occasions where managed charging and discharging tests can be conducted. Given the operation safety is a paramount requirement for battery powered systems, regular maintenance is a necessity, and we strongly recommend including cycling test as one of the key battery maintenance procedures.In summary, according to Table 6 and Figure 10, fine-tuning 4 layers (2 fully-connected layers and 2 convolutional layers) are the best trade-off among estimation performance, model complexity, fine-tuning effort and computational cost. Finally, in this paper, we first build the CNN model offline using both source data and target data, and then use the developed model for online capacity estimation. A potential limitation for this method is that the parameters may drift as the battery cycling life increases, and the estimation error may increase and eventually exceed tolerable limit. This problem however can be overcome by updating the model parameters regularly using cycling test data which could be collected when the battery systems are under maintenance, or in other occasions where managed charging and discharging tests can be conducted. Given the operation safety is a paramount requirement for battery powered systems, regular maintenance is a necessity, and we strongly recommend including cycling test as one of the key battery maintenance procedures.</p>
        <p>This paper has proposed a Convolutional Neural Network-based battery capacity estimation method, which combines the transfer learning and network pruning techniques to improve the estimation accuracy on small degradation dataset and improve the computational efficiency and confirm that:This paper has proposed a Convolutional Neural Network-based battery capacity estimation method, which combines the transfer learning and network pruning techniques to improve the estimation accuracy on small degradation dataset and improve the computational efficiency and confirm that:</p>
        <p>• The estimation performance of the convolutional neural network model on the small target dataset can be improved through the transfer learning, and the normalized estimation error is decreased by 22.52%.• The estimation performance of the convolutional neural network model on the small target dataset can be improved through the transfer learning, and the normalized estimation error is decreased by 22.52%.</p>
        <p>• Applying pruning technique to the transferred convolutional neural network model, the estimation accuracy is further improved, its normalized estimation error is 24.32% smaller than the convolutional neural network model directly trained on the target dataset.• Applying pruning technique to the transferred convolutional neural network model, the estimation accuracy is further improved, its normalized estimation error is 24.32% smaller than the convolutional neural network model directly trained on the target dataset.</p>
        <p>• By selecting neurons with significant contributions and re-assign weights to the selected neurons using fast recursive algorithm in the fully-connected layers, both the model size and computational cost of the pruned model are significantly reduced, while the estimation• By selecting neurons with significant contributions and re-assign weights to the selected neurons using fast recursive algorithm in the fully-connected layers, both the model size and computational cost of the pruned model are significantly reduced, while the estimation</p>
        <p>performance are maintained. Substantial test results have confirmed the efficacy of the proposed method, achieving up to 68.34% size reduction and 80.97% computation savings. In summary, test results have revealed that compared to the original convolutional neural network model, both the model size and computational cost of the proposed model are significantly reduced, while the estimation performance on small degradation dataset is also improved.performance are maintained. Substantial test results have confirmed the efficacy of the proposed method, achieving up to 68.34% size reduction and 80.97% computation savings. In summary, test results have revealed that compared to the original convolutional neural network model, both the model size and computational cost of the proposed model are significantly reduced, while the estimation performance on small degradation dataset is also improved.</p>
        <p>YH. LI would like to thank the China Scholarship Council for sponsoring her research. The work is partially supported by SP Energy Networks on the project 'A holistic approach for power system monitoring to support DSO transition' and EPSRC funded project on 'Creating Resilient Sustainable Microgrids through Hybrid Renewable Energy Systems' under grant EP/R030243/1.YH. LI would like to thank the China Scholarship Council for sponsoring her research. The work is partially supported by SP Energy Networks on the project 'A holistic approach for power system monitoring to support DSO transition' and EPSRC funded project on 'Creating Resilient Sustainable Microgrids through Hybrid Renewable Energy Systems' under grant EP/R030243/1.</p>
        <p>Before introducing the fast recursive algorithm, the recursive matrix R k was defined in Equation (3). When the candidate model terms {ϕ j , j = 1, ..., S} in regression matrix Ψ are mutually linearly independent, R k will has the following distinguished properties:Before introducing the fast recursive algorithm, the recursive matrix R k was defined in Equation (3). When the candidate model terms {ϕ j , j = 1, ..., S} in regression matrix Ψ are mutually linearly independent, R k will has the following distinguished properties:</p>
        <p>, k = 0, 1, ..., (S -1) (A.1), k = 0, 1, ..., (S -1) (A.1)</p>
        <p>With these properties, the fast recursive algorithm can then be derived. For more details please refer to [42].With these properties, the fast recursive algorithm can then be derived. For more details please refer to [42].</p>
    </text>
</tei>
